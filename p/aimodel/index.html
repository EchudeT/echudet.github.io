<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="目录 引言 *PyTorch 模型的基本组成 2.1 模型架构（Architecture） 主干网络（Backbone） 特征金字塔（Feature Pyramid） 特征融合模块（Neck） 头部网络（Head） 2.2 模型参数（Parameters） 2.3 模型配置 Swin Transformer 的详细分析 3.1 *模型概述 3.2 *关键名词解释 3.3 Patch Embedding 3.4 Swin Transformer Block 3.4.1 多头注意力机制（Multi-Head Self-Attention） 3.4.2 窗口注意力（Window Attention） 3.4.3 移动窗口（Shifted Window）机制 3.4.4 前馈网络（Feed-Forward Network, FFN） 3.5 *Stage 和 Block 的层次结构 3.6 模型参数的调整 3.7 *模型的层级结构和通道的关系 MiDaS 模型中的应用 4.1 模型结构图 4.2 特征提取与融合 4.3 深度估计的实现 *模型调用流程 5.1 模型加载与初始化 5.2 数据加载与预处理 5.3 前向传播过程 5.4 后处理与输出 *关键名词解释 6.1 Block（块） 6.2 Patch（补丁） 6.3 Depth（深度） 6.4 Head（注意力头） 6.5 embed_dim（嵌入维度） 6.6 Channels（通道数） 6.7 Hooks（特征提取点） 6.8 .pt（模型文件） 模型训练与参数调整 7.1 损失函数与优化器 7.2 训练循环 7.3 超参数的影响 ">
<title>Aimodel</title>

<link rel='canonical' href='https://echudet.github.io/p/aimodel/'>

<link rel="stylesheet" href="/scss/style.min.86fc5e5efb829fb1bdd7a2d2275017fb3ae01ba7fcf491e347d76495ae5e3ae2.css"><meta property='og:title' content="Aimodel">
<meta property='og:description' content="目录 引言 *PyTorch 模型的基本组成 2.1 模型架构（Architecture） 主干网络（Backbone） 特征金字塔（Feature Pyramid） 特征融合模块（Neck） 头部网络（Head） 2.2 模型参数（Parameters） 2.3 模型配置 Swin Transformer 的详细分析 3.1 *模型概述 3.2 *关键名词解释 3.3 Patch Embedding 3.4 Swin Transformer Block 3.4.1 多头注意力机制（Multi-Head Self-Attention） 3.4.2 窗口注意力（Window Attention） 3.4.3 移动窗口（Shifted Window）机制 3.4.4 前馈网络（Feed-Forward Network, FFN） 3.5 *Stage 和 Block 的层次结构 3.6 模型参数的调整 3.7 *模型的层级结构和通道的关系 MiDaS 模型中的应用 4.1 模型结构图 4.2 特征提取与融合 4.3 深度估计的实现 *模型调用流程 5.1 模型加载与初始化 5.2 数据加载与预处理 5.3 前向传播过程 5.4 后处理与输出 *关键名词解释 6.1 Block（块） 6.2 Patch（补丁） 6.3 Depth（深度） 6.4 Head（注意力头） 6.5 embed_dim（嵌入维度） 6.6 Channels（通道数） 6.7 Hooks（特征提取点） 6.8 .pt（模型文件） 模型训练与参数调整 7.1 损失函数与优化器 7.2 训练循环 7.3 超参数的影响 ">
<meta property='og:url' content='https://echudet.github.io/p/aimodel/'>
<meta property='og:site_name' content='echudet'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2024-12-01T15:02:45&#43;08:00'/><meta property='article:modified_time' content='2024-12-01T23:35:39&#43;08:00'/>
<meta name="twitter:title" content="Aimodel">
<meta name="twitter:description" content="目录 引言 *PyTorch 模型的基本组成 2.1 模型架构（Architecture） 主干网络（Backbone） 特征金字塔（Feature Pyramid） 特征融合模块（Neck） 头部网络（Head） 2.2 模型参数（Parameters） 2.3 模型配置 Swin Transformer 的详细分析 3.1 *模型概述 3.2 *关键名词解释 3.3 Patch Embedding 3.4 Swin Transformer Block 3.4.1 多头注意力机制（Multi-Head Self-Attention） 3.4.2 窗口注意力（Window Attention） 3.4.3 移动窗口（Shifted Window）机制 3.4.4 前馈网络（Feed-Forward Network, FFN） 3.5 *Stage 和 Block 的层次结构 3.6 模型参数的调整 3.7 *模型的层级结构和通道的关系 MiDaS 模型中的应用 4.1 模型结构图 4.2 特征提取与融合 4.3 深度估计的实现 *模型调用流程 5.1 模型加载与初始化 5.2 数据加载与预处理 5.3 前向传播过程 5.4 后处理与输出 *关键名词解释 6.1 Block（块） 6.2 Patch（补丁） 6.3 Depth（深度） 6.4 Head（注意力头） 6.5 embed_dim（嵌入维度） 6.6 Channels（通道数） 6.7 Hooks（特征提取点） 6.8 .pt（模型文件） 模型训练与参数调整 7.1 损失函数与优化器 7.2 训练循环 7.3 超参数的影响 ">
    <link rel="shortcut icon" href="/timer.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu11199958555974308744.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">⌨️</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">echudet</a></h1>
            <h2 class="site-description">探索编程的学生一枚</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/EchudeT'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>档案</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#目录">目录</a></li>
    <li><a href="#引言">引言</a></li>
    <li><a href="#pytorch-模型的基本组成">PyTorch 模型的基本组成</a>
      <ol>
        <li><a href="#模型架构architecture">模型架构（Architecture）</a>
          <ol>
            <li><a href="#主干网络backbone">主干网络（Backbone）</a></li>
            <li><a href="#特征金字塔feature-pyramid">特征金字塔（Feature Pyramid）</a></li>
            <li><a href="#特征融合模块neck">特征融合模块（Neck）</a></li>
            <li><a href="#头部网络head">头部网络（Head）</a></li>
          </ol>
        </li>
        <li><a href="#模型参数parameters">模型参数（Parameters）</a></li>
        <li><a href="#模型配置">模型配置</a></li>
      </ol>
    </li>
    <li><a href="#swin-transformer-的详细分析">Swin Transformer 的详细分析</a>
      <ol>
        <li><a href="#模型概述">模型概述</a></li>
        <li><a href="#关键名词解释">关键名词解释</a></li>
        <li><a href="#patch-embedding">Patch Embedding</a></li>
        <li><a href="#swin-transformer-block">Swin Transformer Block</a>
          <ol>
            <li><a href="#多头注意力机制multi-head-self-attention">多头注意力机制（Multi-Head Self-Attention）</a></li>
            <li><a href="#窗口注意力window-attention">窗口注意力（Window Attention）</a></li>
            <li><a href="#移动窗口shifted-window机制">移动窗口（Shifted Window）机制</a></li>
            <li><a href="#前馈网络feed-forward-network-ffn">前馈网络（Feed-Forward Network, FFN）</a></li>
          </ol>
        </li>
        <li><a href="#stage-和-block-的层次结构">Stage 和 Block 的层次结构</a></li>
        <li><a href="#模型参数的调整">模型参数的调整</a></li>
        <li><a href="#模型的层级结构和通道的关系">模型的层级结构和通道的关系</a></li>
      </ol>
    </li>
    <li><a href="#midas-模型中的应用">MiDaS 模型中的应用</a>
      <ol>
        <li><a href="#模型结构图">模型结构图</a></li>
        <li><a href="#特征提取与融合">特征提取与融合</a></li>
        <li><a href="#深度估计的实现">深度估计的实现</a></li>
      </ol>
    </li>
    <li><a href="#模型调用流程">模型调用流程</a></li>
    <li><a href="#关键名词解释-1">关键名词解释</a>
      <ol>
        <li><a href="#block块">Block（块）</a></li>
        <li><a href="#patch补丁">Patch（补丁）</a></li>
        <li><a href="#depth深度">Depth（深度）</a></li>
        <li><a href="#head注意力头">Head（注意力头）</a></li>
        <li><a href="#embed_dim嵌入维度">embed_dim（嵌入维度）</a></li>
        <li><a href="#channels通道数">Channels（通道数）</a></li>
        <li><a href="#hooks特征提取点">Hooks（特征提取点）</a></li>
        <li><a href="#pt模型文件">.pt（模型文件）</a></li>
      </ol>
    </li>
    <li><a href="#模型训练与参数调整">模型训练与参数调整</a>
      <ol>
        <li><a href="#损失函数与优化器">损失函数与优化器</a></li>
        <li><a href="#训练循环">训练循环</a></li>
        <li><a href="#超参数的影响">超参数的影响</a></li>
      </ol>
    </li>
    <li><a href="#参考">参考</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">


    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/aimodel/">Aimodel</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2024-12-01</time>
            </div>
        

        
            <div>
                <?xml version="1.0" encoding="iso-8859-1"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="792px" height="792px" viewBox="0 0 792 792" style="enable-background:new 0 0 792 792;" xml:space="preserve">
<g>
	<g id="_x31_0_19_">
		<g>
			<path d="M643.5,742.5H594V569.25C594,473.566,516.434,396,420.75,396C516.434,396,594,318.434,594,222.75V49.5h49.5
				c13.662,0,24.75-11.088,24.75-24.75S657.162,0,643.5,0h-495c-13.662,0-24.75,11.088-24.75,24.75S134.838,49.5,148.5,49.5H198
				v173.25C198,318.434,275.566,396,371.25,396C275.566,396,198,473.566,198,569.25V742.5h-49.5c-13.662,0-24.75,11.088-24.75,24.75
				S134.838,792,148.5,792h495c13.662,0,24.75-11.088,24.75-24.75S657.162,742.5,643.5,742.5z M247.5,222.75
				c0-43.387,0-173.25,0-173.25h297c0,0,0,133.427,0,173.25c0,68.335-58.188,123.75-129.938,123.75h-37.125
				C305.687,346.5,247.5,291.085,247.5,222.75z M544.5,742.5h-297c0,0,0-129.888,0-173.25c0-68.335,58.187-123.75,129.938-123.75
				h37.125c71.75,0,129.938,55.415,129.938,123.75C544.5,609.072,544.5,742.5,544.5,742.5z"/>
		</g>
	</g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>

                <time class="article-time--reading">
                    阅读时长: 29 分钟
                </time>
            </div>
        

         
        <div>
            <?xml version="1.0" encoding="iso-8859-1"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="79.536px" height="79.536px" viewBox="0 0 79.536 79.536" style="enable-background:new 0 0 79.536 79.536;"
	 xml:space="preserve">
<g>
	<path style="fill:#010002;" d="M40.162,17.906C37.415,15.275,21.614,6.141,0,15.451c0,0.138,0,2.722,0,6.654v44.418h32.747
		c1.126,0.968,3.798,1.651,6.926,1.651c3.119,0,5.802-0.684,6.931-1.651h32.933V22.105c0-3.933,0-6.517,0-6.654
		C58.813,6.314,42.257,15.982,40.162,17.906z M37.229,63.349c0-1.75-15.372-11.981-33.125-0.818V19.216
		c3.516-2.014,8.246-3.249,13.463-3.249c10.864,0,19.662,3.562,19.662,10.175V63.349z M75.684,62.53
		c-17.751-11.163-33.108-0.932-33.108,0.818V26.146c0-6.612,8.792-10.175,19.646-10.175c5.22,0,9.952,1.235,13.463,3.249V62.53z"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>

            <time class="article-words">
                文章字数：14176字
            </time>
        </div>
        <div class="article-lastmod">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time>
                    Dec 01, 2024 23:35 &#43;0800
                </time>
            </div></footer>
    

    




    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="目录">目录
</h2><ol>
<li><a class="link" href="#%e5%bc%95%e8%a8%80" >引言</a>

</li>
<li><a class="link" href="#PyTorch-%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e6%9c%ac%e7%bb%84%e6%88%90" >*PyTorch 模型的基本组成</a>


<ul>
<li>2.1 <a class="link" href="#%e4%b8%bb%e5%b9%b2%e7%bd%91%e7%bb%9cBackbone" >模型架构（Architecture）</a>


<ul>
<li><a class="link" href="#%e4%b8%bb%e5%b9%b2%e7%bd%91%e7%bb%9cBackbone" >主干网络（Backbone）</a>

</li>
<li><a class="link" href="#%e7%89%b9%e5%be%81%e9%87%91%e5%ad%97%e5%a1%94Feature-Pyramid" >特征金字塔（Feature Pyramid）</a>

</li>
<li><a class="link" href="#%e7%89%b9%e5%be%81%e8%9e%8d%e5%90%88%e6%a8%a1%e5%9d%97Neck" >特征融合模块（Neck）</a>

</li>
<li><a class="link" href="#%e5%a4%b4%e9%83%a8%e7%bd%91%e7%bb%9cHead" >头部网络（Head）</a>

</li>
</ul>
</li>
<li>2.2 <a class="link" href="#%e4%b8%bb%e5%b9%b2%e7%bd%91%e7%bb%9cBackbone" >模型参数（Parameters）</a>

</li>
<li>2.3 <a class="link" href="#%e4%b8%bb%e5%b9%b2%e7%bd%91%e7%bb%9cBackbone" >模型配置</a>

</li>
</ul>
</li>
<li><a class="link" href="#Swin-Transformer-%e7%9a%84%e8%af%a6%e7%bb%86%e5%88%86%e6%9e%90" >Swin Transformer 的详细分析</a>


<ul>
<li>3.1 <a class="link" href="#%e6%a8%a1%e5%9e%8b%e6%a6%82%e8%bf%b0" >*模型概述</a>

</li>
<li>3.2 <a class="link" href="#%e5%85%b3%e9%94%ae%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a" >*关键名词解释</a>

</li>
<li>3.3 <a class="link" href="#Patch-Embedding" >Patch Embedding</a>

</li>
<li>3.4 <a class="link" href="#Swin-Transformer-Block" >Swin Transformer Block</a>


<ul>
<li>3.4.1 <a class="link" href="#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6Multi-Head-Self-Attention" >多头注意力机制（Multi-Head Self-Attention）</a>

</li>
<li>3.4.2 <a class="link" href="#%e7%aa%97%e5%8f%a3%e6%b3%a8%e6%84%8f%e5%8a%9bWindow-Attention" >窗口注意力（Window Attention）</a>

</li>
<li>3.4.3 <a class="link" href="#%e7%a7%bb%e5%8a%a8%e7%aa%97%e5%8f%a3Shifted-Window%e6%9c%ba%e5%88%b6" >移动窗口（Shifted Window）机制</a>

</li>
<li>3.4.4 <a class="link" href="#%e5%89%8d%e9%a6%88%e7%bd%91%e7%bb%9cFeed-Forward-Network-FFN" >前馈网络（Feed-Forward Network, FFN）</a>

</li>
</ul>
</li>
<li>3.5 <a class="link" href="#Stage-%e5%92%8c-Block-%e7%9a%84%e5%b1%82%e6%ac%a1%e7%bb%93%e6%9e%84" >*Stage 和 Block 的层次结构</a>

</li>
<li>3.6 <a class="link" href="#%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e7%9a%84%e8%b0%83%e6%95%b4" >模型参数的调整</a>

</li>
<li>3.7 <a class="link" href="" >*模型的层级结构和通道的关系</a>

</li>
</ul>
</li>
<li><a class="link" href="#MiDaS-%e6%a8%a1%e5%9e%8b%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8" >MiDaS 模型中的应用</a>


<ul>
<li>4.1 <a class="link" href="#%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84%e5%9b%be" >模型结构图</a>

</li>
<li>4.2 <a class="link" href="#%e7%89%b9%e5%be%81%e6%8f%90%e5%8f%96%e4%b8%8e%e8%9e%8d%e5%90%88" >特征提取与融合</a>

</li>
<li>4.3 <a class="link" href="#%e6%b7%b1%e5%ba%a6%e4%bc%b0%e8%ae%a1%e7%9a%84%e5%ae%9e%e7%8e%b0" >深度估计的实现</a>

</li>
</ul>
</li>
<li><a class="link" href="#%e6%a8%a1%e5%9e%8b%e8%b0%83%e7%94%a8%e6%b5%81%e7%a8%8b" >*模型调用流程</a>


<ul>
<li>5.1 <a class="link" href="#%e6%a8%a1%e5%9e%8b%e5%8a%a0%e8%bd%bd%e4%b8%8e%e5%88%9d%e5%a7%8b%e5%8c%96" >模型加载与初始化</a>

</li>
<li>5.2 <a class="link" href="#%e6%95%b0%e6%8d%ae%e5%8a%a0%e8%bd%bd%e4%b8%8e%e9%a2%84%e5%a4%84%e7%90%86" >数据加载与预处理</a>

</li>
<li>5.3 <a class="link" href="#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad%e8%bf%87%e7%a8%8b" >前向传播过程</a>

</li>
<li>5.4 <a class="link" href="#%e5%90%8e%e5%a4%84%e7%90%86%e4%b8%8e%e8%be%93%e5%87%ba" >后处理与输出</a>

</li>
</ul>
</li>
<li><a class="link" href="#%e5%85%b3%e9%94%ae%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a" >*关键名词解释</a>


<ul>
<li>6.1 <a class="link" href="#Block%e5%9d%97" >Block（块）</a>

</li>
<li>6.2 <a class="link" href="#Patch%e8%a1%a5%e4%b8%81" >Patch（补丁）</a>

</li>
<li>6.3 <a class="link" href="#Depth%e6%b7%b1%e5%ba%a6" >Depth（深度）</a>

</li>
<li>6.4 <a class="link" href="#Head%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%a4%b4" >Head（注意力头）</a>

</li>
<li>6.5 <a class="link" href="" >embed_dim（嵌入维度）</a>

</li>
<li>6.6 <a class="link" href="" >Channels（通道数）</a>

</li>
<li>6.7 <a class="link" href="" >Hooks（特征提取点）</a>

</li>
<li>6.8 <a class="link" href="" >.pt（模型文件）</a>

</li>
</ul>
</li>
<li><a class="link" href="#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e4%b8%8e%e5%8f%82%e6%95%b0%e8%b0%83%e6%95%b4" >模型训练与参数调整</a>


<ul>
<li>7.1 <a class="link" href="#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e4%b8%8e%e4%bc%98%e5%8c%96%e5%99%a8" >损失函数与优化器</a>

</li>
<li>7.2 <a class="link" href="#%e8%ae%ad%e7%bb%83%e5%be%aa%e7%8e%af" >训练循环</a>

</li>
<li>7.3 <a class="link" href="#%e8%b6%85%e5%8f%82%e6%95%b0%e7%9a%84%e5%bd%b1%e5%93%8d" >超参数的影响</a>

</li>
</ul>
</li>
</ol>
<hr>
<p><a name="引言"></a></p>
<h2 id="引言">引言
</h2><p>PyTorch 是一个广泛使用于搞研究的深度学习框架，提供了灵活的模型定义和训练方式。理解 PyTorch 模型的结构和工作原理对于赶项目出成果来说至关重要。本分析将深入探讨 PyTorch 模型的组成部分，以 Swin Transformer 和 MiDaS 模型为例，尝试阐述各部分的功能和相互作用，并提供模型的调用流程和关键名词的解释。</p>
<hr>
<p><a name="PyTorch-模型的基本组成"></a></p>
<h2 id="pytorch-模型的基本组成">PyTorch 模型的基本组成
</h2><p>PyTorch 模型通常由以下几个主要组件组成：</p>
<ol>
<li><strong>主干网络（Backbone）</strong>：负责从输入中提取特征。</li>
<li><strong>特征金字塔（Feature Pyramid）</strong>：在不同尺度上提取特征，捕获多尺度信息。</li>
<li><strong>特征融合模块（Neck）</strong>：融合不同层次的特征。</li>
<li><strong>头部网络（Head）</strong>：根据融合后的特征进行预测。</li>
</ol>
<h3 id="模型架构architecture">模型架构（Architecture）
</h3><p><a name="主干网络Backbone"></a></p>
<h4 id="主干网络backbone">主干网络（Backbone）
</h4><ul>
<li><strong>定义</strong>：主干网络是模型的核心部分，包含多个层和块（blocks），负责从输入数据中提取初级和高级特征。</li>
<li><strong>常用模型</strong>：ResNet、VGG、DenseNet、Transformer 等。</li>
</ul>
<p><strong>在 Swin Transformer 中</strong>：</p>
<ul>
<li>Swin Transformer 作为主干网络，利用 Transformer 的自注意力机制进行特征提取。</li>
</ul>
<p><a name="特征金字塔Feature-Pyramid"></a></p>
<h4 id="特征金字塔feature-pyramid">特征金字塔（Feature Pyramid）
</h4><ul>
<li><strong>定义</strong>：特征金字塔网络（FPN）在不同的尺度上提取特征，以捕获多尺度信息并提取不同分辨率的特征图。</li>
<li><strong>作用</strong>：帮助模型同时关注细节和全局信息。</li>
</ul>
<p><strong>在 Swin Transformer 中</strong>：</p>
<ul>
<li>模型通过多个阶段（Stage）形成特征金字塔，每个阶段提取不同尺度的特征。</li>
</ul>
<p><a name="特征融合模块Neck"></a></p>
<h4 id="特征融合模块neck">特征融合模块（Neck）
</h4><ul>
<li><strong>定义</strong>：特征融合模块将来自不同层次的特征进行整合。</li>
<li><strong>作用</strong>：增强特征表示，提高模型的预测能力。</li>
</ul>
<p><strong>在 MiDaS 中</strong>：</p>
<ul>
<li>使用特征融合网络，将来自不同 Stage 的特征进行融合，以生成丰富的特征表示。</li>
</ul>
<p><a name="头部网络Head"></a></p>
<h4 id="头部网络head">头部网络（Head）
</h4><ul>
<li><strong>定义</strong>：根据融合后的特征进行最终的预测任务，如分类、检测、分割等。</li>
<li><strong>结构</strong>：通常由一系列卷积层或全连接层组成。</li>
</ul>
<p><strong>在 MiDaS 中</strong>：</p>
<ul>
<li>头部网络根据融合后的特征，输出深度估计结果，将特征转换为最终输出（深度图）。</li>
</ul>
<h3 id="模型参数parameters">模型参数（Parameters）
</h3><ul>
<li>权重（Weights）：
<ul>
<li>每一层的变换矩阵</li>
<li>通过预训练获得</li>
</ul>
</li>
<li>偏置（Biases）：
<ul>
<li>每一层的偏置项</li>
</ul>
</li>
<li>归一化参数：
<ul>
<li>批归一化（Batch Norm）的均值和方差</li>
<li>层归一化（Layer Norm）的参数</li>
</ul>
</li>
</ul>
<h3 id="模型配置">模型配置
</h3><ul>
<li><strong>超参数</strong>：
<ul>
<li>输入大小：384x384</li>
<li>补丁大小：4x4</li>
<li>窗口大小：12-&gt;24</li>
<li>头数：多头注意力中的头数</li>
<li>隐藏维度：特征维度</li>
</ul>
</li>
<li><strong>训练配置</strong>：
<ul>
<li>预训练数据集：ImageNet-22K</li>
<li>微调数据集：ImageNet-1K</li>
<li>学习率、批量大小等</li>
</ul>
</li>
</ul>
<hr>
<p><a name="Swin-Transformer-的详细分析"></a></p>
<h2 id="swin-transformer-的详细分析">Swin Transformer 的详细分析
</h2><p><a name="模型概述"></a></p>
<h3 id="模型概述">模型概述
</h3><p>Swin Transformer 是一种用于计算机视觉的 Transformer 模型，具有以下特点：</p>
<ul>
<li><strong>层次化设计</strong>：通过分层结构，逐步提取高层次特征。</li>
<li><strong>窗口注意力机制</strong>：在局部窗口内计算注意力，降低计算复杂度。</li>
<li><strong>移动窗口机制</strong>：通过移动窗口，捕获跨窗口的全局信息。</li>
</ul>
<p><strong>模型结构图</strong>：</p>
<blockquote>
<p>下图中对于stage中的输入显示的是序列形式 [B, N, C]，但也存在另一种形式，空间形式 [B, C, H, W]。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">特征表示的两种形式</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mf">1.</span> <span class="n">序列形式</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">]</span><span class="err">：</span>
</span></span><span class="line"><span class="cl"><span class="n">stage1</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>   <span class="c1"># B=批次大小, N=序列长度, C=特征维度</span>
</span></span><span class="line"><span class="cl"><span class="n">stage2</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage3</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage4</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mf">2.</span> <span class="n">空间形式</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">]</span><span class="err">：</span>
</span></span><span class="line"><span class="cl"><span class="n">stage1</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">]</span>   <span class="c1"># B=批次大小, C=通道数, H=高度, W=宽度</span>
</span></span><span class="line"><span class="cl"><span class="n">stage2</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage3</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage4</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>它们的对应关系：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 序列形式 -&gt; 空间形式的转换</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="o">*</span><span class="n">W</span><span class="p">,</span> <span class="n">C</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 具体例子：</span>
</span></span><span class="line"><span class="cl"><span class="n">stage1</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">]</span>    <span class="c1"># 重排序列为96x96的特征图</span>
</span></span><span class="line"><span class="cl"><span class="n">stage2</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>    <span class="c1"># 重排序列为48x48的特征图</span>
</span></span><span class="line"><span class="cl"><span class="n">stage3</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span>    <span class="c1"># 重排序列为24x24的特征图</span>
</span></span><span class="line"><span class="cl"><span class="n">stage4</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>  <span class="c1"># 重排序列为12x12的特征图</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在代码中的转换：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 在swin_common.py中的实现</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim0</span> <span class="o">=</span> <span class="n">dim0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim1</span> <span class="o">=</span> <span class="n">dim1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 转换维度顺序</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 特征后处理</span>
</span></span><span class="line"><span class="cl"><span class="n">pretrained</span><span class="o">.</span><span class="n">act_postprocess1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">Transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># [B, N, C] -&gt; [B, C, N]</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Unflatten</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">]))</span>  <span class="c1"># [B, C, N] -&gt; [B, C, H, W]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>修改后的完整结构图（添加两种表示形式）：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Stage</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">输入</span> <span class="o">-&gt;</span> <span class="n">PatchEmbed</span> <span class="o">-&gt;</span> <span class="n">Transformer</span> <span class="n">Blocks</span> <span class="o">-&gt;</span> <span class="n">输出</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">处理</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Stage</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">输入</span> <span class="o">-&gt;</span> <span class="n">Transformer</span> <span class="n">Blocks</span> <span class="o">-&gt;</span> <span class="n">输出</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">处理</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Stage</span> <span class="mi">3</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">输入</span> <span class="o">-&gt;</span> <span class="n">Transformer</span> <span class="n">Blocks</span> <span class="o">-&gt;</span> <span class="n">输出</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">处理</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Stage</span> <span class="mi">4</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">输入</span> <span class="o">-&gt;</span> <span class="n">Transformer</span> <span class="n">Blocks</span> <span class="o">-&gt;</span> <span class="n">输出</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="n">处理</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关键点：</p>
<ol>
<li>Transformer内部使用序列形式 [B, N, C]</li>
<li>卷积操作使用空间形式 [B, C, H, W]</li>
<li>两种形式可以相互转换</li>
<li>特征维度（C）和空间分辨率（H,W）在两种形式中都保持不变</li>
</ol>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">MidasNet</span> <span class="p">(</span><span class="n">整体模型</span><span class="p">)</span> <span class="p">[</span><span class="n">B</span><span class="o">=</span><span class="n">batch_size</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">├──</span> <span class="mf">1.</span> <span class="n">Backbone</span> <span class="p">(</span><span class="n">SwinTransformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">patch_embed</span> <span class="p">(</span><span class="n">PatchEmbedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">输入图像</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">patch划分</span><span class="p">:</span> <span class="mi">4</span><span class="n">x4</span><span class="p">,</span> <span class="n">得到96x96个patch</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">proj</span> <span class="p">(</span><span class="n">Linear</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">weight</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>  <span class="c1"># 48=4x4x3</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">bias</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage1</span> <span class="p">(</span><span class="n">BasicLayer</span><span class="p">)</span> <span class="o">-</span> <span class="n">特征金字塔第一层</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">输入</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">(</span><span class="n">SwinTransformerBlock</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm1</span> <span class="p">(</span><span class="n">LayerNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">weight</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">bias</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">attn</span> <span class="p">(</span><span class="n">WindowAttention</span><span class="p">)</span> <span class="o">-</span> <span class="mi">4</span><span class="n">个头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">qkv</span> <span class="p">(</span><span class="n">Linear</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">weight</span><span class="p">:</span> <span class="p">[</span><span class="mi">384</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>  <span class="c1"># 384=128*3(Q,K,V)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">bias</span><span class="p">:</span> <span class="p">[</span><span class="mi">384</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="mi">4</span><span class="n">个头每个处理</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">window_size</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">proj</span> <span class="p">(</span><span class="n">Linear</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">weight</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">bias</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">relative_position_bias</span><span class="p">:</span> <span class="p">[</span><span class="mi">169</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm2</span> <span class="p">(</span><span class="n">LayerNorm</span><span class="p">):</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp</span> <span class="p">(</span><span class="n">MLP</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>       <span class="err">├──</span> <span class="n">fc1</span><span class="p">:</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>       <span class="err">└──</span> <span class="n">fc2</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">结构同上</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">downsample</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">reduction</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">norm</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage2</span> <span class="p">(</span><span class="n">BasicLayer</span><span class="p">)</span> <span class="o">-</span> <span class="n">特征金字塔第二层</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">输入</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">结构同stage1</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="mi">8</span><span class="n">个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">每个头</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">window_size</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp维度</span><span class="p">:</span> <span class="mi">256</span> <span class="o">-&gt;</span> <span class="mi">1024</span> <span class="o">-&gt;</span> <span class="mi">256</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">downsample</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage3</span> <span class="p">(</span><span class="n">BasicLayer</span><span class="p">)</span> <span class="o">-</span> <span class="n">特征金字塔第三层</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">输入</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span> <span class="o">-</span> <span class="mi">6</span><span class="n">个Block</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="mi">16</span><span class="n">个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">每个头</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">window_size</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp维度</span><span class="p">:</span> <span class="mi">512</span> <span class="o">-&gt;</span> <span class="mi">2048</span> <span class="o">-&gt;</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">downsample</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">└──</span> <span class="n">stage4</span> <span class="p">(</span><span class="n">BasicLayer</span><span class="p">)</span> <span class="o">-</span> <span class="n">特征金字塔第四层</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">├──</span> <span class="n">输入</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="n">个Block</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">│</span>   <span class="err">├──</span> <span class="mi">32</span><span class="n">个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">│</span>   <span class="err">├──</span> <span class="n">每个头</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">window_size</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp维度</span><span class="p">:</span> <span class="mi">1024</span> <span class="o">-&gt;</span> <span class="mi">4096</span> <span class="o">-&gt;</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">├──</span> <span class="mf">2.</span> <span class="n">Neck</span> <span class="p">(</span><span class="n">特征融合网络</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">输入特征维度</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">feat1</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">feat2</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">feat3</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">feat4</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">融合层1</span><span class="p">:</span> <span class="n">feat4</span> <span class="p">[</span><span class="mi">1024</span><span class="p">]</span> <span class="o">+</span> <span class="n">feat3</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">融合层2</span><span class="p">:</span> <span class="n">feat3</span> <span class="p">[</span><span class="mi">512</span><span class="p">]</span> <span class="o">+</span> <span class="n">feat2</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">└──</span> <span class="n">融合层3</span><span class="p">:</span> <span class="n">feat2</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">+</span> <span class="n">feat1</span> <span class="p">[</span><span class="mi">128</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">└──</span> <span class="mf">3.</span> <span class="n">Head</span> <span class="p">(</span><span class="n">输出头</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">输入</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">Conv2d</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">ReLU</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">Conv2d</span><span class="p">:</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="err">└──</span> <span class="n">输出</span><span class="p">:</span> <span class="p">[</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">(</span><span class="n">深度图</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>特征金字塔在这个结构中体现为：</p>
<ol>
<li>Stage1: 96x96, 128维 (4个头)</li>
<li>Stage2: 48x48, 256维 (8个头)</li>
<li>Stage3: 24x24, 512维 (16个头)</li>
<li>Stage4: 12x12, 1024维 (32个头)</li>
</ol>
<p>每个Stage都是特征金字塔的一层，随着层数增加：</p>
<ul>
<li>空间分辨率降低（图像尺寸变小）</li>
<li>通道数增加（特征维度增加）</li>
<li>注意力头数量增加（更多的&quot;专家&quot;视角）</li>
</ul>
<p>所以：</p>
<ul>
<li>Backbone就是这整个SwinTransformer结构</li>
<li>特征金字塔就是这四个Stage形成的多尺度特征</li>
<li>注意力头在每个Stage的WindowAttention模块中</li>
<li>特征融合在Neck部分进行</li>
</ul>
<p>这就像一个层层递进的系统：</p>
<ol>
<li>底层Stage：大量简单特征，少量注意力头</li>
<li>高层Stage：少量复杂特征，大量注意力头</li>
<li>Neck：将各层特征智能融合</li>
<li>Head：生成最终输出</li>
</ol>
<p><a name="关键名词解释"></a></p>
<h3 id="关键名词解释">关键名词解释
</h3><p>在深入分析模型之前，先对一些关键名词进行解释：</p>
<ul>
<li><strong>Patch（补丁）</strong>：将输入图像划分为固定大小的小块，每个小块称为一个补丁（Patch）。</li>
<li><strong>Embed_dim（嵌入维度）</strong>：每个补丁被映射为高维特征向量，嵌入维度指这个特征向量的维度。</li>
<li><strong>Block（块）</strong>：Swin Transformer 的基本组成单元，包括注意力机制和前馈网络等。</li>
<li><strong>Depth（深度）</strong>：每个 Stage 中包含的 Block 数量，决定了模型的深度。</li>
<li><strong>Head（注意力头）</strong>：在多头注意力机制中，多个独立的注意力计算单元称为注意力头。</li>
</ul>
<p><a name="Patch-Embedding"></a></p>
<h3 id="patch-embedding">Patch Embedding
</h3><p><strong>功能</strong>：</p>
<ul>
<li>将输入图像划分为固定大小的补丁（Patch）。</li>
<li>将每个补丁映射为高维特征向量。</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchEmbed</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 卷积映射</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 展平并交换维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>解释</strong>：</p>
<ul>
<li><strong>Patch（补丁）</strong>：将输入图像按照 <code>patch_size</code> 划分为多个小块。例如，<code>patch_size=4</code> 时，输入 <code>384x384</code> 的图像会被划分为 <code>96x96</code> 个补丁。</li>
<li><strong>Embed_dim（嵌入维度）</strong>：每个补丁通过卷积映射，转换为 <code>embed_dim=128</code> 维的特征向量。</li>
</ul>
<p><a name="Swin-Transformer-Block"></a></p>
<h3 id="swin-transformer-block">Swin Transformer Block
</h3><p>Swin Transformer Block 是模型的基本组成单元，包含以下组件：</p>
<ol>
<li><strong>LayerNorm 层</strong>：归一化输入特征。</li>
<li><strong>多头自注意力层（使用窗口注意力）</strong>。</li>
<li><strong>残差连接</strong>：保留输入特征，促进梯度传播。</li>
<li><strong>前馈网络（FFN）</strong>：进一步处理特征。</li>
</ol>
<p><a name="多头注意力机制Multi-Head-Self-Attention"></a></p>
<h4 id="多头注意力机制multi-head-self-attention">多头注意力机制（Multi-Head Self-Attention）
</h4><p><strong>功能</strong>：</p>
<ul>
<li>捕获输入特征中不同位置之间的关联关系。</li>
<li>不同的注意力头可以关注不同的特征模式。</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算注意力得分并应用于值向量</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>解释</strong>：</p>
<ul>
<li><strong>Head（注意力头）</strong>：将特征维度 <code>C</code> 分为 <code>num_heads</code> 个部分，每个部分独立计算注意力。</li>
<li><strong>多头的优势</strong>：并行处理，可以关注不同的特征模式，增强模型的表达能力。</li>
</ul>
<p><a name="窗口注意力Window-Attention"></a></p>
<h4 id="窗口注意力window-attention">窗口注意力（Window Attention）
</h4><p><strong>功能</strong>：</p>
<ul>
<li>在局部窗口内计算自注意力，降低计算复杂度。</li>
<li>每个窗口独立计算，不同窗口之间不共享信息。</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">WindowAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入特征划分为窗口</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对每个窗口应用多头自注意力</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>解释</strong>：</p>
<ul>
<li><strong>窗口大小（window_size）</strong>：定义局部计算注意力的范围。例如，<code>window_size=7</code> 时，每个 <code>7x7</code> 的区域作为一个窗口。</li>
<li><strong>优势</strong>：降低计算复杂度，使得模型能够处理高分辨率图像。</li>
</ul>
<p><a name="移动窗口Shifted-Window机制"></a></p>
<h4 id="移动窗口shifted-window机制">移动窗口（Shifted Window）机制
</h4><p><strong>功能</strong>：</p>
<ul>
<li>通过在不同的 Swin Transformer Block 中移动窗口，实现跨窗口的信息交流。</li>
<li>增强模型的全局建模能力。</li>
</ul>
<p><strong>实现</strong>：</p>
<ul>
<li><strong>标准窗口划分</strong>：在偶数层，直接按照窗口大小划分。</li>
<li><strong>移动窗口划分</strong>：在奇数层，将特征图平移半个窗口大小，然后再进行窗口划分。</li>
</ul>
<p><strong>解释</strong>：</p>
<ul>
<li><strong>移动窗口的目的</strong>：让相邻窗口之间的信息能够互相交流，克服窗口注意力的局部性限制。</li>
</ul>
<p><a name="前馈网络Feed-Forward-Network-FFN"></a></p>
<h4 id="前馈网络feed-forward-network-ffn">前馈网络（Feed-Forward Network, FFN）
</h4><p><strong>功能</strong>：</p>
<ul>
<li>对注意力层的输出进行进一步处理。</li>
<li>通常由两个线性层和一个激活函数组成。</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">FeedForwardNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>解释</strong>：</p>
<ul>
<li><strong>扩展维度</strong>：通常将特征维度扩大 4 倍（<code>hidden_dim = 4 * dim</code>），然后再缩小回原来的维度。</li>
<li><strong>作用</strong>：增加模型的非线性表达能力。</li>
</ul>
<p><a name="Stage-和-Block-的层次结构"></a></p>
<h3 id="stage-和-block-的层次结构">Stage 和 Block 的层次结构
</h3><p><strong>整体结构</strong>：</p>
<ul>
<li><strong>Stage</strong>：模型被划分为多个阶段，每个阶段处理不同尺度的特征。</li>
<li><strong>Block（块）</strong>：每个 Stage 包含多个 Swin Transformer Block。</li>
</ul>
<p><strong>示例</strong>（以 Swin V2-Base 为例）：</p>
<ul>
<li><strong>Embed_dim</strong>：<code>128</code>。</li>
<li><strong>Depths（深度）</strong>：<code>[2, 2, 18, 2]</code>，表示每个 Stage 包含的 Block 数量。
<ul>
<li><strong>Stage 1</strong>：<code>2</code> 个 Block。</li>
<li><strong>Stage 2</strong>：<code>2</code> 个 Block。</li>
<li><strong>Stage 3</strong>：<code>18</code> 个 Block。</li>
<li><strong>Stage 4</strong>：<code>2</code> 个 Block。</li>
</ul>
</li>
<li><strong>Num_heads（注意力头数量）</strong>：<code>[4, 8, 16, 32]</code>，表示每个 Stage 中的注意力头数量。</li>
</ul>
<p><strong>解释</strong>：</p>
<ul>
<li>
<p><strong>Depth（深度）</strong>：决定了每个 Stage 的深度，Block 数量越多，模型可以提取更复杂的特征。</p>
<p>depths设置的原理：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">为什么Stage3的depth最大（6）？
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">1. 计算复杂度考虑：
</span></span></span><span class="line"><span class="cl"><span class="s2">   - Stage1: 96x96分辨率，depth小可以节省计算
</span></span></span><span class="line"><span class="cl"><span class="s2">   - Stage3: 24x24分辨率，可以承担更多计算
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">2. 特征提取需求：
</span></span></span><span class="line"><span class="cl"><span class="s2">   - Stage1: 主要处理低级特征，不需要太多block
</span></span></span><span class="line"><span class="cl"><span class="s2">   - Stage3: 需要提取复杂的语义特征，需要更多block
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">3. 信息流动：
</span></span></span><span class="line"><span class="cl"><span class="s2">   Stage1 (2 blocks)
</span></span></span><span class="line"><span class="cl"><span class="s2">      ↓ [降采样]
</span></span></span><span class="line"><span class="cl"><span class="s2">   Stage2 (2 blocks)
</span></span></span><span class="line"><span class="cl"><span class="s2">      ↓ [降采样]
</span></span></span><span class="line"><span class="cl"><span class="s2">   Stage3 (6 blocks) ← 关键特征提取阶段
</span></span></span><span class="line"><span class="cl"><span class="s2">      ↓ [降采样]
</span></span></span><span class="line"><span class="cl"><span class="s2">   Stage4 (2 blocks)
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1. 浅层配置</span>
</span></span><span class="line"><span class="cl"><span class="n">shallow_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">shallow_effects</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;优点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;计算速度&#39;</span><span class="p">:</span> <span class="s1">&#39;快&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;内存占用&#39;</span><span class="p">:</span> <span class="s1">&#39;小&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;训练简单&#39;</span><span class="p">:</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;缺点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;特征提取&#39;</span><span class="p">:</span> <span class="s1">&#39;不够深入&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;模型表达能力&#39;</span><span class="p">:</span> <span class="s1">&#39;有限&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;复杂场景处理&#39;</span><span class="p">:</span> <span class="s1">&#39;欠佳&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. 深层配置</span>
</span></span><span class="line"><span class="cl"><span class="n">deep_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">deep_effects</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;优点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;特征提取&#39;</span><span class="p">:</span> <span class="s1">&#39;非常深入&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;模型表达能力&#39;</span><span class="p">:</span> <span class="s1">&#39;强&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;复杂场景处理&#39;</span><span class="p">:</span> <span class="s1">&#39;优秀&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;缺点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;计算速度&#39;</span><span class="p">:</span> <span class="s1">&#39;慢&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;内存占用&#39;</span><span class="p">:</span> <span class="s1">&#39;大&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;训练难度&#39;</span><span class="p">:</span> <span class="s1">&#39;高&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3. 平衡配置（MiDaS使用）</span>
</span></span><span class="line"><span class="cl"><span class="n">balanced_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">balanced_effects</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;优点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;特征提取&#39;</span><span class="p">:</span> <span class="s1">&#39;充分&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;计算效率&#39;</span><span class="p">:</span> <span class="s1">&#39;适中&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;内存占用&#39;</span><span class="p">:</span> <span class="s1">&#39;合理&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;缺点&#39;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;可能不适合&#39;</span><span class="p">:</span> <span class="s1">&#39;特别复杂的场景&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><strong>Head（注意力头）</strong>：注意力头数量随着 Stage 的深入而增加，能够捕获更丰富的特征模式。</p>
<p>注意力头的工作机制：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">WindowAttention</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># dim: 输入特征维度（例如128）</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># num_heads: 注意力头数量（例如4）</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># 每个头的维度（128/4=32）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  <span class="c1"># x: [B, N, C]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 将输入分给各个头</span>
</span></span><span class="line"><span class="cl">        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [B, N, 3*C]</span>
</span></span><span class="line"><span class="cl">        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 每个头独立工作</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        以4个头为例：
</span></span></span><span class="line"><span class="cl"><span class="s2">        头1：专注于边缘特征
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 查询：这个位置有强边缘吗？
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 键值对：其他位置的边缘信息
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        头2：专注于纹理模式
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 查询：这里的纹理是什么样的？
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 键值对：相似纹理的区域
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        头3：专注于色彩分布
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 查询：这个区域的颜色特征？
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 键值对：相似颜色的区域
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        头4：专注于形状结构
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 查询：这里是什么形状？
</span></span></span><span class="line"><span class="cl"><span class="s2">            - 键值对：相似形状的区域
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>头数增加的影响：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Stage1: 4个头</span>
</span></span><span class="line"><span class="cl"><span class="n">attention_stage1</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;head_dim&#39;</span><span class="p">:</span> <span class="mi">128</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span>    <span class="c1"># 每个头32维</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;capacity&#39;</span><span class="p">:</span> <span class="s1">&#39;基础特征提取&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;complexity&#39;</span><span class="p">:</span> <span class="s1">&#39;低&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;computation&#39;</span><span class="p">:</span> <span class="s1">&#39;少&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="s1">&#39;小&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage4: 32个头</span>
</span></span><span class="line"><span class="cl"><span class="n">attention_stage4</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;head_dim&#39;</span><span class="p">:</span> <span class="mi">1024</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span>  <span class="c1"># 每个头32维</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;capacity&#39;</span><span class="p">:</span> <span class="s1">&#39;复杂特征提取&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;complexity&#39;</span><span class="p">:</span> <span class="s1">&#39;高&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;computation&#39;</span><span class="p">:</span> <span class="s1">&#39;多&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="s1">&#39;大&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">头数增加的效果：
</span></span></span><span class="line"><span class="cl"><span class="s2">1. 优点：
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 可以同时关注更多不同类型的特征
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 提高模型的表达能力
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 增强特征的多样性
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">2. 缺点：
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 计算量增加
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 内存消耗增加
</span></span></span><span class="line"><span class="cl"><span class="s2">   - 可能过度拟合（如果数据不足）
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><strong>Block</strong></p>
<p>关键点：</p>
<ol>
<li>
<p><strong>Block不需要手动写</strong>：</p>
<ul>
<li>使用timm库提供的预定义模型</li>
<li>block的结构已经在模型中定义好了</li>
</ul>
</li>
<li>
<p><strong>Block的数量由depths控制</strong>：</p>
<ul>
<li>
<p>depths=[2,2,6,2]表示每个stage的block数量</p>
</li>
<li>
<p>这些block会自动创建</p>
</li>
</ul>
</li>
<li>
<p><strong>Block的参数会随着stage变化</strong>：</p>
<ul>
<li>
<p>维度会翻倍：128-&gt;256-&gt;512-&gt;1024</p>
</li>
<li>
<p>注意力头数会增加：4-&gt;8-&gt;16-&gt;32</p>
</li>
</ul>
</li>
<li>
<p><strong>特征提取通过hooks实现</strong>：</p>
<ul>
<li>
<p>指定要提取特征的block位置</p>
</li>
<li>
<p>自动注册hook函数来获取中间特征</p>
</li>
</ul>
</li>
</ol>
<p>Block的创建：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 在timm库中，Swin Transformer的block是这样定义的：</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SwinTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 注意力层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">WindowAttention</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>           <span class="c1"># 特征维度</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>  <span class="c1"># 注意力头数</span>
</span></span><span class="line"><span class="cl">            <span class="n">window_size</span><span class="o">=</span><span class="n">window_size</span>  <span class="c1"># 窗口大小</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. MLP层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Mlp</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_features</span><span class="o">=</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">act_layer</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. 层归一化</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Block的组织：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 在Swin Transformer中，blocks是通过depths参数组织的</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="n">depths</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Stage 1</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">stage1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="n">SwinTransformerBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 创建2个block</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Stage 2</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">stage2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="n">SwinTransformerBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depths</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 创建2个block</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Stage 3 (最深的stage)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">stage3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="n">SwinTransformerBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depths</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># 创建6个block</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Stage 4</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">stage4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">        <span class="n">SwinTransformerBlock</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depths</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>  <span class="c1"># 创建2个block</span>
</span></span><span class="line"><span class="cl">    <span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在MiDaS中的使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 在MiDaS中，通过timm库加载预训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">timm</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;swinv2_base_window12to24_192to384_22kft1k&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 然后通过hooks获取中间特征</span>
</span></span><span class="line"><span class="cl"><span class="n">hooks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 指定每个stage中要提取特征的block索引</span>
</span></span><span class="line"><span class="cl"><span class="n">pretrained</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">hooks</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>  <span class="c1"># Stage 1的第1个block</span>
</span></span><span class="line"><span class="cl"><span class="n">pretrained</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">hooks</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># Stage 2的第1个block</span>
</span></span><span class="line"><span class="cl"><span class="n">pretrained</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">hooks</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>  <span class="c1"># Stage 3的第17个block</span>
</span></span><span class="line"><span class="cl"><span class="n">pretrained</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">hooks</span><span class="p">[</span><span class="mi">3</span><span class="p">]]</span>  <span class="c1"># Stage 4的第1个block</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p><a name="模型参数的调整"></a></p>
<h3 id="模型参数的调整">模型参数的调整
</h3><p><strong>主要参数</strong>：</p>
<ul>
<li><strong>Embed_dim（嵌入维度）</strong>：影响特征表示的丰富程度。</li>
<li><strong>Depths（深度）</strong>：每个 Stage 中的 Block 数量，影响模型的深度和表达能力。</li>
<li><strong>Num_heads（注意力头数量）</strong>：影响模型捕获不同特征模式的能力。</li>
</ul>
<p><strong>调整策略</strong>：</p>
<ul>
<li><strong>计算资源有限时</strong>：减小 <code>embed_dim</code>、<code>depths</code> 和 <code>num_heads</code>。</li>
<li><strong>需要更强的特征表示时</strong>：增大 <code>embed_dim</code> 和 <code>depths</code>。</li>
<li><strong>平衡性能和效率</strong>：根据任务需求选择合适的参数配置。</li>
</ul>
<h3 id="模型的层级结构和通道的关系">模型的层级结构和通道的关系
</h3><p>以Swin V2-Base为例：</p>
<p><strong>模型的基本组成</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SwinV2Base</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. Patch Embedding层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>          <span class="c1"># 将图像分成4x4的patch</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>         <span class="c1"># 输入RGB三通道</span>
</span></span><span class="line"><span class="cl">            <span class="n">embed_dim</span><span class="o">=</span><span class="mi">128</span>          <span class="c1"># 每个patch嵌入维度为128</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 四个Transformer Stage</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stage1</span> <span class="o">=</span> <span class="n">BasicLayer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># 处理1/4分辨率特征</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stage2</span> <span class="o">=</span> <span class="n">BasicLayer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># 处理1/8分辨率特征</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stage3</span> <span class="o">=</span> <span class="n">BasicLayer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># 处理1/16分辨率特征</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stage4</span> <span class="o">=</span> <span class="n">BasicLayer</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># 处理1/32分辨率特征</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>通道数在各层级的变化</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 输入图像: [384, 384, 3]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ↓ Patch Embedding</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>  <span class="c1"># 96x96个patch，每个128维</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage1: 保持通道数</span>
</span></span><span class="line"><span class="cl"><span class="n">stage1_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>   <span class="c1"># 维持空间和通道维度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage2: 通道数翻倍，空间分辨率减半</span>
</span></span><span class="line"><span class="cl"><span class="n">stage2_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">48</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>   <span class="c1"># 48x48特征图，256通道</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage3: 再次翻倍</span>
</span></span><span class="line"><span class="cl"><span class="n">stage3_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>   <span class="c1"># 24x24特征图，512通道</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage4: 最终特征</span>
</span></span><span class="line"><span class="cl"><span class="n">stage4_out</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>  <span class="c1"># 12x12特征图，1024通道</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>每个Stage的内部结构</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BasicLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. Window Attention块</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">window_attn</span> <span class="o">=</span> <span class="n">WindowAttention</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>                <span class="c1"># 特征维度</span>
</span></span><span class="line"><span class="cl">            <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>           <span class="c1"># 注意力头数</span>
</span></span><span class="line"><span class="cl">            <span class="n">window_size</span><span class="o">=</span><span class="mi">12</span>         <span class="c1"># 窗口大小</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. Feed Forward Network (FFN)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>  <span class="c1"># 内部扩展维度</span>
</span></span><span class="line"><span class="cl">            <span class="n">out_features</span><span class="o">=</span><span class="mi">256</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. Layer Norm层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>通道数增加的原因和作用</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Stage 1: 128通道</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 每个通道关注不同特征：</span>
</span></span><span class="line"><span class="cl"><span class="n">channels_stage1</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="mi">1</span><span class="o">-</span><span class="mi">32</span><span class="p">:</span>    <span class="s2">&#34;边缘和纹理特征&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">33</span><span class="o">-</span><span class="mi">64</span><span class="p">:</span>   <span class="s2">&#34;颜色和亮度模式&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">65</span><span class="o">-</span><span class="mi">96</span><span class="p">:</span>   <span class="s2">&#34;局部形状特征&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">97</span><span class="o">-</span><span class="mi">128</span><span class="p">:</span>  <span class="s2">&#34;基本结构特征&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage 2: 256通道</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 更复杂的特征组合：</span>
</span></span><span class="line"><span class="cl"><span class="n">channels_stage2</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="mi">1</span><span class="o">-</span><span class="mi">64</span><span class="p">:</span>    <span class="s2">&#34;组合边缘特征&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">65</span><span class="o">-</span><span class="mi">128</span><span class="p">:</span>  <span class="s2">&#34;复杂纹理模式&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">129</span><span class="o">-</span><span class="mi">192</span><span class="p">:</span> <span class="s2">&#34;部件级特征&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">193</span><span class="o">-</span><span class="mi">256</span><span class="p">:</span> <span class="s2">&#34;中等规模结构&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stage 3: 512通道</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 高级语义特征：</span>
</span></span><span class="line"><span class="cl"><span class="n">channels_stage3</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="mi">1</span><span class="o">-</span><span class="mi">128</span><span class="p">:</span>   <span class="s2">&#34;物体部件关系&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">129</span><span class="o">-</span><span class="mi">256</span><span class="p">:</span> <span class="s2">&#34;场景组成模式&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">257</span><span class="o">-</span><span class="mi">384</span><span class="p">:</span> <span class="s2">&#34;空间排布特征&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="mi">385</span><span class="o">-</span><span class="mi">512</span><span class="p">:</span> <span class="s2">&#34;上下文信息&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>具体示例：处理一张图片的过程</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 假设输入图片: [1, 3, 384, 384]</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SwinV2Process</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">process_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. Patch Embedding</span>
</span></span><span class="line"><span class="cl">        <span class="n">patches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [1, 96*96, 128] - 9216个patch，每个128维</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. Stage 1处理</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage1_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage1</span><span class="p">(</span><span class="n">patches</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 保持维度但增强特征表示</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 仍然是[1, 96*96, 128]但包含更丰富的特征</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. Stage 2处理</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage2_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage2</span><span class="p">(</span><span class="n">stage1_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 降采样+通道扩展</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [1, 48*48, 256]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. Stage 3处理</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage3_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage3</span><span class="p">(</span><span class="n">stage2_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [1, 24*24, 512]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 5. Stage 4处理</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage4_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage4</span><span class="p">(</span><span class="n">stage3_out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># [1, 12*12, 1024]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关键理解：</p>
<ol>
<li><strong>层级的作用</strong>：
<ul>
<li>每个Stage负责特定尺度的特征提取</li>
<li>随着层级加深，感受野变大</li>
<li>特征从具体到抽象</li>
</ul>
</li>
<li><strong>通道的作用</strong>：
<ul>
<li>每个通道像一个&quot;专家&quot;</li>
<li>通道数增加=更多&quot;专家&quot;观点</li>
<li>高层通道组合低层特征</li>
</ul>
</li>
<li><strong>为什么需要这种结构</strong>：
<ul>
<li>渐进式特征提取</li>
<li>多尺度信息捕获</li>
<li>局部到全局的理解</li>
</ul>
</li>
</ol>
<p>这就像是一个层级化的观察系统：</p>
<ol>
<li>底层：大量简单&quot;专家&quot;看细节</li>
<li>中层：较少但更专业的&quot;专家&quot;归纳特征</li>
<li>高层：精英&quot;专家&quot;总结全局信息</li>
</ol>
<hr>
<p><a name="MiDaS-模型中的应用"></a></p>
<h2 id="midas-模型中的应用">MiDaS 模型中的应用
</h2><p>MiDaS 是一种用于单目深度估计的模型，利用了 Swin Transformer 作为主干网络。</p>
<p><a name="模型结构图"></a></p>
<h3 id="模型结构图">模型结构图
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">MidasNet</span> <span class="p">(</span><span class="n">整体模型</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">├──</span> <span class="mf">1.</span> <span class="n">Backbone</span> <span class="p">(</span><span class="n">SwinTransformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">patch_embed</span> <span class="p">(</span><span class="n">PatchEmbedding</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">将图像分成patches</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage1</span> <span class="p">(</span><span class="n">特征金字塔第一层</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">(</span><span class="n">SwinTransformerBlock</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm1</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">attn</span> <span class="p">(</span><span class="n">WindowAttention</span><span class="p">)</span> <span class="err">←</span> <span class="n">这里有4个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">head1</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">head2</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">head3</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">head4</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm2</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">(</span><span class="n">结构同上</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">downsample</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage2</span> <span class="p">(</span><span class="n">特征金字塔第二层</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">(</span><span class="n">SwinTransformerBlock</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm1</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">attn</span> <span class="p">(</span><span class="n">WindowAttention</span><span class="p">)</span> <span class="err">←</span> <span class="n">这里有8个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">head1</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">head2</span><span class="p">:</span> <span class="mi">32</span><span class="n">维特征</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="o">...</span> <span class="p">(</span><span class="n">共8个</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">norm2</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">mlp</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">(</span><span class="n">结构同上</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">downsample</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">stage3</span> <span class="p">(</span><span class="n">特征金字塔第三层</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">├──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span> <span class="p">(</span><span class="mi">6</span><span class="n">个Block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">每个block有16个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>   <span class="err">└──</span> <span class="n">downsample</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">└──</span> <span class="n">stage4</span> <span class="p">(</span><span class="n">特征金字塔第四层</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>       <span class="err">└──</span> <span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">(</span><span class="mi">2</span><span class="n">个Block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>           <span class="err">└──</span> <span class="n">每个block有32个注意力头</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">├──</span> <span class="mf">2.</span> <span class="n">Neck</span> <span class="p">(</span><span class="n">特征融合网络</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">融合层1</span> <span class="p">(</span><span class="n">feat4</span> <span class="o">+</span> <span class="n">feat3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">├──</span> <span class="n">融合层2</span> <span class="p">(</span><span class="n">feat3</span> <span class="o">+</span> <span class="n">feat2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>   <span class="err">└──</span> <span class="n">融合层3</span> <span class="p">(</span><span class="n">feat2</span> <span class="o">+</span> <span class="n">feat1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">│</span>
</span></span><span class="line"><span class="cl"><span class="err">└──</span> <span class="mf">3.</span> <span class="n">Head</span> <span class="p">(</span><span class="n">输出头</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">Conv2d</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="n">ReLU</span>
</span></span><span class="line"><span class="cl">    <span class="err">└──</span> <span class="n">Conv2d</span> <span class="p">(</span><span class="n">最终深度图</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a name="特征提取与融合"></a></p>
<h3 id="特征提取与融合">特征提取与融合
</h3><p><strong>特征提取</strong>：</p>
<ul>
<li><strong>Backbone</strong> 提取不同尺度的特征，形成特征金字塔。</li>
<li><strong>各 Stage 输出的特征</strong>：
<ul>
<li><strong>Stage 1</strong>：高分辨率，低级特征。</li>
<li><strong>Stage 4</strong>：低分辨率，高级语义特征。</li>
</ul>
</li>
</ul>
<p><strong>特征融合（Neck）</strong>：</p>
<ul>
<li><strong>目的</strong>：将不同尺度的特征进行融合，整合局部和全局信息。</li>
<li><strong>过程</strong>：
<ol>
<li><strong>从高层特征开始</strong>，逐步上采样。</li>
<li><strong>与下一层特征融合</strong>（通常通过加法或连接）。</li>
<li><strong>重复步骤</strong>，直到融合所有层的特征。</li>
</ol>
</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">FeatureFusionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">ResidualConvUnit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">up</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">skip_connection</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">skip_connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a name="深度估计的实现"></a></p>
<h3 id="深度估计的实现">深度估计的实现
</h3><p><strong>头部网络（Head）</strong>：</p>
<ul>
<li><strong>结构</strong>：由一系列卷积层和激活函数组成。</li>
<li><strong>作用</strong>：将融合后的特征映射为深度估计结果。</li>
</ul>
<p><strong>实现</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DepthEstimationHead</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>完整的前向传播过程</strong>：</p>
<ol>
<li><strong>输入图像经过 Backbone</strong>，提取多尺度特征。</li>
<li><strong>Neck 模块融合特征</strong>，生成综合特征表示。</li>
<li><strong>Head 输出深度图</strong>。</li>
</ol>
<hr>
<p><a name="模型调用流程"></a></p>
<h2 id="模型调用流程">模型调用流程
</h2><p>以Swin V2-Base为例，详细解释MiDaS的完整工作流程：</p>
<p><strong>模型初始化和加载</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1. 创建MiDaS模型实例</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">MidasNet</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model_path</span><span class="o">=</span><span class="s2">&#34;weights/dpt_swin2_base_384.pt&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>  <span class="c1"># 特征维度</span>
</span></span><span class="line"><span class="cl">    <span class="n">non_negative</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># 确保深度为非负</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. 加载Swin V2-Base backbone</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_make_pretrained_swin2b24_384</span><span class="p">(</span><span class="n">pretrained</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载预训练的Swin V2-Base模型</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">timm</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;swinv2_base_window12to24_192to384_22kft1k&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>图像预处理和特征提取</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1. 输入图像预处理</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 原始图像: [H, W, 3] -&gt; [1, 3, 384, 384]</span>
</span></span><span class="line"><span class="cl"><span class="n">input_image</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>  <span class="c1"># 归一化、调整大小等</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. Swin V2-Base特征提取</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Swin V2-Base有4个stage，每个stage提取不同尺度的特征</span>
</span></span><span class="line"><span class="cl"><span class="n">stage1_feature</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># [1, 128, 96, 96]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage2_feature</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">stage1</span><span class="p">)</span>  <span class="c1"># [1, 256, 48, 48]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage3_feature</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">stage2</span><span class="p">)</span>  <span class="c1"># [1, 512, 24, 24]</span>
</span></span><span class="line"><span class="cl"><span class="n">stage4_feature</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">3</span><span class="p">](</span><span class="n">stage3</span><span class="p">)</span>  <span class="c1"># [1, 1024, 12, 12]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>特征重映射和统一</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 将不同stage的特征映射到统一维度（256通道）</span>
</span></span><span class="line"><span class="cl"><span class="n">layer1_rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">layer1_rn</span><span class="p">(</span><span class="n">stage1_feature</span><span class="p">)</span>  <span class="c1"># [1, 256, 96, 96]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer2_rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">layer2_rn</span><span class="p">(</span><span class="n">stage2_feature</span><span class="p">)</span>  <span class="c1"># [1, 256, 48, 48]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer3_rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">layer3_rn</span><span class="p">(</span><span class="n">stage3_feature</span><span class="p">)</span>  <span class="c1"># [1, 256, 24, 24]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer4_rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">layer4_rn</span><span class="p">(</span><span class="n">stage4_feature</span><span class="p">)</span>  <span class="c1"># [1, 256, 12, 12]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>特征融合过程</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 自底向上的特征融合</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 1. 最深层特征处理</span>
</span></span><span class="line"><span class="cl"><span class="n">path_4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">refinenet4</span><span class="p">(</span><span class="n">layer4_rn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [1, 256, 12, 12]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. 融合第三层</span>
</span></span><span class="line"><span class="cl"><span class="n">path_3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">refinenet3</span><span class="p">(</span><span class="n">path_4</span><span class="p">,</span> <span class="n">layer3_rn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># path_4上采样到[1, 256, 24, 24]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 与layer3_rn [1, 256, 24, 24]融合</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3. 融合第二层</span>
</span></span><span class="line"><span class="cl"><span class="n">path_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">refinenet2</span><span class="p">(</span><span class="n">path_3</span><span class="p">,</span> <span class="n">layer2_rn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># path_3上采样到[1, 256, 48, 48]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 与layer2_rn [1, 256, 48, 48]融合</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 4. 融合第一层</span>
</span></span><span class="line"><span class="cl"><span class="n">path_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">refinenet1</span><span class="p">(</span><span class="n">path_2</span><span class="p">,</span> <span class="n">layer1_rn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># path_2上采样到[1, 256, 96, 96]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 与layer1_rn [1, 256, 96, 96]融合</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>深度图生成</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 通过输出头生成深度图</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">output_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. 256-&gt;128通道，保持空间分辨率</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [1, 128, 96, 96]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. 上采样2倍</span>
</span></span><span class="line"><span class="cl">    <span class="n">Interpolate</span><span class="p">(</span><span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;bilinear&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [1, 128, 192, 192]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. 128-&gt;32通道</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [1, 32, 192, 192]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 4. 32-&gt;1通道（最终深度图）</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 确保深度非负</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># [1, 1, 192, 192]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>后处理和输出</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1. 调整深度图大小到原始图像尺寸</span>
</span></span><span class="line"><span class="cl"><span class="n">depth</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">depth</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">original_height</span><span class="p">,</span> <span class="n">original_width</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;bilinear&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. 深度图归一化（可选）</span>
</span></span><span class="line"><span class="cl"><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">-</span> <span class="n">depth</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">/</span> <span class="n">depth</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关键特点：</p>
<ol>
<li><strong>Swin V2-Base的优势</strong>：
<ul>
<li>动态窗口大小（12-&gt;24）</li>
<li>高效的层次化特征提取</li>
<li>预训练在ImageNet-22K和1K上</li>
</ul>
</li>
<li><strong>特征提取的层次性</strong>：
<ul>
<li>Stage 1：捕获局部纹理和边缘</li>
<li>Stage 2：感知基本形状</li>
<li>Stage 3：理解物体部件</li>
<li>Stage 4：把握场景结构</li>
</ul>
</li>
<li><strong>特征融合的策略</strong>：
<ul>
<li>自底向上融合</li>
<li>残差连接保持信息</li>
<li>逐步整合全局和局部信息</li>
</ul>
</li>
<li><strong>深度估计的精确性</strong>：
<ul>
<li>多尺度特征利用</li>
<li>渐进式信息融合</li>
<li>高分辨率输出</li>
</ul>
</li>
</ol>
<p>数据流动过程：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1. 输入图像</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">]</span>  <span class="c1"># [批次, 通道, 高, 宽]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. Patch Embedding</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">96</span><span class="o">*</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>   <span class="c1"># [批次, patch数, 特征维度]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3. Stage 1</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">48</span><span class="o">*</span><span class="mi">48</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>   <span class="c1"># 空间降采样，通道数增加</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 4. Stage 2</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>   <span class="c1"># 继续降采样和增加通道</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 5. Stage 3</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="o">*</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>  <span class="c1"># 继续降采样和增加通道</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 6. Stage 4</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>    <span class="c1"># 最终特征图</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个过程就像是：</p>
<ol>
<li>先用不同&quot;专家&quot;（通道）从不同角度观察图像</li>
<li>将这些观察逐层提炼（特征提取）</li>
<li>整合不同层次的理解（特征融合）</li>
<li>最终得出深度判断（深度图）</li>
</ol>
<hr>
<p><a name="关键名词解释"></a></p>
<h2 id="关键名词解释-1">关键名词解释
</h2><p>本节对文中提到的关键名词进行详细解释，帮助读者更好地理解模型的工作原理。</p>
<p><a name="Block块"></a></p>
<h3 id="block块">Block（块）
</h3><ul>
<li><strong>定义</strong>：模型的基本组成单元，包含多个层和操作。</li>
<li><strong>在 Swin Transformer 中</strong>：
<ul>
<li><strong>Swin Transformer Block</strong>：包括 LayerNorm、多头注意力机制、残差连接和前馈网络等组件。</li>
<li><strong>作用</strong>：在每个 Stage 中，多个 Block 依次堆叠，逐步提取更高级的特征。</li>
</ul>
</li>
</ul>
<p><a name="Patch补丁"></a></p>
<h3 id="patch补丁">Patch（补丁）
</h3><ul>
<li>
<p><strong>定义</strong>：将输入图像划分为固定大小的小块，每个小块称为一个补丁（Patch）。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 假设输入图像: [1, 3, 384, 384]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># patch_size = 4，意味着将图像切分成4x4的小块</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 图像切分过程：</span>
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">原始图像(384x384)
</span></span></span><span class="line"><span class="cl"><span class="s2">┌────────────────┐
</span></span></span><span class="line"><span class="cl"><span class="s2">│ ┌──┐┌──┐┌──┐  │
</span></span></span><span class="line"><span class="cl"><span class="s2">│ │P1││P2││P3│  │
</span></span></span><span class="line"><span class="cl"><span class="s2">│ └──┘└──┘└──┘  │
</span></span></span><span class="line"><span class="cl"><span class="s2">│ ┌──┐┌──┐┌──┐  │
</span></span></span><span class="line"><span class="cl"><span class="s2">│ │P4││P5││P6│  │
</span></span></span><span class="line"><span class="cl"><span class="s2">│ └──┘└──┘└──┘  │
</span></span></span><span class="line"><span class="cl"><span class="s2">└────────────────┘
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">每个Patch(4x4):
</span></span></span><span class="line"><span class="cl"><span class="s2">P1 = ┌────┐
</span></span></span><span class="line"><span class="cl"><span class="s2">     │RGB │
</span></span></span><span class="line"><span class="cl"><span class="s2">     │RGB │
</span></span></span><span class="line"><span class="cl"><span class="s2">     │RGB │
</span></span></span><span class="line"><span class="cl"><span class="s2">     └────┘
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 384/4 = 96，所以水平和垂直方向各有96个patch</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 因此总共有96x96=9216个patch</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><strong>在 Swin Transformer 中</strong>：</p>
<ul>
<li><strong>Patch Embedding</strong>：将每个补丁转换为高维特征向量，作为模型的输入。</li>
<li><strong>优势</strong>：减少计算复杂度，将二维图像数据转换为一维序列，方便 Transformer 处理。</li>
</ul>
</li>
</ul>
<p><a name="Depth深度"></a></p>
<h3 id="depth深度">Depth（深度）
</h3><ul>
<li><strong>在模型结构中</strong>：
<ul>
<li><strong>Depth（深度）</strong>：指每个 Stage 中包含的 Block 数量，影响模型的深度和特征提取能力。</li>
</ul>
</li>
<li><strong>在深度估计任务中</strong>：
<ul>
<li><strong>Depth（深度）</strong>：指场景中物体距离相机的距离，深度估计的目标就是预测每个像素的深度值。</li>
</ul>
</li>
<li>depths参数的选择建议：
<ol>
<li><strong>任务复杂度</strong>：
<ul>
<li>简单任务：使用较小的depths</li>
<li>复杂任务：增加关键stage的depth</li>
</ul>
</li>
<li><strong>计算资源</strong>：
<ul>
<li>限制严格：使用较小的depths</li>
<li>资源充足：可以使用更大的depths</li>
</ul>
</li>
<li><strong>数据规模</strong>：
<ul>
<li>数据少：避免使用过大的depths（防止过拟合）</li>
<li>数据充足：可以使用更大的depths</li>
</ul>
</li>
<li><strong>实时性要求</strong>：
<ul>
<li>要求高：使用较小的depths</li>
<li>要求低：可以使用更大的depths</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><a name="Head注意力头"></a></p>
<h3 id="head注意力头">Head（注意力头）
</h3><ul>
<li><strong>定义</strong>：在多头注意力机制中，多个独立的注意力计算单元称为注意力头（Head）。</li>
<li><strong>在 Swin Transformer 中</strong>：
<ul>
<li><strong>多头注意力机制</strong>：将特征维度分割成多个子空间，每个 Head 在各自的子空间中计算注意力，增强模型的特征表示能力。</li>
</ul>
</li>
</ul>
<h3 id="embed_dim嵌入维度">embed_dim（嵌入维度）
</h3><ul>
<li>
<p><strong>定义</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 每个4x4的patch包含：</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_pixels</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">3</span>  <span class="c1"># 48个值(4x4个像素，每个像素3个RGB通道)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 嵌入过程：</span>
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">一个Patch(48个值)     -&gt;     嵌入向量(128维)
</span></span></span><span class="line"><span class="cl"><span class="s2">[R1,G1,B1,           -&gt;     [e1,
</span></span></span><span class="line"><span class="cl"><span class="s2"> R2,G2,B2,                   e2,
</span></span></span><span class="line"><span class="cl"><span class="s2"> R3,G3,B3,                   e3,
</span></span></span><span class="line"><span class="cl"><span class="s2"> ...                         ...
</span></span></span><span class="line"><span class="cl"><span class="s2"> R16,G16,B16]               e128]
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">这是一个从48维到128维的转换
</span></span></span><span class="line"><span class="cl"><span class="s2">就像用128个词来描述一个4x4的图像块
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 这个转换通过线性投影实现：</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">patch_pixels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># 48 -&gt; 128</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<h3 id="channels通道数">Channels（通道数）
</h3><p><strong>在MiDaS这个具体场景中的通道数</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 输入图像的通道数</span>
</span></span><span class="line"><span class="cl"><span class="n">input_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="c1"># [batch_size, channels, height, width]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># channels = 3: RGB三个颜色通道</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Backbone提取的特征通道数</span>
</span></span><span class="line"><span class="cl"><span class="n">layer_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># [1, 256, H/4, W/4]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">layer_1</span><span class="p">)</span>  <span class="c1"># [1, 512, H/8, W/8]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer_3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">layer_2</span><span class="p">)</span>  <span class="c1"># [1, 1024, H/16, W/16]</span>
</span></span><span class="line"><span class="cl"><span class="n">layer_4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pretrained</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">layer_3</span><span class="p">)</span>  <span class="c1"># [1, 2048, H/32, W/32]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在这里，通道数的变化表示：</p>
<ul>
<li>开始：3个通道（RGB）</li>
<li>第一层：扩展到256个通道（256个不同的特征检测器）</li>
<li>第二层：512个通道（更多的特征检测器）</li>
<li>第三层：1024个通道</li>
<li>第四层：2048个通道</li>
</ul>
<p><strong>在抽象意义上的通道数</strong>： 想象你在看一幅画：</p>
<p>A. <strong>单通道（Grayscale）</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">grayscale_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 就像黑白照片，只有明暗信息</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>每个像素只有一个值</li>
<li>类比：用铅笔画素描，只有深浅变化</li>
</ul>
<p>B. <strong>三通道（RGB）</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">color_image</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 像彩色照片，有RGB三个颜色通道</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>每个像素有三个值（红、绿、蓝）</li>
<li>类比：用三支彩色笔画画，可以混合出各种颜色</li>
</ul>
<p>C. <strong>多通道特征图</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">feature_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 256个不同的&#34;视角&#34;看同一张图</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>类比：想象256个不同的专家同时在观察同一个物体：</p>
<ul>
<li>专家1关注边缘</li>
<li>专家2关注颜色</li>
<li>专家3关注纹理</li>
<li>专家4关注形状</li>
<li>&hellip;以此类推</li>
</ul>
<p><strong>通道数增加的意义</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1x1卷积调整通道数</span>
</span></span><span class="line"><span class="cl"><span class="n">conv1x1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 从3通道扩展到64通道</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像：</p>
<ul>
<li>一个人看到一个苹果：只能说&quot;这是个红色的圆形物体&quot;</li>
<li>64个专家看同一个苹果：
<ul>
<li>专家1：&ldquo;表面有光泽&rdquo;</li>
<li>专家2：&ldquo;有细微的斑点&rdquo;</li>
<li>专家3：&ldquo;果皮有特定纹理&rdquo;</li>
<li>专家4：&ldquo;边缘有特定曲率&rdquo;</li>
<li>&hellip;更多细节描述</li>
</ul>
</li>
</ul>
<p><strong>通道数在特征融合中的作用</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">FeatureFusionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># features=256意味着256个&#34;专家&#34;的意见</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像：</p>
<ul>
<li>不同层的256个专家各自提供观点</li>
<li>通过融合得到更全面的理解</li>
<li>保持通道数相同确保&quot;专家数量&quot;一致，便于信息整合</li>
</ul>
<p><strong>实际应用中的通道数变化</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 在MiDaS中的输出层</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">scratch</span><span class="o">.</span><span class="n">output_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="c1"># 256-&gt;128通道</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>   <span class="c1"># 128-&gt;32通道</span>
</span></span><span class="line"><span class="cl">    <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>      <span class="c1"># 32-&gt;1通道（深度图）</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这就像：</p>
<ol>
<li>先有256个专家的详细分析</li>
<li>整合为128个关键观点</li>
<li>进一步提炼为32个核心发现</li>
<li>最终得出一个统一的结论（深度值）</li>
</ol>
<p>通道数的本质是特征维度，它决定了模型能够同时关注多少种不同的特征模式。就像人类观察事物时，可以同时注意到形状、颜色、纹理、边缘等多个方面，深度学习模型通过多通道来实现这种多角度的特征提取和分析。</p>
<h3 id="hooks特征提取点">Hooks（特征提取点）
</h3><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">hooks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 在不同层设置特征提取点</span>
</span></span><span class="line"><span class="cl"><span class="c1"># - 第1层：第1个block的输出</span>
</span></span><span class="line"><span class="cl"><span class="c1"># - 第2层：第1个block的输出</span>
</span></span><span class="line"><span class="cl"><span class="c1"># - 第3层：第17个block的输出</span>
</span></span><span class="line"><span class="cl"><span class="c1"># - 第4层：第1个block的输出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">这些组件协同工作的过程：
</span></span></span><span class="line"><span class="cl"><span class="s1">输入图像首先被分割成补丁并嵌入
</span></span></span><span class="line"><span class="cl"><span class="s1">通过多个Transformer块进行特征提取
</span></span></span><span class="line"><span class="cl"><span class="s1">在指定的hook点提取多尺度特征
</span></span></span><span class="line"><span class="cl"><span class="s1">特征被送入头部网络进行处理
</span></span></span><span class="line"><span class="cl"><span class="s1">最终输出深度估计结果
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="pt模型文件">.pt（模型文件）
</h3><ol>
<li><code>.pt</code>文件通常包含：
<ul>
<li>模型参数（state_dict）</li>
<li>模型配置（config）</li>
<li>优化器状态（如果保存）</li>
<li>训练元数据（如果保存）</li>
</ul>
</li>
<li>访问特定部分：
<ul>
<li>Backbone: <code>model.pretrained</code>或<code>state_dict['pretrained.*']</code></li>
<li>特征提取器: <code>model.scratch</code>或<code>state_dict['scratch.*']</code></li>
<li>特征融合块: <code>model.scratch.refinenet*</code></li>
</ul>
</li>
<li>特征的处理流程：
<ul>
<li>输入图像 → Backbone提取特征 → 特征融合块处理 → 输出深度图</li>
</ul>
</li>
</ol>
<p>实际示例：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 加载并查看模型结构</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">inspect_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;检查模型结构和参数&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;模型的主要组成部分：&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;- </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">模型层的结构：&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;state_dict&#39;</span> <span class="ow">in</span> <span class="n">model</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;层名称: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;参数形状: </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 使用示例</span>
</span></span><span class="line"><span class="cl"><span class="c1"># inspect_model(&#39;path_to_your_model.pt&#39;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a name="模型训练与参数调整"></a></p>
<h2 id="模型训练与参数调整">模型训练与参数调整
</h2><p><a name="损失函数与优化器"></a></p>
<h3 id="损失函数与优化器">损失函数与优化器
</h3><p><strong>损失函数</strong>：</p>
<ul>
<li><strong>L1 损失</strong>：计算预测深度与真实深度之间的绝对误差。</li>
<li><strong>L2 损失</strong>：计算预测深度与真实深度之间的平方误差。</li>
<li><strong>自定义损失</strong>：可以结合结构相似性（SSIM）等指标，提升模型性能。</li>
</ul>
<p><strong>优化器</strong>：</p>
<ul>
<li><strong>AdamW</strong>：适用于 Transformer 结构的优化器，具有权重衰减的功能。</li>
<li><strong>学习率</strong>：需要根据数据集和模型复杂度进行调整。</li>
</ul>
<p><a name="训练循环"></a></p>
<h3 id="训练循环">训练循环
</h3><p><strong>代码示例</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">depths</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">depths</span> <span class="o">=</span> <span class="n">depths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 前向传播</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">depths</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 反向传播和优化</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 可选：验证模型性能，调整学习率等</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>注意事项</strong>：</p>
<ul>
<li><strong>梯度裁剪</strong>：防止梯度爆炸，特别是在深度模型中。</li>
<li><strong>学习率调度</strong>：使用 <code>CosineAnnealingLR</code> 或 <code>ReduceLROnPlateau</code> 等调度策略。</li>
</ul>
<p><a name="超参数的影响"></a></p>
<h3 id="超参数的影响">超参数的影响
</h3><p><strong>模型参数</strong>：</p>
<ul>
<li><strong>Embed_dim（嵌入维度）</strong>：
<ul>
<li><strong>影响</strong>：决定了特征向量的维度，增大 <code>embed_dim</code> 可以提升特征表示能力，但会增加计算量。</li>
</ul>
</li>
<li><strong>Depths（深度）</strong>：
<ul>
<li><strong>影响</strong>：每个 Stage 中的 Block 数量，增加 Block 数量可以提取更深层次的特征，但也会增加模型复杂度。</li>
</ul>
</li>
<li><strong>Num_heads（注意力头数量）</strong>：
<ul>
<li><strong>影响</strong>：决定了多头注意力机制的并行度，增加注意力头数量可以捕获更多特征模式，但会增加内存占用。</li>
</ul>
</li>
</ul>
<p><strong>训练参数</strong>：</p>
<ul>
<li><strong>学习率</strong>：需要根据模型和数据集进行调整，过大会导致训练不稳定，过小会导致收敛缓慢。</li>
<li><strong>批次大小</strong>：需要根据 GPU 内存大小进行设置，较大的批次大小可以稳定梯度，但会占用更多内存。</li>
<li><strong>权重衰减</strong>：防止模型过拟合，通常设置为 <code>1e-4</code> 或 <code>1e-5</code>。</li>
</ul>
<hr>
<h2 id="参考">参考
</h2><ol>
<li><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong>
<ul>
<li>作者：Ze Liu, Yutong Lin, Yue Cao, et al.</li>
<li>链接：<a class="link" href="https://arxiv.org/abs/2103.14030"  target="_blank" rel="noopener"
    >https://arxiv.org/abs/2103.14030</a>

</li>
</ul>
</li>
<li><strong>MiDaS: Robust Monocular Depth Estimation with Mix and Match of Dense and Sparse Depth Samples</strong>
<ul>
<li>作者：René Ranftl, Katrin Lasinger, David Hafner, et al.</li>
<li>链接：<a class="link" href="https://arxiv.org/abs/1907.01341"  target="_blank" rel="noopener"
    >https://arxiv.org/abs/1907.01341</a>

</li>
</ul>
</li>
<li><strong>PyTorch 官方文档</strong>
<ul>
<li>链接：<a class="link" href="https://pytorch.org/docs/"  target="_blank" rel="noopener"
    >https://pytorch.org/docs/</a>

</li>
</ul>
</li>
<li><strong>timm: PyTorch Image Models</strong>
<ul>
<li>作者：Ross Wightman</li>
<li>链接：<a class="link" href="https://github.com/rwightman/pytorch-image-models"  target="_blank" rel="noopener"
    >https://github.com/rwightman/pytorch-image-models</a>

</li>
</ul>
</li>
</ol>

</section>


    <footer class="article-footer">
    

    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            最后更新于 Dec 01, 2024 23:35 &#43;0800
        </span>
    </section></footer>




    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

     
    
        
    

    <footer class="site-footer">

    <section class="copyright">
        &copy; 
        
        2024 echudet
    </section>
    
    <section class="powerby">
        


        <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>

        <script language="javascript"> 
            var now = new Date();
            function createtime(){
                now.setTime(now.getTime()+250);
                var grt= new Date("2024/11/10 00:00:00"); 
                days = (now - grt ) / 1000 / 60 / 60 / 24;
                dnum = Math.floor(days);
                hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
                hnum = Math.floor(hours);
                if(String(hnum).length ==1 ){hnum = "0" + hnum;}
                minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
                mnum = Math.floor(minutes);
                if(String(mnum).length ==1 ){mnum = "0" + mnum;}
                seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
                snum = Math.round(seconds);
                if(String(snum).length ==1 ){snum = "0" + snum;}

                document.getElementById("timeDate").innerHTML = "本站已稳定运行"+dnum+" 天 ";
                document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒<br>"
            }
            setInterval("createtime()",250); 
        </script> 

        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        
    
        
         
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
        

        共 733820 字 , 33 篇文章<br>使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>

</footer>



    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>


    </body>
</html>
