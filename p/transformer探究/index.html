<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="transformer">
<title>Transformer探究</title>

<link rel='canonical' href='https://echudet.github.io/p/transformer%E6%8E%A2%E7%A9%B6/'>

<link rel="stylesheet" href="/scss/style.min.209f2315995b57a9cdd733e5afe32e05ebb1138a0b3bb750cfcb86dff885e546.css"><meta property='og:title' content="Transformer探究">
<meta property='og:description' content="transformer">
<meta property='og:url' content='https://echudet.github.io/p/transformer%E6%8E%A2%E7%A9%B6/'>
<meta property='og:site_name' content='echudet'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-09-13T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2025-11-01T19:46:45&#43;08:00'/>
<meta name="twitter:title" content="Transformer探究">
<meta name="twitter:description" content="transformer">
    <link rel="shortcut icon" href="/timer.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_c2920364e6534697.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">⌨️</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">echudet</a></h1>
            <h2 class="site-description">探索编程的学生一枚</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/EchudeT'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://twitter.com'
                        target="_blank"
                        title="Twitter"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>档案</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#编码器">编码器</a></li>
    <li><a href="#self-attention计算过程">Self-Attention计算过程</a></li>
    <li><a href="#the-beast-with-many-heads">The Beast With Many Heads</a></li>
    <li><a href="#使用位置编码来表示序列的顺序">使用位置编码来表示序列的顺序</a></li>
    <li><a href="#残差连接">残差连接</a></li>
    <li><a href="#解码器">解码器</a></li>
    <li><a href="#最后的线性层和-softmax-层">最后的线性层和 Softmax 层</a></li>
    <li><a href="#transformer-的训练过程">Transformer 的训练过程</a></li>
    <li><a href="#损失函数">损失函数</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">


    
    <header class="article-category">
        
            <a href="/categories/ai/" >
                Ai
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/transformer%E6%8E%A2%E7%A9%B6/">Transformer探究</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            transformer
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-09-13</time>
            </div>
        

        
            <div>
                <?xml version="1.0" encoding="iso-8859-1"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="792px" height="792px" viewBox="0 0 792 792" style="enable-background:new 0 0 792 792;" xml:space="preserve">
<g>
	<g id="_x31_0_19_">
		<g>
			<path d="M643.5,742.5H594V569.25C594,473.566,516.434,396,420.75,396C516.434,396,594,318.434,594,222.75V49.5h49.5
				c13.662,0,24.75-11.088,24.75-24.75S657.162,0,643.5,0h-495c-13.662,0-24.75,11.088-24.75,24.75S134.838,49.5,148.5,49.5H198
				v173.25C198,318.434,275.566,396,371.25,396C275.566,396,198,473.566,198,569.25V742.5h-49.5c-13.662,0-24.75,11.088-24.75,24.75
				S134.838,792,148.5,792h495c13.662,0,24.75-11.088,24.75-24.75S657.162,742.5,643.5,742.5z M247.5,222.75
				c0-43.387,0-173.25,0-173.25h297c0,0,0,133.427,0,173.25c0,68.335-58.188,123.75-129.938,123.75h-37.125
				C305.687,346.5,247.5,291.085,247.5,222.75z M544.5,742.5h-297c0,0,0-129.888,0-173.25c0-68.335,58.187-123.75,129.938-123.75
				h37.125c71.75,0,129.938,55.415,129.938,123.75C544.5,609.072,544.5,742.5,544.5,742.5z"/>
		</g>
	</g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>

                <time class="article-time--reading">
                    阅读时长: 13 分钟
                </time>
            </div>
        

         
        <div>
            <?xml version="1.0" encoding="iso-8859-1"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="79.536px" height="79.536px" viewBox="0 0 79.536 79.536" style="enable-background:new 0 0 79.536 79.536;"
	 xml:space="preserve">
<g>
	<path style="fill:#010002;" d="M40.162,17.906C37.415,15.275,21.614,6.141,0,15.451c0,0.138,0,2.722,0,6.654v44.418h32.747
		c1.126,0.968,3.798,1.651,6.926,1.651c3.119,0,5.802-0.684,6.931-1.651h32.933V22.105c0-3.933,0-6.517,0-6.654
		C58.813,6.314,42.257,15.982,40.162,17.906z M37.229,63.349c0-1.75-15.372-11.981-33.125-0.818V19.216
		c3.516-2.014,8.246-3.249,13.463-3.249c10.864,0,19.662,3.562,19.662,10.175V63.349z M75.684,62.53
		c-17.751-11.163-33.108-0.932-33.108,0.818V26.146c0-6.612,8.792-10.175,19.646-10.175c5.22,0,9.952,1.235,13.463,3.249V62.53z"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
</svg>

            <time class="article-words">
                文章字数：6173字
            </time>
        </div>
        <div class="article-lastmod">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time>
                    Nov 01, 2025 19:46 &#43;0800
                </time>
            </div></footer>
    

    




    
</div>

</header>

    <section class="article-content">
    
    
    <p>参考+图片来源：<a class="link" href="https://jalammar.github.io/illustrated-transformer/"  target="_blank" rel="noopener"
    >https://jalammar.github.io/illustrated-transformer/</a>

</p>
<p>Transformer由谷歌于2017年在《Attention Is All You Need》论文中提出，凭借其完全基于注意力机制的创新设计，彻底改变了自然语言处理领域的格局，成为BERT、Llama等主流模型的核心组件。</p>
<p>Transformer模型的其中一个优点，就是使得模型训练过程能够并行计算。在 RNN 中，每一个 time step 的计算都依赖于上一个 time step 的输出，这就使得所有的 time step 必须串行化，无法并行计算。</p>
<p>而在 Transformer 中，所有 time step 的数据，都是经过 Self Attention 计算，使得整个运算过程可以并行化计算。</p>
<p>Transformer 使用了Seq2Seq任务中常用的结构——包括两个部分：Encoder 和 Decoder，这两大部分根据模型参数量的不同堆叠的层数也是不同的。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/en-de.png"
	width="1319"
	height="935"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/en-de_hu_37dd6d825a2f6f89.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/en-de_hu_2b65e097f734f1ef.png 1024w"
	loading="lazy"
	
		alt="en-de"
	
	
		class="gallery-image" 
		data-flex-grow="141"
		data-flex-basis="338px"
	
></p>
<h2 id="编码器">编码器
</h2><p>每个编码器在结构上是相同的，并且可以分为两部分看待。</p>
<p>第一部分是Self-Attention Layer层。这个层处理一个词的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息。</p>
<p>下面来分析 Self-Attention 的具体机制。
假设我们想要翻译的句子是：The animal didn&rsquo;t cross the street because it was too tired
这个句子中的it是一个指代词，那么it指的是什么呢？它是指animal还是street？这个问题对人来说，是很简单的，但是对算法来说并不是那么容易。当模型在处理（翻译）it的时候，Self Attention机制能够让模型把it和animal关联起来。</p>
<p>同理，当模型处理句子中的每个词时，Self Attention机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词，作为辅助线索，进而可以更好地编码当前位置的词。</p>
<p>如果你熟悉 RNN，回忆一下：RNN 在处理一个词时，会考虑前面传过来的hidden state，而hidden state就包含了前面的词的信息。而 Transformer 使用Self Attention机制，会把其他单词的理解融入处理当前的单词。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention.png"
	width="630"
	height="618"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention_hu_cd716d2dff48e181.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/self-attention_hu_12d73356d9401c7.png 1024w"
	loading="lazy"
	
		alt="self-attention"
	
	
		class="gallery-image" 
		data-flex-grow="101"
		data-flex-basis="244px"
	
></p>
<p>当我们在第五层编码器中（编码部分中的最后一层编码器）编码“it”时，有一部分注意力集中在“The animal”上，并且把这两个词的信息融合到了&quot;it&quot;这个单词中。</p>
<p>第二部分是Feed Forward Neural Network层（前馈神经网络）。</p>
<p>作用很简单，乘以一定的参数并且向前传递。</p>
<p>Transformer 有一个重要特性：每个位置的词向量经过编码器都有自己单独的路径。具体来说，在 Self Attention 层中，这些路径之间是有依赖关系的；而在 Feed Forward （前馈神经网络）层中，这些路径之间是没有依赖关系的。因此这些词向量在经过 Feed Forward 层中可以并行计算。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/encoder_with_tensors_2.png"
	width="1268"
	height="771"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/encoder_with_tensors_2_hu_bc89721e601002a3.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/encoder_with_tensors_2_hu_a80f107ae2b9f9d0.png 1024w"
	loading="lazy"
	
		alt="encoder_with_tensors_2"
	
	
		class="gallery-image" 
		data-flex-grow="164"
		data-flex-basis="394px"
	
></p>
<h2 id="self-attention计算过程">Self-Attention计算过程
</h2><p>下面我们先看下如何使用向量来计算 Self Attention，然后再看下如何使用矩阵来实现 Self Attention。矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式。</p>
<p>第 1 步是对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量。这 3 个向量是词向量分别和 3 个矩阵相乘得到的，而这个矩阵是我们要学习的参数。</p>
<p>注意，这 3 个新得到的向量一般比原来的词向量的长度更小。</p>
<p>Query 向量，Key 向量，Value 向量是什么含义呢？</p>
<p>其实它们就是 3 个向量，给它们加上一个名称，可以让我们更好地理解 Self-Attention 的计算过程和逻辑含义。</p>
<p>第 2 步是计算 Attention Score（注意力分数）。</p>
<p>假设我们现在计算第一个词Thinking的 Attention Score（注意力分数），需要根据Thinking这个词，对句子中的其他每个词都计算一个分数。这些分数决定了我们在编码Thinking这个词时，需要对句子中其他位置的每个词放置多少的注意力。</p>
<p>这些分数，是通过计算 &ldquo;Thinking&rdquo; 对应的 Query 向量和其他位置的每个词的 Key 向量的点积，而得到的。</p>
<p>第 3 步就是把每个分数除以Key 向量的长度。你也可以除以其他数，除以一个数是为了在反向传播时，求取梯度更加稳定。</p>
<p>第 4 步，接着把这些分数经过一个 Softmax 层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于 1。</p>
<p>这些分数决定了在编码当前位置（这里的例子是第一个位置）的词时，对所有位置的词分别有多少的注意力。</p>
<p>一般来说，对自己的注意力是最多的。</p>
<p>第 5 步，得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘。</p>
<p>这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。</p>
<p>第 6 步是把上一步得到的向量相加，就得到了 Self Attention 层在这个位置（这里的例子是第一个位置）的输出。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention2.png"
	width="1109"
	height="1162"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention2_hu_6b4f3efe9943d1a2.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/self-attention2_hu_57381d88438b7f1f.png 1024w"
	loading="lazy"
	
		alt="self-attention2"
	
	
		class="gallery-image" 
		data-flex-grow="95"
		data-flex-basis="229px"
	
></p>
<p>上面这张图，包含了 Self Attention 的全过程，最终得到的当前位置（这里的例子是第一个位置）的向量会输入到前馈神经网络。</p>
<p>但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。下面让我们来看，如何使用矩阵来计算所有位置的输出向量。</p>
<p>又知道Q，K，W可以由X和相关的权重矩阵W得到。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation.png"
	width="581"
	height="658"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation_hu_b95fb88addb8957a.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation_hu_1d1ae9aeeeae6448.png 1024w"
	loading="lazy"
	
		alt="self-attention-matrix-calculation"
	
	
		class="gallery-image" 
		data-flex-grow="88"
		data-flex-basis="211px"
	
></p>
<p>综合上述步骤，可以得到Self-Attention的一个简便矩阵运算</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation-2.png"
	width="893"
	height="349"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation-2_hu_a35bcc245811b4b6.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/self-attention-matrix-calculation-2_hu_b5ab2e60c4f0e05e.png 1024w"
	loading="lazy"
	
		alt="self-attention-matrix-calculation-2"
	
	
		class="gallery-image" 
		data-flex-grow="255"
		data-flex-basis="614px"
	
></p>
<h2 id="the-beast-with-many-heads">The Beast With Many Heads
</h2><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了 Self Attention 层。这种机制从如下两个方面增强了 attention 层的能力：</p>
<ol>
<li>
<p>它扩展了模型关注不同位置的能力。</p>
<p>在上面的例子中，第一个位置的输出 z1 包含了句子中其他每个位置的很小一部分信息，但 z1 可能主要是由第一个位置的信息决定的。</p>
<p>当我们翻译句子：The animal didn’t cross the street because it was too tired时，我们想让机器知道其中的it指代的是什么。这时，多头注意力机制会有帮助。</p>
</li>
<li>
<p>多头注意力机制赋予 attention 层多个“子表示空间”。</p>
<p>下面我们会看到，多头注意力机制会有多组的权重矩阵（在 Transformer 的论文中，使用了 8 组注意力（attention heads）。因此，接下来我也是用 8 组注意力头 （attention heads））。</p>
<p>每一组注意力的权重矩阵都是随机初始化的。经过训练之后，每一组注意力可以看作是把输入的向量映射到一个”子表示空间“。</p>
</li>
</ol>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_qkv.png"
	width="1310"
	height="774"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_qkv_hu_bee1f5273ba0ca4a.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_qkv_hu_2561badbe5db3408.png 1024w"
	loading="lazy"
	
		alt="transformer_attention_heads_qkv"
	
	
		class="gallery-image" 
		data-flex-grow="169"
		data-flex-basis="406px"
	
></p>
<p>在多头注意力机制中，我们为每组注意力维护单独的 WQ, WK, WV 权重矩阵。将输入 X 和每组注意力的WQ, WK, WV 相乘，得到 8 组 Q, K, V 矩阵。</p>
<p>接着，我们把每组 K, Q, V 计算得到每组的 Z 矩阵，就得到 8 个 Z 矩阵。</p>
<p>接下来就有点麻烦了，因为前馈神经网络层接收的是 1 个矩阵，而不是 8 个矩阵。所以我们需要一种方法，把 8 个矩阵整合为一个矩阵。</p>
<p>怎么才能做到呢？我们把矩阵拼接起来，然后和另一个权重矩阵相乘。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_weight_matrix_o.png"
	width="1372"
	height="759"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_weight_matrix_o_hu_8a75e75079b48bf.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_attention_heads_weight_matrix_o_hu_1310815a6ede0e81.png 1024w"
	loading="lazy"
	
		alt="transformer_attention_heads_weight_matrix_o"
	
	
		class="gallery-image" 
		data-flex-grow="180"
		data-flex-basis="433px"
	
></p>
<p>这就是多头注意力的全部内容。</p>
<p>既然我们已经谈到了多头注意力，现在让我们重新回顾之前的翻译例子，看下当我们编码单词it时，不同的 attention heads （注意力头）关注的是什么部分。</p>
<p>当我们编码单词&quot;it&quot;时，其中一个 attention head （注意力头）最关注的是&quot;the animal&quot;，另外一个 attention head 关注的是&quot;tired&quot;。因此在某种意义上，&ldquo;it&quot;在模型中的表示，融合了&quot;animal&quot;和&quot;word&quot;的部分表达。</p>
<p>然而，当我们把所有 attention heads（注意力头） 都在图上画出来时，多头注意力又变得难以解释了。</p>
<img src="transformer_self-attention_visualization_2.png" alt="transformer_self-attention_visualization_2" style="zoom: 80%;" />
<img src="transformer_self-attention_visualization_3.png" alt="transformer_self-attention_visualization_3" style="zoom: 80%;" />
<h2 id="使用位置编码来表示序列的顺序">使用位置编码来表示序列的顺序
</h2><p>到目前为止，我们阐述的模型中缺失了一个东西，那就是表示序列中单词顺序的方法。</p>
<p>为了解决这个问题，Transformer 模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。</p>
<p>这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到 Q/K/V，然后计算点积得到 attention 时，可以提供有意义的信息。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_vectors.png"
	width="1438"
	height="793"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_vectors_hu_193de9b8083cc85c.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_vectors_hu_c015391a3ee56a5c.png 1024w"
	loading="lazy"
	
		alt="transformer_positional_encoding_vectors"
	
	
		class="gallery-image" 
		data-flex-grow="181"
		data-flex-basis="435px"
	
></p>
<p>为了让模型了解单词的顺序，我们添加了带有位置编码的向量&ndash;这些向量的值遵循特定的模式。</p>
<p>那么带有位置编码的向量到底遵循什么模式？</p>
<p>在下图中，每一行表示一个带有位置编码的向量。所以，第一行对应于序列中第一个单词的位置编码向量。每一行都包含 512 个值，每个值的范围在 -1 和 1 之间。我对这些向量进行了涂色可视化，你可以从中看到向量遵循的模式。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_large_example.png"
	width="1238"
	height="907"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_large_example_hu_94bf188211c845db.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_positional_encoding_large_example_hu_617a367a7c4ccb47.png 1024w"
	loading="lazy"
	
		alt="transformer_positional_encoding_large_example"
	
	
		class="gallery-image" 
		data-flex-grow="136"
		data-flex-basis="327px"
	
></p>
<p>这是一个真实的例子，包含了 20 个词，每个词向量的维度是 512。你可以看到，它看起来像从中间一分为二。这是因为左半部分的值是由 sine 函数产生的，而右半部分的值是由 cosine 函数产生的，然后将他们拼接起来，得到每个位置编码向量。</p>
<p>此为生成位置编码的公式，在 Transformer 论文的 3.5 节中有详细说明。
这不是唯一一种生成位置编码的方法。但这种方法的优点是：可以扩展到未知的序列长度。</p>
<p>例如：当我们的模型需要翻译一个句子，而这个句子的长度大于训练集中所有句子的长度，这时，这种位置编码的方法也可以生成一样长的位置编码向量。</p>
<h2 id="残差连接">残差连接
</h2><p>编码器结构中有一个需要注意的细节是：编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization）。</p>
<p>将 Self-Attention 层的层标准化（layer-normalization）和向量都进行可视化，如下所示：</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_2.png"
	width="712"
	height="666"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_2_hu_6ca7359a8dfa2afa.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_2_hu_4ede346610adf1d2.png 1024w"
	loading="lazy"
	
		alt="transformer_resideual_layer_norm_2"
	
	
		class="gallery-image" 
		data-flex-grow="106"
		data-flex-basis="256px"
	
></p>
<p>在解码器的子层里面也有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，如下图所示。</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_3.png"
	width="1415"
	height="804"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_3_hu_bed3f55e2282630d.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/transformer_resideual_layer_norm_3_hu_7387b3fd343b5989.png 1024w"
	loading="lazy"
	
		alt="transformer_resideual_layer_norm_3"
	
	
		class="gallery-image" 
		data-flex-grow="175"
		data-flex-basis="422px"
	
></p>
<h2 id="解码器">解码器
</h2><p>解码器由三层构成。</p>
<p>第一层是Self-Attention Layer层。</p>
<p>第二层是Encoder-Decoder Attention层，这个层能帮助解码器聚焦于输入句子的相关部分（类似于 seq2seq 模型 中的 Attention）。</p>
<p>第三层是Feed Forward Neural Network层。</p>
<p>现在让我们来看下， 编码器和解码器是如何协同工作的。</p>
<p>上面说了，编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。</p>
<p>在完成了编码（encoding）阶段之后，我们开始解码（decoding）阶段。解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词。</p>
<p>接下来会重复这个过程，直到输出一个结束符，Transformer 就完成了所有的输出。</p>
<p>每一步的输出都会在下一个时间步输入到下面的第一个解码器。Decoder 就像 Encoder 那样，从下往上一层一层地输出结果。正对如编码器的输入所做的处理，我们把解码器的输入向量，也加上位置编码向量，来指示每个词的位置。</p>
<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层不太一样：在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。</p>
<p>具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置。</p>
<p>Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，不同之处是：Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于编码器（Encoder）最终的输出。</p>
<h2 id="最后的线性层和-softmax-层">最后的线性层和 Softmax 层
</h2><p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是由 Softmax 层后面的线性层来完成的。</p>
<p>线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量。</p>
<p>现在假设我们的模型有 10000 个英语单词（模型的输出词汇表），这些单词是从训练集中学到的。因此 logits 向量有 10000 个数字，每个数表示一个单词的分数。我们就是这样去理解线性层的输出。</p>
<p>然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。</p>
<h2 id="transformer-的训练过程">Transformer 的训练过程
</h2><p>现在我们已经了解了 Transformer 的前向传播过程，下面讲讲 Transformer 的训练过程，这也是非常有用的知识。</p>
<p>在训练过程中，模型会经过上面讲的所有前向传播的步骤。但是，当我们在一个标注好的数据集上训练这个模型的时候，我们可以对比模型的输出和真实的标签。</p>
<p>为了可视化这个对比，让我们假设输出词汇表只包含 6 个单词（“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;”（“&lt;eos&gt;”表示句子末尾））。</p>
<p>我们模型的输出词汇表，是在训练之前的数据预处理阶段构造的。当我们确定了输出词汇表，我们可以用向量来表示词汇表中的每个单词。这个表示方法也称为 one-hot encoding。例如，我们可以把单词 “am” 用下面的向量来表示：</p>
<p><img src="/p/transformer%E6%8E%A2%E7%A9%B6/one-hot-vocabulary-example.png"
	width="1259"
	height="640"
	srcset="/p/transformer%E6%8E%A2%E7%A9%B6/one-hot-vocabulary-example_hu_8ee80695fb6c0987.png 480w, /p/transformer%E6%8E%A2%E7%A9%B6/one-hot-vocabulary-example_hu_c725b61cc494d3.png 1024w"
	loading="lazy"
	
		alt="one-hot-vocabulary-example"
	
	
		class="gallery-image" 
		data-flex-grow="196"
		data-flex-basis="472px"
	
></p>
<p>介绍了训练过程，我们接着讨论模型的损失函数，这我们在训练时需要优化的目标，通过优化这个目标来得到一个训练好的、非常精确的模型。</p>
<h2 id="损失函数">损失函数
</h2><p>用一个简单的例子来说明训练过程，比如：把“merci”翻译为“thanks”。
这意味着我们希望模型最终输出的概率分布，会指向单词 ”thanks“（在“thanks”这个词的概率最高）。但模型还没训练好，它输出的概率分布可能和我们希望的概率分布相差甚远。</p>
<p>由于模型的参数都是随机初始化的。模型在每个词输出的概率都是随机的。我们可以把这个概率和正确的输出概率做对比，然后使用反向传播来调整模型的权重，使得输出的概率分布更加接近震数输出。</p>
<p>那我们要怎么比较两个概率分布呢？我们可以简单地用一个概率分布减去另一个概率分布。关于更多细节，你可以查看交叉熵(cross-entropy)]和KL 散度(Kullback–Leibler divergence)的相关概念。</p>
<p>但上面的例子是经过简化的，因为我们的句子只有一个单词。在实际中，我们使用的句子不只有一个单词。例如&ndash;输入是：“je suis étudiant” ，输出是：“i am a student”。这意味着，我们的模型需要输出多个概率分布，满足如下条件：</p>
<ol>
<li>每个概率分布都是一个向量，长度是 vocab_size（我们的例子中，向量长度是 6，但实际中更可能是 30000 或者 50000）</li>
<li>第一个概率分布中，最高概率对应的单词是“i”</li>
<li>第二个概率分布中，最高概率对应的单词是“am”</li>
<li>以此类推，直到第 5 个概率分布中，最高概率对应的单词是“&lt;eos&gt;”，表示没有下一个单词了</li>
</ol>
<p>希望经过训练，模型会输出我们希望的正确翻译。当然，如果你要翻译的句子是训练集中的一部分，那输出的结果并不能说明什么。</p>
<p>我们希望的是模型在没见过的句子上也能够准确翻译。需要注意的是：概率分布向量中，每个位置都会有一点概率，即使这个位置不是输出对应的单词&ndash;这是 Softmax 中一个很有用的特性，有助于帮助训练过程。</p>
<p>现在，由于模型每个时间步只产生一个输出，我们可以认为：模型是从概率分布中选择概率最大的词，并且丢弃其他词。这种方法叫做贪婪解码（greedy decoding）。</p>
<p>另一种方法是每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程：假设第一个位置概率最高的两个输出的词是”I“和”a“，这两个词都保留，然后根据第一个词计算第二个位置的词的概率分布，再取出 2 个概率最高的词，对于第二个位置和第三个位置，我们也重复这个过程。</p>
<p>这种方法称为集束搜索(beam search)，在我们的例子中，beam_size 的值是 2（含义是：在所有时间步，我们保留两个最高概率），top_beams 的值也是 2（表示我们最终会返回两个翻译的结果）。beam_size 和 top_beams 都是你可以在实验中尝试的超参数。</p>

</section>


    <footer class="article-footer">
    

    <section class="article-lastmod">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <span>
            最后更新于 Nov 01, 2025 19:46 &#43;0800
        </span>
    </section></footer>




    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/vllm-service/">
        
        

        <div class="article-details">
            <h2 class="article-title">vLLM Service</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/aimodel/">
        
        

        <div class="article-details">
            <h2 class="article-title">aimodel</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">

    <section class="copyright">
        &copy; 
        
            2024 - 
        
        2025 echudet
    </section>
    
    <section class="powerby">
        


        <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>

        <script language="javascript"> 
            var now = new Date();
            function createtime(){
                now.setTime(now.getTime()+250);
                var grt= new Date("2024/11/10 00:00:00"); 
                days = (now - grt ) / 1000 / 60 / 60 / 24;
                dnum = Math.floor(days);
                hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum);
                hnum = Math.floor(hours);
                if(String(hnum).length ==1 ){hnum = "0" + hnum;}
                minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
                mnum = Math.floor(minutes);
                if(String(mnum).length ==1 ){mnum = "0" + mnum;}
                seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
                snum = Math.round(seconds);
                if(String(snum).length ==1 ){snum = "0" + snum;}

                document.getElementById("timeDate").innerHTML = "本站已稳定运行"+dnum+" 天 ";
                document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒<br>"
            }
            setInterval("createtime()",250); 
        </script> 

        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        
    
        
         
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
             
        
        

        共 745148 字 , 39 篇文章<br>使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.29.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>

</footer>



    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>


    </body>
</html>
