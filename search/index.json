[{"content":" 为什么要学习Verilog 因为本人在学校的五个verilog的数逻实验都是借助了魔法海螺(ai)来完成的，临近考试，诚惶诚恐，故补此知识点。（并且之后的龙芯杯也需要使用）\nVerliog知识点 Verilog 数据类型 基本数据类型\nwire reg 定义与用途 wire 用于声明连接不同模块或逻辑单元的信号，代表物理电路中的导线。 适用于组合逻辑和连续赋值（assign 语句）。 reg 用于在过程块（always 或 initial 块）中存储和保持值，类似于变量。 常用于时序逻辑（如寄存器）和组合逻辑中的状态存储。 赋值方式 只能通过连续赋值（assign）或模块实例化中的端口连接来驱动。 只能在过程块中通过阻塞赋值（=）或非阻塞赋值（\u0026lt;=）来驱动。 特性 不存储值，每当驱动信号发生变化时，wire 的值会立即更新。 可以被多个驱动源同时驱动（需要小心处理以避免竞争）。 能够存储值，保持其状态直到被新赋值。 在时序逻辑中用于描述寄存器和状态。 常见易错点\n错误使用赋值方式\n错误：在过程块外对 reg 进行赋值，或在连续赋值中对 reg 进行驱动。\n1 2 3 4 5 6 7 8 reg a; assign a = b \u0026amp; c; // 错误：不能使用 assign 语句驱动 reg 类型 //正确做法 reg a; always @(*) begin a = b \u0026amp; c; // 使用过程块内的赋值 end 错误：在过程块内对 wire 进行赋值。\n1 2 3 4 5 6 7 8 wire a; always @(*) begin a = b \u0026amp; c; // 错误：不能在过程块内赋值 wire 类型 end //正确做法 wire a; assign a = b \u0026amp; c; // 使用 assign 语句 混淆 wire 和 reg 的用途\n错误：将 reg 用于需要被多个模块驱动的信号，或者将 wire 用于需要存储状态的信号。\n1 2 3 4 5 6 7 8 9 10 11 // 错误示例：将需要存储状态的信号声明为 wire wire [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // 错误：wire 类型不能在过程块中赋值 end // 正确做法 reg [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // 使用 reg 类型 end 多重驱动信号\n错误：多个驱动源同时驱动同一个 wire 信号，可能导致竞态条件（race condition）或短路。\n1 2 3 wire a; assign a = b; assign a = c; // 错误：多个驱动源 解决方法：确保每个 wire 信号只有一个驱动源，或使用三态缓冲区控制信号驱动。\n忘记初始化 reg\n错误：在仿真中忘记初始化 reg，可能导致不确定的初始值。\n1 2 3 4 reg [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // count 未初始化，仿真时可能为 x end 解决方法：\n使用复位信号初始化 reg。 在仿真中使用 initial 块赋初值。 1 2 3 4 5 6 7 reg [3:0] count; always @(posedge clk or posedge rst) begin if (rst) count \u0026lt;= 4\u0026#39;b0000; else count \u0026lt;= count + 1; end 误解 reg 的硬件含义\n误区：reg 不一定对应硬件中的寄存器，取决于代码的具体描述。\n1 2 3 4 reg a; always @(*) begin a = b \u0026amp; c; // 组合逻辑中的 reg，实际硬件中不生成寄存器 end 解释：在组合逻辑中使用 reg 只是因为需要在过程块内赋值，并不意味着硬件中会有实际的寄存器。\n向量\n在Verilog中，向量类型用于表示多位信号或数据。与单比特（单一的0或1）信号不同，向量可以表示多个比特的组合，如字节（8位）、字（16位或32位）等。向量使得处理和传输多位数据更加便捷和高效。在Verilog中，向量通过指定范围来声明。范围由高位和低位索引组成，格式如下：\n1 2 3 4 5 6 7 8 9 10 11 wire [高位:低位] 信号名; reg [高位:低位] 信号名; //具体来说 wire [7:0] data_bus; // 8位数据总线，位7到位0 wire [3:0] nibble; // 4位半字节，位3到位0 reg [31:0] address; // 32位地址信号，位31到位0 //Verilog允许指定向量的高位到低位，也可以反向指定 wire [7:0] data_bus; // 位7是最高位，位0是最低位 wire [0:7] data_bus_rev; // 位0是最高位，位7是最低位 在verilog中，还允许对向量进行诸多操作，如连接与拼接、位选取与范围选择、位切片、赋值等，详细见下。\n连接与拼接\nVerilog允许通过连接运算符（{}）将多个信号拼接成一个向量，或将一个向量拆分成多个信号。\n1 2 3 4 5 6 7 8 9 10 11 12 wire [3:0] nibble1, nibble2; wire [7:0] byte; // 拼接 assign byte = {nibble1, nibble2}; // 将nibble1作为高位，nibble2作为低位 // 拆分 assign {nibble1, nibble2} = byte; // 从byte中提取高4位到nibble1，低4位到nibble2 //甚至还可以↓ {4{w}} //这等同于{w,w,w,w} {b,{3{a,b}}} //这等同于{b,a,b,a,b,a,b} 位选取与范围选择\nVerilog允许访问向量中的单个比特或一段范围。\n1 2 3 4 5 6 7 8 9 //单比特选取 wire [7:0] data_bus; wire bit7 = data_bus[7]; // 访问第7位 wire bit0 = data_bus[0]; // 访问第0位 //范围选取 wire [15:0] word; wire [7:0] lower_byte = word[7:0]; // 访问低8位 wire [7:0] upper_byte = word[15:8]; // 访问高8位 位切片（Bit Slicing）\n可以通过动态计算的索引来访问向量的一部分，但需要使用生成语句或任务/函数。\n1 2 3 4 5 6 7 wire [31:0] data; wire [7:0] byte0, byte1, byte2, byte3; assign byte0 = data[7:0]; assign byte1 = data[15:8]; assign byte2 = data[23:16]; assign byte3 = data[31:24]; 向量的赋值\n向量可以通过连续赋值（assign）或过程赋值（always块内）进行赋值。\n1 2 3 4 5 6 7 8 9 //连续赋值 wire [7:0] a, b, c; assign c = a \u0026amp; b; // 位与操作 //过程赋值 reg [3:0] counter; always @(posedge clk) begin counter \u0026lt;= counter + 1; end 缩减运算符 1 2 3 4 5 reg [3:0] B; reg C; C = \u0026amp;B; //相当于C =( (B[0]\u0026amp;B[1]) \u0026amp; B[2] ) \u0026amp; B[3]; 整数，实数，时间寄存器变量\n整数（integer）\n整数类型用关键字 integer 来声明。声明时不用指明位宽，位宽和编译器有关，一般为32 bit。reg 型变量为无符号数，而 integer 型变量为有符号数。例如：\n1 2 3 4 5 6 7 8 9 10 reg [31:0] data1 ; reg [7:0] byte1 [3:0]; //数组变量 integer j; //整型变量，用来辅助生成数字电路 always@* begin for (j=0; j\u0026lt;=3;j=j+1) begin byte1[j] = data1[(j+1)*8-1 : j*8]; //把data1[7:0]…data1[31:24] 依次赋值给 byte1[0][7:0]…byte[3][7:0] end end integer 信号 j 作为辅助信号，将 data1 的数据依次赋值给数组 byte1。综合后实际电路里并没有 j 这个信号，j 只是辅助生成相应的硬件电路。\n实数（real）\n实数用关键字 real 来声明，可用十进制或科学计数法来表示。实数声明不能带有范围，默认值为 0。如果将一个实数赋值给一个整数，则只有实数的整数部分会赋值给整数。例如：\n1 2 3 4 5 6 7 8 9 10 real data1 ; integer temp ; initial begin data1 = 2e3 ; data1 = 3.75 ; end initial begin temp = data1 ; //temp 值的大小为3 end 时间（time）\nVerilog 使用特殊的时间寄存器 time 型变量，对仿真时间进行保存。其宽度一般为 64 bit，通过调用系统函数 $time 获取当前仿真时间。例如：\n1 2 3 4 5 time current_time ; initial begin #100 ; current_time = $time ; //current_time 的大小为 100 end 数组\n在 Verilog 中允许声明 reg, wire, integer, time, real 及其向量类型的数组。数组维数没有限制。线网数组也可以用于连接实例模块的端口。数组中的每个元素都可以作为一个标量或者向量，以同样的方式来使用，形如：\u0026lt;数组名\u0026gt;[\u0026lt;下标\u0026gt;]。对于多维数组来讲，用户需要说明其每一维的索引。例如：\n1 2 3 4 5 6 integer flag [7:0] ; //8个整数组成的数组 reg [3:0] counter [3:0] ; //由4个4bit计数器组成的数组 wire [7:0] addr_bus [3:0] ; //由4个8bit wire型变量组成的数组 wire data_bit[7:0][5:0] ; //声明1bit wire型变量的二维数组 reg [31:0] data_4d[11:0][3:0][3:0][255:0] ; //声明4维的32bit数据变量数组 虽然数组与向量的访问方式在一定程度上类似，但不要将向量和数组混淆。向量是一个单独的元件，位宽为 n；数组由多个元件组成，其中每个元件的位宽为 n 或 1。它们在结构的定义上就有所区别。\n存储器\n存储器变量就是一种寄存器数组，可用来描述 RAM 或 ROM 的行为。例如：\n1 2 3 reg membit[0:255] ; //256bit的1bit存储器 reg [7:0] mem[0:1023] ; //1Kbyte存储器，位宽8bit mem[511] = 8\u0026#39;b0 ; //令第512个8bit的存储单元值为0 参数\n参数用来表示常量，用关键字 parameter 声明，只能赋值一次。例如：\n1 2 3 parameter data_width = 10\u0026#39;d32 ; parameter i=1, j=2, k=3 ; parameter mem_size = data_width * 10 ; 字符串\n字符串保存在 reg 类型的变量中，每个字符占用一个字节（8bit）。因此寄存器变量的宽度应该足够大，以保证不会溢出。字符串不能多行书写，即字符串中不能包含回车符。如果寄存器变量的宽度大于字符串的大小，则使用 0 来填充左边的空余位；如果寄存器变量的宽度小于字符串大小，则会截去字符串左边多余的数据。例如，为存储字符串 \u0026ldquo;hello-verilog\u0026rdquo;, 需要 13*8bit 的存储单元：\n1 2 3 4 reg [0: 13*8-1] str ; initial begin str = \u0026#34;hello-verilog\u0026#34;; end Verilog 操作符分类表 操作符类别 操作符 描述 示例 优先级 结合性 算术操作符 + 加法 a + b 14 左结合 - 减法 a - b 14 左结合 * 乘法 a * b 13 左结合 / 除法 a / b 13 左结合 % 取模（取余） a % b 13 左结合 ** 指数（幂运算） a ** b 12 右结合 - (一元) 取负（改变符号） -a 15 右结合 + (一元) 正号（通常不改变值） +a 15 右结合 关系操作符 \u0026lt; 小于 a \u0026lt; b 10 左结合 \u0026lt;= 小于或等于 a \u0026lt;= b 10 左结合 \u0026gt; 大于 a \u0026gt; b 10 左结合 \u0026gt;= 大于或等于 a \u0026gt;= b 10 左结合 等价操作符 == 相等 a == b 9 左结合 != 不相等 a != b 9 左结合 === 严格相等（包括X和Z） a === b 9 左结合 !== 严格不相等（包括X和Z） a !== b 9 左结合 逻辑操作符 \u0026amp;\u0026amp; 逻辑与 a \u0026amp;\u0026amp; b 5 左结合 || 逻辑或 a || b 5 左结合 ! 逻辑非 !a 15 右结合 按位操作符 \u0026amp; 按位与 a \u0026amp; b 8 左结合 | 按位或 a | b 8 左结合 ^ 按位异或 a ^ b 6 左结合 ~ 按位非 ~a 15 右结合 ~\u0026amp; 按位与非（NAND） ~\u0026amp;a 8 左结合 ~ 按位或非（NOR） ~a 8 左结合 ~^ 或 ^~ 按位异或非（XNOR） ~^a 或 ^~a 6 左结合 归约操作符 \u0026amp; 归约与（所有位进行逻辑与） \u0026amp;a - - | 归约或（所有位进行逻辑或） |a - - ^ 归约异或（所有位进行逻辑异或） ^a - - ~\u0026amp; 归约与非（所有位进行逻辑与后取反） ~\u0026amp;a - - ~ 归约或非（所有位进行逻辑或后取反） ~a - - ~^ 或 ^~ 归约异或非（所有位进行逻辑异或后取反） ~^a 或 ^~a - - 移位操作符 \u0026lt;\u0026lt; 逻辑左移 a \u0026lt;\u0026lt; 1 11 左结合 \u0026gt;\u0026gt; 逻辑右移 a \u0026gt;\u0026gt; 1 11 左结合 \u0026lt;\u0026lt;\u0026lt; 算术左移（通常与逻辑左移相同） a \u0026lt;\u0026lt;\u0026lt; 1 11 左结合 \u0026gt;\u0026gt;\u0026gt; 算术右移（保持符号位） a \u0026gt;\u0026gt;\u0026gt; 1 11 左结合 拼接操作符 {} 拼接多个信号或值为一个向量 {a, b} 3 左结合 {{}} 重复拼接（如 {3{a}} 表示 a 重复3次拼接） {{3{a}}} 3 左结合 条件操作符 ?: 条件（三元）操作符，例如 condition ? expr1 : expr2 sel ? a : b 2 右结合 Verilog 数值表示 Verilog HDL 有下列四种基本的值来表示硬件电路中的电平逻辑：\n0：逻辑 0 或 \u0026ldquo;假\u0026rdquo; 1：逻辑 1 或 \u0026ldquo;真\u0026rdquo; x 或 X：未知 z 或 Z：高阻 x 意味着信号数值的不确定，即在实际电路里，信号可能为 1，也可能为 0。\nz 意味着信号处于高阻状态，常见于信号（input, reg）没有驱动时的逻辑结果。例如一个 pad 的 input 呈现高阻状态时，其逻辑值和上下拉的状态有关系。上拉则逻辑值为 1，下拉则为 0 。\n整数数值表示方法\n数字声明时，合法的基数格式有 4 中，包括：十进制(\u0026rsquo;d 或 \u0026lsquo;D)，十六进制(\u0026lsquo;h 或 \u0026lsquo;H)，二进制（\u0026lsquo;b 或 \u0026lsquo;B），八进制（\u0026lsquo;o 或 \u0026lsquo;O）。数值可指明位宽，也可不指明位宽。\n1 2 3 4\u0026#39;b1011 // 4bit 数值 32\u0026#39;h3022_c0de // 32bit 的数值 //下划线 _ 是为了增强代码的可读性 不指明位宽时，一般直接写数字，默认为十进制表示，例如下面的 3 种写法是等效的：\n1 2 3 counter = \u0026#39;d100 ; //一般会根据编译器自动分频位宽，常见的为32bit* counter = 100 ; counter = 32\u0026#39;h64 ; 负数表示，通常在表示位宽的数字前面加一个减号来表示负数。例如：\n1 2 3 4 5 6 -6\u0026#39;d15 -15 //-15 在 5 位二进制中的形式为 5\u0026#39;b10001, 在 6 位二进制中的形式为 6\u0026#39;b11_0001 //需要注意的是，减号放在基数和数字之间是非法的 4\u0026#39;d-2 //非法说明 实数表示方法\n实数表示方法主要有两种方式\n1 2 3 4 5 6 7 8 9 10 //十进制 30.123 6.0 3.0 0.001 //科学计数法 1.2e4 //大小为12000 1_0001e4 //大小为100010000 1E-3 //大小为0.001 字符串表示方法\n字符串是由双引号包起来的字符队列。字符串不能多行书写，即字符串中不能包含回车符。Verilog 将字符串当做一系列的单字节 ASCII 字符队列。例如，为存储字符串 \u0026ldquo;hello-verilog\u0026rdquo;, 需要 13*8bit 的存储单元。\n1 2 3 4 reg [0: 13*8-1] str ; initial begin str = \u0026#34;hello-verilog\u0026#34;; end Verilog 状态机 有限状态机（Finite-State Machine，FSM），简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。状态机不仅是一种电路的描述工具，而且也是一种思想方法，在电路设计的系统级和 RTL 级有着广泛的应用。\n状态机类型\nVerilog 中状态机主要用于同步时序逻辑的设计，能够在有限个状态之间按一定要求和规律切换时序电路的状态。状态的切换方向不但取决于各个输入值，还取决于当前所在状态。 状态机可分为 2 类：Moore 状态机和 Mealy 状态机。\nMoore 型状态机\nMoore 型状态机的输出只与当前状态有关，与当前输入无关。输出会在一个完整的时钟周期内保持稳定，即使此时输入信号有变化，输出也不会变化。输入对输出的影响要到下一个时钟周期才能反映出来。这也是 Moore 型状态机的一个重要特点：输入与输出是隔离开来的。\nMealy 型状态机\nMealy 型状态机的输出，不仅与当前状态有关，还取决于当前的输入信号。Mealy 型状态机的输出是在输入信号变化以后立刻发生变化，且输入变化可能出现在任何状态的时钟周期内。因此，同种逻辑下，Mealy 型状态机输出对输入的响应会比 Moore 型状态机早一个时钟周期。\n状态机设计：3 段式\n众所周知，许多设计规范和最佳实践都推荐使用三段式状态机。这有助于确保设计的一致性和可靠性，尤其在团队合作和大规模项目中尤为重要。将状态机分为三个独立的部分（状态寄存器、下一个状态逻辑、输出逻辑）使得设计更加模块化。每个部分有明确的职责，便于理解和管理。由于各部分职责分明，修改或扩展某一部分（如添加新的状态或修改输出逻辑）不会影响其他部分。这大大提高了设计的可维护性和可扩展性。并且，通过将状态寄存器与状态转移逻辑分离，避免了时序逻辑和组合逻辑的混杂。这有助于减少时序相关的问题，如竞态条件和冒险（hazards）。\n状态机设计如下：\n(0) 首先，根据状态机的个数确定状态机编码。利用编码给状态寄存器赋值，代码可读性更好。 (1) 状态机第一段，时序逻辑，非阻塞赋值，传递寄存器的状态。 (2) 状态机第二段，组合逻辑，阻塞赋值，根据当前状态和当前输入，确定下一个状态机的状态。 (3) 状态机第三代，时序逻辑，非阻塞赋值，因为是 Mealy 型状态机，根据当前状态和当前输入，确定输出信号。 下面给出一个简单的状态机代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 module fsm_example ( input wire clk, input wire rst_n, input wire input_signal, output reg output_signal ); // 状态编码 using localparam localparam [1:0] STATE_IDLE = 2\u0026#39;b00, STATE_ACTIVE = 2\u0026#39;b01, STATE_DONE = 2\u0026#39;b10; // 状态寄存器 reg [1:0] current_state, next_state; // 状态机第一段，时序逻辑，非阻塞赋值，传递寄存器的状态 always @(posedge clk or negedge rst_n) begin if (!rst_n) current_state \u0026lt;= STATE_IDLE; else current_state \u0026lt;= next_state; end // 状态机第二段，组合逻辑，阻塞赋值，根据当前状态和当前输入，确定下一个状态机的状态 always @(*) begin case (current_state) STATE_IDLE: begin if (input_signal) next_state = STATE_ACTIVE; else next_state = STATE_IDLE; end STATE_ACTIVE: begin next_state = STATE_DONE; end STATE_DONE: begin next_state = STATE_IDLE; end default: begin next_state = STATE_IDLE; end endcase end // 状态机第三代，时序逻辑，非阻塞赋值，因为是 Mealy 型状态机， // 根据当前状态和当前输入，确定输出信号 always @(*) begin case (current_state) STATE_IDLE: begin output_signal = 0; end STATE_ACTIVE: begin output_signal = 1; end STATE_DONE: begin output_signal = 0; end default: begin output_signal = 0; end endcase end endmodule Verilog 竞争与冒险 产生原因\n数字电路中，信号传输与状态变换时都会有一定的延时。\n在组合逻辑电路中，不同路径的输入信号变化传输到同一点门级电路时，在时间上有先有后，这种先后所形成的时间差称为竞争（Competition）。 由于竞争的存在，输出信号需要经过一段时间才能达到期望状态，过渡时间内可能产生瞬间的错误输出，例如尖峰脉冲。这种现象被称为冒险（Hazard）。 竞争不一定有冒险，但冒险一定会有竞争。 避免方法\n在编程时多注意以下几点，也可以避免大多数的竞争与冒险问题。\n1）时序电路建模时，用非阻塞赋值。 2）组合逻辑建模时，用阻塞赋值。 3）在同一个 always 块中建立时序和组合逻辑模型时，用非阻塞赋值。 4）在同一个 always 块中不要既使用阻塞赋值又使用非阻塞赋值。 5）不要在多个 always 块中为同一个变量赋值。 6）避免 latch 产生。 时序电路中非阻塞赋值可以消除竞争冒险。由于无法确定 a 与 b 阻塞赋值的操作顺序，就有可能带来竞争冒险。使用非阻塞赋值时，赋值操作是同时进行的，所以就不会带来竞争冒险。\n1 2 3 4 5 6 7 8 9 10 always @(posedge clk) begin a = b ; b = a ; end //不会带来竞争冒险 always @(posedge clk) begin a \u0026lt;= b ; b \u0026lt;= a ; end 我们想实现 C = A\u0026amp;B, F=C\u0026amp;D 的组合逻辑功能，用非阻塞赋值语句如下。两条赋值语句同时赋值，F \u0026lt;= C \u0026amp; D 中使用的是信号 C 的旧值，所以导致此时的逻辑是错误的，F 的逻辑值不等于 A\u0026amp;B\u0026amp;D。而且，此时要求信号 C 具有存储功能，但不是时钟驱动，所以 C 可能会被综合成锁存器（latch），导致竞争冒险。对代码修改后，F = C \u0026amp; D 的操作一定是在 C = A \u0026amp; B 之后，此时 F 的逻辑值等于 A\u0026amp;B\u0026amp;D，符合设计。\n1 2 3 4 5 6 7 8 9 always @(*) begin C \u0026lt;= A \u0026amp; B ; F \u0026lt;= C \u0026amp; D ; end //fix ↓ always @(*) begin C = A \u0026amp; B ; F = C \u0026amp; D ; end 同一个 always 块中建立时序和组合逻辑模型时，用非阻塞赋值。虽然时序电路中可能涉及组合逻辑，但是如果赋值操作使用非阻塞赋值，仍然会导致如规范 1 中所涉及的类似问题。\n1 2 3 4 5 6 7 8 always @(posedge clk or negedge rst_n) if (!rst_n) begin q \u0026lt;= 1\u0026#39;b0; end else begin q \u0026lt;= a \u0026amp; b; //即便有组合逻辑，也不要写成：q = a \u0026amp; b end end 同一个 always 块中不要既使用阻塞赋值又使用非阻塞赋值。always 涉及的组合逻辑中，既有阻塞赋值又有非阻塞赋值时，会导致意外的结果，例如下面代码描述。此时信号 C 阻塞赋值完毕以后，信号 F 才会被非阻塞赋值，仿真结果可能正确。但如果 F 信号有其他的负载，F 的最新值并不能马上传递出去，数据有效时间还是在下一个触发时刻。此时要求 F 具有存储功能，可能会被综合成 latch，导致竞争冒险。\n1 2 3 4 always @(*) begin C = A \u0026amp; B ; F \u0026lt;= C \u0026amp; D ; end 不要在多个 always 块中为同一个变量赋值。此时信号拥有多驱动端（Multiple Driver），是禁止的。当然，也不允许 assign 语句为同一个变量进行多次连线赋值。\nTestbench 概述 定义与目的\n定义：Testbench 是一个独立的Verilog模块，用于仿真和验证被测模块（Module Under Test，MUT）的功能。 目的：提供输入信号、监控输出信号、验证设计的正确性。 基本特点\n不包含端口（port）。 包含信号声明、被测模块实例化、刺激生成和输出监控等部分。 Testbench 的基本组成部分\ntimescale 指令\n1 2 3 4 `timescale \u0026lt;时间单位\u0026gt; / \u0026lt;时间精度\u0026gt; //\u0026lt;时间单位\u0026gt;：定义延迟和仿真时间的基本单位。 //\u0026lt;时间精度\u0026gt;：定义仿真内部时间的最小步长。 // 对于大多数数字电路设计，1ns / 1ps 或 10ns / 1ps 是常见且高效的选择。 时间单位（Time Unit）\n定义：时间单位指定了Verilog中时间延迟和仿真时间的基本单位。例如，1ns 表示1纳秒。 用途：用于定义延迟（如 #5）、仿真时间的显示格式等。 时间精度（Time Precision）\n定义：时间精度指定了时间单位中最小的时间步长。例如，1ps 表示时间步长为1皮秒。 用途：影响仿真中时间的分辨率和计算精度。 模块声明\n使用 module 关键字声明Testbench模块。 不需要端口列表。 1 module testbench_name; 信号声明\n输入信号：通常声明为 reg 类型，因为它们将在过程块中被驱动。 输出信号：声明为 wire 类型，因为它们由被测模块驱动。 1 2 3 4 reg clk; reg rst_n; reg en; wire [3:0] count; 被测模块实例化\n使用被测模块的名称和实例名（如 uut，代表 Unit Under Test）。 通过端口映射（.port_name(signal_name)）连接Testbench中的信号与被测模块的端口。 1 2 3 4 5 6 module_name uut ( .clk(clk), .rst_n(rst_n), .en(en), .count(count) ); 生成输入信号\n时钟信号生成：通常使用 initial 块与 forever 循环结合 # 延迟来实现时钟信号。 复位信号生成：在 initial 块中设置复位信号的初始状态和变化。 其他输入信号生成：使用 initial 或 always 块控制其他输入信号的变化。 1 2 3 4 5 6 7 8 9 initial begin clk = 0; forever #5 clk = ~clk; // 生成时钟，周期10个时间单位 end initial begin rst_n = 0; #10 rst_n = 1; // 释放复位 end 监控输出信号\n使用系统任务如 $monitor、$display 来观察和记录被测模块的输出信号变化。 $monitor：实时监控指定信号的变化。 $display：在特定时刻输出信号信息。 1 2 3 initial begin $monitor(\u0026#34;Time=%0t | rst_n=%b | en=%b | count=%d\u0026#34;, $time, rst_n, en, count); end 结束仿真\n使用 $finish 任务在特定条件下结束仿真。 可以在 initial 块中通过延迟控制仿真结束的时机。 1 2 3 4 initial begin #100; // 仿真时间 $finish; // 结束仿真 end Testbench 的高级功能\n任务（Tasks）和函数（Functions）\n任务：封装重复使用的操作，如复位信号生成。 函数：封装返回值的操作，适用于计算和逻辑判断。 1 2 3 4 5 6 7 task reset; begin rst_n = 0; #10; rst_n = 1; end endtask 生成波形文件\n使用 $dumpfile 和 $dumpvars 生成波形文件（如 .vcd 文件），用于后续波形查看和分析。\n1 2 3 4 initial begin $dumpfile(\u0026#34;testbench.vcd\u0026#34;); // 指定波形文件名 $dumpvars(0, testbench_name); // 记录所有信号 end 断言（Assertions）\n使用 SystemVerilog 的断言功能自动检查设计的正确性，提高测试效率。\n1 2 3 always @(posedge clk) begin assert (count \u0026lt;= 4\u0026#39;d15) else $fatal(\u0026#34;Count overflowed!\u0026#34;); end Testbench 编写步骤总结\n模块声明：不包含端口，使用 module 和 endmodule 包围整个Testbench。 信号声明 使用 reg 类型声明将被驱动的输入信号。 使用 wire 类型声明被测模块的输出信号。 被测模块实例化：使用模块名称和实例名，连接Testbench中的信号与被测模块的端口。 生成输入信号：使用 initial 块和 always 块生成时钟、复位及其他输入信号。 监控输出信号：使用 $monitor、$display 等任务实时观察被测模块的输出。 结束仿真：使用 $finish 任务在合适的时机结束仿真。 常用的系统任务和函数\n$monitor\n持续监控并输出指定信号的变化。 语法：$monitor(\u0026quot;格式字符串\u0026quot;, 参数列表); $display\n在特定时刻输出信号信息。 语法：$display(\u0026quot;格式字符串\u0026quot;, 参数列表); $finish\n结束仿真。 语法：$finish; $dumpfile 和 $dumpvars\n生成波形文件，用于后续的波形分析。 语法： 1 2 $dumpfile(\u0026#34;filename.vcd\u0026#34;); $dumpvars(level, module_name); $stop\n暂停仿真，进入交互模式（部分仿真工具支持）。 语法：$stop; 语法补充 循环语句\n在Verilog HDL中也存在循环语句。可以综合的循环语句为for语句，并且这个for语句和C语句中的for语句使用形式完全一样。for语句的形式为：for (表达式1；表达式2；表达式3）语句；\n也即:for（循环变量赋初值；循环结束条件；循环变量增值）执行语句；上述的展开表达分别解释了表达形式中的表达式1，表达式2和表达式3的含义。可以看到，确实与C语言中的对应的for循环语句完全是一样的。\n对于循环语句来说，不太容易想象得出综合之后的效果，因为这方面并不直观。这一点确实是这样，对于循环来说，综合器处理起来也并不容易，并且对于不同的综合器来说，不一定是可以综合的。相比于前面讨论的语句（赋值和判断语句），for循环语句描述的功能更加高层和抽象（也包括其它类型的循环语句）。虽然写程序容易，但是转化为硬件的难度会更大。即便转化完成，可能所需要的硬件资源也很多，效率不高。因此，除非是一些对语句进行重复设置的情况，尽量不要使用循环语句，以免对综合器造成困扰。\n除了上述的for语句之外，在Verilog HDL中也有其它三个循环语句，分别为forever语句，repeat语句，while语句。其中forever语句会连续执行语句，主要在仿真中使用，用以生成周期性的波形，例如时钟信号。\n1 2 3 4 5 6 7 8 9 10 11 // repeat语句的使用格式为： repeat(循环次数的表达式) begin //语句或者语句块 end //单个语句不需要begin和end // while语句的使用格式为： while(循环执行的条件表达式) begin //语句或者语句块 end //单个语句不需要begin和end 由于repeat语句和while语句的功能实际上都可以通过for语句表达出来，另外也由于for语句在大部分的EDA工具中都是可以综合的，而repeat和while往往是不可综合的，因此在自己编写代码的时，如果需要生成可以综合的代码，尽量使用for语句来实现循环。但是，正如上面所说的一样，综合出来的效率不一定很高，应该谨慎使用。\nVerilog HDL的设计层次与风格 在Verilog HDL中，可以使用不同的方式来进行电路的设计，有的时候也会给初学者很大的困扰。因为语言有很大的灵活性，对于相同的电路可以有不同的设计方法。这种灵活性可能会对初学者来说不太好掌握。下面就通过一个1位全加器的简单例子来说明在Verilog HDL中的不同的设计层次与设计方法。\n1位的全加器的输入包括1位的低位进位cin，两个一位的输入信号a和b，输出则包括了一个当前位的和sum以及向高位的进位cout。全加器的电路图可以直接从1位全加器的真值表中获得，大部分的数字电路以及组成原理的教科书都有1位全加器的例子，可做参考。从1位全加器的真值表可以获得1位全加器的逻辑表达式：\nCarryOut = (¬ABCarryIn)+(A*¬BCarryIn)+(AB*¬CarryIn)+(ABCarryIn)\n=(BCarryIn)+(ACarryIn)+(A*B)\nSum = (¬A*¬BCarryIn) + (¬AB*¬CarryIn) + (A*¬B*¬CarryIn) +（ABCarryIn)\n当然，上面已经完成了一个一位全加器的设计，并且已经设计出了电路。也就是说，真正的工作已经完成了，这实际上就是传统的硬件设计方法。作为例子，在这里首先把一个已经实现的电路绘制出来，然后再逐步讨论使用Verilog HDL进行不同的描述。目的就是展示出真正的硬件之后，再展示不同的描述，用以说明硬件与对应的描述之间的关联。\n在上述的电路中，使用了三个非门（not），四个3输入的与门（and），三个2输入的与门（and），一个4输入的或门（or），一个3输入的或门（or）。这里的非门，与门和或门都是Verilog HDL中的内置的门电路，可以直接使用。这样，可以将上述电路中的每一条线进行命名，然后就可以直接构造出Verilog HDL的结构描述（调用门元件）。当然，如果是直接是输入线，就不必命名了，使用输入的名称即可，同理对于输出线可以使用输出的名称。\n1 2 3 4 5 6 7 8 9 //1位全加器的门级结构描述 module full_adder1(a,b,cin,sum,cout); input a,b,cin; output sum,cout; wire a_n, b_n, cin_n, sum_p1,sum_p2,sum_p3,sum_p4, cout_p1, cout_p2, cout_p3; not(a_n,a),(b_n,b),(cin_n,cin); and(sum_p1,a_n,b_n,cin),(sum_p2,a_n,b,cin_n),(sum_p3,a,b_n,cin_n),(sum_p4,a,b,cin),(cout_p1,b,cin),(cout_p2,a,cin),(cout_p3,a,b); or(sum,sum_p1,sum_p2,sum_p3,sum_p4),(cout,cout_p1,cout_p2,cout_p3); endmodule 可以看到，上述1位全加器的门级结构描述直接描述了各个门之间是如何连线的。由于是文本文件，因此连线不像图形那么直观。但是，从代码中的各个信号线的命名，以及对应于命名完成的信号线作为门级的调用参数来看，各个门原件的连线就比较清楚了。门级结构描述虽然不是最底层的描述（比如直接用晶体管搭建），但是已经非常接近底层的描述，可以被直接综合出来，使用元件进行直接映射即可。但是，门级结构描述的缺点也很明显，要求用户自行完成门级的设计，直接映射到硬件。这种方法一般用于设计比较简单的电路，或者设计非常高效工作的电路，方便综合器直接进行综合。当然，这里的门级结构描述还有一个作用，即其结构描述的方式可用于通过部分的逻辑电路模块来构造更加大型的电路模块。这个等讨论完其它两种描述之后再回顾一下会更加清楚。\n门级结构描述虽然方便了底层的综合器，但是对于编程来说不太方便，开发者希望能够进行更加高层的设计。一个选择就是将上述的逻辑表达式直接写到程序里面这就是数据流描述的方法。\n1 2 3 4 5 6 7 //1位全加器的数据流描述 module full_adder1(a,b,cin,sum,cout); input a,b,cin; output sum,cout; assign sum = (~a\u0026amp;~b\u0026amp;cin)|(~a\u0026amp;b\u0026amp;~cin)|(a\u0026amp;~b\u0026amp;~cin)|(a\u0026amp;b\u0026amp;cin); assign cout = (b\u0026amp;cin)|(a\u0026amp;cin)|(a\u0026amp;b); endmodule 这里的数据流描述方法，说明的就是在组合逻辑中，输出是如何随着输入数据的变化而变化，使用的是持续赋值语句assign。这些持续赋值语句说明了数据流的变化情况以及它们之间的逻辑关系。可以看到，持续赋值语句的抽象层次要比前面的门级结构描述更加抽象。只要有了逻辑表达式关系，直接翻译为Verilog HDL中的运算符即可，而不用仔细考虑底层的门电路构成。但是，数据流描述的抽象层次还不是很高，因为已经很难从给出的数据流中看到这是一个1位的全加器。实际上，对于复杂的硬件逻辑设计来说，使用行为级描述更为妥当，即直接描述出硬件所需要完成的功能，而不需要考虑这些硬件具体是如何实现的。具体如何实现交给EDA综合软件去做。\n1 2 3 4 5 6 7 8 //1位全加器的行为级描述 module full_adder1(a,b,cin,sum,cout); input wire a, b,cin; output reg sum,cout; always @* begin {cout,sum}=a+b+cin; end endmodule 从行为级描述中完全看不到最终的电路是怎样使用元件以及怎样布线的，但是这个程序非常简单明了。这对于开发人员来说极为方便，直接能够看出是一个全加器，因为这一段程序完整的描述了一个全加器所需要完成的功能。\n上面的结构级描述，数据流级描述以及行为级描述就是Verilog HDL开发过程中可以使用的三种不同层次的对于硬件的描述方法。结构级描述直接描述了硬件电路的结构，最为具体，但是不够抽象。数据流描述更加接近传统的逻辑设计，抽象程度中等。行为级描述只需要抽象描述一个硬件的功能单元完成什么样的功能即可，不需要说明硬件是如何构造的，最为抽象。在实际的设计过程中，这三种方式可以互相混合使用，针对不同的电路可以选择不同的描述方式。\n在设计更加大型的硬件电路的时候，使用结构级描述是必不可少的。在前面的例子中，已经看到是如何通过调用门级的基本逻辑单元来完成全加器的功能。这样的方法在设计更加大型的电路中也是相同的。可以设计一些小型的电路模块，然后通过结构的描述来设计出规模更大的电路。下面就通过设计4位的加法器来说明结构描述在设计大型电路时候的作用。\n通过对其中的信号进行命名，可以通过结构描述的方式来描述上述的电路。实际上，这里需要命名的信号就是进位到前一级的进位，可以分别命名为cin1，cin2，cin3。上述电路的描述如下。\n1 2 3 4 5 6 7 8 9 10 11 //1位全加器的行为级描述 module full_adder4(a,b,cin,sum,cout); input cin; input [3:0]a,b; output [3:0]sum; output cout; full_adder1 a0(a[0],b[0],cin,sum[0],cin1); full_adder1 a1(a[1],b[1],cin1sum[1],cin2); full_adder1 a2(a[2],b[2],cin2sum[2],cin3); full_adder1 a3(a[3],b[3],cin3sum[3],cout); endmodule 可以看到，这里的结构级描述与之前的门级结构描述在形式上是完全一样的。例 17.12门级结构描述调用的是语言内建的元件，但是在这里调用的是开发者自己的模块。同时，在这里也看到了调用的另外一种形式，即可以对每一次调用进行命名，分别命名为a0, a1, a2, a3。\n到现在为止，基本上对于Verilog HDL的编程语言进行了一个基本的介绍。当然，这里介绍的是Verilog HDL中的最基本的内容，希望能够帮助读者对Verilog HDL这样一个语言有一个最基本的了解。在实际进行硬件设计的时候，最基本的方法还是自顶向下的方法，对硬件的总体先分成多个互相独立的模块，然后定义模块之间的连线关系。之后，每一个独立的模块可以进行分别设计，连线关系即是它们之间的接口。最终完成的硬件通过结构描述的方式将设计完成的模块连接在一起。\n编程建议与经验 下面的内容与Verilog语言的语法不直接相关，但是会讨论一些经验以及常见的错误。这里的一些常见错误同学平时也会碰到，建议先阅读理解大概的含义，在具体编程碰到问题的时候也可以返回本节查看与理解。也注意多与同学讨论，少走弯路，减少调试难度。另外，一些具体实验中可能碰到的困难与错误已经列举在前面的实验提示部分。\n计算机组成原理的实验是硬件实验，使用的是硬件描述语言Verilog。做实验之前一定要首先熟悉一下Verilog硬件描述语言的语法以及惯用的方法。惯用的方法可以从例子中学习。当然，编写正确代码也需要经验。Verilog代码是硬件描述语言，这个跟软件的代码语言完全不同，不能想当然从软件的角度去思考。Verilog只是描述硬件模块应该达到的功能，但是没有描述内部的结构，不能直接说明内部是如何的。这里关键的一点是所有的硬件模块都是并行执行的，是信号从输入到输出的传递的过程。除了Verilog的语法之外，下面的一些与语言相关的提示需要大家注意一下，会有一些帮助。\n`default_nettype none\n`default_nettype none 这是建议的做法。在Verilog中，所有没有被定义的标记label都被默认认为是wire类型的。但是，这种默认的行为有的是非常危险的，比如在信号名字上出现拼写错误不会被探测出来。因此，建议在所有的源文件的开始加上这一句，取消默认行为。\n锁相环电路\nPLL是FPGA上专用的时钟生成模块，内部是模拟电路。在启动时需要一段时间才能进入稳定状态，所以有个locked信号输出，表示它稳定了。具体的可以参考前面关于锁相环电路的讨论。\n调时序：\n调时序：不同模块之间，由于寄存器的关系，会有相位差，然后需要增加几个空的状态机节拍。哪个path太长了就加寄存器打一拍。\n阻塞赋值语句和非阻塞赋值语句：\n关于=和\u0026lt;=：一般来说，组合逻辑用=，时序逻辑用\u0026lt;=。但是实际上wire和reg仅仅是语法层面的东西，assign的左值必须是wire，always里的左值必须是reg，否则综合就会报错。最终是否综合成触发器，是根据有没有时钟信号决定的。综合器怎么知道哪个是时钟信号呢？通过posedge的描述方法可以知道对应的模块里面需要响应正边沿（posedge）或者是负边沿（negedge），从而会综合出触发器。\n在Verilog的语法上，阻塞的过程赋值语句=，非阻塞的过程赋值语句\u0026lt;=。在always过程里面的赋值语句被称为是过程赋值语句，一般用以对reg类型的变量进行赋值。过程赋值语句分为两种类型，一个是非阻塞赋值语句（\u0026lt;=），一个是阻塞赋值语句（=）。它们之间的区别是：\n1）非阻塞non-blocking赋值语句（\u0026lt;=）在赋值语句出现的地方不是立即发生的，而是等到整个过程块结束的时候才发生。由于不是立即发生的，在过程内的描述中，仿佛这条语句不存在一样，因此被称为是非阻塞的。只是在过程的最后会执行所有的非阻塞赋值语句，在这个执行的过程中，所有的右值会维持原来的值不变。\n2）阻塞blocking赋值语句（=）在赋值语句出现的地方就立即完成赋值操作，左值立刻发生变化。一个块语句中存在多条阻塞赋值语句的话，这些阻塞赋值语句会按照先后顺序关系，一条一条的执行，前面的赋值语句没有执行完，后面的赋值语句不会执行。\n程序的可读性：\n写Verilog的代码和写软件应用程序的代码一样，有一件事情需要特别注意，就是提高程序的可读性，增加程序的可维护性。在选择信号名称的时候需要按照名称选择的惯例，有一些名称是常用的，按照其惯常的用法就可以了，不要改变其名字的用法。下面是一些名字使用的惯例，同学们应当遵守来提高自己程序的可读性。\n_i, _o，分别代表一个模块的输入信号和输出信号。\nn或者_n为后缀，表明这个信号是0使能。\nclk，clock时钟信号，后面或者前面接上频率，可以显示时钟信号的频率。\nrst，reset重置信号，使得信号可以重置，一般在重置响应中写入状态机的初值。\nwe，write enable信号，对应于模块的写入使能。\noe，output entable信号，对应于模块的读取使能。\nce，chip enable，对应于模块的总体使能信号。上述的信号几乎在所有的模块中都会有。（注意信号是正向的还是反向的，即1使能还是0使能。在说明书中，0使能会在信号名称的上面带有横线。）\nselect，sel，这个信号一般用于对芯片的选择。\n其它的信号也会有惯例使用的情况，大家在平时的时候可以多注意学习。\n代码检查工具：\n下面的网址中有一些Verilog的工具可以供大家参考。\nhttps://www.veripool.org/ verilator \u0026ndash;lint-only -Wall [source_files.v]\u0026hellip; 可以帮助做一些检查。\n一些特殊的语法点：\nram_address = pc[2+:21];\n含义就是从第2位开始的21位，就是2:22，就是把最后两位去掉。\n或者写成pc[22:2]也是一样的。\ncase语句可能出现错误的情况：\n在使用case的时候，一定要注意把所有的信号在所有的情况下写全了，要不然有一些信号就没有定义，会造成错误。或者可以灵活使用赋值语句，=，在过程最前面的时候先进行赋值。\n下面的case代码块是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 always @(*) begin ram_we_n = 1\u0026#39;b1; ram_oe_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end endcase end 下面的case语句代码块也是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 always @(*) begin case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_we_n = 1\u0026#39;b1; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end default: begin ram_oe_n = 1\u0026#39;b1; ram_we_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; end endcase end 但是下面的case语句代码块是错误的，为什么？（所有情况没有考虑完善）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 always @(*) begin case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end default: begin ram_oe_n = 1\u0026#39;b1; ram_we_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; end endcase end always过程语句的敏感信号\nVerilog规定，always@()中的是指该always块内的所有输入信号的变化为敏感列表，也就是仿真时只有当always@(*)块内的输入信号产生变化，该块内描述的信号才会产生变化。\nWarning: empty statement in seqential block\n如果两个分号放在一起的话\u0026quot;;;\u0026quot;\u0026quot;，就会出现这个警告。删掉一个分号即可。这里一个容易出现的错误是在信号常数定义define的时候，不小心在信号后面跟了一个分号，例如define OP_ADD 4\u0026rsquo;h9; 这里多了一个分号。在模块代码里面直接使用的时候就会出现上面的情况。\nVerilog的实战引入 为了解verilog的语法，此处通过实战引入\n示例程序：4位二进制计数器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // 模块声明，定义模块名和参数 module counter #( parameter WIDTH = 4 // 参数定义，计数器的位宽 )( input wire clk, // 输入信号：时钟 input wire rst_n, // 输入信号：同步复位（低有效） input wire en, // 输入信号：使能 output reg [WIDTH-1:0] count // 输出信号：计数值 ); // 内部信号声明 wire rst = ~rst_n; // 使用wire类型定义内部复位信号 // 计数逻辑 always @(posedge clk or posedge rst) begin if (rst) begin count \u0026lt;= 0; // 复位时计数器清零 end else if (en) begin count \u0026lt;= count + 1; // 使能时计数器加一 end end // 初始块，用于仿真时初始化 initial begin count = 0; end endmodule 语法规则详解\n模块声明 (module):\nmodule counter: 定义一个名为 counter 的模块。#(parameter WIDTH = 4): 定义参数 WIDTH，默认值为4，用于设置计数器的位宽。这使得模块更具通用性。\n端口声明:\ninput wire clk: 定义输入端口 clk，类型为 wire，表示时钟信号。 input wire rst_n: 定义输入端口 rst_n，类型为 wire，表示同步复位信号，低有效。 input wire en: 定义输入端口 en，类型为 wire，表示计数使能信号。 output reg [WIDTH-1:0] count: 定义输出端口 count，类型为 reg，位宽为 WIDTH，用于存储计数值。 内部信号 (wire): wire rst = ~rst_n;: 定义内部信号 rst，通过取反 rst_n 得到高有效的复位信号。\n行为描述 (always 块):\nalways @(posedge clk or posedge rst): 定义一个 always 块，敏感于 clk 的上升沿和 rst 的上升沿。 在always块内部： if (rst): 如果复位信号有效，则将 count 清零。 else if (en): 否则，如果使能信号有效，则将 count 加一。 使用非阻塞赋值 \u0026lt;= 来描述时序逻辑，确保在时钟边沿正确更新信号。 初始块 (initial 块):initial begin count = 0; end: 在仿真开始时，将 count 初始化为0。这在实际硬件中通常由复位信号处理，但在仿真中有助于明确初始状态。 initial过程语句不带出发条件且仅执行一次。initial语句通常用于仿真模块中对激励向量的描述，或者用于给寄存器变量赋初值。它是面向模拟仿真的过程语句，通常不能被逻辑综合工具支持。\n数据类型:\nwire: 用于连续赋值，表示信号之间的连接。 reg: 用于在 always 块中赋值，表示需要在时序逻辑中存储值的信号。 4位二进制计数器testbench 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // counter_tb.v module counter_tb; // 输入信号 reg clk; reg rst_n; reg en; // 输出信号 wire [3:0] count; // 实例化被测模块 counter uut ( .clk(clk), .rst_n(rst_n), .en(en), .count(count) ); // 生成波形文件 initial begin $dumpfile(\u0026#34;counter_tb.vcd\u0026#34;); // 指定波形文件名 $dumpvars(0, counter_tb); // 记录所有信号 end // 生成时钟信号，周期10个时间单位 initial begin clk = 0; forever #5 clk = ~clk; // 每5个时间单位反转一次 end // 生成复位和使能信号的任务 task reset; begin rst_n = 0; en = 0; #10; rst_n = 1; #10; end endtask task enable_count; begin en = 1; #50; en = 0; end endtask // Testbench 主过程 initial begin // 初始化信号 rst_n = 0; en = 0; // 调用复位任务 reset; // 启动计数 enable_count; // 再次复位 reset; // 结束仿真 #20; $finish; end // 监控信号变化 initial begin $monitor(\u0026#34;Time=%0t | rst_n=%b | en=%b | count=%d\u0026#34;, $time, rst_n, en, count); end endmodule 示例程序：交通信号灯控制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 // 顶层模块声明 module traffic_light_controller ( input wire clk, // 时钟信号 input wire rst_n, // 复位信号，低有效 output reg red, // 红灯输出 output reg yellow, // 黄灯输出 output reg green // 绿灯输出 ); // 状态定义 using localparam localparam [1:0] RED_STATE = 2\u0026#39;b00, GREEN_STATE = 2\u0026#39;b01, YELLOW_STATE = 2\u0026#39;b10; // 当前状态和下一个状态 reg [1:0] current_state, next_state; // 连续赋值使用 assign wire rst = ~rst_n; // 状态转移逻辑 always @(posedge clk or posedge rst) begin if (rst) begin current_state \u0026lt;= RED_STATE; end else begin current_state \u0026lt;= next_state; end end // 下一个状态的组合逻辑 always @(*) begin case (current_state) RED_STATE: begin next_state = GREEN_STATE; end GREEN_STATE: begin next_state = YELLOW_STATE; end YELLOW_STATE: begin next_state = RED_STATE; end default: begin next_state = RED_STATE; end endcase end // 输出逻辑 always @(*) begin // 默认所有灯关闭 red = 0; yellow = 0; green = 0; case (current_state) RED_STATE: begin red = 1; end GREEN_STATE: begin green = 1; end YELLOW_STATE: begin yellow = 1; end endcase end endmodule 语法规则详解\nlocalparam:\n使用 localparam 定义本地参数，用于状态编码，避免在模块外部被修改。例如：\n1 2 3 4 localparam [1:0] RED_STATE = 2\u0026#39;b00, GREEN_STATE = 2\u0026#39;b01, YELLOW_STATE = 2\u0026#39;b10; assign 语句:\n用于连续赋值，适用于组合逻辑。在本例中，将rst定义为rst_n的反转：\n1 wire rst = ~rst_n; always @(\\*) 块:\nalways @(*) 表示组合逻辑块，自动包含所有右边信号作为敏感信号。用于定义下一个状态和输出逻辑：\n1 2 3 always @(*) begin // 组合逻辑内容 end case 语句:\n用于多分支条件选择，类似于编程语言中的 switch 语句。在状态转移和输出逻辑中使用：\n1 2 3 4 5 6 7 8 9 10 case (current_state) RED_STATE: begin next_state = GREEN_STATE; end // 其他状态 default:begin //default分支:在case语句中使用default处理未定义的情况 //.... end endcase 组合逻辑与时序逻辑的分离:\n使用不同的 always 块分别处理时序逻辑（状态转移）和组合逻辑（下一个状态及输出）。\n信号驱动类型:\nreg 类型用于在 always 块中赋值的信号，如 current_state 和输出信号 red、yellow、green。wire 类型用于连续赋值的信号，如内部复位信号 rst。\n示例程序：UART的收发 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 `timescale 1ns / 1ps //该模块实现了一个UART（通用异步收发传输器）接收器，用于接收串行数据并将其转换为并行数据。 //主要功能包括检测起始位、接收数据位、检测停止位，并在接收完成后输出有效信号和接收到的数据。 module uart_recv( input clk, input rst, input din, output reg valid, output reg [7:0] data ); localparam IDLE = 2\u0026#39;b00; localparam START = 2\u0026#39;b01; localparam DATA = 2\u0026#39;b10; localparam STOP = 2\u0026#39;b11; reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; reg din_r1, din_r2; wire rx_data = din_r2; always @(posedge clk or posedge rst) begin if(rst) begin din_r1 \u0026lt;= 1\u0026#39;b1; din_r2 \u0026lt;= 1\u0026#39;b1; end else begin din_r1 \u0026lt;= din; din_r2 \u0026lt;= din_r1; end end assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if(rst) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(current_state == IDLE) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(baud_counter == 14\u0026#39;d0) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else baud_counter \u0026lt;= baud_counter - 14\u0026#39;d1; end always @(posedge clk or posedge rst) begin if(rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end always @(*) begin case(current_state) IDLE: begin if(!rx_data) next_state = START; else next_state = IDLE; end START: begin if(baud_tick) next_state = (!rx_data) ? DATA : IDLE; else next_state = START; end DATA: begin if(baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if(baud_tick) // Sample stop bit next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end always @(posedge clk or posedge rst) begin if(rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(current_state != DATA) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; end always @(posedge clk or posedge rst) begin if(rst) data_reg \u0026lt;= 8\u0026#39;d0; else if(current_state == DATA \u0026amp;\u0026amp; baud_tick) data_reg \u0026lt;= {rx_data, data_reg[7:1]}; end always @(posedge clk or posedge rst) begin if(rst) begin valid \u0026lt;= 1\u0026#39;b0; data \u0026lt;= 8\u0026#39;d0; end else if(current_state == STOP \u0026amp;\u0026amp; baud_tick) begin valid \u0026lt;= 1\u0026#39;b1; data \u0026lt;= data_reg; end else begin valid \u0026lt;= 1\u0026#39;b0; end end endmodule //该模块实现了一个UART发送器，用于将并行数据转换为串行数据进行发送。 //主要功能包括发送起始位、数据位和停止位，并在发送完成后返回到空闲状态。 module uart_send( input clk, input rst, input valid, input [7:0] data, output reg dout ); localparam IDLE = 2\u0026#39;b00; localparam START = 2\u0026#39;b01; localparam DATA = 2\u0026#39;b10; localparam STOP = 2\u0026#39;b11; reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if (rst) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else if (current_state != IDLE) baud_counter \u0026lt;= (baud_counter == 14\u0026#39;d0) ? BAUD_COUNT_MAX - 14\u0026#39;d1 : baud_counter - 14\u0026#39;d1; else baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; end always @(posedge clk or posedge rst) begin if (rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end always @(posedge clk or posedge rst) begin if (rst) data_reg \u0026lt;= 8\u0026#39;d0; else if (valid \u0026amp;\u0026amp; current_state == IDLE) data_reg \u0026lt;= data; end always @(posedge clk or posedge rst) begin if (rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if (current_state == DATA \u0026amp;\u0026amp; baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; else if (current_state == IDLE) bit_cnt \u0026lt;= 3\u0026#39;d0; end always @(*) begin case (current_state) IDLE: begin if (valid) next_state = START; else next_state = IDLE; end START: begin if (baud_tick) next_state = DATA; else next_state = START; end DATA: begin if (baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if (baud_tick) next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end always @(posedge clk or posedge rst) begin if (rst) dout \u0026lt;= 1\u0026#39;b1; else begin case (current_state) IDLE: dout \u0026lt;= 1\u0026#39;b1; START: dout \u0026lt;= 1\u0026#39;b0; DATA: dout \u0026lt;= data_reg[bit_cnt]; STOP: dout \u0026lt;= 1\u0026#39;b1; default: dout \u0026lt;= 1\u0026#39;b1; endcase end end endmodule 设计思想：两者均采用有限状态机（FSM）的设计方法，定义了四个状态：IDLE（空闲）、START（起始）、DATA（数据）、STOP（停止）。这种设计有助于清晰地划分不同的操作阶段，便于管理和扩展。\n时钟生成与计数器：\nbaud_counter：用于生成波特率时钟脉冲。BAUD_COUNT_MAX 定义了一个波特周期的计数值（10416），对应于特定的波特率（如9600波特，假设时钟频率为100MHz）。 baud_tick：当 baud_counter 计数到0时产生一个波特率脉冲，用于状态转换和数据采样。 复位逻辑：在复位信号有效时，将状态机置于 IDLE 状态，并重置相关计数器和寄存器。\nuart_recv 模块详细分析\n信号声明\n1 2 3 4 5 6 7 8 9 10 11 12 reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; reg din_r1, din_r2; wire rx_data = din_r2; 状态相关信号：\ncurrent_state：当前状态。 next_state：下一个状态。 波特率计数器：\nbaud_counter：用于生成波特率脉冲。 BAUD_COUNT_MAX：波特率计数的最大值（10416）。 baud_tick：当 baud_counter 为0时产生脉冲。 数据相关信号：\nbit_cnt：数据位计数器，记录当前接收的数据位。 data_reg：临时存储接收的数据位。 data：输出的并行数据。 输入信号去抖动：\ndin_r1, din_r2：用于同步和去抖动串行输入信号 din。 rx_data：同步后的输入信号。 输入信号同步\n1 2 3 4 5 6 7 8 9 always @(posedge clk or posedge rst) begin if(rst) begin din_r1 \u0026lt;= 1\u0026#39;b1; din_r2 \u0026lt;= 1\u0026#39;b1; end else begin din_r1 \u0026lt;= din; din_r2 \u0026lt;= din_r1; end end 功能：将异步的串行输入 din 同步到本地时钟域，减少亚稳态问题。 初始状态：在复位时，将 din_r1 和 din_r2 设为1（空闲状态，UART线默认为高电平）。 波特率计数器\n1 2 3 4 5 6 7 8 9 10 11 12 assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if(rst) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(current_state == IDLE) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(baud_counter == 14\u0026#39;d0) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else baud_counter \u0026lt;= baud_counter - 14\u0026#39;d1; end 当在 IDLE 状态时，计数器初始化为 BAUD_COUNT_MAX / 2 - 1，以便在检测到起始位后，等待半个波特周期后开始采样，确保采样在中间位置。 在其他状态，计数器在每个波特周期递减，直到达到0时产生 baud_tick，并重新加载计数值。 状态寄存器更新\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if(rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end 功能：在每个时钟上升沿或复位信号有效时，更新当前状态。 下一个状态逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 always @(*) begin case(current_state) IDLE: begin if(!rx_data) next_state = START; else next_state = IDLE; end START: begin if(baud_tick) next_state = (!rx_data) ? DATA : IDLE; else next_state = START; end DATA: begin if(baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if(baud_tick) // Sample stop bit next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end IDLE： 等待检测到起始位（!rx_data）。 检测到起始位后，进入 START 状态。 START： 等待一个波特周期（baud_tick），确认起始位的稳定性。 如果仍然检测到低电平，进入 DATA 状态；否则，返回 IDLE。 DATA： 每个波特周期采样一个数据位。 计数器达到7（接收完8位数据）后，进入 STOP 状态；否则，继续 DATA 状态。 STOP： 等待一个波特周期采样停止位。 采样完成后，返回 IDLE 状态。 default： 遇到未定义状态时，返回 IDLE。 数据位计数器\n1 2 3 4 5 6 7 8 always @(posedge clk or posedge rst) begin if(rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(current_state != DATA) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; end 在 DATA 状态下，每个波特周期递增 bit_cnt。 在其他状态（IDLE、START、STOP）复位计数器。 数据寄存器\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if(rst) data_reg \u0026lt;= 8\u0026#39;d0; else if(current_state == DATA \u0026amp;\u0026amp; baud_tick) data_reg \u0026lt;= {rx_data, data_reg[7:1]}; end 在 DATA 状态下，每个波特周期将接收到的位 rx_data 右移并存入 data_reg。 实现串行到并行的数据转换。 输出信号生成\n1 2 3 4 5 6 7 8 9 10 11 always @(posedge clk or posedge rst) begin if(rst) begin valid \u0026lt;= 1\u0026#39;b0; data \u0026lt;= 8\u0026#39;d0; end else if(current_state == STOP \u0026amp;\u0026amp; baud_tick) begin valid \u0026lt;= 1\u0026#39;b1; data \u0026lt;= data_reg; end else begin valid \u0026lt;= 1\u0026#39;b0; end end 在 STOP 状态且 baud_tick 时，表示数据接收完成，设置 valid 为高，并将接收到的数据 data_reg 赋值给输出 data。 其他时间，valid 保持低。 uart_recv 模块通过四状态的有限状态机，实现了UART接收的基本功能，包括检测起始位、采样数据位、验证停止位，并在接收完成后输出有效信号和接收到的数据。关键设计点包括输入信号同步、波特率生成、状态机设计和数据采样等。\nuart_send 模块详细分析\n信号声明\n1 2 3 4 5 6 7 8 9 reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; 状态相关信号： current_state：当前状态。 next_state：下一个状态。 波特率计数器： baud_counter：用于生成波特率脉冲。 BAUD_COUNT_MAX：波特率计数的最大值（10416）。 baud_tick：当 baud_counter 为0时产生脉冲。 数据相关信号： bit_cnt：数据位计数器，记录当前发送的数据位。 data_reg：临时存储待发送的数据位。 dout：串行数据输出。 波特率计数器\n1 2 3 4 5 6 7 8 9 10 assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if (rst) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else if (current_state != IDLE) baud_counter \u0026lt;= (baud_counter == 14\u0026#39;d0) ? BAUD_COUNT_MAX - 14\u0026#39;d1 : baud_counter - 14\u0026#39;d1; else baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; end 在非 IDLE 状态下，计数器递减，直到达到0时产生 baud_tick，并重新加载计数值。 在 IDLE 状态，计数器保持在 BAUD_COUNT_MAX - 1，等待发送触发。 状态寄存器更新\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if (rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end 功能：在每个时钟上升沿或复位信号有效时，更新当前状态。 数据寄存器\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if (rst) data_reg \u0026lt;= 8\u0026#39;d0; else if (valid \u0026amp;\u0026amp; current_state == IDLE) data_reg \u0026lt;= data; end 在 IDLE 状态且 valid 为高时，将输入数据 data 载入 data_reg，准备发送。 数据位计数器\n1 2 3 4 5 6 7 8 always @(posedge clk or posedge rst) begin if (rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if (current_state == DATA \u0026amp;\u0026amp; baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; else if (current_state == IDLE) bit_cnt \u0026lt;= 3\u0026#39;d0; end 在 DATA 状态下，每个波特周期递增 bit_cnt，记录当前发送的数据位。 在 IDLE 状态复位计数器。 下一个状态逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 always @(*) begin case (current_state) IDLE: begin if (valid) next_state = START; else next_state = IDLE; end START: begin if (baud_tick) next_state = DATA; else next_state = START; end DATA: begin if (baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if (baud_tick) next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end IDLE： 等待发送有效信号（valid）。 接收到 valid 后，进入 START 状态。 START： 等待一个波特周期（baud_tick），然后进入 DATA 状态。 DATA： 在每个波特周期发送一个数据位。 发送完第8位数据后，进入 STOP 状态。 STOP： 等待一个波特周期采样停止位，然后返回 IDLE 状态。 default： 遇到未定义状态时，返回 IDLE。 串行数据输出逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 always @(posedge clk or posedge rst) begin if (rst) dout \u0026lt;= 1\u0026#39;b1; else begin case (current_state) IDLE: dout \u0026lt;= 1\u0026#39;b1; START: dout \u0026lt;= 1\u0026#39;b0; DATA: dout \u0026lt;= data_reg[bit_cnt]; STOP: dout \u0026lt;= 1\u0026#39;b1; default: dout \u0026lt;= 1\u0026#39;b1; endcase end end IDLE：dout 保持高电平（UART空闲状态）。 START：发送低电平起始位。 DATA：按位发送数据，按照 bit_cnt 从 data_reg 中取出对应位。 STOP：发送高电平停止位。 default：默认保持高电平。 uart_send 模块通过四状态的有限状态机，实现了UART发送的基本功能，包括发送起始位、数据位和停止位。关键设计点包括数据寄存器加载、波特率生成、状态机设计和数据位发送等。\n","date":"2024-11-20T00:00:00Z","image":"https://echudet.github.io/p/intro-verilog/bg_hu8738848294577397739.png","permalink":"https://echudet.github.io/p/intro-verilog/","title":"Intro Verilog"},{"content":"Lecture 23 RCU 课前预习 论文速览 在Introduction (简介)部分，介绍了Read-Copy Update (RCU)作为Linux内核中一种高性能的同步机制。它支持并发的读写操作，且自引入以来，RCU的使用不断扩大，成为Linux内核中的关键组件。文章回顾了RCU的发展背景，并说明了其高效性能在内核中带来的巨大优势。\n在RCU Requirements (RCU要求)部分提到，RCU在Linux内核中的实现需要满足三个关键要求：支持更新时的并发读取；低计算和存储开销；确定的操作完成时间，尤其是在实时响应和系统工程方面具有重要意义。\n在RCU Design (RCU设计)部分，详细介绍了RCU的设计，包括核心的两个基本操作：\nRCU读侧临界区：用来保证线程可以安全读取数据结构。 RCU同步：用来确保所有并发的读操作完成后再进行更新操作。通过这些机制，RCU允许多线程并发读取数据，同时其他线程可以更新数据。 在Using RCU (RCU的使用)部分，详细阐述了RCU在Linux内核中的多种使用模式：\n4.1 Wait for Completion (等待操作完成)：RCU用于等待预先存在的操作完成，以确保安全地移除数据。 4.2 Reference Counting (引用计数)：RCU作为引用计数的替代方案，提供了更高效的内存释放机制。 4.3 Type Safe Memory (类型安全内存)：确保在多线程操作中，被释放的内存不会被错误地重新分配。 4.4 Publish-Subscribe (发布-订阅模式)：提供一种安全发布数据的方式，确保并发读取操作安全。 4.5 Read-Write Lock Alternative (读写锁的替代方案)：RCU作为读写锁的高效替代方案，在并发读取场景下提供了更好的性能。 在Algorithmic Transformations (算法转换)部分，讨论了为了适应RCU而需要进行的几种常见算法转换：\n5.1 Impose Level of Indirection (引入间接层次)：通过间接访问数据结构来保证数据的一致性。 5.2 Mark Obsolete Objects (标记过时对象)：对数据对象进行标记，防止读取到已过时的数据。 5.3 Retry Readers (读者重试)：当检测到数据已被更新时，读线程可以选择重试操作。 在RCU Usage Statistics (RCU使用统计)部分，提供了RCU在不同Linux子系统中的使用统计数据，展示了自RCU引入以来的使用增长趋势，尤其是在网络栈、System V IPC以及其他内核子系统中的广泛应用。\n在Related Work (相关工作)部分，作者回顾了与RCU相关的其他并发控制机制，包括类似RCU的机制和不同的内存回收技术。这些机制在不同操作系统中被广泛使用。\n在Conclusions (结论)部分，总结了RCU在Linux内核中作为一种关键同步机制的成功，讨论了RCU的多种应用模式和转换方法，并展望了未来在内核中更多RCU应用的可能性。\nRCU的工作原理 RCU在Linux内核中的设计与工作原理，核心在于两个基本操作：\nRCU读侧临界区：使用rcu_read_lock和rcu_read_unlock实现。进入读侧临界区时，线程调用rcu_read_lock以防止被抢占，确保其对共享数据的读取不会与其他更新操作冲突。当线程退出临界区时调用rcu_read_unlock。 RCU同步：用 synchronize_rcu实现。这个操作确保在调用时所有正在执行的读侧临界区已结束，但不阻止新的读操作。该操作常用于等待之前的读操作完成，从而安全地进行数据更新或释放内存。 假设我们需要从目录缓存中删除一个dentry结构体（文件目录缓存项）。删除过程包含以下步骤：\n一个线程在读取dentry时会进入RCU读侧临界区，使用rcu_read_lock保护数据读取。 当需要删除dentry时，系统首先将其从目录缓存中移除（但不立即释放内存），然后调用synchronize_rcu等待当前所有对该dentry的读取操作完成。 在synchronize_rcu返回后，系统可以安全地释放这个dentry的内存，因为所有之前的读操作都已完成。 这种方法通过允许读者线程在临界区内访问数据，而更新线程可以异步等待所有读操作完成，从而避免锁定导致的性能损失和死锁。\n这种设计实现了RCU对高性能、并发访问和低开销的要求。在Linux内核中，这种机制广泛应用于诸如网络栈和文件系统等高并发场景。\nsynchronize_rcu 在Linux内核中，synchronize_rcu是一个重要的同步原语，用于确保所有现存的RCU读操作完成之后才继续执行接下来的更新或内存释放操作。下面我将详细解释synchronize_rcu的工作机制、使用场景及其背后的工作原理，并通过示例来说明其实际应用。\n在RCU (Read-Copy-Update) 中，读者线程可以无锁地访问共享数据，而写者线程在更新数据时则需要等待所有并发的读者线程完成当前的读取操作，以避免数据不一致或内存被过早释放。\n当写者线程想要修改或删除某个共享数据结构时，它不能直接对数据结构进行操作，因为可能有其他线程正在读取这个数据结构。为了避免这种情况，写者线程需要调用synchronize_rcu，该函数确保在它返回之前，所有使用过旧版本数据的读者线程都已经完成它们的临界区。\nsynchronize_rcu的基本工作流程是等待所有处理器（CPU）执行至少一次上下文切换。每个RCU的读侧临界区通过rcu_read_lock和rcu_read_unlock进行标记，当rcu_read_lock被调用时，它会禁止内核调度器抢占线程。这样，在临界区内，读者线程可以安全地读取数据结构，而不必担心写操作导致的并发问题。\nsynchronize_rcu不会阻止新的RCU读侧临界区的进入，而是等待所有当前活跃的RCU读侧临界区结束。一旦所有读者线程退出临界区，synchronize_rcu函数就会返回，此时可以保证之前版本的数据不会再被读取，从而可以安全地进行数据更新或内存释放。\n简化版的实现示例：\n1 2 3 4 void synchronize_rcu(void) { for_each_cpu(int cpu) run_on(cpu); } 这个伪代码的基本含义是让调用线程在每个CPU上运行一次，以确保所有CPU上的上下文切换已完成。这种设计避免了在每个读者线程之间进行显式的通信，极大地简化了同步机制的复杂性和性能开销。\nsynchronize_rcu主要用于以下几种典型场景：\n等待数据更新安全完成：当写者线程想要删除或更新一个数据结构时，它会首先从共享数据结构中移除旧数据（但不立即释放内存），并调用synchronize_rcu确保所有并发的读者线程完成对该数据的读取。只有在synchronize_rcu返回之后，旧数据的内存才可以被安全地释放。 实现类型安全的内存管理：在多线程环境下，写者线程在释放旧对象的内存时，需要确保不会有线程继续访问该对象。通过synchronize_rcu，写者线程可以等待所有的读操作结束，保证该内存块不再被引用，然后安全地释放内存。 引用计数替代方案：传统的引用计数需要在每次引用时都对共享计数器进行原子操作，这在高并发情况下性能较差。而RCU提供了一种替代方案：只需确保在删除对象前所有引用操作都完成，无需每次修改计数器。synchronize_rcu就是这种场景下的核心机制，确保对象被安全释放。 为了更好地理解synchronize_rcu的实际应用，以下是一个文件系统目录项缓存（dentry）的删除示例：\n在Linux的虚拟文件系统（VFS）中，目录项（dentry）缓存是一个共享的数据结构，频繁用于文件路径解析。假设某个线程要删除一个目录项，具体步骤如下：\n读取操作：读者线程通过rcu_read_lock进入读侧临界区，开始读取dentry结构的数据。在此过程中，它不会阻塞其他读者线程，也不受写者线程的影响。 删除操作：写者线程想要删除一个目录项时，首先将该目录项从缓存中移除，但不会立即释放内存。接下来，它会调用synchronize_rcu。 等待同步：synchronize_rcu会等待所有当前正在读取该目录项的读者线程完成读取操作。这是通过等待所有CPU上的上下文切换来实现的。 释放内存：当synchronize_rcu返回时，表示所有读操作都已经完成。此时，写者线程可以安全地释放该目录项的内存。 通过这样的设计，读者线程在访问共享数据时不需要加锁，从而避免了锁竞争和死锁的风险，提升了系统的并发性能。而写者线程则通过synchronize_rcu确保更新操作的安全性。\n优势 局限性 高性能：synchronize_rcu的最大优势是它允许读操作完全无锁化，极大地提高了高并发场景下的读取性能。 写操作开销较高：虽然读操作几乎是无开销的，但写操作（特别是synchronize_rcu的调用）可能会带来较高的延迟，尤其是在高并发的系统中。 延迟友好：在实时系统中，synchronize_rcu的确定性完成时间有助于避免不可预测的长时间延迟。 实时性限制：虽然有快速同步版本（如call_rcu），但在某些对实时性要求极高的系统中，等待所有CPU的上下文切换可能仍然不足够快。 简单的同步模型：写者只需等待所有读者完成操作，而不需要直接和每个读者进行通信。 \u0026hellip; synchronize_rcu是Linux内核中一种强大的同步工具，它通过等待所有读者线程完成操作，确保共享数据在更新或删除时的安全性。尽管它引入了一定的写操作开销，但它极大地提升了并发读取场景中的性能。因此，RCU及其synchronize_rcu函数在诸如网络栈、文件系统和虚拟文件系统等读密集型子系统中得到了广泛的应用。\n关于 synchronize_rcu 的深入探讨 在Linux内核的RCU机制中，synchronize_rcu 是关键的一环，负责确保数据的安全更新。它的作用是在修改某个数据结构之前，等待所有现存的读取操作完成，以便写操作可以在不干扰正在进行的读操作的前提下进行。\n当一个线程调用rcu_read_lock进入读侧临界区时，它被标记为正在读取一个RCU保护的数据结构。写者线程不能阻塞正在读取的线程，因此rcu_read_lock和rcu_read_unlock提供了一种轻量级的机制来避免锁竞争。\n在需要修改或删除数据结构时，写者线程调用synchronize_rcu，该函数会等待所有活跃的RCU读侧临界区结束。只有当所有现有的读操作完成后，synchronize_rcu才会返回，这样可以确保没有读者再访问旧版本的数据结构。\nsynchronize_rcu 通过等待内核中的每个CPU执行至少一次上下文切换来确认所有的RCU临界区都已经完成。这意味着当每个CPU完成一次任务切换时，所有当前的读侧临界区都会终止，确保任何线程不再访问旧数据。\n在实现层面，Linux并没有直接创建额外的线程在每个CPU上运行（尽管一些初步的实现看起来像是在这样做），而是通过一种更优化的方式来检查上下文切换是否完成。这种方式显著减少了等待开销，而不需要额外的系统资源去处理CPU间的调度。\n关于时间开销的问题是合理的：如果我们简单地让调用线程在每个CPU上运行一次，确实会产生较高的延迟。这个问题在早期的系统设计中显得尤为重要，特别是在高并发的系统中，涉及大量的CPU核心和繁忙的处理任务。\n不过，synchronize_rcu 的实现已经针对这一问题进行了优化，它不需要在每个CPU上调度一个独立的线程来执行上下文切换。实际上，它利用了系统的调度器和现有的上下文切换事件来跟踪CPU的状态。通过监视调度器的状态变化，synchronize_rcu 可以检测到每个CPU是否已经完成了必要的上下文切换。因此，它在性能上有较大的提升，而不必实际等待每个CPU独立运行。\n为了进一步减少延迟，Linux还提供了更快速的同步版本，称为call_rcu。call_rcu 是 synchronize_rcu的异步版本，允许线程在不需要同步等待的情况下发起一个回调函数来释放内存。当所有的读者线程退出RCU临界区后，这个回调函数会被调用，完成最终的内存清理工作。\n异步的call_rcu可以极大地减少线程的阻塞时间，尤其是在高并发的场景下。这对于内核中的许多时间敏感的子系统，例如网络栈和文件系统，都是至关重要的。\n一个实际例子：文件目录项（dentry）删除\n通过一个常见的例子来说明synchronize_rcu的实际应用：删除文件目录项（dentry）缓存。\n读侧操作：假设某个线程正在通过rcu_read_lock进入读侧临界区，读取文件目录项的相关信息。读操作不会被阻塞，它继续无锁地访问共享数据。 写侧操作：当需要删除一个dentry时，写者线程首先从目录缓存中移除该dentry，但不会立即释放它所占用的内存。为了确保安全，写者线程随后调用synchronize_rcu，等待所有并发的读者线程完成操作。 等待同步：在synchronize_rcu的执行期间，系统将等待所有正在执行的读操作完成。这是通过上下文切换跟踪来实现的。它不会阻塞新读者的进入，只是确保在调用时已经存在的读者完成任务。 安全释放内存：一旦synchronize_rcu返回，意味着没有线程还在访问被删除的dentry，此时系统可以安全地释放它所占用的内存。 synchronize_rcu 是Linux内核中处理并发更新操作的关键工具。尽管它需要等待系统中所有读者线程完成操作，但其设计通过上下文切换检测机制有效地避免了额外的系统开销。它在提升并发性能的同时，还为内核的许多子系统提供了稳定和确定的响应时间。对于高性能网络和文件系统等场景，它尤其具有显著的优势。\n异步版本的 synchronize_rcu: call_rcu call_rcu 是 Linux 内核 RCU 机制中用于延迟内存释放或资源清理的异步函数。与同步等待所有现有读操作完成的 synchronize_rcu 不同，call_rcu 会安排一个回调函数在所有 CPU 都完成至少一次上下文切换后执行。这种机制允许写者线程无需阻塞，继续执行其他操作，从而提高系统的并发性和性能。\ncall_rcu 的函数原型如下：\n1 void call_rcu(struct rcu_head *head, rcu_callback_t func); struct rcu_head *head：这是嵌入在要被释放或修改的对象中的 RCU 头结构，用于跟踪延迟操作。 rcu_callback_t func：指定的回调函数，当所有 RCU 临界区完成后调用这个函数。通常，这个回调函数包含释放内存或清理资源的代码。 在内核中，call_rcu 主要用于延迟资源回收，例如内存释放。以下是一个常见的使用场景，涉及删除数据结构中的对象：\n读取数据（读侧临界区）： 多个读者线程可能会在 RCU 读侧临界区内访问数据结构，通过 rcu_read_lock() 和 rcu_read_unlock() 确保不会被中断。 删除对象（写侧操作）： 当一个线程需要删除某个对象时，它会从数据结构中将该对象移除，但不立即释放内存。随后，调用 call_rcu，指定一个回调函数来在所有读者操作完成后释放内存。 1 2 3 4 5 6 7 8 9 10 11 void my_callback(struct rcu_head *head) { my_data_t *data = container_of(head, my_data_t, rcu); free(data); // 安全地释放内存 } void remove_data(my_data_t *data) { // 从数据结构中移除 data call_rcu(\u0026amp;data-\u0026gt;rcu, my_callback); // 异步安排内存清理 } 回调函数执行： 当所有 CPU 都完成至少一次上下文切换时，系统将调用 my_callback，安全地释放内存。 call_rcu 的优点\n非阻塞执行： 相比于 synchronize_rcu，call_rcu 不会让调用线程等待，因此在高并发系统中能够更高效地进行资源管理。 降低延迟： 对于实时或性能敏感的应用程序，call_rcu 的异步特性可以减少阻塞时间，避免长时间等待所有读者完成操作。 批处理更新： 由于 call_rcu 是非阻塞的，可以同时处理大量更新。内核的 RCU 实现会将多个 call_rcu 的调用批量处理，从而减少上下文切换的开销。 call_rcu 通过每个 CPU 的数据结构维护回调列表。当所有 CPU 都完成至少一次上下文切换时，系统会调用这些回调函数，执行延迟的清理操作。其具体的工作流程为：\n标记上下文切换：当调用 call_rcu 时，它不会立即执行回调，而是记录一个待处理的回调，并开始跟踪所有 CPU 的上下文切换。 检测上下文切换：RCU 系统周期性检查每个 CPU 是否完成了至少一次上下文切换，这个过程经过优化，避免了不必要的性能开销。 执行回调：当所有 CPU 都完成至少一次上下文切换时，系统会在安排的 CPU 上执行回调，进行内存释放或资源清理。 在实践中，call_rcu 具有极高的可扩展性，可同时处理数千个回调请求。通过批量处理上下文切换的检测，系统大大降低了每次调用的开销。\ncall_rcu 的一个重要应用案例是在 Linux 网络栈中。网络栈通常需要高吞吐量和低延迟，使用同步等待的方式效率不高。例如，在删除 IP 选项或更新路由表时，call_rcu 能够在所有读者完成后异步释放旧数据，从而避免了不必要的阻塞。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 void udp_sendmsg(sock_t *sock, msg_t *msg) { ip_options_t *opts; opts = rcu_dereference(sock-\u0026gt;opts); if (opts != NULL) { // 处理 IP 选项 } rcu_read_unlock(); } void setsockopt(sock_t *sock, int opt, void *arg) { if (opt == IP_OPTIONS) {s ip_options_t *old = sock-\u0026gt;opts; ip_options_t *new = arg; rcu_assign_pointer(sock-\u0026gt;opts, new); if (old != NULL) call_rcu(\u0026amp;old-\u0026gt;rcu, kfree_rcu); } } 在此示例中，call_rcu 确保在释放旧的 IP 选项之前，所有可能使用这些选项的线程都已经完成操作。\ncall_rcu 是一种延迟执行的异步机制，通过在所有 CPU 完成上下文切换后执行指定的回调函数，解决了高并发场景下的内存释放和资源管理问题。相比于 synchronize_rcu，它具有显著的性能优势和延迟优化，特别适用于网络栈、文件系统等读操作占优的高性能系统。\nLinux NMI Linux 的 NMI (Non-Maskable Interrupt) 系统是处理一种高优先级硬件中断的机制，这种中断无法被屏蔽或忽略。NMI 通常用于非常关键的任务，比如检测系统错误、硬件问题或执行高精度的性能监控。它的优先级高于普通中断，在某些情况下，即使系统中断被禁用或屏蔽，NMI 依然会被处理。其主要应用有如下几点：\n硬件错误检测：NMI 被广泛用于报告严重的硬件错误，比如内存纠错（ECC）错误、CPU 错误或其他硬件问题。当硬件检测到这些严重的错误时，它会触发 NMI，系统会立即进入 NMI 处理程序，快速响应这些错误。 性能监控：NMI 系统常用于高精度的性能监控工具，如 perf 或 OProfile，这些工具依赖 NMI 来收集系统性能数据。通过 NMI，这些工具能够定期中断 CPU，获取系统状态和性能计数器的精确信息。 看门狗定时器：NMI 还常用于看门狗定时器，当系统进入死锁或长时间无响应时，看门狗定时器会触发 NMI 来执行系统恢复操作，防止系统完全崩溃。 在 NMI 系统中，Linux 内核通常会结合 RCU (Read-Copy-Update) 机制来处理并发访问的同步问题，尤其是 NMI 处理程序的动态注册和注销。由于 NMI 具有非常高的优先级，并且可以在任何时间打断系统操作，传统的锁机制在此场景中可能会引发死锁或性能问题。因此，RCU 提供了一种无锁机制来管理 NMI 处理程序列表。RCU 在 NMI 系统中的具体作用：\n动态注册和注销 NMI 处理程序：在 NMI 系统中，处理程序可能会根据需要动态注册和注销。在注销处理程序时，需要确保没有 CPU 正在执行该处理程序。通过 RCU，可以在注销 NMI 处理程序后调用 synchronize_rcu 来等待所有 NMI 处理操作完成，确保不会有 CPU 再次访问已被删除的处理程序。 高并发性能：RCU 提供了一种无锁机制，允许多个 CPU 高效访问 NMI 处理程序列表。这对于依赖频繁调用 NMI 的应用（如性能监控工具）至关重要，可以避免因频繁的锁竞争而导致的性能瓶颈。 确定性的执行时间：在高优先级的 NMI 系统中，使用 RCU 机制可以保证进入和退出 NMI 处理程序的执行时间是确定的，这对于实时系统尤为关键。相比于读写锁等阻塞同步机制，RCU 能避免不确定的执行时间问题。 以下是一个使用 RCU 进行 NMI 处理程序注册和注销的简化代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 rcu_list_t nmi_list; spinlock_t nmi_list_lock; void handle_nmi() { rcu_read_lock(); // 进入 RCU 临界区 rcu_list_for_each(\u0026amp;nmi_list, handler_t cb) { cb(); // 调用每个 NMI 处理程序 } rcu_read_unlock(); // 退出 RCU 临界区 } void register_nmi_handler(handler_t cb) { spin_lock(\u0026amp;nmi_list_lock); rcu_list_add(\u0026amp;nmi_list, cb); // 注册新的 NMI 处理程序 spin_unlock(\u0026amp;nmi_list_lock); } void unregister_nmi_handler(handler_t cb) { spin_lock(\u0026amp;nmi_list_lock); rcu_list_remove(\u0026amp;nmi_list, cb); // 从列表中移除处理程序 spin_unlock(\u0026amp;nmi_list_lock); synchronize_rcu(); // 等待所有 NMI 操作完成 } 通过上面的代码，NMI 处理程序的注册和注销可以在高并发的环境下安全进行。synchronize_rcu 用于确保注销处理程序时，所有正在执行的 NMI 操作已经结束，防止无效指针访问导致的系统崩溃。\nLinux NMI 系统是用于处理高优先级硬件中断的关键机制，通常用于检测硬件故障、执行高精度性能监控以及管理看门狗定时器。结合 RCU 机制，NMI 系统能够实现动态的处理程序管理，并提高系统的并发性能和可靠性。\n传统锁与 RCU 在多线程或多处理器系统中，如何有效地同步共享数据的访问是一个关键问题。传统的锁机制和 RCU（Read-Copy-Update）是两种广泛使用的同步机制。以下是对比这两种同步方法的详细分析。\n传统锁的核心原理是通过互斥锁来确保只有一个线程可以访问某个共享资源，而其他线程必须等待，直到该资源被释放。这种方式包括多种类型的锁，比如互斥锁（mutex）、读写锁（read-write lock）等。\n互斥锁（Mutex）：互斥锁确保同一时刻只有一个线程能够访问共享数据。它会阻塞其他试图访问相同资源的线程，直到锁被释放。 读写锁（Read-Write Lock）：允许多个读者线程并发访问资源，但写者线程必须独占该资源。读者之间不会相互阻塞，但写者和其他读者之间互斥。 传统锁优点 传统锁缺点 简单直观：传统锁的使用方法非常简单明确，只需要在进入临界区前加锁，退出时解锁即可。 性能瓶颈：传统锁的一个主要问题是它可能导致性能瓶颈，尤其是在高并发读操作的场景中。读者线程和写者线程之间的互斥可能导致大量等待，进而拖慢整个系统的性能。 对写密集型操作有优势：传统锁对于写操作频繁的场景非常有效，因为它通过强制序列化写操作来确保数据一致性。 死锁风险：不正确的锁顺序或嵌套锁的使用可能导致死锁问题，即多个线程彼此等待锁释放，从而永久阻塞。 RCU 是 Linux 内核中一种非常高效的同步机制，专为读多写少的场景设计。它通过允许读者线程和写者线程同时访问共享资源，而不会彼此阻塞，从而极大地提高了并发性。\nRCU 的基本原理是读者线程不需要锁来访问数据，而写者线程在更新数据时，会首先创建数据的副本，然后通过同步机制（如 synchronize_rcu 或 call_rcu）来确保所有当前正在读取该数据的线程完成后，才会删除或释放旧数据。\nRCU优点 RCU缺点 高并发性：RCU 允许读者线程无锁访问数据，因此在大量读操作场景下，RCU 提供了极高的并发性。多个读者线程可以同时读取数据，而不会发生互相等待。 写操作的复杂性：虽然 RCU 在读操作上效率极高，但写操作需要复杂的同步机制，如 synchronize_rcu 来确保所有读者线程完成后再进行更新。这意味着在写密集型场景中，RCU 的性能可能并不优于传统锁。 低开销：RCU 不涉及复杂的锁操作，避免了传统锁中因加锁和解锁带来的性能开销。 内存消耗：RCU 的设计要求在更新数据时创建数据副本，因此在高频率更新的场景下，可能会导致内存使用量增加。 确定性时间：RCU 中的读操作具有确定性时间，不会像传统锁那样因锁争用而导致不确定的等待时间。 \u0026hellip; RCU 和传统锁在多核系统上的性能差异较为明显，尤其是在读操作占多数的场景下。\n读写锁 vs RCU\n读写锁 RCU 读操作 使用传统的读写锁来保护同样的读操作，在同样的系统中，获取和释放读锁的开销为 89 纳秒。当 CPU 核心数量增加时，读写锁的开销显著增加。在 16 核心的系统上，获取和释放读锁的时间增至 6654 纳秒。 RCU 的读操作不需要任何锁机制，因此非常快速。在一个典型的多核系统中，进入和退出 RCU 临界区（调用 rcu_read_lock 和 rcu_read_unlock）的开销大约是 6 纳秒，无论系统中有多少 CPU 核心，RCU 的读操作开销基本保持不变。 并发性 读写锁会在多个读者竞争锁时导致缓存失效，降低并发性能。 RCU 的并发读操作不会相互影响，读者线程之间不需要等待。 死锁风险 传统锁，特别是嵌套锁和多个锁的场景中，容易导致死锁。例如，如果一个线程获取了一个锁后被中断，而另一个线程同时尝试获取相同的锁，那么死锁可能发生。 RCU 由于其无锁的读操作，不存在读者线程之间的锁竞争，也不会出现读者线程和写者线程之间的死锁问题。只有在某些特殊情况下，写者线程可能会因为等待 RCU 临界区的完成而阻塞，但这不会导致传统意义上的死锁。 RCU 和传统锁的应用场景有所不同。\n适合使用传统锁的场景 适合使用 RCU 的场景 写操作占主导：在写操作频繁的场景下，传统锁机制更加适合，因为它能够通过强制互斥确保数据的一致性，避免复杂的同步机制。 读操作占主导：RCU 特别适合读操作占多数的场景，尤其是内核中的文件系统、网络栈等场景。在这些场景中，读者线程不需要等待其他读者或写者，系统能够提供极高的并发性能。 实时系统的精确控制：在某些实时系统中，需要严格的顺序保证，并且必须确保某些操作独占资源。在这种场景下，传统锁可能提供更好的控制。 高并发性和低延迟：对于对并发性和低延迟有较高要求的系统，RCU 是理想的选择。由于其读操作几乎无阻塞，因此可以减少延迟，提升系统的整体响应速度。 传统锁和 RCU 各有其优缺点，适合不同的应用场景。传统锁提供了简单、可靠的互斥访问方式，适用于写操作频繁的场景，但在高并发读操作下性能表现不佳。相比之下，RCU 是一种高效的并发同步机制，能够在读密集型场景中大幅提高性能，但它在写操作频繁的场景中表现稍弱。\nRCU 替代读写锁的典型场景\nLinux 内核中的 PID 哈希表 就是使用 RCU 替代读写锁的一个典型例子。PID 哈希表用于将进程标识符（PID）映射到进程信息。在这个例子中，读取者通过调用 rcu_read_lock 来获取进程信息，读取完成后调用 rcu_read_unlock 释放锁。而写者线程通过自旋锁来同步写操作。\n以下是一个简化的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 process_t *pid_lookup(int pid) { process_t *p; rcu_read_lock(); // 进入 RCU 临界区 p = pid_table[pid_hash(pid)].process; if (p) atomic_inc(\u0026amp;p-\u0026gt;ref); // 增加引用计数 rcu_read_unlock(); // 退出 RCU 临界区 return p; } void pid_remove(int pid) { process_t **p; spin_lock(\u0026amp;pid_table[pid_hash(pid)].lock); // 使用自旋锁同步写操作 p = \u0026amp;pid_table[pid_hash(pid)].process; rcu_assign_pointer(p, NULL); // 安全发布 NULL spin_unlock(\u0026amp;pid_table[pid_hash(pid)].lock); if (*p) call_rcu(pid_free, *p); // 使用 RCU 异步释放旧数据 } 在这个例子中，RCU 的读操作是并发的，且不需要阻塞写操作。写操作通过自旋锁保护，并在合适的时间点调用 call_rcu 异步释放旧数据。\nLinux 的 slab 分配器 Linux 的 slab 分配器是一种内存管理机制，用于高效地管理和分配内核中频繁使用的小对象内存。它通过将一块大的内存页面分割成多个相同大小的对象，并维护对象的生命周期，以便快速分配和释放内存。与常规的动态内存分配器相比，slab 分配器有以下几个特点：\n高效性：slab 分配器通过减少内存碎片和频繁的内存分配开销来提高性能，特别是在需要频繁创建和销毁对象的场景中。 类型安全：slab 分配器为每种类型的对象创建特定的内存缓存区，每个缓存区中包含相同类型和大小的对象。因此，这些对象的内存分配和释放在类型上是安全的，避免了对象混淆。 缓存复用：对象在释放后，slab 不会立即将其内存返回给系统，而是将该对象保留在缓存中，供下次分配时复用。这样可以避免频繁的内存分配和释放操作。 类型安全 在 slab 分配器中，开发者可以设置一个特殊的 SLAB_DESTROY_BY_RCU 标志，这个标志会确保在 RCU 同步完成之前，不会将内存重新分配给不同类型的对象。如果使用 SLAB_DESTROY_BY_RCU 标志，意味着对象的内存不会在 RCU 同步完成之前被销毁或重新分配。这就实现了“类型安全内存”的效果。举个例子：\n内存分配：当内核使用 slab 分配器分配对象时，slab 会从一个页面（page）中分割出相同类型的对象，并在该页面中分配所需的内存。 内存回收：当一个页面中的所有对象都被释放后，该页面可以返回给系统。如果设置了 SLAB_DESTROY_BY_RCU 标志，那么在释放这些对象之前，RCU 会确保没有线程仍然在访问这些对象的内存。 类型安全：在这种方式下，即使某个线程在 RCU 临界区内仍然引用了一个对象，系统也能确保在对象的内存被重新分配给另一个对象类型之前完成同步，避免了类型混乱的问题。 反向页表是一个典型的使用类型安全内存的例子。反向页表用于管理物理页和虚拟地址之间的映射。在 Linux 内核中，每个物理页通过 page_t 表示，虚拟地址映射则通过 anon_vma_t 来管理。为了防止在线程引用的 anon_vma_t 被释放时重新分配给其他类型的对象，内核可以通过 RCU 机制实现类型安全的内存分配。具体来说：\n当一个物理页被解映射时，anon_vma_t 对象可能会被释放，但 RCU 确保在释放之前，所有引用该对象的线程已经完成了其操作。 如果某个线程仍然在读取 anon_vma_t，它可以通过检测对象的状态来避免访问无效的内存。 类型安全内存的防护：传统方式与RCU机制的比较 在多线程或多处理器系统中，内存管理和对象生命周期管理是非常复杂的任务。尤其是在对象释放过程中，如果某个对象被多个线程共享访问，那么在某个线程释放该对象内存的同时，其他线程可能仍然在使用该对象，这就会导致悬空指针（dangling pointer）和类型安全问题。为了解决这些问题，系统在没有 RCU 机制的情况下，通常采用传统的防护方式来实现类型安全内存，而引入 RCU 之后，内存管理的复杂性和性能问题得到了很大程度的简化。\n在没有 RCU 机制时，内核通常使用以下几种方法来确保类型安全内存：\n引用计数（Reference Counting）： 工作机制：通过在对象结构中维护一个引用计数器，来跟踪当前对象被多少个线程引用。每当一个线程开始使用对象时，引用计数加 1，当线程停止使用时，引用计数减 1。只有当引用计数降至 0 时，才能安全地释放该对象的内存。 防护方式：通过引用计数，系统能够确保在所有线程都不再引用该对象时，才释放内存，避免了悬空指针的问题。这种机制可以有效防止某个线程访问已释放对象的问题。 缺点：引用计数的更新需要原子操作（如 atomic_inc 和 atomic_dec），在高并发的场景中，频繁的计数器更新会带来性能瓶颈。此外，在某些情况下（如循环引用），引用计数机制可能会失效，导致内存无法及时释放。 互斥锁（Mutex）和自旋锁（Spinlock）： 工作机制：使用锁机制来保护共享对象的访问。在访问或修改对象前，线程必须先获取锁，操作完成后释放锁。这样可以防止多个线程同时访问对象导致数据不一致或非法内存访问。 防护方式：锁机制可以确保在访问和释放对象时，不会有其他线程正在使用该对象，从而避免内存重用时出现类型不匹配的问题。 缺点：锁机制容易导致性能问题，特别是在高并发场景下，频繁的加锁和解锁会带来显著的性能开销。此外，锁的使用不当可能引发死锁和优先级反转等问题。 延迟释放（Deferred Freeing）： 工作机制：在对象释放时，使用某种机制延迟内存的真正释放，直到确定没有线程仍在访问该对象。例如，通过设置标志位或使用线程等待等方式，确保内存释放的安全性。 防护方式：延迟释放可以防止悬空指针，但需要额外的同步操作和检测机制来判断对象是否可以安全释放，这增加了系统的复杂性。 缺点：延迟释放机制可能导致内存占用增多，如果管理不当，甚至会导致内存泄漏。 RCU（Read-Copy-Update）是一种高效的同步机制，可以大大简化类型安全内存管理中的问题。RCU 的核心思想是允许读者线程在无锁的情况下访问数据，同时写者线程可以异步地进行数据更新，并在合适的时机释放旧数据。这种机制极大地降低了传统方式中同步操作的复杂性和性能开销。\nRCU 的类型安全内存防护机制 类型安全内存指的是当对象被释放后，其内存不会立即被重用为其他类型的对象，这样可以防止访问到已释放对象的线程遇到类型不匹配的问题。 在 RCU 中，为了实现类型安全内存，通常会结合**SLAB_DESTROY_BY_RCU 标志**来使用 slab 分配器（内核中一种高效的内存分配器）。这个标志会确保对象在 RCU 同步完成之前，其内存不会被重新分配为不同类型的对象。 在实际应用中，RCU 通过以下方式实现类型安全内存： 读者线程在 rcu_read_lock 和 rcu_read_unlock 之间无锁地访问对象，写者线程在更新或删除对象时，会调用 call_rcu 或 synchronize_rcu 来等待所有读操作完成，然后安全地释放内存。 在使用 SLAB_DESTROY_BY_RCU 标志时，内存分配器会确保对象内存在 RCU 临界区完成之前，不会被其他类型的对象重用。 RCU 机制的优势 高效性：RCU 的读操作几乎无锁，避免了传统锁机制中因加锁解锁带来的性能损失。在高并发读操作场景中，RCU 提供了极高的并发性和低延迟。 低内存开销：RCU 的类型安全内存管理通过 SLAB_DESTROY_BY_RCU 的延迟释放机制，避免了引用计数器和锁带来的额外内存开销。 避免死锁：RCU 的读操作和写操作分离机制有效地避免了传统锁机制中可能发生的死锁问题。 以下是一个简化的内核中 RCU 和 引用计数 两种类型安全内存防护方式的比较：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // 引用计数方式实现类型安全内存 struct my_object { atomic_t refcount; // 数据字段 }; void acquire_object(struct my_object *obj) { atomic_inc(\u0026amp;obj-\u0026gt;refcount); // 增加引用计数 } void release_object(struct my_object *obj) { if (atomic_dec_and_test(\u0026amp;obj-\u0026gt;refcount)) { free(obj); // 当引用计数为 0 时，释放对象 } } // RCU 方式实现类型安全内存 struct my_rcu_object { struct rcu_head rcu; // 数据字段 }; void free_rcu_object(struct rcu_head *head) { struct my_rcu_object *obj = container_of(head, struct my_rcu_object, rcu); free(obj); // 释放对象 } void release_rcu_object(struct my_rcu_object *obj) { call_rcu(\u0026amp;obj-\u0026gt;rcu, free_rcu_object); // 安排异步释放 } 在传统引用计数方式中，每次对象的引用和释放都需要更新引用计数，这在高并发环境中可能带来性能瓶颈。 而在 RCU 方式中，call_rcu 可以异步地释放对象，在释放前无需同步等待所有引用完成，避免了频繁的锁操作，提高了系统性能。 总的来说\n没有 RCU 时：传统的类型安全内存防护主要依赖于引用计数和锁机制。这些方法能够确保在释放对象前，所有引用该对象的线程已经完成访问，从而避免悬空指针和类型安全问题。然而，这种方式在高并发环境中容易带来性能瓶颈和复杂的死锁问题。 引入 RCU 之后：RCU 提供了更高效的类型安全内存防护机制。通过 RCU 的异步释放机制，可以在不阻塞读者线程的情况下安全释放对象，并结合 slab 分配器的 SLAB_DESTROY_BY_RCU 标志，确保内存不会被重新分配给不同类型的对象。这种机制在内核中得到了广泛的应用，特别是在文件系统和网络栈等高并发读操作的场景中。 RCU 的引入显著提升了内核的性能和并发性，简化了类型安全内存管理的复杂性，并减少了传统锁机制带来的问题。\nreverse-mapping 在Linux内核的虚拟内存管理中，**反向映射（Reverse Mapping）**是一个重要机制，用于管理物理内存页与虚拟地址空间之间的映射关系。具体来说，反向映射允许系统通过物理页找到哪些进程正在引用该页以及对应的虚拟地址是什么。\n反向映射的基本功能：\n通过物理页找到虚拟地址：传统的映射是从虚拟地址空间查找对应的物理页，但反向映射的目的是反过来，通过物理页找到所有引用该物理页的虚拟地址。这对于一些内存管理操作非常关键，特别是在共享内存页或分页回收时。 多进程共享页的管理：在现代操作系统中，多个进程可以共享相同的物理页，比如当多个进程使用同一段共享内存或代码段时。反向映射帮助内核追踪这些共享情况，确保在回收或修改页时，可以正确处理所有共享关系。 反向映射的具体用途：\n分页回收：当内存不足时，操作系统会将不常使用的页交换到磁盘上。这时，内核需要知道哪些页可以被安全回收，以及哪些进程的页表需要更新。反向映射使内核能够通过物理页找到对应的虚拟地址，并确保在回收物理页时正确更新相应进程的页表。 内存管理优化：反向映射还能帮助优化内存回收策略。通过了解哪些物理页被多个进程共享，内核可以避免不必要的交换操作（即不轻易将共享频繁的页回收），从而提高系统性能。 页表更新：当物理页的内容发生变化或需要被换出到磁盘时，反向映射允许内核找到所有引用该页的虚拟地址，以便相应地更新这些进程的页表。 在Linux内核中，反向映射的实现依赖于page structure和anon_vma等数据结构：\npage structure每个物理页对应一个page结构体，其中包含了有关该页的信息，包括哪些进程引用了该页。page 是内核为每个物理内存页维护的数据结构。每个物理页都对应一个 page 结构，其中包含了关于该页的元数据，包括该页的状态、引用计数以及通过反向映射找到引用该页的进程。 anon_vma匿名映射（比如进程的堆或栈）使用anon_vma来追踪虚拟内存区域。通过anon_vma，系统可以追踪到所有引用该物理页的虚拟地址。在匿名内存映射（例如进程的堆和栈）中，anon_vma 结构用来管理虚拟内存区域。它包含了一个链表，链表中的每一项都对应一个 vm_area_struct，这些结构用于表示该匿名页映射的进程及其虚拟地址范围。通过 anon_vma，内核能够从一个物理页找到所有引用该页的虚拟地址。 当需要通过一个物理页找到它对应的虚拟地址时，反向映射机制的工作流程如下：\n首先，内核通过 page 结构找到所有引用该页的 anon_vma 结构。 然后，遍历 anon_vma 中的链表，获取所有引用该物理页的 vm_area_struct 结构。 最后，vm_area_struct 中包含了该物理页对应的虚拟地址范围，内核可以根据这些信息对所有引用该物理页的虚拟内存地址进行操作。 反向映射在提高内存管理效率的同时也引入了一些开销。因为内核需要维护额外的数据结构来记录每个物理页与虚拟地址空间的关系，所以需要一定的内存开销。此外，在多进程大量共享内存的情况下，反向映射的管理会增加系统复杂度。为了优化反向映射，Linux 内核引入了多种优化技术，例如：\n多级映射：通过分层次的映射结构减少对所有进程的遍历时间。 延迟更新：当一个物理页的虚拟地址映射发生改变时，内核可以选择延迟更新页表，这样可以减少频繁的页表更新带来的性能开销。 总的来说，反向映射的主要作用是在物理内存和虚拟内存之间提供一个高效的查询机制，特别是在需要从物理页追溯到对应的虚拟地址时。它在内存回收、页表更新和内存共享管理等方面起着重要作用。在现代操作系统中，反向映射是内存管理不可或缺的一部分，能够提升系统的整体性能并确保内存操作的正确性。\n内存回收（Memory Reclamation） 在操作系统中，内存回收是一个非常重要的机制，用于在系统内存资源紧张时释放不再需要的内存。4.5 章节重点讨论了**页面回收（page reclaim）**的机制，以及如何通过反向映射来辅助高效的内存回收。\n页面回收是操作系统的一种内存管理机制，当系统内存资源紧张时，操作系统会通过扫描内存，找到那些不再需要频繁访问的页面（如被交换到磁盘的页面或不活跃的页面）并将它们回收以释放物理内存。回收的页面可能会被重新分配给其他进程或操作系统使用。\n内存回收机制主要包括以下几类操作：\n交换（Swap）：将不活跃的页面写入磁盘，以释放物理内存。 页面缓存回收：回收不再需要的缓存页面，例如文件系统缓存页面。 匿名页回收：回收匿名内存页（如进程的堆和栈）中的不活跃页面。 反向映射对于页面回收至关重要。当内核决定回收一个物理页时，它需要通过反向映射找到所有引用该页的进程，并更新这些进程的页表。这是确保页面回收不会破坏系统的一致性和正确性的重要步骤。\n具体而言，当系统决定回收一个物理页时，反向映射提供了以下关键信息：\n哪些进程引用了该页：反向映射帮助内核找到所有使用该物理页的进程。 该页在进程中的虚拟地址：通过虚拟地址，内核能够更新页表，确保这些进程不会继续访问已回收的页面。 Linux 内核中的内存回收策略依赖于多种因素，包括页面的访问频率、共享情况和物理内存的使用情况。主要的内存回收策略包括：\nLRU（Least Recently Used）算法：内核使用 LRU 算法来判断哪些页面最不常使用，并优先回收这些页面。LRU 算法会将不常使用的页面移动到内存回收列表的尾部，以便被优先回收。 优先回收共享少的页面：通过反向映射，内核能够找到那些被少数进程共享的页面。这些页面通常更容易被回收，因为它们的影响面较小。 Swap 优化：当内存非常紧张时，内核可能需要将一些页面交换到磁盘上。反向映射机制在这时能够提供有用的信息，以确保被交换的页面不会在短时间内被频繁访问，从而减少交换操作的开销。 在内存回收中，一个主要的挑战是确保系统性能不受太大影响。频繁的页面回收和页表更新操作可能导致系统性能下降，尤其是在多进程共享大量内存的场景中。因此，内核引入了多种优化措施，以确保内存回收过程的高效性：\n批量回收：内核可以通过批量回收多个不活跃的页面，减少每次回收的开销。 异步回收：为了减少对应用程序的影响，内存回收通常以异步方式进行。内核会在后台逐步回收内存，而不会阻塞正在执行的用户进程。 逐步更新页表：当系统决定回收某个页时，内核可以延迟对页表的更新，以减少频繁的页表更新操作。 内存回收在释放系统资源的同时，也带来了一定的性能开销。内核必须在释放内存和保持系统性能之间找到平衡。如果内存回收策略过于激进，可能会导致频繁的页面置换和磁盘交换操作，从而拖慢系统性能。反之，如果内存回收不足，则系统可能会遇到内存不足的问题，影响应用程序的正常运行。\n发布-订阅模型 (Publish-Subscribe) 在发布-订阅（Publish-Subscribe）模型中，写入者（发布者）会初始化一个数据项，然后通过 rcu_assign_pointer 来发布一个指向该数据项的指针。读取者（订阅者）通过 rcu_dereference 来获取并读取这个指针指向的数据项。这两个原语分别保证了数据的发布和读取过程的正确性。\nrcu_assign_pointer: 该函数负责将一个新初始化的指针赋值给全局可见的指针，确保在写操作完成之后，该指针所指向的数据才对读取者可见。 rcu_dereference: 读取者在访问 RCU 临界区中的指针时使用 rcu_dereference，确保数据在正确的时间顺序下被访问。 这种模式常常与存在性保证（existence guarantees）结合使用，以便发布新版本的数据项、并在保证旧版本不被并发访问的情况下回收它们。这一模式允许系统中的多个读取者在并发的情况下访问数据，同时允许写入者在后台安全地更新或替换数据项。\nLinux 内核中的一个经典例子是 动态替换系统调用。在某些体系结构（例如 PowerPC Cell 架构）中，系统会在运行时向系统调用表追加新的系统调用。这个扩展表的指针通过 rcu_assign_pointer 发布，确保扩展表的初始化完成后，读取者才能正确访问扩展表中的内容。\n在执行系统调用时，内核会首先调用 rcu_read_lock 进入 RCU 临界区，然后通过 rcu_dereference 来读取系统调用表的指针，确保系统调用表的扩展部分在被使用时已完全初始化。如果需要修改扩展表（例如撤回系统调用），内核会先将扩展表的指针设置为 NULL，确保不再有线程访问扩展表中的内容，随后调用 synchronize_rcu，等待所有正在执行的 RCU 临界区结束。这样确保没有线程会继续执行那些已经被移除的系统调用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 syscall_t *table; spinlock_t table_lock; int invoke_syscall(int number, void *args...) { syscall_t *local_table; int r = -1; rcu_read_lock(); local_table = rcu_dereference(table); // 获取最新的系统调用表 if (local_table != NULL) r = local_table[number](args); // 执行系统调用 rcu_read_unlock(); return r; } void retract_table() { syscall_t *local_table; spin_lock(\u0026amp;table_lock); local_table = table; rcu_assign_pointer(\u0026amp;table, NULL); // 将系统调用表指针设置为 NULL spin_unlock(\u0026amp;table_lock); synchronize_rcu(); // 等待所有读取操作结束 kfree(local_table); // 释放旧的系统调用表 } 发布-订阅模式关键在于：\n同步与并发保证: 发布的指针在 RCU 临界区内能够被读取者并发访问，而不会导致数据竞态问题。使用 rcu_assign_pointer 和 rcu_dereference 能够确保在多处理器系统上数据的发布和读取过程中的内存屏障和编译器指令正确执行。 旧版本的数据管理: 当写入者需要发布新版本的数据时，旧版本的数据仍可能被读取者访问，因此 RCU 提供了确保在读取者完成访问后安全地回收旧数据的机制。 高效性: 发布-订阅模式结合 RCU 能够在极少锁开销的情况下实现高并发的读写分离操作，大大提高系统的吞吐量。 Cache miss 在多处理器系统中，Cache miss 是影响性能的重要因素。传统的同步方法（如互斥锁、读写锁等）和 RCU（Read-Copy-Update） 方法在处理数据结构时，产生 Cache miss 的概率差异主要来源于两者对共享数据访问的方式和内存一致性的处理方式不同。\n传统同步方法，尤其是使用锁机制的场景中，多个线程或处理器为了读取或修改共享数据，需要通过加锁来保护数据的完整性。在这种情况下，容易产生 Cache miss，主要有以下几个原因：\n写-写竞争：多个处理器试图同时写入共享变量时，缓存一致性协议需要通过无效化其他处理器的缓存来保证一致性。比如，一个处理器写入某个缓存行的数据后，其他处理器需要重新加载该缓存行，导致 写 miss 频繁出现。 写-读竞争：一个处理器正在写某个共享变量时，其他处理器可能正试图读取这个变量。每当写操作发生时，读取该变量的处理器需要重新加载最新的数据到它的缓存中，造成 Cache miss。 锁的开销：当多个处理器竞争同一个锁时，锁变量本身也会被频繁读写，导致锁所在的缓存行失效，从而引发 Cache miss。 这些竞争加剧了缓存行在多个处理器之间的传输，增加了缓存失效（Cache miss）的可能性。\nRCU 提供了一种优化读操作的机制，减少了缓存一致性协议带来的负面影响。它的工作原理是通过“读者不阻塞写者，写者不阻塞读者”的方式来管理并发数据访问，具体来说有以下几种特性：\n读者不加锁：RCU 允许多个读者同时读取数据，而不需要对共享数据加锁。这样，读操作就不会引发任何缓存一致性问题，也不会导致缓存行失效，因为读者访问的是共享数据的快照，不会修改数据。这大大减少了读操作的 Cache miss。 写操作延迟并且分阶段进行：RCU 在写操作时，会先创建一个数据的副本（Copy），然后修改这个副本。当没有读者在访问旧的数据时，才会进行数据的更新（Update），将新版本替换旧版本。由于读者和写者之间不直接竞争缓存行，写操作发生时不会影响读者，从而减少了 Cache miss。 批量更新：RCU 通常会将多个更新操作批量执行，而不是频繁地对共享数据进行修改。这种批量操作降低了写操作触发的缓存失效，从而降低了 Cache miss 的概率。 传统同步方法 由于频繁的读写竞争、缓存一致性维护，以及锁的使用，导致缓存失效较多，从而增加了 Cache miss 的频率。 RCU 方法 通过减少读操作的干扰和写操作的延迟更新，避免了读写竞争，因而大幅减少了 Cache miss。 RCU 通过“读-写分离”和“数据复制”机制有效地减少了处理器间的缓存竞争，尤其是在多线程环境中，大量的读操作不会引发 Cache miss，这使得它在高并发场景中表现出色。\n间接访问（indirection） 间接访问（indirection）在 RCU 设计中是为了确保 原子性和一致性，使得更新操作对读者看来是原子的，而不会暴露出“更新中的状态”。\n在直接存储的模式下，如果对某个对象进行写操作，比如在 PID 表中直接存储 process_t 实例，那么更新 process_t 的各个字段就会发生在原对象上。这样一来，读者可能会在对象更新还未完成时，读取到尚未完全初始化的对象，这就是所谓的“中间状态”。\n举个例子，假设一个 process_t 对象有多个字段需要逐一更新。如果 PID 表直接存储了这个对象的实例：\n当写者正在修改 process_t 的字段时，某个读者可能正在读取该对象，读取的字段可能只被部分更新。 这会导致不一致的读取情况，比如某个字段是旧值，而另一个字段是新值。这在多线程或并发环境中可能会引发严重的问题，因为读者期望读取的是一个一致的、完全更新的对象。 为了避免这种情况，RCU 使用间接访问：PID 表存储的是指向 process_t 的指针，而不是 process_t 实例本身。这种方式确保更新过程可以通过更换指针来实现，从而使得新对象的发布是原子的：\n更新副本：当需要更新 process_t 时，写者不会直接修改现有的 process_t，而是创建一个新的 process_t 副本，完全更新新对象的所有字段。 发布新指针：在新对象的所有字段都更新完毕后，写者将 PID 表中指向 process_t 的指针更新为新对象的地址。这一操作是原子的：所有的字段更新都是一次性发布的，不会暴露出中间状态。 在这个过程中，指针充当了一个“指向新状态”的媒介。由于指针的更换是原子的，任何新的读者都会看到完整的新对象，而不是一个正在更新的对象。示例：\n初始状态：PID 表中的指针 ptr 指向 process_t_A。 写操作：创建 process_t_B 副本并更新所有字段。 指针更新：将 ptr 更新为指向 process_t_B。此时，所有读者读取到的都是 process_t_B，不可能再读取到部分更新的 process_t_A。 总而言之，通过间接访问：\n可以确保数据的更新对读者来说是一次性的、原子的，避免了不一致的读取情况。 读者在读取数据时，始终会看到一个完全初始化的数据对象，而不会被暴露给“部分更新”或“中间状态”。 在多线程编程中，使用间接访问为数据一致性提供了重要保障，这也解释了为什么在 RCU 中间接访问是一个常见的设计模式。\n正课部分 使用锁带来的问题 今天的话题是如何在多核CPU计算机上获得好的性能，这是一个非常有趣，深入且令人着迷的话题。今天我们只会涉及这个话题的很小的一个部分，也就是在面对内核中需要频繁读但是不需要频繁写的共享数据时，如何获得更好的性能。\n在不同的场景下有不同的方法可以在多核CPU的机器上获得更好的性能，我们今天要看的是Linux的RCU，它对于需要频繁读的内核数据来说是一种非常成功的方法。\n如果你有一个现代的计算机，或许包含了4、8、16、64个并行运行的CPU核，这些CPU核共享了内存数据，操作系统内核将会是一个并行运行的程序。如果你想要获得好的性能，你需要确保内核能尽可能的在多个CPU核上并行的完成它的工作。\n如果你能将内核并行的运行在8个CPU核上，并且它们都能完成有效的工作，那么相比运行在单个CPU核上，你就能获得8倍的性能。从理论上来说，这明显是可能的。\n如果你在内核中有大量的进程，那就不太用担心，在不做任何额外工作的前提下，这些进程极有可能是并行运行的。另一方面，如果你有很多应用程序都在执行系统调用，很多时候，不同的应用程序执行的不同系统调用也应该是相互独立的，并且在很多场景下应该在相互不影响的前提下运行。\n例如，通过fork产生的两个进程，或者读取不同pipe的两个进程，或者读写不同文件的两个进程。表面上看，这些进程之间没有理由会相互影响，也没有理由不能并行运行并获得n倍的吞吐量。\n但问题是内核中包含了大量的共享数据。\n出于一些其他的原因，内核共享了大量的资源，例如内存，CPU，磁盘缓存，inode缓存，这些东西都在后台被不同的进程所共享。这意味着，即使两个完全不相关的进程在执行两个系统调用，如果这两个系统调用需要分配内存或使用磁盘缓存或者涉及到线程调度决策，它们可能最终会使用内核中相同的数据结构，因此我们需要有办法能让它们在使用相同数据的同时，又互不影响。\n在过去的许多年里，人们付出了巨大的努力来让内核中的这些场景能更快的运行。\n我们之前看过其中一种可以保证正确性的方法，也就是spinlock。\nspinlock很直观，它的工作就是当两个进程可能会相互影响时，阻止并行运行。所以spinlock的直接效果就是降低性能。它使得正确性有了保障，但是又绝对的阻止了并行执行，这并不总是能令人满意。\n今天我们会关注需要频繁读的数据，也就是说你的数据主要是在被读取，相对来说很少被写入。我将使用单链表来作为主要的例子。\n对于单链表，会存在一个指向头指针（head）的全局变量，之后是一些链表元素，每个链表元素都包含了一个数据，假设是字符串。第一个链表元素包含了“hello”。每个链表元素还包含了一个next指针，指向了下一个链表元素。最后一个链表元素的next指针指向空指针。\n接下来我们假设对于这个链表的大部分操作是读，比如说内核线程大部分时候只会扫描链表来找到某些数据，而不会修改链表。假设一个写请求都没有的话，我们就根本不必担心这个链表，因为它是完全静态的，它从来都不会更新，我们可以自由的读它。\n但是接下来我们假设每隔一会，一些其他的线程会来修改链表元素中的数据；删除一个链表元素；又或者是在某个位置插入链表元素。所以尽管我们关注的主要是读操作，我们也需要关心写操作，我们需要保证读操作在面对写操作时是安全的。\n在XV6中，我们是通过锁来保护这个链表。\n在XV6中，不只是修改数据的线程，读取数据的线程也需要获取锁，因为我们需要排除当我们在读的时候某人正在修改链表的可能，否则的话会导致读取数据的线程可能读到更新一半的数据或者是读到一个无效的指针等等，所以XV6使用了锁。\n但是使用锁有个缺点，如果通常情况下没有修改数据的线程，那么意味着每次有一个读取数据的线程，都需要获取一个排他的锁。XV6中的spinlock是排他的，即使只是两个读取数据的线程也只能一次执行一个线程。\n所以一种改进这里场景的方法是使用一种新的锁，它可以允许多个读取线程和一个写入线程。接下来我们来看看这种锁，不仅因为它是有趣的，也因为它的不足促成了对于RCU的需求。\n读写锁 (Read-Write Lock) 这种锁被称为读写锁（Read-Write Lock），它的接口相比spinlock略显复杂。\n如果只是想要读取数据，那么可以调用r_lock，将锁作为参数传入，同样的还会有个r_unlock，数据的读取者使用这些接口。数据的写入者调用w_lock和w_unlock接口。\n这里的语义是，要么你可以有多个数据的读取者获取了读锁，这样可以获得并行执行读操作的能力；要么你只能有一个数据写入者获取了写锁。但是不能两种情况同时发生，读写锁排除了某人获取了数据的写锁，同时又有别人获取读锁的可能性。\n你要么只有一个数据写入者，要么有多个数据读取者，不可能有别的可能。\n学生提问：当某人持有了读锁时，读写锁是采用什么方案来阻止其他人写入数据的？\nRobert教授：并没有什么方案，这就像XV6的锁一样。我们这里讨论的是由值得信赖且负责的开发人员编写的内核代码，所以就像XV6的spinlock一样，如果使用锁的代码是不正确的，那么结果就是不正确的，这是内核代码典型的编写方式，你只能假设开发内核的人员遵循这里的规则。\n如果我们有一个大部分都是读取操作的数据结构，我们会希望能有多个用户能同时使用这个数据结构，这样我们就可以通过多核CPU获得真正的性能提升。\n如果没有其他问题的话，那么读写锁就可以解决今天的问题，我们也没有必要读RCU这篇论文 。但实际上如果你深入细节，你会发现当你使用读写锁时，尤其对于大部分都是读取操作的数据结构，会有一些问题。为了了解实际发生了什么，我们必须看一下读写锁的代码实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //A simplified version of Linux\u0026#39;s read/write lock. //n=0 -\u0026gt; not locked //n=-1 -\u0026gt; locked by one writer //n\u0026gt;0 -\u0026gt; locked by n readers struct rwlock{ int n; }: r_lock(l): while 1: x = l-\u0026gt;n if x \u0026lt; 0 continue if CAS(\u0026amp;l-\u0026gt;n,X,×+1) return //CAS(p,a,b)is atomic compare-and-swap instruction //if *p ==a,set *p = b,return true //else return false w_lock(1): while 1: if CAS(\u0026amp;l-\u0026gt;n,0,-1) return Linux实际上有读写锁的实现，上面是一种简化了的Linux代码。首先有一个结构体是rwlock，这与XV6中的lock结构体类似。rwlock结构体里面有一个计数器n，\n如果n等于0那表示锁没有以任何形式被被任何人持有 如果n等于-1那表示当前有一个数据写入者持有写锁 如果n大于0表示有n个数据读取者持有读锁。我们需要记录这里的数字，因为我们只有在n减为0的时候才能让数据写入者持有写锁。 r_lock函数会一直在一个循环里面等待数据写入者释放锁。\n首先它获取读写锁中计数器n的拷贝，如果n的拷贝小于0的话那意味着存在一个数据写入者，我们只能继续循环以等待数据写入者退出。\n如果n的拷贝不小于0，我们会增加读写锁的计数器。但是我们只能在读写锁的计数器仍然大于等于0的时候，对其加1。\n所以我们不能直接对n加1，因为如果一个数据写入者在我们检查n和我们增加n之间潜入了，那么我们有可能在数据写入者将n设置为-1的同时，将n又加了1。所以我们只能在检查完n大于等于0，且n没有改变的前提下，将其加1。\n人们通过利用特殊的原子指令来实现这一点，我们之前在看XV6中spinlock的实现时看过类似的指令（注，详见10.7中的test_and_set指令）。\n其中一个使用起来很方便的指令是compare-and-swap（CAS）。CAS接收三个参数，第一个参数是内存的某个地址，第二个参数是我们认为内存中该地址持有的数值，第三个参数是我们想设置到内存地址的数值。\nCAS的语义是，硬件首先会设置一个内部的锁，使得一个CAS指令针对一个内存地址原子的执行；之后硬件会检查当前内存地址的数值是否还是x；如果是的话，将其设置为第三个参数，也就是x+1，之后CAS指令会返回1；如果不是的话，并不会改变内存地址的数值，并返回0。\n这里必须是原子性，因为这里包含了两个操作，首先是检查当前值，其次是设置一个新的数值。\n学生提问：有没有可能计算x的过程中，发生了一个中断？\nRobert教授：你是指我们在执行CAS指令之前计算它的第三个参数的过程中发生中断吗？CAS实际上是一个指令，如果中断发生在我们计算x+1的过程中，那么意味着我们还没有调用CAS，这时包括中断在内的各种事情都可能发生。\n如果我们在最初读取n的时候读到0，那么不管发不发生中断，我们都会将1作为CAS的第三个参数传入，因为中断并不会更改作为本地变量的x，所以CAS的第二个和第三个参数会是0和1。如果n还是0，我们会将其设置为1，这是我们想看到的；如果n不是0，那么CAS并不会更新n。\n如果这里没有使用本地变量x，那么就会有大问题了，因为n可能在任意时间被修改，所以我们需要在最开始在本地变量x中存储n的一个固定的值。\n上面介绍了w_lock与r_lock同时调用的场景。多个r_lock同时调用的场景同样也很有趣。\n假设n从0开始，当两个r_lock同时调用时，我们希望当两个r_lock都返回时，n变成2，因为我们希望两个数据读取者可以并行的使用数据。两个r_lock在最开始都将看到n为0，并且都会通过传入第二个参数0，第三个参数1来调用CAS指令，但是只有一个CAS指令能成功。\nCAS是一个原子操作，一次只能发生一个CAS指令。不管哪个CAS指令先执行，将会看到n等于0，并将其设置为1。另一个CAS指令将会看到n等于1，返回失败，并回到循环的最开始，这一次x可以读到1，并且接下来执行CAS的时候，第二个参数将会是1，第三个参数是2，这一次CAS指令可以执行成功。\n最终两次r_lock都能成功获取锁，其中一次r_lock在第一次尝试就能成功，另一次r_lock会回到循环的最开始再次尝试并成功。\n学生提问：如果开始有一堆数据读取者在读，之后来了一个数据写入者，但是又有源源不断的数据读取者加入进来，是不是就轮不到数据写入者了？\nRobert教授：如果多个数据读取者获取了锁，每一个都会通过CAS指令将n加1，现在n会大于0。如果这时一个数据写入者尝试要获取锁，它的CAS指令会将n与0做对比，只有当n等于0时，才会将其设置为-1。\n但是因为存在多个数据读取者，n不等于0，所以CAS指令会失败。数据写入者会在w_lock的循环中不断尝试并等待n等于0，如果存在大量的数据读取者，这意味着数据写入者有可能会一直等待。这是这种锁机制的一个缺陷。\n学生提问：在刚刚两个数据读取者要获取锁的过程中，第二个数据读取者需要再经历一次循环，这看起来有点浪费，如果有多个数据读取者，那么它们都需要重试。\nRobert教授：你说到了人们为什么不喜欢这种锁的点子上了。即使没有任何的数据写入者，仅仅是在多个CPU核上有大量的数据读取者，r_lock也可能会有非常高的代价。\n在一个多核的系统中，每个CPU核都有一个关联的cache，也就是L1 cache。每当CPU核读写数据时，都会保存在cache中。除此之外，还有一些内部连接的线路使得CPU可以彼此交互，因为如果某个CPU核修改了某个数据，它需要告诉其他CPU核不要去缓存这个数据，这个过程被称为(cache) invalidation。\n如果有多个数据读取者在多个CPU上同时调用r_lock，它们都会读取读写锁的计数l-\u0026gt;n，并将这个数据加载到CPU的cache中，它们也都会调用CAS指令，但是第一个调用CAS指令的CPU会修改l-\u0026gt;n的内容。作为修改的一部分，它需要使得其他CPU上的cache失效。\n所以执行第一个CAS指令的CPU需要通过线路发送invalidate消息给其他每一个CPU核，之后其他的CPU核在执行CAS指令时，需要重新读取l-\u0026gt;n，但是这时CAS指令会失败，因为l-\u0026gt;n已经等于1了，但x还是等于0。\n之后剩下的所有数据读取者都会回到循环的最开始，重复上面的流程，但这一次还是只有一个数据读取者能成功。\n假设有n个数据读取者，那么每个r_lock平均需要循环n/2次，每次循环都涉及到O(n)级别的CPU消息，因为至少每次循环中所有CPU对于l-\u0026gt;n的cache需要被设置为无效。\n这意味着，对于n个CPU核来说，同时获取一个锁的成本是O(n^2)，当你为一份数据增加CPU核时，成本以平方增加。\n这是一个非常糟糕的结果，因为你会期望如果有10个CPU核完成一件事情，你将获得10倍的性能，尤其现在还只是读数据并没有修改数据。\n你期望它们能真正的并行运行，当有多个CPU核时，每个CPU核读取数据的时间应该与只有一个CPU核时读取数据的时间一致，这样并行运行才有意义，因为这样你才能同时做多件事情。但是现在，越多的CPU核尝试读取数据，每个CPU核获取锁的成本就越高。\n对于一个只读数据，如果数据只在CPU的cache中的话，它的访问成本可能只要几十个CPU cycle。但是如果数据很受欢迎，由于O(n^2)的效果，光是获取锁就要消耗数百甚至数千个CPU cycle，因为不同CPU修改数据（注，也就是读写锁的计数器）需要通过CPU之间的连线来完成缓存一致的操作。\n所以这里的读写锁，将一个原本成本很低的读操作，因为要修改读写锁的l-\u0026gt;n，变成了一个成本极高的操作。如果你要读取的数据本身就很简单，这里的锁可能会完全摧毁任何并行带来的可能的性能提升。\n读写锁糟糕的性能是RCU存在的原因，因为如果读写锁足够有效，那么就没有必要做的更好。\n除了在有n个CPU核时，r_lock的成本是O(n^2)之外，这里的读写锁将一个本来可以缓存在CPU中的，并且可能会很快的只读的操作，变成了需要修改锁的计数器l-\u0026gt;n的操作。\n如果我们写的是可能与其他CPU核共享的数据，写操作通常会比读操作成本高得多。因为读一个未被修改的数据可以在几个CPU cycle内就从CPU cache中读到，但是修改可能被其他CPU核缓存的数据时，需要涉及CPU核之间的通信来使得缓存失效。\n不论如何修改数据结构，任何涉及到更改共享数据的操作对于性能来说都是灾难。\n1 2 3 4 5 6 7 r_lock(l): while 1: x = l-\u0026gt;n //THIS if x \u0026lt; 0 continue if CAS(\u0026amp;l-\u0026gt;n,X,×+1) return 所以r_lock中最关键的就是它对共享数据做了一次写操作。所以我们期望找到一种方式能够在读数据的同时，又不需要写数据，哪怕是写锁的计数器也不行。这样读数据实际上才是一个真正的只读操作。\nRCU实现(1) - 基本实现 一种可能的解决方案是：数据读取者完全不使用锁。\n在有些场景数据读取者可以直接读数据，只有数据的写入者才需要锁。我们接下来快速的看一下能不能让数据读取者在不上锁的时候直接读取链表。\n假设我们有个链表，链表元素中存的数据是字符串，我们将读取链表中的数据。如果没有数据的写入者，那么不会有任何问题。\n接下来我们看一下存在数据写入者时的三种可能场景：\n首先是数据的写入者只修改了链表元素的内容，将链表元素中的字符串改成了其他的字符串。 第二种场景是数据写入者插入了一个链表元素。 第三种场景是数据写入者删除了一个链表元素。 因为RCU需要分别考虑这三种场景，我们将会分别审视这三种场景并看一下同时发生数据的读写会有什么问题？\n如果数据写入者想要修改链表元素内的字符串，而数据读取者可能正在读取相同字符串。如果不做任何特殊处理，数据读取者可能会读到部分旧的字符串和部分新的字符串。这是我们需要考虑的一个问题。\n如果数据写入者正在插入一个链表元素，假设要在链表头部插入一个元素，数据写入者需要将链表的头指针指向新元素，并将新元素的next指针指向之前的第一个元素。\n这里的问题是，数据的写入者可能在初始化新元素之前，就将头指针指向新元素，也就是说这时新元素包含的字符串是无效的并且新元素的next指针指向的是一个无效的地址。这是插入链表元素时可能出错的地方。\n如果数据写入者正在删除一个链表元素，我们假设删除的是第一个元素，所以需要将链表的头指针指向链表的第二个元素，之后再释放链表的第一个元素。\n这里的问题是，如果数据读取者正好在读链表的第一个元素，而数据写入者又释放了这个元素，那么数据读取者看到的是释放了的元素，这个链表元素可能接下来被用作其他用途，从数据读取者的角度来说看到的是垃圾数据。\n如果我们完全不想为数据读取者提供任何锁，那么我们需要考虑这三个场景。我将不会讨论数据写入者对应的问题，因为在整个课程中我将会假设数据写入者在完成任何操作前，都会使用类似spinlock的锁。\n我们不能直接让数据读取者在无锁情况下完成读取操作，但是我们可以修复上面提到的问题，这就带出了RCU（Read Copy Update）这个话题。\nRCU是一种实现并发的特殊算法，它是一种组织数据读取者和写入者的方法，通过RCU数据读取者可以不用使用任何锁。RCU的主要任务就是修复上面的三种数据读取者可能会陷入问题的场景，它的具体做法是让数据写入者变得更加复杂一些，所以数据写入者会更慢一些。\n除了锁以外它还需要遵循一些额外的规则，但是带来的好处是数据读取者因为可以不使用锁、不需要写内存而明显的变快。\n在之前讨论的第一个场景中，数据写入者会更新链表元素的内容。RCU将禁止这样的行为，也就是说数据写入者不允许修改链表元素的内容。假设我们有一个链表，数据写入者想要更新链表元素E2。\n现在不能直接修改E2的内容，RCU会创建并初始化一个新的链表元素。所以新的内容会写到新的链表元素中，之后数据写入者会将新链表元素的next指针指向E3，之后在单个的写操作中将E1的next指针指向新的链表元素。\n所以这里不是修改链表元素的内容，而是用一个包含了更新之后数据的新链表元素代替之前的链表元素。对于数据读取者来说，如果遍历到了E1并正在查看E1的next指针：\n要么看到的是旧的元素E2，这并没有问题，因为E2并没有被改变； 要么看到的是新版本的E2，这也没有问题，因为数据写入者在更新E1的next指针之前已经完全初始化好了新版本的E2。 不管哪种情况，数据读取者都将通过正确的next指针指向E3。这里核心的点在于，数据读取者永远也不会看到一个正在被修改的链表元素内容。\n学生提问：旧的E2和E3之间的关系会被删除吗？\nRobert教授：会被保留。这是个好问题，并且这也是RCU中较为复杂的主要部分，现在我们假设旧的E2被保留了。\n学生提问：我们并不用担心E2和E3之间的关系，因为在普通的实现中，E2也会被释放，就算没有RCU我们也不用担心这里的关系，是吗（注，这里应该说的是GC会回收E2）？\nRobert教授：这里的问题是，在我们更新E1的next指针时，部分数据读取者通过E1的旧的next指针走到了旧的E2，所以当完成更新时，部分数据读取者可能正在读取旧的E2，我们最好不要释放它。\n这里将E1的next指针从旧的E2切换到新的E2，在我（Robert教授）脑海里，我将其称为committing write。\n这里能工作的部分原因是，单个committing write是原子的，从数据读取者的角度来说更新指针要么发生要么不发生。通过这一条不可拆分的原子指令，我们将E1的next指针从旧的E2切换到的新的E2。写E1的next指针完成表明使用的是新版本的E2。\n这是对于RCU来说一个非常基本同时也是非常重要的技术，它表示RCU主要能用在具备单个committing write的数据结构上。这意味着一些数据结构在使用RCU时会非常的奇怪，例如一个双向链表，其中的每个元素都有双向指针，这时就不能通过单个committing write来删除链表元素，因为在大多数机器上不能同时原子性的更改两个内存地址。所以双向链表对于RCU来说不太友好。\n相反的，树是一个好的数据结构。\n如果我们要更新图中的节点，我们可以构造树的虚线部分，然后再通过单个committing write更新树的根节点指针，切换到树的新版本。\n数据写入者会创建树中更新了的那部分，同时再重用树中未被修改的部分，最后再通过单个committing write，将树的根节点更新到新版本的树的根节点。\n但是对于其他的数据结构，就不一定像树一样能简单的使用RCU。以上就是实现RCU的第一部分。\nRCU实现(2) - Memory barrier 在前一部分介绍的方法中，存在一个问题。在前一部分中，如果要更新E2的内容，需要先创建一个E2‘ 并设置好它的内容，然后将E2’ 的next指针指向E3，最后才会将E1的next指针指向E2’。\n你们或许还记得在XV6中曾经介绍过（注，详见10.8），许多计算机中都不存在“之后”或者“然后”这回事，通常来说所有的编译器和许多微处理器都会重排内存操作。如果我们用C代码表示刚才的过程：\n1 2 3 e = alloc() e-\u0026gt; next = E3 E1-\u0026gt;next = e 如果你测试这里的代码，它可能可以较好的运行，但是在实际中就会时不时的出错。\n这里的原因是编译器或者计算机可能会重排这里的写操作，也有可能编译器或者计算机会重排数据读取者的读操作顺序。如果我们在初始化E2’的内容之前，就将E1的next指针设置成E2‘，那么某些数据读取者可能就会读到垃圾数据并出错。\n所以实现RCU的第二个部分就是数据读取者和数据写入者都需要使用memory barriers，这里背后的原因是因为我们这里没有使用锁。对于数据写入者来说，memory barrier应该放置在committing write之前，\n1 2 3 4 e = alloc() e-\u0026gt; next = E3 BARRIER E1-\u0026gt;next = e 这样可以告知编译器和硬件，先完成所有在barrier之前的写操作，再完成barrier之后的写操作。所以在E1设置next指针指向E2‘的时候，E2’必然已经完全初始化完了。\n对于数据读取者，需要先将E1的next指针加载到某个临时寄存器中，我们假设r1保存了E1的next指针，之后数据读取者也需要一个memory barrier，然后数据读取者才能查看r1中保存的指针。\n1 2 3 4 r1 = E1 -\u0026gt; next BARRIER r1 -\u0026gt; x -\u0026gt; next 这里的barrier表明的意思是，在完成E1的next指针读取之前，不要执行其他的数据读取，这样数据读取者从E1的next指针要么可以读到旧的E2，要么可以读到新的E2‘。\n通过barrier的保障，我们可以确保成功在r1中加载了E1的next指针之后，再读取r1中指针对应的内容。\n因为数据写入者中包含的barrier确保了在committing write时，E2’已经初始化完成。如果数据读取者读到的是E2‘，数据读取者中包含的barrier确保了可以看到初始化之后E2’的内容。\n学生提问：什么情况下才可能在将E1的next指针加载到r1之前，就先读取r1中指针指向的内容？\nRobert教授：我觉得你难住我了。一种可能是，不论r1指向的是什么，它或许已经在CPU核上有了缓存，或许一分钟之前这段内存被用作其他用途了，我们在CPU的缓存上有了E1-\u0026gt;next对应地址的一个旧版本。\n我不确定这是不是会真的发生，这里都是我编的，如果r1-\u0026gt;x可以使用旧的缓存的数据，那么我们将会有大麻烦。\n说实话我不知道这个问题的答案，呵呵。我课下会想一个具体的例子。\nRCU实现(3) - 读写规则 前面有同学也提到过，数据写入者会将E1的next指针从指向旧的E2切换到指向新的E2‘，但是可能有数据读取者在切换之前读到了旧的E2，并且仍然在查看旧的E2。\n我们需要在某个时候释放旧的E2，但是最好不要在某些数据读取者还在读的时候释放。所以我们需要等待最后一个数据读取者读完旧的E2，然后才能释放旧的E2。\n这就是RCU需要解决的第三个问题，数据写入者到底要等待多久才能释放E2？你可以想到好几种方法来实现这里的等待。\n例如，我们可以为每个链表元素设置一个引用计数，并让数据读取者在开始使用链表元素时对引用计数加1，用完之后对引用计数减1，然后让数据写入者等待引用计数为0。\n但是我们会第一时间就否定这个方案，因为RCU的核心思想就是在读数据的时候不引入任何的写操作，因为我们前面看过了，如果有大量的数据读取者同时更新引用计数，相应的代价将十分高。所以我们绝对不会想要有引用计数的存在。\n另一种可能是使用自带垃圾回收（Garbage Collect）的编程语言。\n在带有GC的编程语言中，你不用释放任何对象，相应的GC会记住是否有任何线程或者任何数据结构对于某个对象还存在引用。如果GC发现对象不可能再被使用时，就会释放对象。\n这也是一种可能且合理的用来释放链表元素的方法。但是使用了RCU的Linux系统，并不是由带有GC的编程语言编写，并且我们也不确定GC能不能提升性能，所以这里我们也不能使用一个标准GC来释放E2。\nRCU使用的是另一种方法，数据读取者和写入者都需要遵循一些规则，使得数据写入者可以在稍后再释放链表元素。规则如下：\n数据读取者不允许在context switch（注，是指线程切换的context switch，详见11.4）时持有一个被RCU保护的数据（也就是链表元素）的指针。所以数据读取者不能在RCU critical 区域内出让CPU。 对于数据写入者，它会在每一个CPU核都执行过至少一次context switch之后再释放链表元素。 这里的第一条规则也是针对spin lock的规则，在spin lock的加锁区域内是不能出让CPU的。\n第二条规则更加复杂点，但是相对来说也更清晰，因为每个CPU核都知道自己有没有发生context switch，所以第二条规则是数据写入者需要等待的一个明确条件。\n数据写入者或许要在第二条规则上等待几个毫秒的时间才能确保没有数据读取者还在使用链表元素，进而释放链表元素。\n人们创造了多种技术来实现上面第二条规则中的等待，论文 里面讨论的最简单的一种方法是通过调整线程调度器，使得写入线程简短的在操作系统的每个CPU核上都运行一下，这个过程中每个CPU核必然完成了一次context switching。\n因为数据读取者不能在context switch的时候持有数据的引用，所以经过这个过程，数据写入者可以确保没有数据读取者还在持有数据。\n所以数据写入者的代码实际上看起来是这样的：\n首先完成任何对于数据的修改 之后调用实现了上面第二条规则synchronize_rcu函数 最后才是释放旧的链表元素 synchronize_rcu迫使每个CPU核都发生一次context switch，所以在synchronize_rcu函数调用之后，由于前面的规则1，任何一个可能持有旧的E1 next指针的CPU核，都不可能再持有指向旧数据的指针，这意味着我们可以释放旧的链表元素。\n你们可能会觉得synchronize_rcu要花费不少时间，可能要将近1个毫秒，这是事实并且不太好。\n其中一种辩解的方法是：对于RCU保护的数据来说，写操作相对来说较少，写操作多花费点时间对于整体性能来说不会太有影响。\n对于数据写入者不想等待的场景，可以调用另一个函数call_rcu，将你想释放的对象和一个执行释放的回调函数作为参数传入，RCU系统会将这两个参数存储到一个列表中，并立刻返回。\n之后在后台，RCU系统会检查每个CPU核的context switch计数，如果每个CPU核都发生过context switch，RCU系统会调用刚刚传入的回调函数，并将想要释放的对象作为参数传递给回调函数。这是一种避免等待的方法，因为call_rcu会立即返回。\n但是另一方面不推荐使用call_rcu，因为如果内核大量的调用call_rcu，那么保存call_rcu参数的列表就会很长，这意味着需要占用大量的内存，因为每个列表元素都包含了一个本该释放的指针。\n在一个极端情况下，如果你不够小心，大量的调用call_rcu可能会导致系统OOM，因为所有的内存都消耗在这里的列表上了。所以如果不是必须的话，人们一般不会使用call_rcu。\n学生提问：这里的机制阻止了我们释放某些其他人还在使用的对象，但是并没有阻止数据读取者看到更新了一半的数据，对吗？\nRobert教授：23.3中的基本实现阻止了你说的情况，在23.3中，我们并不是在原地更新链表元素，如果是的话绝对会造成你说的那种情况。\nRCU不允许在原地更新数据，它会创建一个新的数据元素然后通过单个committing write替换原有数据结构中的旧数据元素。因为这里的替换是原子的，所以数据读取者看不到更新了一半的数据。\n学生提问：上面提到的条件1，是不是意味着我们必须关注在RCU read crtical区域内的代码执行时间，因为它限制了CPU核在这个区域内不能context switch？\nRobert教授：是的，在RCU区域内，数据读取者会阻止CPU发生context switch，所以你会想要让这个区域变得较短，这是个需要考虑的地方。\nRCU使用的方式是，在Linux中本来有一些被普通锁或者读写锁保护的代码，然后某人会觉得锁会带来糟糕的性能问题，他会将Locking区域替换成RCU区域，尽管实际中会更加复杂一些。\nLocking区域已经尽可能的短了，因为当你持有锁的时候，可能有很多个CPU核在等待锁，所以普通锁保护的区域会尽量的短。因为RCU区域通常是用来替代Lock区域，它也趋向于简短，所以通常情况下不用担心RCU区域的长短。\n这里实际的限制是，数据读取者不能在context switch时持有指针指向被RCU保护的数据，这意味着你不能读磁盘，然后在等读磁盘返回的过程中又持有指针指向被RCU保护的数据。\n所以通常的问题不是RCU区域的长短，而是禁止出让CPU。\nRCU用例代码 为了巩固前面介绍的内容，接下来看一段使用了RCU的简单代码。上半段是读取被RCU保护的链表 ，下半段代码是替换链表的第一个元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //list reader: rcu_read_lock() e = head while(p){ e = rcu_dereference(e) look at e-\u0026gt;x .. e = e-\u0026gt;next } rcu_read_unlock() //replace the first list element: acquire(lock) old = head e = alloc() e-\u0026gt;x = ... e-\u0026gt;next = head-\u0026gt;next rcu_assign_pointer(\u0026amp;head,e) release(lock) synchronize_rcu() free(old) 数据读取位于rcu_read_lock和rcu_read_unlock之间，这两个函数几乎不做任何事情。\nrcu_read_lock会设置一个标志位，表明如果发生了定时器中断，请不要执行context switch，因为接下来要进入RCU critical区域。\n所以rcu_read_lock会设置一个标志位来阻止定时器中断导致的context switch，中断或许还会发生，但是不会导致context switch（注，也就是线程切换）。rcu_read_unlock会取消该标志位。\n所以这是一个集成在RCU critical区域的计数器。rcu_read_lock和rcu_read_unlock因为几乎不做任何工作所以极其的快。\n注，这里有个问题，23.2中描述的读写锁慢的原因是因为在读数据的时候引入了写计数器的操作，这里同样也是需要额外的写操作，为什么这里不会有问题？这是因为读写锁的计数器是所有CPU共享的，而这里的标志位是针对每个CPU的，所以修改这里的标志位并不会引起CPU之间的缓存一致消息\n其中的while循环会扫描链表，rcu_dereference函数会插入memory barrier，它首先会从内存中拷贝e，触发一个memory barrier，之后返回指向e的指针。之后我们就可以读取e指针指向的数据内容，并走向下一个链表元素。数据读取部分非常简单。\n数据写入部分更复杂点。\nRCU并不能帮助数据写入者之间避免相互干扰，所以必须有一种方法能确保一次只能有一个数据写入者更新链表。这里我们假设我们将使用普通的spinlock，所以最开始数据写入者获取锁。 如果我们要替换链表的第一个元素，我们需要保存先保存链表第一个元素的拷贝，因为最后我们需要释放它，所以有old=head。 接下来的代码执行的是之前介绍的内容，首先是分配一个全新的链表元素，之后是设置该链表元素的内容，设置该链表元素的next指针指向旧元素的next指针。 之后的rcu_assign_pointer函数会设置一个memory barrier，以确保之前的所有写操作都执行完，再将head指向新分配的链表元素e。 之后就是释放锁。 之后调用synchronize_rcu确保任何一个可能持有了旧的链表元素的CPU都执行一次context switch，因此这些CPU会放弃指向旧链表元素的指针。 最后是释放旧的链表元素。 这里有件事情需要注意，在数据读取代码中，我们可以在循环中查看链表元素，但是我们不能将链表元素返回。例如，我们使用RCU的时候，不能写一个list_lookup函数来返回链表元素，也不能返回指向链表元素中数据的指针，也就是不能返回嵌入在链表元素中的字符串。\n我们必须只在RCU critical区域内查看被RCU保护的数据，如果我们写了一个通用的函数返回链表元素，或许我们能要求这个函数的调用者也遵循一些规则，但是函数的调用者还是可能会触发context switch。\n如果我们在函数的调用者返回之前调用了rcu_read_unlock，这将会违反23.5中的规则1，因为现在定时器中断可以迫使context switch，而被RCU保护的数据指针仍然被持有者。所以使用RCU的确会向数据读取者增加一些之前并不存在的限制。\n学生提问：这样是不是说我们不可能返回下标是i的元素所包含的内容？\nRobert教授：可以返回一个拷贝，如果e-\u0026gt;x是个字符串，那么我们可以返回一个该字符串的拷贝，这是没有问题的。\n但是如果我们直接返回一个指针指向e-\u0026gt;x，那就违反了RCU规则。实际上返回e中的任何指针都是错误的，因为我们不能在持有指向RCU保护数据的指针时，发生context switch。通常的习惯是直接在RCU critical区域内使用这些数据。\n接下来我将再简短的介绍性能。\n如果你使用RCU，数据读取会非常的快，除了读取数据本身的开销之外就几乎没有别的额外的开销了。如果你的链表有10亿个元素，读取链表本身就要很长的时间，但是这里的时间消耗并不是因为同步（注，也就是类似加锁等操作）引起的。\n所以你几乎可以认为RCU对于数据读取者来说没有额外的负担。唯一额外的工作就是在rcu_read_lock和rcu_read_unlock里面设置好不要触发context switch，并且在rcu_dereference中设置memory barrier，这些可能会消耗几十个CPU cycle，但是相比锁来说代价要小的多。\n对于数据写入者，性能会更加的糟糕。\n首先之前使用锁的时候所有的工作仍然需要做，例如获取锁和释放锁。其次，现在还有了一个可能非常耗时的synchronize_rcu函数调用。实际上在synchronize_rcu内部会出让CPU，所以代码在这不会通过消耗CPU来实现等待，但是它可能会消耗大量时间来等待其他所有的CPU核完成context switch。\n所以基于数据写入时的多种原因，和数据读取时的工作量，数据写入者需要消耗更多的时间完成操作。如果数据读取区域很短（注，这样就可以很快可以恢复context switch），并且数据写入并没有很多，那么数据写入慢一些也没关系。所以当人们将RCU应用到内核中时，必须要做一些性能测试来确认使用RCU是否能带来好处，因为这取决于实际的工作负载。\n你们应该已经看到了RCU并不是广泛通用的，你不能把所有使用spinlock并且性能很差的场景转化成使用 RCU，并获得更好的性能。\n主要的原因是RCU完全帮不到写操作，甚至会让写操作更慢，只有当读操作远远多于写操作时才有可能应用RCU。\n因为RCU有这样的限制：代码不能在sleep的时候持有指针指向被RCU保护的数据，所以这会使得一些代码非常奇怪。当一定要sleep的时候，在sleep结束之后需要重新进入RCU critical区域再次查找之前已经看过的数据，前提是这些数据还存在。所以RCU使得代码稍微复杂了一些。\n另一方面可以直接应用RCU的数据结构在更新时，需要能支持单个操作的committing write。\n你不能在原地更新数据，而是必须创建一个新的链表元素对象来替代之前的元素对象。所以单链表，树是可以应用RCU的数据结构，但是一些复杂的数据结构不能直接使用RCU。\n论文 里面提到了一些更复杂的方法，例如sequence lock，可以在允许原地更新数据的同时，又不用数据读取者使用锁。但是这些方法要复杂一些，并且能够提升性能的场景也是受限的。\n另一个小问题是，RCU并没有一种机制能保证数据读取者一定看到的是新的数据。\n因为如果某些数据读取者在数据写入者替换链表元素之前，获取了一个指针指向被RCU保护的旧数据，数据读取者可能会在较长的时间内持有这个旧数据。大部分时候这都无所谓，但是论文提到了在一些场景中，人们可能会因为读到旧数据而感到意外。\n作为一个独立的话题，你们或许会想知道对于一个写操作频繁的数据该如何提升性能。\nRCU只关心读操作频繁的数据，但是这类数据只代表了一种场景。在一些特殊场景中，写操作频繁的数据也可以获取好的性能，但是我还不知道存在类似RCU这样通用的方法能优化写操作频繁的数据。不过仍然有一些思路可以值得借鉴。\n最有效的方法就是重新构造你的数据结构，这样它就不是共享的。有的时候共享数据完全是没必要的，一旦你发现数据共享是个问题，你可以尝试让数据不共享。\n但是某些时候你又的确需要共享的数据，而这些共享数据并没有必要被不同的CPU写入。\n实际上你们已经在lab中见过这样的数据，在locking lab的kalloc部分，你们重构了free list使得每个CPU核都有了一个专属的free list，这实际上就是将一个频繁写入的数据转换成了每个CPU核的半私有数据。\n大部分时候CPU核不会与其他CPU核的数据有冲突，因为它们都有属于自己的free list。唯一的需要查看其他CPU核的free list的场景是自己的free list用光了。\n有很多类似的例子用来处理内核中需要频繁写入的数据，例如Linux中的内存分配，线程调度列表。对于每个CPU核都有一套独立的线程对象以供线程调度器查看（注，详见11.8，线程对象存储在struct cpu中）。CPU核只有在自己所有工作都完成的时候才会查看其他CPU核的线程调度列表。\n另一个例子是统计计数，如果你在对某个行为计数，但是计数变化的很频繁，同时又很少被读出，你可以重构你的计数器，使得每个CPU核都有一个独立的计数器，这样每个CPU核只需要更新属于自己的计数器。当你需要读取计数值时，你只需要通过加锁读出每个CPU核的计数器，然后再加在一起。\n这些都是可以让写操作变得更快的方法，因为数据写入者只需要更新当前CPU核的计数器，但是数据读取者现在变得更慢了。如果你的计数器需要频繁写入，实际上通常的计数器都需要频繁写入，通过将更多的工作转换到数据读取操作上，这将会是一个巨大的收益。\n这里想说的是，即使我们并没有太讨论，但是的确存在一些技术在某些场合可以帮助提升需要频繁写入数据的性能。\n最后总结一下，论文中介绍的RCU对于Linux来说是一个巨大的成功。\n它在Linux中各种数据都有使用，实际中需要频繁读取的数据还挺常见的，例如block cache基本上就是被读取，所以一种只提升读性能的技术能够应用的非常广泛。\n尽管已经有了许多有趣的并发技术，同步（synchronization）技术，RCU还是很神奇，因为它对数据读取者完全去除了锁和数据写入（注，这里说的数据写入是指类似读写锁时的计数值，但是RCU在读数据的时候还是需要写标志位关闭context switch，只是这里的写操作代价并不高），所以相比读写锁，RCU是一个很大的突破。\nRCU能工作的核心思想是为资源释放（Garbage Collection）增加了grace period，在grace period中会确保所有的数据读取者都使用完了数据。所以尽管RCU是一种同步技术，也可以将其看做是一种特殊的GC技术。\n学生提问：为什么数据读取者可以读到旧数据呢？在RCU critical区域里，你看到的应该就是实际存在的数据啊？\nRobert教授：通常来说这不是个问题。通常来说，你写代码，将1赋值给x，之后print ”done“。\n在print之后，如果有人读取x，可能会看到你在将1赋值给x之前x的数值，这里或许有些出乎意料。而RCU允许这种情况发生，如果我们在使用RCU时，并将数据赋值改成list_replace，将包含1的元素的内容改成2。\n在函数结束后，我们print ”done“。\n如果一些其他的数据读取者在查看链表，它们或许刚刚看到了持有1的链表元素，之后它们过了一会才实际的读取链表元素内容，并看到旧的数值1（注，因为RCU是用替换的方式实现更新，数据读取者可能读到了旧元素的指针，里面一直包含的是旧的数值）。\n所以这就有点奇怪了，就算添加memory barrier也不能避免这种情况。不过实际上大部分场景下这也没关系，因为这里数据的读写者是并发的，通常来说如果两件事情是并发执行的，你是不会认为它们的执行顺序是确定的。\n但是论文中的确举了个例子说读到旧数据是有关系的，并且会触发一个实际的问题，尽管我并不太理解为什么会有问题。\n学生提问：RCU之所以被称为RCU，是因为它的基本实现对吧？\nRobert教授：Read-Copy-Update，是的我认为是因为它的基本实现，它不是在原地修改数据，你是先创建了一个拷贝再来更新链表。\n学生提问：在介绍读写锁时，我们讨论了为了实现缓存一致需要O(n^2)时间。对于spinlock这是不是也是个问题，为什么我们在之前在介绍spinlock的时候没有讨论这个问题，是因为spinlock有什么特殊的操作解决了这个问题吗？\nRobert教授：并没有，锁的代价都很高。如果没有竞争的话，例如XV6中的标准spinlock会非常快。但是如果有大量的CPU核在相同的时候要获取相同的锁就会特别的慢。存在一些其他的锁，在更高负载的时候性能更好，但是在更低负载的时候性能反而更差。这里很难有完美的方案。\n学生提问：或许并不相关，可能存在不同操作系统之间的锁吗？\nRobert教授：在分布式系统中，有一种锁可以存在于多个计算机之间。\n一个场景是分布式数据库，你将数据分发给多个计算机，但是如果你想要执行一个transaction，并使用分布在多个计算机上的数据，你将需要从多个计算机上收集锁。\n另一个场景是，有一些系统会尝试在独立的计算机之间模拟共享内存，比如说一个计算机使用了另一个计算机的内存，背后需要有一些工具能够使得计算机之间能交互并请求内存。\n这样就可以在一个集群的计算机上运行一些现有的并行程序，而不是在一个大的多核计算机上，这样成本会更低。\n这时需要对spinlock或者任何你使用的锁做一些额外的处理，人们发明了各种技术来使得锁能很好的工作，这些技术与我们介绍的技术就不太一样了，尽管避免性能损失的压力会更大。\n","date":"2024-10-08T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08123rcu/bg_hu17832527176933493116.jpg","permalink":"https://echudet.github.io/p/mit6s08123rcu/","title":"MIT6S081：23RCU"},{"content":"Lecture 22 Meltdown 课前预习 论文速览 摘要部分概述了Meltdown漏洞的关键概念。Meltdown利用现代处理器的乱序执行特性来从用户空间读取内核内存数据。它不依赖于任何软件漏洞，而是利用硬件层面的设计缺陷。这一漏洞广泛影响了个人计算机、云服务器中的虚拟机等设备。文章还特别提到KAISER防御机制在阻止Meltdown方面的意外效果。漏洞的严重性在于，它打破了内存隔离的安全保证，使攻击者能够读取其他进程或虚拟机的内存。\n引言部分深入讨论了现代操作系统中内存隔离的重要性，特别是用户进程和内核之间的隔离。文章指出，Meltdown漏洞破坏了这种隔离机制，使攻击者能够访问内核内存。这一漏洞不依赖操作系统或软件漏洞，而是通过利用处理器硬件特性来实施攻击。引言还强调了Meltdown可以在不需要权限的情况下读取敏感数据，这对个人计算机用户和云计算环境中的用户都构成了重大威胁。\n背景部分介绍了Meltdown攻击所依赖的关键技术，包括乱序执行、地址翻译和缓存攻击。乱序执行是现代处理器的性能优化机制，它允许CPU提前执行指令以提高效率，然而这种执行方式会带来缓存的副作用，成为信息泄露的基础。地址翻译部分解释了虚拟地址如何映射到物理地址，并描述了内核地址如何与用户空间隔离。缓存攻击则涉及利用缓存的时序差异来推断内存中的数据。\n玩具示例提供了一个简单的代码示例，用以演示乱序执行如何通过缓存的副作用泄露数据。代码中，虽然某些指令会由于异常而未实际执行，但其带来的缓存变化仍然能够被利用来推测数据。这一部分的核心在于展示了即使未完成的指令也可能对微架构状态产生影响，而这正是Meltdown攻击的基础。\n攻击组成部分描述了Meltdown的两个主要组成部分。首先是“瞬态指令”的执行，这些指令是由于乱序执行而提前执行的，尽管它们在架构层面上没有影响，但它们可以通过微架构层次泄露数据。其次是通过隐蔽通道将泄露的信息传递到外部世界。文章详细解释了如何通过这些瞬态指令访问和泄露内核中的数据。\nMeltdown攻击部分具体介绍了如何在实际系统中实施攻击。通过利用乱序执行，攻击者可以访问操作系统的内核内存，包括敏感的用户数据、密码等。Meltdown能够在多种操作系统上进行攻击，包括Linux、Windows和Android。文中还描述了在这些系统中读取整个物理内存的方法，并讨论了攻击的速度和成功率。\n评估部分通过实验验证了Meltdown的攻击效果。实验在不同的硬件平台上进行了攻击测试，结果表明攻击的成功率和速度与缓存状态密切相关。评估数据展示了在最佳情况下，攻击者可以以数百KB每秒的速度读取内核内存数据，并且攻击在不同操作系统上的表现也有所不同。\n局限性部分讨论了Meltdown在ARM和AMD处理器上的效果。虽然Meltdown能够在大多数Intel处理器上成功运行，但在某些ARM和AMD架构上，Meltdown无法被复现。文章推测这是由于这些处理器在处理权限检查和内存访问时的不同实现方式所致。\n防御措施部分概述了针对Meltdown的可能防御手段。主要提到了通过硬件和软件改进来阻止攻击的方式。硬件设计的改变可以有效阻止此类漏洞的再次出现，例如通过严格的内存访问权限检查。文章特别提到，KAISER补丁是目前最有效的短期解决方案，因为它阻止了攻击者访问内核内存。\n相关工作部分回顾了与Meltdown相关的研究，包括之前的缓存侧信道攻击和推测执行攻击。文中强调了Meltdown与这些攻击的区别，并说明了Meltdown在攻击范围和成功率上的独特之处。\n结论部分总结了Meltdown漏洞对现代计算系统的严重威胁，并呼吁立即采取防御措施，特别是部署KAISER补丁。文章还指出，长期的解决方案可能需要从硬件设计的角度进行修复，以防止未来类似漏洞的产生。\n侧信道攻击（Side-Channel Attack） 侧信道攻击（Side-Channel Attack）是一类通过分析计算机系统中硬件、软件执行时产生的物理信息或微观行为特征来获取敏感信息的攻击手段，而不是通过传统的软件漏洞或直接的破解加密算法等方式进行攻击。侧信道攻击通常利用处理器执行时的时序、功耗、电磁辐射、缓存访问行为等外部特征来推测出某些敏感数据。\n在论文中，讨论的是乱序处理器（out-of-order processor）的缓存侧信道攻击，并且是与Meltdown漏洞相关。这涉及到现代处理器中复杂的执行优化技术，以及这些技术在某些极端情况下可能暴露的信息安全隐患。\nMeltdown 漏洞是什么？Meltdown 是在 2018 年公开的一类严重的硬件漏洞，主要影响英特尔的处理器（也有部分 AMD 和 ARM 处理器）。该漏洞利用了现代处理器的乱序执行机制，允许攻击者在未授权的情况下读取内核空间或其他进程中的敏感数据。\n乱序执行是处理器为了提高性能，提前执行一些看似可以立即进行的指令，即使这些指令的前置条件（如内存访问权限检查）还未完成。而 Meltdown 恰恰利用了这个特性：虽然最终系统会撤销未经授权的操作，但在执行期间，数据可能已经进入了处理器的缓存中，从而留下痕迹。\n缓存侧信道攻击是什么？侧信道攻击常与缓存的行为密切相关。缓存是一种快速存取的数据存储区域，存放处理器频繁访问的数据。不同级别的缓存（L1、L2、L3）会极大地影响程序的执行效率和数据访问时延。\n在 Meltdown 攻击中，攻击者利用乱序执行机制，通过分析内存访问时引发的缓存行为，泄露出敏感信息。具体来说，缓存侧信道攻击可以通过缓存命中与失效的时序差异来判断某些内存单元是否已被访问（即是否已加载到缓存中）。这可以通过类似于Flush+Reload或Prime+Probe这样的攻击技术实现：\nFlush+Reload：攻击者使用 clflush 指令清空特定的缓存行，然后观察该缓存行的加载时延。如果攻击目标的某个内存地址被加载回缓存，攻击者通过时延测量可以推断出相关数据。 Prime+Probe：攻击者在缓存中占据一定的空间，然后测量其他程序对该缓存区域的影响，进而推测程序行为。 Meltdown 漏洞与缓存侧信道攻击的结合非常关键。Meltdown 攻击本质上是通过乱序执行让处理器提前访问到本不应被访问的数据，虽然这些数据无法直接被读取或使用，但它们会短暂存储在缓存中。然后，攻击者可以通过侧信道分析技术（如缓存时序分析）推测出这些数据。\n例如，攻击者在利用 Meltdown 读取未经授权的内核空间数据时，可以在数据进入缓存后，使用缓存时序分析来判断访问了哪些内存地址，以及这些地址存放了什么数据。\n所以，在论文讨论的对乱序处理器缓存进行侧信道攻击，是在描述一种通过缓存行为监控的方式，利用处理器在乱序执行过程中暴露的安全漏洞（如 Meltdown），从而窃取敏感数据的攻击手段。\n这种攻击方式的流程可能如下：\n攻击者诱使处理器触发乱序执行。 在处理器访问未经授权的内核空间或其他进程的数据时，这些数据会暂时存入缓存中。 攻击者通过缓存侧信道技术（如 Flush+Reload 或 Prime+Probe）分析缓存行为，获取敏感数据的相关信息。 这种类型的攻击利用了硬件的特性（如乱序执行、缓存行为）以及微架构中的信息泄漏漏洞，超越了传统的安全模型，因此在公开时引发了广泛的关注和讨论。\nMeltdown + 侧信道攻击 的完整过程 从攻击的准备开始，逐步展示攻击者是如何利用处理器的乱序执行特性、缓存行为以及侧信道分析来泄露内核或其他敏感数据的。\n攻击准备：隔离的内存空间\n在现代操作系统中，用户进程和内核空间（包括操作系统的关键数据和其他进程的内存）是隔离的。通常情况下，用户进程不能直接访问内核空间的数据，因为这种访问会触发页面故障（page fault）并**作系统阻止。\n但是，Meltdown 攻击利用处理器的乱序执行特性，使得在某些情况下，即使内核数据不应被用户态进程访问，处理器仍可能提前执行读取内核内存的指令——即使最终这些指令会因为权限问题被回滚。\n利用乱序执行访问未经授权的内核数据\nMeltdown 的核心在于处理器的乱序执行机制。乱序执行允许处理器为了优化性能，提前执行一些指令，而不必等待前面的指令完成。例如，当处理器等待较慢的内存访问完成时，它可以提前执行某些后续指令。\n在 Meltdown 攻击中，攻击者可以诱使处理器访问属于内核空间的数据，尽管这类访问通常是不被允许的。这可以通过构造如下代码实现：\n1 unsigned char kernel_data = *(unsigned char*)kernel_address; 此代码试图从内核地址 kernel_address 读取数据。尽管内核数据通常不能被用户态程序直接访问，处理器可能会通过乱序执行提前读取该数据。但是，一旦处理器发现该读取操作无权限，它将撤销此操作，并触发页面故障异常。但在 Meltdown 攻击中，尽管读取的结果不能被直接使用，该数据已被暂时存入缓存中。\n数据进入缓存，留下痕迹\n乱序执行导致的数据被存储到缓存，即使随后这些数据被撤销且处理器不允许访问它们。然而，这些内存访问留下了一个副作用：数据被加载到了缓存中。此时，数据的物理内容可能无法被直接访问，但缓存的行为泄露了这些信息。\n通过缓存侧信道攻击获取数据\n接下来，攻击者利用缓存侧信道技术，如 Flush+Reload，来提取数据。这个步骤关键在于，通过测量缓存的访问时延，攻击者可以推测哪些数据已经加载到了缓存中，从而间接推测内核数据。\nFlush+Reload 攻击机制的具体步骤：\n步骤 1：清空缓存 攻击者首先使用 clflush 指令清空缓存中的特定内存行。通过清空缓存，攻击者确保之后的访问会重新加载数据到缓存中，从而使得缓存命中与失效的时间差异显著。 步骤 2：诱导乱序执行读取内核数据 通过前面提到的代码，诱导处理器乱序执行去读取未经授权的内核数据。尽管最终读取会被操作系统阻止，但在短时间内该数据已经被加载到缓存中。 步骤 3：测量时延，推测数据 攻击者再次尝试访问缓存中被加载的数据，通过测量访问的时间差，可以推测出这些数据是否已经在缓存中。如果数据已经被缓存，访问时间会非常短（命中缓存）；如果数据没有被缓存，访问时间则会显著增加（缓存未命中，需要从内存加载）。通过这种时延分析，攻击者可以逐字节推测内核数据的值。 整个流程的总结\n从上面的步骤可以看出，Meltdown + 侧信道攻击的关键点在于处理器的乱序执行和缓存行为。攻击者利用乱序执行机制读取到不应访问的内核数据，并利用缓存时延差异通过侧信道技术来间接推测数据的内容。这个攻击过程分为以下主要阶段：\n触发乱序执行：攻击者构造代码，迫使处理器访问未经授权的内核数据。 数据进入缓存：虽然访问被撤销，但数据暂时存入缓存。 缓存时延分析：通过 Flush+Reload 等侧信道攻击技术，利用缓存命中与失效的时间差，攻击者间接获取敏感数据。 例子：泄露内核空间中的一个字节\n假设攻击者想要泄露内核地址 0xFFFF0000 中的数据，该地址包含一个字节的敏感数据。\n首先，攻击者清空缓存中的某些内存行，确保后续缓存行为是可测量的。\n通过乱序执行读取内核数据：\n攻击者构造代码，如：\n1 2 unsigned char kernel_data = *(unsigned char*)0xFFFF0000; array[kernel_data * 4096] = 1; 这个操作试图从内核地址读取一个字节，并将其值用于访问一个数组。这段代码会被乱序执行提前处理，而敏感数据会被存入缓存。\n缓存中的数据被测量：\n由于内存访问时敏感数据已经进入缓存，攻击者通过测量缓存时延来判断 array[kernel_data * 4096] 是否已经在缓存中。 通过多个猜测，最终可以推断出 kernel_data 的实际值。 总而言之，Meltdown 攻击利用了处理器的乱序执行，允许攻击者访问通常无法读取的内核数据，而缓存侧信道攻击则为这些数据的提取提供了有效手段。通过缓存命中和未命中的时延差异，攻击者能够逐步推测出内核空间中的数据，尽管系统本身阻止了未经授权的访问。\n数据读取过程（微架构状态如何转换为架构状态） 虽然侧信道可以让我们知道缓存的某个位置是否被加载了数据，但如何从缓存中真正推测出数据的值似乎还不够清晰。确实，侧信道攻击通过分析缓存访问时延，能够推测哪些数据被缓存，但问题是如何利用这些信息来推断具体的数值。在下面，我将进一步细化这个过程，以解释数据具体是如何从缓存中读取出来的。\n关键点：使用缓存时延推测数据值\n侧信道攻击本质上不是直接读取数据，而是通过缓存行为推测出数据的内容。在 Meltdown + 侧信道攻击中，缓存的命中或失效是由我们控制的代码触发的，而我们可以利用这个控制来获取敏感数据。\n为了更直观地理解，一个具体的例子将详细解释，如何通过缓存时延分析逐字节提取敏感数据。\n攻击环境准备，假设攻击者正在尝试通过 Meltdown 漏洞访问内核空间中某个敏感字节的值。为了实现这一目的，攻击者构建如下代码：\n1 2 unsigned char kernel_data = *(unsigned char*)kernel_address; array[kernel_data * 4096] = 1; kernel_address 是攻击者试图访问的敏感内核数据的地址。\narray[] 是一个攻击者可控的数组，大小足够大，通常为 256 * 4096 字节，这样可以利用每个缓存行（通常是 64 字节）的不同位置来探测数据。\n关键在于 kernel_data 是从内核空间读取的未经授权的数据。虽然处理器最终会拒绝这一访问并触发页面故障（权限不允许访问），但在触发异常之前，乱序执行可能已经将数据暂时加载到缓存中，并用于后续的代码执行。\n使用乱序执行加载数据，当处理器提前执行这段代码时，kernel_data 的值会用于数组 array[] 的索引计算。即使最终这个操作因为权限问题被撤销，处理器已经使用了 kernel_data 的值，并将它反映在缓存行为中。\n测量缓存时延来推测数据，关键步骤来了：攻击者如何通过缓存侧信道推测出 kernel_data 的具体值？这就是通过测量缓存时延来完成的，通常使用类似 Flush+Reload 或 Prime+Probe 技术。以 Flush+Reload 为例，具体操作步骤如下：\n清空缓存（Flush）：在执行上述代码之前，攻击者先通过 clflush 指令清空缓存中 array[] 的所有内容。这样可以确保接下来的操作导致的数据访问都会重新加载。\n执行代码，触发乱序执行：处理器在运行乱序执行时，提前读取了 kernel_address 并且把这个读取的结果 kernel_data 作为索引用于访问 array[kernel_data * 4096]。此时，虽然实际的 kernel_data 未被攻击者直接读取到，但它的值影响了数组访问的缓存命中。\n测量时延（Reload）：代码执行后，攻击者尝试依次访问 array[] 的不同部分（每个间隔 4096 字节），并测量每次访问的时延。由于缓存命中时访问会比缓存失效时更快，攻击者可以通过时延分析发现哪一部分的 array[] 访问速度明显较快。\n如果某个地址 array[kernel_data * 4096] 的访问非常快（即缓存命中），攻击者就可以推测 kernel_data 的值为该部分的索引。 具体例子：逐字节提取数据。假设 kernel_address 指向的内存地址包含一个字节的值，例如 0x41（ASCII 对应 \u0026lsquo;A\u0026rsquo;）。此时，kernel_data = 0x41。攻击者通过以下方式逐步推测出 kernel_data 的值：\n处理器执行代码 array[kernel_data * 4096] = 1;，此时 array[0x41 * 4096] 被访问，导致这部分数据加载到缓存中。 攻击者接着通过 clflush 清除缓存，并开始依次访问 array[0 * 4096], array[1 * 4096], ..., array[255 * 4096]，测量每个访问的时延。 当攻击者访问 array[0x41 * 4096] 时，发现该访问非常快（缓存命中），因此可以推测出 kernel_data = 0x41，即这个字节的值是 0x41。 整个攻击过程的核心是：通过乱序执行使得敏感数据进入缓存，并通过缓存时延分析侧信道技术，间接推测出这些敏感数据的值。\n侧信道分析并不是直接读取缓存中的数据，而是通过控制访问和测量缓存时延，来推测哪些数据在缓存中被命中了，从而一步步推导出 kernel_data 的值。\n完整过程回顾\n诱导乱序执行：通过访问内核数据并构建某种操作，使得数据暂时存入缓存。 清空缓存：在测量之前清除缓存，以确保后续时延测量的准确性。 测量缓存时延：通过访问特定缓存行并测量访问时间差，推测缓存中已命中的数据值。 逐字节推测内核数据：通过这种方式，可以字节级地推测出敏感数据。 通过这种缓存时延分析，Meltdown 能够从无法直接读取的内存区域中逐字节地提取出数据。\n预测执行 为了确保 预测执行（或称为乱序执行）一定发生，特别是在你提到的循环中，可以采取一些策略，这些策略能够诱导处理器执行提前加载（prefetching）或预测性执行，以提高成功的概率。在详细解释之前，我们先定义一些相关概念：\n预测执行与乱序执行\n预测执行（Speculative Execution）是处理器在不确定某个条件是否成立时，提前执行可能需要执行的指令。比如，在条件判断（如 if 或 while）之前，处理器可能会提前执行某些后续的指令，假设条件为真或假。\n乱序执行（Out-of-Order Execution）是指处理器根据当前的资源可用性、指令的依赖关系等，提前执行不依赖于当前正在等待的指令的部分操作。这种执行机制有助于提高处理器的吞吐量。\n如何确保预测执行一定发生，为了确保预测执行在循环（或其他控制结构）中尽可能发生，可以设计代码，使处理器尽量进入预测执行路径。以下是常用的策略：\n使用条件分支和预测，预测执行经常发生在条件分支的地方，特别是 if-else 或 switch 语句。处理器通过预测某个条件将会成立（例如分支预测器认为 if 条件为真），进而提前执行某些指令。为了确保预测执行可以发生，你可以故意引入复杂的条件判断： 1 2 3 4 5 for (int i = 0; i \u0026lt; 1000; i++) { if (condition) { // 关键指令在这里执行 } } 这种情况下，处理器会根据历史记录和分支预测算法，决定是否提前执行 if 分支中的代码。你可以通过构造复杂且动态的条件，让预测执行变得更加可能。\n控制指令顺序，确保代码中有足够多的指令可以并行执行，特别是在循环体中。当前面的指令可能导致内存访问等需要等待的操作时，处理器会试图执行后续的指令。这种环境下，乱序执行也会发生。你可以在循环中引入一些独立的指令，从而确保处理器在等待某些内存访问时，仍然可以预测性地执行其他指令。 prefetch 的含义，Prefetch 是指提前加载数据到缓存中，目的是减少内存访问时的延迟。现代处理器会自动进行预取操作，但程序员也可以手动触发预取指令，将将来可能需要的数据提前加载到缓存中。它的主要作用是提高程序的性能，避免因访问主内存而产生的高延迟。\n在具体操作中，预取可以通过硬件自动完成，也可以通过显式的指令实现，比如 __builtin_prefetch()，它允许程序员手动指定要提前加载的数据。这在需要频繁访问大量数据的情况下尤其有用。\n1 __builtin_prefetch(\u0026amp;data, 0, 3); 这里 \u0026amp;data 是预取的数据地址，0 表示这是只读预取，3 是预取的优先级。通过预取，程序可以在需要数据之前将它加载到缓存中，避免内存访问延迟。\n如何结合预取与预测执行，在使用 Meltdown 或其他类似攻击时，如果你希望诱导处理器提前加载某些数据（例如敏感数据）到缓存中，你可以：\n利用循环和条件分支来确保乱序执行或预测执行。\n结合预取，让处理器将目标内存数据提前加载到缓存中。\n例如，你可以在一个循环中使用 __builtin_prefetch() 来让处理器提前加载内核中的某些内存地址。尽管这些内核地址在用户态不允许直接访问，但利用 Meltdown 类漏洞，攻击者可以通过侧信道技术推测它们的内容。\n假设你有一个想要预取的内核地址 kernel_address，你可以在循环中手动预取该地址：\n1 2 3 4 for (int i = 0; i \u0026lt; 1000; i++) { __builtin_prefetch((void*)kernel_address, 0, 3); // 后续操作触发缓存侧信道攻击 } 总结\n预测执行可以通过复杂的条件判断和控制指令顺序来诱发，特别是在循环和分支结构中，它能提高触发 Meltdown 类攻击的可能性。 乱序执行是处理器自动进行的一种优化，程序设计中可以通过提供独立的操作来诱导它提前执行某些指令。 Prefetch 是提前将数据加载到缓存的手段，可以手动指定特定的内存地址进行预取，从而在攻击中加快对缓存状态的推测。 Flush+Reload Flush+Reload 是一种典型的缓存侧信道攻击方法，广泛应用于如 Meltdown、Spectre 等漏洞的利用中。它主要利用了共享内存（如共享库或页面）中的缓存行为，结合缓存命中与失效的时延差异，来推测内存中的数据内容。Flush+Reload 的核心在于通过清除缓存和测量缓存命中时的时延，获取受害者程序对特定内存区域的访问情况。\nFlush+Reload 攻击依赖于现代处理器的多级缓存体系和内存共享机制。其基本工作流程如下：\nFlush（清空缓存）：攻击者首先通过 clflush 指令将目标内存地址对应的缓存行从缓存中移除，确保接下来对该地址的访问会从主内存重新加载，而不是从缓存读取。 Victim 访问（受害者访问）：受害者程序运行期间，可能会访问这个内存地址。当受害者访问该地址时，数据会被加载到缓存中，而这个缓存行为是攻击者所无法直接观察到的。 Reload（重新加载并测量时延）：接着，攻击者再次访问该地址，并测量访问的时延。如果访问时延较短，说明数据已经在缓存中（受害者程序访问过该地址）；如果时延较长，说明缓存未命中，数据需要从主内存加载。 Flush+Reload 的详细流程\n准备阶段：确定共享内存区域。Flush+Reload 攻击通常需要一个共享的内存区域，攻击者和受害者程序都可以访问这一内存区域。这可以是一个共享的库（如 .so 文件），或者是一个共享的内存页面。在 Meltdown 攻击中，攻击者可能会试图访问内核空间的某个地址。\nFlush：清除缓存行。攻击者首先使用 clflush 指令将目标地址对应的缓存行从缓存中清除。clflush 是一个处理器指令，它会强制清空某个内存地址所在的缓存行，无论该地址是否在 L1、L2 或 L3 缓存中。\n1 clflush(\u0026amp;shared_memory_address); // 清空 shared_memory_address 对应的缓存行 通过这一步操作，攻击者确保缓存中没有目标地址的数据，确保接下来对该地址的访问一定会引发缓存行为（缓存命中或未命中）。\n受害者程序执行。在攻击者执行 clflush 后，受害者程序继续执行。在它的执行过程中，如果受害者程序访问了之前被清空缓存的那个地址，数据会被重新加载到缓存中。\n这一步对于攻击者是不可见的，攻击者无法直接知道受害者程序是否访问了该地址，但这正是 Flush+Reload 要通过测量时延来推测的部分。\nReload：访问并测量时延。攻击者接下来会重新访问刚刚清空缓存的那个内存地址，并测量访问的时延。这里使用类似 rdtsc（读取时间戳计数器）的指令来精确计量该地址的访问时延。\n缓存命中：如果受害者程序访问了该地址，数据已经被加载到缓存中，那么攻击者的访问会非常快，通常在几个 CPU 周期内完成。\n缓存未命中：如果受害者程序没有访问该地址，数据没有被加载到缓存中，攻击者的访问会较慢，因为处理器需要从主内存重新加载数据。\n代码示例：\n1 2 3 4 start_time = rdtsc(); access(shared_memory_address); // 访问目标内存地址 end_time = rdtsc(); time_diff = end_time - start_time; 通过访问时延的差异，攻击者可以推测受害者程序是否访问了该地址。如果访问时间短，则说明该地址已经在缓存中（缓存命中）；如果访问时间长，则说明该地址未被缓存（缓存未命中）。\n多次重复，推测数据。Flush+Reload 攻击的威力在于，它可以通过多次采样，并结合对多个地址的访问时延，逐步推测出受害者程序访问了哪些数据，并且可能进一步推测这些数据的值。\n在某些攻击场景中，例如 Meltdown，攻击者会构造某种数据依赖关系，使得内核数据（或其他敏感数据）加载到缓存中，通过时延差异来推测这些数据。\nFlush+Reload 的应用场景\n加密算法泄露：Flush+Reload 经常被用来攻击基于内存访问模式的加密算法（如 AES）。攻击者可以监控加密算法在执行过程中对内存的访问，从而推测出加密密钥的部分或全部。 Meltdown/Spectre 攻击：Flush+Reload 在 Meltdown 和 Spectre 攻击中被广泛使用。Meltdown 攻击诱导处理器乱序执行访问不该访问的内核空间数据，而 Flush+Reload 则用来通过缓存时延分析间接提取这些数据。 共享库攻击：攻击者可以通过 Flush+Reload 分析共享库的缓存行为，监控其他程序对共享库函数的调用情况，从而推测出程序的行为或数据。 优点 局限 精确性高：Flush+Reload 的缓存时延测量非常精确，可以区分出极小的访问时间差异。 需要共享内存：Flush+Reload 依赖于攻击者和受害者程序共享某个内存区域，才能有效地监控缓存行为。 高效性：因为只需通过测量缓存行为进行推测，Flush+Reload 可以在无需直接访问内存数据的情况下泄露信息。 防御机制有效：现代操作系统和处理器在修复 Meltdown、Spectre 等漏洞时，采取了一些防御机制，如缓存隔离、限制缓存刷新指令的使用等，可能会降低 Flush+Reload 的效果。 总的来说，Flush+Reload 是一种强大的缓存侧信道攻击方法，利用缓存时延分析推测内存访问行为。它通过 clflush 指令清空缓存、受害者程序访问内存、攻击者重新加载数据并测量时延，来判断受害者程序是否访问了特定的内存区域。这种方法在加密算法泄露和 Meltdown、Spectre 等漏洞的利用中具有广泛的应用，但它也需要依赖共享内存和较高精度的时延测量技术。\nFlush+Reload 查询对象 在 Flush+Reload 攻击中，查询的是缓存中的数据，而不是直接查询页表或页表中的缓存行。Flush+Reload 的核心是基于缓存行的状态（命中或未命中），而不涉及具体的页表结构。\n为了更清楚地解释这一点，让我们深入了解 Flush+Reload 的运作机制：\n缓存与内存的关系\n在现代处理器中，内存访问的速度相对较慢，而缓存（L1、L2、L3）是一种高速的存储层，用于存放处理器频繁使用的数据。缓存以 缓存行（通常 64 字节）的单位进行管理。\n当处理器访问某个内存地址时，它首先会检查该地址是否已经在缓存中：\n命中（Cache Hit）：如果该数据已经在缓存中，处理器可以直接读取，速度极快。\n未命中（Cache Miss）：如果缓存中没有该数据，处理器需要从更慢的主内存中加载，造成较大的时延。\nFlush+Reload 的基本原理\nFlush+Reload 是一种缓存侧信道攻击技术，它的工作原理基于对特定缓存行的命中与未命中状态的监控。具体操作步骤如下：\nFlush：首先，攻击者使用 clflush 指令，将目标内存地址对应的缓存行从缓存中移除，确保缓存行处于未命中的状态。这个目标地址通常指向一个共享的内存区域，比如一个共享的库或页面（在 Meltdown 攻击中，可能是敏感数据的地址）。\nVictim 访问：然后，受害者程序可能会访问这个内存地址。这会导致数据被重新加载到缓存中，但攻击者此时不知道受害者是否真的访问了这个地址。\nReload：攻击者随后再访问相同的内存地址，并测量访问时延。如果访问速度非常快（缓存命中），攻击者可以推测该地址已经被受害者程序访问并加载到缓存中。\n与物理页表无关\nFlush+Reload 攻击关注的是内存地址对应的缓存行，而不是查询或直接操作物理页表或页表缓存。\n页表是操作系统管理虚拟内存和物理内存映射的结构。它决定虚拟地址映射到哪个物理地址。然而，Flush+Reload 的核心关注点是物理内存地址的缓存状态，并不涉及页表的具体内容或查询页表中的缓存行。\n攻击对象是缓存行，而非页表\n在 Flush+Reload 中：\nFlush 清除的是目标内存地址的缓存行。\nReload 通过测量时延，检查是否该地址对应的缓存行已经被加载到缓存中。\n虽然页表决定了虚拟地址和物理地址之间的映射，但在缓存系统中，数据是按照物理地址进行缓存的。因此，当处理器访问某个内存地址时，处理器会先查找这个物理地址对应的缓存行。\nFlush+Reload 主要是查询内存地址对应的缓存行状态（命中或未命中），而不是直接查询物理页表或页表中的缓存行。\n缓存未命中复习 当缓存未命中时，加载的内容不是物理页表本身，而是内存地址对应的实际数据。具体来说：\n未命中时，处理器会从主内存中将目标内存地址对应的数据块（即缓存行）加载到缓存中，而不是加载物理页表。缓存行通常是内存中某个物理地址的一小块（比如 64 字节）。 页表的作用是将虚拟地址映射到物理地址。当处理器需要访问某个虚拟地址时，它首先通过页表查找该虚拟地址对应的物理地址。这个查找过程会通过 TLB（Translation Lookaside Buffer） 来加速。 如果TLB 命中，处理器可以快速得到物理地址，然后访问该物理地址并加载相应的数据到缓存中。 如果TLB 未命中，处理器需要从页表中查找对应的物理地址，这会导致页表的查找过程。而这个过程通常涉及访问页表（页表存放在主内存中），然后将查找到的物理地址对应的数据加载到缓存中。 缓存行（Cache Line）：\n缓存是按块（即缓存行）来管理的，每个缓存行对应内存中连续的一段数据。 例如，当处理器访问某个内存地址时，如果该地址所在的缓存行未命中，则处理器会从主内存中加载整个缓存行（如 64 字节）到缓存，而不是只加载单个字节或单个指令。 空间局部性原理：\n缓存设计通常基于空间局部性原理，这意味着如果处理器访问了某个内存地址，它很有可能会在接下来访问相邻的地址。因此，处理器每次加载一个较大的数据块（即缓存行）到缓存中，而不是只加载一个特定字节或单个数据项。 假设处理器访问内存地址 0x1000，该地址对应的缓存行未命中：\n如果缓存行大小为 64 字节，处理器会从 0x1000 开始加载一段内存，可能从 0x1000 到 0x103F（64 字节的范围）。 整个缓存行（64 字节）会被存储在缓存中，之后处理器对这些地址范围内的数据访问就可以直接从缓存中读取，避免再次访问主内存。 缓存行大小的选择:\n常见缓存行大小：典型的缓存行大小为 64 字节，但某些系统可能会有不同的缓存行大小，如 32 字节或 128 字节。这种大小通常是处理器设计时的固定参数，旨在平衡性能和内存带宽使用。 优势：一次加载较大的数据块有助于提高缓存命中率，并减少处理器与主内存之间的数据传输开销，因为后续访问同一缓存行的相邻数据不需要再从主内存读取。 加载整个缓存物理页表的地方，指的是 TLB（Translation Lookaside Buffer，翻译后备缓冲区），它与虚拟地址和物理地址映射过程中的缓存机制有关。TLB 是一种专用缓存，用于加速虚拟地址到物理地址的转换。\nTLB 是处理器内的一个小型高速缓存，它存储了最近使用的虚拟地址到物理地址的映射，即页表条目。当处理器访问虚拟内存时，TLB 可以直接提供虚拟地址对应的物理地址，而不必每次都查找完整的页表。这样极大地提升了地址转换的效率。\nTLB 缓存的是页表的部分条目，通常是页表中的一组映射关系，而不是整个页表本身。具体来说，每个 TLB 条目包含虚拟页面号（VPN）和物理页面号（PPN）之间的映射关系。每当 TLB 未命中时，处理器需要从内存中的页表中查找该虚拟地址对应的物理地址，并将这一映射加载到 TLB 中，以便加速后续的内存访问。典型的 TLB 加载过程：\nTLB 命中：处理器首先查找 TLB。如果命中，虚拟地址直接被转换为物理地址，继续执行程序。\nTLB 未命中: 如果未命中，处理器会访问内存中的页表结构，找到对应的物理地址映射，然后将这一映射加载到 TLB 中。\n页表是分层的，因此处理器可能需要多次内存访问来查找页表，尤其是在多级页表的系统中。 一旦找到对应的物理页表条目，处理器会将其加载到 TLB 中。 “加载一张缓存的物理页表”是指 TLB 在加载页表条目时，起到了页表缓存的作用。它并不缓存整个页表，而是缓存最近使用的页表条目。因为页表查找相对耗时，通过 TLB 缓存，可以避免每次都进行完整的页表查找。\n假设系统使用的是 4KB 的页（常见的页面大小）：\n虚拟地址访问：处理器访问一个虚拟地址时，首先会查询 TLB 是否存在该地址对应的物理地址映射。 TLB 未命中：如果 TLB 没有该虚拟地址的映射，处理器需要通过页表查找物理地址（这涉及访问内存中的多级页表）。 更新 TLB：查找到物理地址后，该映射会被存入 TLB，后续对该虚拟地址范围内的访问会直接命中 TLB，不再需要查页表。 TLB 通常非常小，可能只有几十到几百个条目，但它的存在可以显著减少处理器访问完整页表的频率。通常情况下，每个 TLB 条目会缓存一整个页面的映射信息。例如，如果页面大小是 4KB，那么一次 TLB 加载可以加速对该 4KB 内存范围内的所有地址访问。\n总的来说，当缓存未命中时，处理器加载的是目标物理地址对应的数据块，即缓存行，而不是直接加载页表。页表的作用是在虚拟地址到物理地址的转换过程中使用，而最终缓存的内容是物理地址对应的数据本身，不是页表结构。一般情况下，处理器在缓存未命中时会从主内存加载一个缓存行的数据，大小通常为 64 字节，这是缓存设计中常用的大小。\n“加载一张缓存物理页表”的地方指的是 TLB，它缓存的是页表中的部分条目，即最近使用的虚拟地址到物理地址的映射。TLB 的作用是加速地址转换，避免每次访问虚拟内存时都查找页表，而是通过缓存常用的页表条目来提高效率。\nMeltdown 的 异常 0 值 在 Meltdown + 侧信道攻击 中，出现“异常读取到 0 值”的情况，通常与以下几个方面有关：\n权限问题导致的异常访问：由于内存访问违规（试图访问不允许的地址，如内核空间），导致读取的数据无效，处理器返回 0 值或触发异常。 缓存行为异常：某些情况下，处理器在非法访问内存时，会导致数据未能正确加载到缓存中，进而在侧信道攻击中读取时得到错误的 0 值。 为了避免在这种攻击中读取到无效的 0 值，需要从以下几个方面进行调整：\n避免异常提前终止执行。Meltdown 攻击依赖于处理器的乱序执行，即使触发了非法内存访问异常，处理器仍然会提前执行部分指令。因此，以下措施可以帮助减少读取到 0 值的可能性：\n限制异常处理\n在进行 Meltdown 攻击时，尽量减少异常的影响。例如，通过构造某种条件，使得异常发生后处理器仍然可以继续执行读取操作。可以使用信号处理机制来捕捉异常，并避免程序在异常发生后立即崩溃。\n方法：使用类似以下代码结构捕捉异常：\n1 2 3 4 5 6 7 8 if (setjmp(buf) == 0) { // 这里执行 Meltdown 攻击的代码 unsigned char kernel_data = *(unsigned char*)kernel_address; array[kernel_data * 4096] = 1; } else { // 异常发生时的处理逻辑 // 可以重试或者记录错误 } 通过 setjmp 和 longjmp，程序可以在发生非法访问异常后返回并继续执行。\n多次采样，减少误差\n由于缓存命中与失效的时间差异较小，有时单次攻击可能因数据加载失败或异常读取到 0。为了解决这个问题，可以使用多次采样的方式：\n多次尝试访问同一个地址，并记录多次访问的结果。\n统计访问时的时延，并通过取平均值或最大值的方式，确定数据是否正确。\n通过多次采样，可以减少单次访问异常带来的误差，增加推测数据的准确性。\n优化侧信道分析。当缓存行为异常，或者数据没有正确加载到缓存时，侧信道技术可能读取到无效数据。在这种情况下，可以优化缓存时延的测量方法，确保对缓存命中状态的判断更准确。\n更精准的时延测量\n有时，测量时延的精度不够高，导致读取的结果不准确。可以通过更精准的计时器（如 rdtsc 指令）来测量缓存访问的时间差，确保能够正确区分缓存命中与未命中。\n设置更明确的触发条件\n通过调整缓存的刷新和测量时机，可以确保攻击中处理器将数据正确加载到缓存中，避免读取到无效的 0 值。例如，控制 clflush 指令的调用时机，以确保在读取敏感数据之前缓存被正确刷新。\n避免处理器的零填充机制。某些现代处理器在检测到非法内存访问时，可能会出于安全考虑返回 0（或其他固定值），而不是读取实际的数据。这是一种防止侧信道攻击的硬件机制。为了应对这种情况，攻击者可以：\n使用较旧的硬件\n某些较旧的处理器没有这种安全机制，因此可以使用这些处理器进行攻击，从而避免数据被零填充。\n探测并绕过零填充\n在硬件中如果存在零填充机制，可以通过探测不同的访问模式，分析哪些数据读取到的 0 值是由于这个机制引起的，进而在攻击逻辑中规避这些地址。\n处理异常返回的零值。在攻击过程中，针对出现的异常情况返回 0 值，可以通过重试策略来解决。每当读取到 0 值时，可以重新尝试访问同一个内存地址，直到获取到非 0 值为止。例如：\n1 2 3 4 unsigned char result; do { result = attempt_to_read_data(kernel_address); } while (result == 0); // 重试直到获取到非 0 的数据 在 Meltdown + 侧信道攻击 中，读取到 0 值的原因可能是由于异常处理、缓存行为或硬件机制。要减少这种情况的发生，可以通过捕捉异常、增加多次采样、优化时延测量等手段改进攻击的准确性。此外，了解处理器的硬件特性也是关键，尤其是在现代处理器中，某些硬件防护机制可能会主动返回 0 或其他无效数据。\n区分内核值 0 与异常 0 在 Meltdown + 侧信道攻击 中，确实存在一种情况：内核中的值本身可能为 0，而你又可能因为异常处理（如权限问题）得到了一个 0。那么，如何区分这两种 0，即区分真实的内存值为 0 和异常导致的 0，是侧信道攻击中的一个挑战。\n可以通过多次采样和时延分析等技术来区分这两种情况。具体的方法如下：\n多次采样。通过多次采样可以有效地减少由于异常返回 0 的影响。基本思路是，如果数据在多次采样过程中稳定地返回 0，那么它更可能是内存中的真实值；而如果采样结果波动不定（有时返回 0，有时返回其他值或异常），那么就可能是由于异常导致的。步骤：\n对同一个内存地址进行多次攻击，测量每次的访问结果。\n如果多次采样都返回 0，可以初步判断内核中的值可能确实是 0。\n如果 0 只是偶尔出现，并且伴随有其他非 0 的返回值，则可以推测该 0 可能是由于异常处理机制导致的。\n缓存时延分析。为了进一步提高区分的精度，缓存时延分析仍然是关键。你可以结合时延测量来帮助判断：\n真实的内存值为 0 时，读取缓存中的 0 值通常伴随着缓存命中，此时访问的时延应该是很短的。\n异常导致返回 0 时，由于处理器未能正常加载该内存地址的数据，缓存未命中时的访问时延通常会更长。\n通过时延测量，攻击者可以识别出缓存命中的情况，从而区分正常的内存访问（真实的 0）和异常情况导致的返回值 0。假设要判断 kernel_address 的值是否为 0：\n刷新缓存：使用 clflush 指令清除缓存中 kernel_address 对应的缓存行。\n执行读取操作：触发 Meltdown 攻击，尝试从 kernel_address 读取数据。\n测量时延：使用 rdtsc（读取时间戳计数器）测量对 kernel_address 的访问时延。\n多次采样: 重复步骤 1-3，采集多个结果。\n如果时延总是很短且返回 0，则说明内存中的真实值可能就是 0。\n如果时延较长，或是时延不稳定、偶尔返回 0，则很可能是由于异常导致的。\n信号处理与异常控制。如前面提到的，你可以通过 setjmp 和 longjmp 等机制捕捉异常并确保程序在发生异常时不会崩溃。这种方法不仅可以减少因异常直接返回 0，还可以通过异常处理来重试访问，进一步确保结果的准确性。\n当你捕获到异常时，可以记录异常次数和触发位置。如果某个内存地址频繁触发异常且返回 0，则这个 0 可能与异常有关。\n随机访问测试。另一个技术是对目标内存地址周围的地址进行随机访问。如果周围地址的访问正常且没有返回异常的 0，但目标地址却持续返回 0，这更有可能表明目标地址中的数据确实是 0。可以通过以下策略进行随机测试：\n对比 kernel_address 的相邻地址（例如 kernel_address - 1 和 kernel_address + 1）的访问情况。\n如果相邻地址的数据返回正常，但目标地址频繁出现异常 0，则该 0 可能是由于异常引起的。\n重试机制。为了应对异常返回 0 的情况，可以设计一个重试机制，即每次读取到 0 时，进行多次重试，直到结果稳定。如果经过多次重试结果仍然是 0，则该 0 更有可能是内存中的真实值。\n1 2 3 4 5 6 unsigned char result; int retries = 5; // 设定重试次数 do { result = attempt_to_read_data(kernel_address); retries--; } while (result == 0 \u0026amp;\u0026amp; retries \u0026gt; 0); // 重试，直到获取到非 0 的数据或达到重试上限 总而言之，要区分 Meltdown + 侧信道攻击中读取到的 0 是内存中的真实值还是异常导致的返回值，可以通过以下几种方法：\n多次采样：多次读取同一地址，判断 0 是否稳定出现。 缓存时延分析：通过测量缓存访问时延，判断是否发生了缓存命中，命中意味着数据可能是真实的。 信号处理与异常控制：捕捉异常，记录异常次数，并重试访问来确认结果。 随机访问测试：对目标地址附近的地址进行测试，通过对比结果进一步验证。 重试机制：当读取到 0 时，尝试重试多次，以确保返回值的准确性。 通过以上手段，可以大大提高攻击的成功率，并有效区分异常导致的 0 和内存中的真实值。\nMeltdown调整内存值大小 在 Meltdown 的侧信道攻击中，要调整每次能得到的内存值的大小，通常涉及到以下几个技巧：\n控制访问粒度。一般来说，Meltdown 攻击可以逐字节读取数据。如果你想调整每次读取的数据大小，可以通过以下方式：\n逐字节访问\n典型实现：构造攻击代码，使每次读取一个字节。\n示例代码：\n1 2 3 4 for (size_t offset = 0; offset \u0026lt; data_size; offset++) { unsigned char value = read_byte(kernel_address + offset); // 处理读取的 value } 逐多字节访问\n如果希望每次读取多字节，可以将多次字节读取合并，例如使用memcpy优化攻击代码：\n1 2 3 4 5 for (size_t offset = 0; offset \u0026lt; data_size; offset += n) { unsigned char buffer[n]; memcpy(buffer, kernel_address + offset, n); // 处理读取的 buffer } 这个方法需要确保攻击代码能连续加载所需字节到缓存。\n优化缓存行为。缓存行是以固定大小（通常 64 字节）的块进行管理的。调整数据读取大小时，需要考虑缓存行对齐的问题：\n缓存对齐：确保读取的数据恰好在一个或多个完整的缓存行中。这样能减少多余的内存访问和缓存未命中。\n预取优化：通过 prefetch 指令提前加载大块数据，帮助提高获取率。\n使用批量处理。如果希望每次读取更多数据，考虑使用批量攻击来同时处理多个字节：\n结合流水线和乱序执行：调整内存访问模式，让处理器能够提前执行多个字节加载。\n批量缓存刷新：在攻击中，批量刷新缓存行以准备加载更多字节。\n利用高级指令。某些处理器架构支持高级指令，可以用于更大范围的内存操作。有助于提高每次读取的字节量。\n异常处理调整。在 Meltdown 攻击中，如果要提高每次读取数据的粒度，可以结合异常处理调整策略：\n细化异常捕获：确保每个读取操作的边界条件和异常处理得当，以避免数据丢失或错误。\n调整 setjmp 和 longjmp 的使用：确保在更大连续内存读取中保持执行连续性。\n要在 Meltdown 的侧信道攻击中调整每次能得到的内存值大小，可以通过控制访问粒度、优化缓存行为、使用批量处理和高级指令来实现。同时调整异常处理策略，确保读取准确和有效。通过这些方法，可以更灵活地控制每次读取的数据量。\nKAISER KAISER 是一种改进的内存隔离技术，旨在强化用户空间和内核空间之间的隔离。其主要步骤包括：\n内存映射调整：KAISER 会确保在用户空间中不映射任何内核内存，只保留一些x86架构必须的部分（例如中断处理程序）。 消除直接物理映射：用户空间无法访问物理内存或内核内存，因此无法通过地址解析来泄露这些内存。 防止漏洞利用：通过不在用户空间映射敏感内核内存，阻止像Meltdown这样的攻击从内核中泄露信息。 假设一个攻击者正在尝试定位某个关键的内存处理函数，该函数在用户空间中只留下了微小的踪迹（比如只有几千字节），而整体用户空间的大小可能是数十GB。同时，如果这些内存位置随着KASLR（内核地址空间布局随机化）不断随机化，攻击者需要经历极多次尝试才能有效猜测到这些位置，因此增加了攻击的难度。\nKAISER通过以下方式保护这个攻击面：\n使用跳板函数：对必要的内核指针位置使用\u0026quot;跳板（trampoline）\u0026ldquo;函数。这意味着中断处理程序和其他必要的内核代码不直接调用内核，而是通过一个单独的跳板函数进行。 跳板函数内存分离：确保这些跳板函数只被映射到内核中，并且与其他内核代码使用不同的随机偏移量进行随机化。 隔离作用：即使Meltdown攻击成功，攻击者只能获取到跳板代码的指针，而无法获取到内核的其余随机化偏移，从而保护内核地址的随机化信息。 假设一个系统需要在用户空间中保持中断处理程序的映射，以正常操作。\n传统方法：可能直接映射中断处理程序，容易让攻击者通过Meltdown攻击获取其地址并推断其他内核信息。 通过KAISER防护： 将中断处理程序地址指向一个跳板函数。 这个跳板函数的地址在内核中被随机化。 当需要访问中断处理程序时，首先通过被完全随机化了的跳板函数进行处理，以覆盖整个内核的直接地址。 通过这种方式，KAISER减少了直接暴露给用户空间的内核地址数量，即使在攻击者可以通过Meltdown获得一些地址，也无法得知内核的具体随机化偏移和其他信息。这种方式在安全和性能之间找到了妥协点，保护了“剩余攻击面”。\nMemory-Type Range Registers (MTRRs) MTRRs 是一个特性，允许系统定义内存区域的缓存特性。通过这些寄存器，系统可以指定某些内存区域是可缓存的、不可缓存的、写合并等。这对于优化系统性能和特定硬件行为很重要。\nMicroarchitectural state 和 architectural state \u0026ldquo;Microarchitectural state\u0026rdquo; 和 \u0026ldquo;architectural state\u0026rdquo; 是计算机体系结构中两个不同的概念，涉及处理器如何管理和表现数据。\n微架构状态（Microarchitectural state）：这是处理器内部的细节层次，它包括缓存、分支预测器、管道、加载/存储队列等部件的状态。这些状态反映了处理器如何在硬件层面优化执行指令，但这些信息通常对软件程序不可见，也不会直接影响程序执行的逻辑结果。 架构状态（Architectural state）：这是处理器从程序员或系统的角度所能看到的状态，主要包括寄存器、内存中的数据、程序计数器等。这些状态决定了程序的执行逻辑和可见结果。 “Microarchitectural state is translated into an architectural state” 指的是微架构中的某些临时状态，最终可能影响到架构状态，也就是说，处理器内部的一些优化行为（例如缓存命中或未命中）可能在某些情况下改变程序的实际执行结果或可见结果。\n缓存和微架构攻击：\n在微架构层面上，缓存中的数据是可见的，但这不属于程序的架构状态。然而，通过侧信道攻击（如Meltdown或Spectre），攻击者可以通过观察缓存行为（例如缓存命中与未命中）推断出某些本应不可见的架构状态。 假设一个程序访问了一个数据地址，如果这个地址在缓存中（微架构状态），访问速度会快。如果攻击者能够利用侧信道推断出这个缓存访问时间的差异，他们可以推断出该数据地址的位置或内容，进而影响架构状态（例如，暴露寄存器或内存中的数据）。 微架构状态（如缓存中的数据或管道中的指令）通常对程序不可见，但某些侧信道攻击能够利用这些状态的变化，最终推导出影响程序执行的架构状态。这种“转换”通常并不是直接的，而是通过观察微架构行为来推断出架构级别的信息。\n正课部分 Meltdown发生的背景 今天讲的是Meltdown，之所以我会读这篇论文，是因为我们在讲解如何设计内核时总是会提到安全。内核提供安全性的方法是隔离，用户程序不能读取内核的数据，用户程序也不能读取其他用户程序的数据。\n我们在操作系统中用来实现隔离的具体技术是硬件中的User/Supervisor mode，硬件中的Page Table，以及精心设计的内核软件，例如系统调用在使用用户提供的指针具备防御性。\n但是同时也值得思考，如何可以破坏安全性？实际上，内核非常积极的提供隔离性安全性，但总是会有问题出现。\n今天的论文 讨论的就是最近在操作系统安全领域出现的最有趣的问题之一，它发表于2018年。包括我在内的很多人发现对于用户和内核之间的隔离进行攻击是非常令人烦恼的，因为它破坏了人们对于硬件上的Page Table能够提供隔离性的设想。这里的攻击完全不支持这样的设想。\n同时，Meltdown也是被称为Micro-Architectural Attack的例子之一，这一类攻击涉及利用CPU内隐藏的实现细节。通常来说CPU如何工作是不公开的，但是人们会去猜，一旦猜对了CPU隐藏的实现细节，就可以成功的发起攻击。\nMeltdown是可被修复的，并且看起来已经被完全修复了。然后它使得人们担心还存在类似的Micro-Architectural Attack。所以这是最近发生的非常值得学习的一个事件。\n让我从展示攻击的核心开始，之后我们再讨论具体发生了什么。\n1 2 3 4 5 6 char buf [8192] r1 = \u0026lt;a kernel virtual address\u0026gt; r2 = *r1 r2 = r2 \u0026amp; 1 r2 = r2 * 4096 r3 = buf [r2] 这是论文中展示攻击是如何工作的代码的简化版。\n如果你是攻击者，出于某种原因你可以在计算机上运行一些软件，这个计算机上有一些你想要窃取的数据。虽然你不能直接访问这些数据，但是这些数据还是位于内存中，或许是内核内存，或许是另一个进程的内存。\n你可以在计算机上运行一个进程，或许因为你登录到了分时共享的机器，也或许你租用了运行在主机上的服务。你可以这样发起攻击：\n在程序中你在自己的内存中声明了一个buffer，这个buffer就是普通的用户内存且可以被正常访问。 然后你拥有了内核中的一个虚拟内存地址，其中包含了一些你想要窃取的数据。 这里的程序是C和汇编的混合，第3行代码的意思是你拥有了内核的虚拟内存地址，你从这个内存地址取值出来并保存在寄存器r2中。 第4行获取寄存器r2的低bit位，所以这里这种特定的攻击只是从内核一个内存地址中读取一个bit。 第5行将这个值乘以4096，因为低bit要么是1，要么是0，所以这意味着r2要么是4096，要么是0。 第6行中，我们就是读取前面申请的buffer，要么读取位置0的buffer，要么读取位置4096的buffer。 这就是攻击的基本流程。\n这里的一个问题是，为什么这里不能直接工作？在第3行，我们读取了内核的内存地址指向的数据，我们可以直接读取内核的内存地址吗？并不能，我们相信答案是否定的。\n如果我们在用户空间，我们不可能直接从内核读取数据。我们知道CPU不能允许这样的行为，因为当我们使用一个内核虚拟地址时，这意味着我们会通过Page Table进行查找，而Page Table有权限标志位，我们现在假设操作系统并没有在PTE中为内核虚拟地址设置标志位来允许用户空间访问这个地址，这里的标志位在RISC-V上就是pte_u标位置。\n因此这里的读取内核内存地址指令必然会失败，必然会触发Page Fault。实际中如果我们运行代码，这些代码会触发Page Fault。如果我们在代码的最后增加printf来打印r3寄存器中的值，我们会在第3行得到Page Fault，我们永远也走不到printf。这时我们发现我们不能直接从内核中偷取数据。\n然而，如论文展示的一样，这里的指令序列是有用的。虽然现在大部分场景下已经不是事实了，但是论文假设内核地址被映射到了每个用户进程的地址空间中了。也就是说，当用户代码在运行时，完整的内核PTE也出现在用户程序的Page Table中，但是这些PTE的pte_u比特位没有被设置，所以用户代码在尝试使用内核虚拟内存地址时，会得到Page Fault。\n在论文写的时候，所有内核的内存映射都会在用户程序的Page Table中，只是它们不能被用户代码使用而已，如果用户代码尝试使用它们，会导致Page Fault。操作系统设计人员将内核和用户内存地址都映射到用户程序的Page Table中的原因是，这使得系统调用非常的快，因为这使得当发生系统调用时，你不用切换Page Table。\n切换Page Table本身就比较费时，同时也会导致CPU的缓存被清空，使得后续的代码执行也变慢。所以通过同时将用户和内核的内存地址都映射到用户空间可以提升性能。但是上面的攻击依赖了这个习惯。我将会解释这里发生了什么使得上面的代码是有用的。\n学生提问：能重复一下上面的内容吗？\nRobert教授：在XV6中，当进程在用户空间执行时，如果你查看它的Page Table，其中包含了用户的内存地址映射，trampoline和trap frame page的映射，除此之外没有别的映射关系，这是XV6的工作方式。\n而这篇论文假设的Page Table不太一样，当这篇论文在写的时候，大部分操作系统都会将内核内存完整映射到用户空间程序。所以所有的内核PTE都会出现在用户程序的Page Table中，但是因为这些PTE的pte_u比特位没有被设置，用户代码并不能实际的使用内核内存地址。\n这么做的原因是，当你执行系统调用时，你不用切换Page Table，因为当你通过系统调用进入到内核时，你还可以使用同一个Page Table，并且因为现在在Supervisor mode，你可以使用内核PTE。\n这样在系统调用过程中，进出内核可以节省大量的时间。所以大家都使用这个技术，并且几乎可以肯定Intel也认为一个操作系统该这样工作。\n在论文中讨论的攻击是基于操作系统使用了这样的结构。最直接的摆脱攻击的方法就是不使用这样的结构。但是当论文还在写的时候，所有的内核PTE都会出现在用户空间。\n学生提问：所以为了能够攻击，需要先知道内核的虚拟内存地址？\nRobert教授：是的。或许找到内存地址本身就很难，但是你需要假设攻击者有无限的时间和耐心，如果他们在找某个数据，他们或许愿意花费几个月的时间来窃取这个数据。\n有可能这是某人用来登录银行账号或者邮件用的密码。这意味着攻击者可能需要尝试每一个内核内存地址，以查找任何有价值的数据。\n或许攻击者会研究内核代码，找到内核中打印了数据的地址，检查数据结构和内核内存，最后理解内核是如何工作的，并找到对应的虚拟内存地址。因为类似的攻击已经存在了很长的时间，内核实际上会保护自己不受涉及到猜内核内存地址的攻击的影响。\n论文中提到了Kernal address space layout randomization。所以现代的内核实际上会将内核加载到随机地址，这样使得获取内核虚拟地址更难。这个功能在论文发表很久之前就存在，因为它可以帮助防御攻击。\n在这个攻守双方的游戏中，我们需要假设攻击者最后可以胜出并拿到内核的虚拟内存地址。所以我们会假设攻击者要么已经知道了一个内核虚拟地址，要么愿意尝试每一个内核虚拟内存地址。\n我们会好奇，上面的代码怎么会对攻击者是有用的？如果CPU如手册中一样工作，那么这里的攻击是没有意义的，在第三行会有Page Fault。但是实际上CPU比手册中介绍的要复杂的多，而攻击能生效的原因是一些CPU的实现细节。\n这里攻击者依赖CPU的两个实现技巧，一个是Speculative execution（预测执行），另一个是CPU的缓存方式。\nSpeculative execution(1) 首先来看Speculative execution（预测执行），这里也有一个示例代码。\n1 2 3 4 5 6 7 8 r0 = \u0026lt;something\u0026gt; r1 = valid //r1 is a register; valid is in RAM if(r1 == 1){ r2 = *r0 r3 = r2 + 1 } else{ r3 = 0 } 现在我并没有讨论安全性，Speculative execution是一种用来提升CPU性能的技术，所以这是CPU使用的一些优化技巧。假设我们在运行这里的代码：\n在r0寄存器保存了一个内存地址，地址可能是有效的也可能是无效的，这取决于我代码的逻辑。 我们假设内存中还保存了一个valid变量。在使用r0中保存地址之前，我们会先将valid从内存中加载到r1。 并且只有当valid等于1时，才使用r0中的地址。如果valid等于0，我们将不会使用r0中的地址。 如果valid等于1，我们会将r0的地址指向的内容加载到r2。 并对r2寄存器加1，保存在r3寄存器中。 在一个简单的CPU实现中，在代码的第2行，你会将valid从内存中加载到r1，这里对应了从内存中读取数据的load指令。任何一个需要从内存中读取数据的load指令都会花费2GHZ CPU的数百个CPU cycle。CPU最多可以在每个cycle执行一条指令，如果我们需要在代码的第2行等待几百个CPU cycle，那么机器会闲置数百个CPU cycle。\n这是一个明显的降低性能的地方，因为如果一切都正常的话，CPU可以在每个cycle内执行一条指令，而不是每几百个cycle才执行一条指令。\n所有现在的CPU都使用了叫做branch prediction的功能。第3行的if语句是一个branch，如果我们将其转换成机器指令，我们可以发现这里有一个branch，并且这是一个带条件的branch用来测试r1寄存器是否等于1。\nCPU的branch prediction会至少为每个最近执行过的branch保存一个缓存，并记住这个branch是否被选中了，所以这里可能是基于上次branch的选择的预测。\n但是即使CPU没有足够的信息做预测，它仍然会选择一个branch，并执行其中的指令。也就是说在CPU在知道第3行代码是否为true之前，它会选择某一个branch并开始执行。或许branch选错了，但是CPU现在还不知道。\n所以在上面的代码中，或许在第2行代码的load结束之前，也就是在知道valid变量的值之前，CPU会开始执行第4行的指令，并通过load指令读取r0指向的内存地址的内容。\n而r0中的内存地址或许是，也或许不是一个有效的指针。一旦load指令返回了一些内容，在代码的第5行对返回内容加1并设置到r3寄存器中。\n或许很久之后，第2行的load指令终于完成了，现在我们知道valid变量的值。如果valid等于1，那么一切都好，如果valid等于0，CPU会取消它执行第4、5行代码的效果，并重新执行合适的分支代码，也就是第7行代码。\n这里在确定是否应该执行之前就提前执行分支代码的行为，被称作预测执行。这是为了提升性能，如果CPU赌对了，那么它就可以超前执行一些指令，而不用等待费时的内存加载。\nCPU中为了支持预测执行的硬件及其复杂，CPU里面有大量的设计来让这里能工作，但是没有一个设计被公开了，这些都是Intel的内部信息，并且不在手册中。所以在Meltdown Attack时，涉及到大量有关CPU是如何工作的猜测来确保攻击能生效。\n为了能回滚误判的预测执行，CPU需要将寄存器值保存在别处。虽然代码中第4行，第5行将值保存在了r2，r3，但是实际上是保存在了临时寄存器中。\n如果CPU赌对了，那么这些临时寄存器就成了真实寄存器，如果赌错了，CPU会抛弃临时寄存器，这样代码第4，5行就像从来没有发生过一样。\n在这里的代码中，我们需要考虑如果r0中是有效的指针会发生什么，如果不是有效的指针，又会发生什么。如果我们在超前执行代码第4行，并且r0中是有效的指针，那么CPU会真实的加载指针的内容到r2寄存器的临时版本中。\n如果r0中的指针指向的内容位于CPU的cache中，那么必然可以将内容拷贝到r2寄存器的临时版本。如果CPU的cache中没有包含数据，我并不清楚CPU是否会会从内存中读取r0中指针指向的内容。\n对于我们来说，更有趣的一个问题是，如果r0中的指针不是一个有效的指针，会发生什么？如果r0中的指针不是一个有效的地址，并且我们在超前执行代码第4行，机器不会产生Fault。\n机器或许知道r0是无效的地址，并且代码第4行尝试使用一个无效的地址，但是它不能产生Page Fault，因为它不能确定代码第4行是否是一个正确的代码分支，因为有可能CPU赌错了。\n所以直到CPU知道了valid变量的内容，否则CPU不能在代码第4行生成Page Fault。也就是说，如果CPU发现代码第4行中r0内的地址是无效的，且valid变量为1，这时机器才会生成Page Fault。如果r0是无效的地址，且valid变量为0，机器不会生成Page Fault。\n所以是否要产生Page Fault的决定，可能会推迟数百个CPU cycle，直到valid变量的值被确定。\n当我们确定一条指令是否正确的超前执行了而不是被抛弃了这个时间点，对应的技术术语是Retired。\n所以当我们说一个指令被超前执行，在某个时间点Retired，这时我们就知道这条指令要么会被丢弃，要么它应该实际生效，并且对机器处于可见状态。\n一条指令如果是Retired需要满足两个条件，首先它自己要结束执行，比如说结束了从内存加载数据，结束了对数据加1；其次，所有之前的指令也需要Retired。\n所以上面代码第4行在直到valid变量被从内存中加载出来且if被判定之前不能Retired，所以第4行的Retirement可能会延后数百个CPU cycle。\n这是Meltdown攻击非常关键的一个细节。\nSpeculative execution(2) 1 2 3 4 5 6 7 8 r0 = \u0026lt;something\u0026gt; r1 = valid //r1 is a register; valid is in RAM if(r1 == 1){ r2 = *r0 r3 = r2 + 1 } else{ r3 = 0 } 如果r0中的内存地址是无效的，且在Page Table中完全没有映射关系，那么我也不知道会发生什么。如果r0中的内存地址在Page Table中存在映射关系，只是现在权限不够，比如说pte_u标志位为0，那么Intel的CPU会加载内存地址对应的数据，并存储在r2寄存器的临时版本中。\n之后r2寄存器的临时版本可以被代码第5行使用。所以尽管r0中的内存地址是我们没有权限的内存，比如说一个内核地址，它的数据还是会被加载到r2，之后再加1并存储在r3中。\n之后，当代码第4行Retired时，CPU会发现这是一个无效的读内存地址行为，因为PTE不允许读取这个内存地址。这时CPU会产生Page Fault取消执行后续指令，并回撤对于r2和r3寄存器的修改。\n所以，在这里的例子中，CPU进行了两个推测：一个是CPU推测了if分支的走向，并选择了一个分支提前执行；除此之外，CPU推测了代码第4行能够成功完成。\n对于load指令，如果数据在CPU缓存中且相应的PTE存在于Page Table，不论当前代码是否有权限，Intel CPU总是能将数据取出。如果没有权限，只有在代码第4行Retired的时候，才会生成Page Fault，并导致预测执行被取消。\n学生提问：我对CPU的第二个预测，也就是从r0中保存的内存地址加载数据有一些困惑，这是不是意味着r0对应的数据先被加载到了r2，然后再检查PTE的标志位？\nRobert教授：完全正确。在预测的阶段，不论r0指向了什么地址，只要它指向了任何东西，内存中的数据会被加载到r2中。\n之后，当load指令Retired时才会检查权限。如果我们并没有权限做操作，所有的后续指令的效果会被取消，也就是对于寄存器的所有修改会回滚。同时，Page Fault会被触发，同时寄存器的状态就像是预测执行的指令没有执行过一样。\n学生提问：难道不能限制CPU在Speculative execution的时候，先检查权限，再执行load指令吗？看起来我们现在的问题就是我们在不知道权限的情况下读取了内存，如果我们能先知道权限，那么Speculative execution能不能提前取消？\nRobert教授：这里有两个回答。首先，Intel芯片并不是这样工作的。其次，是的，我相信对于Intel来说如果先做权限检查会更简单，这样的话，在上面的例子中，r2寄存器就不会被修改。\n你们或许注意到论文中提到，尽管AMD CPU的手册与Intel的一样，它们有相同的指令集，Meltdown Attack并不会在AMD CPU上生效。普遍接受的观点是，AMD CPU在Speculative execution时，如果没有权限读取内存地址，是不会将内存地址中的数据读出。\n这就是为什么Meltdown Attack在AMD CPU上不生效的原因。最近的Intel CPU明显也采用了这种方法，如果程序没有权限，在Speculative execution的时候也不会加载内存数据。\n这里使用哪种方式对于性能来说没有明显区别，或许在指令Retired的时候再检查权限能省一些CPU的晶体管吧。这里我要提醒一下，这里有很多内容都是猜的，不过我认为我说的都是对的。Intel和AMD并没有太披露具体的细节。\n这里有一些重要的术语。你可以从CPU手册中读到的，比如说一个add指令接收两个寄存器作为参数，并将结果存放在第三个寄存器，这一类设计被称为CPU的Architectural，或者通告的行为。\n如果你读取一个你没有权限的内存地址，你会得到一个Page Fault，你不允许读取这个内存地址，这就是一种通告的行为。\nCPU的实际行为被称作Micro-Architectural，CPU的通告行为与实际行为是模糊不清的。比如说CPU会悄悄的有Speculative execution。\nCPU设计者在设计Micro-Architectural时的初衷是为了让它是透明的。的确有很多行为都发生在CPU内部，但是结果看起来就像是CPU完全按照手册在运行。\n举个例子，在上面代码的第4行，或许Intel的CPU在读取内存时没有检查权限，但是如果权限有问题的话，在指令Retired的时候，所有的效果都会回滚，你永远也看不到你不该看到的内存内容。\n所以看起来就跟CPU的手册一样，你不允许读取你没有权限的内存地址。这里Architectural和Micro-Architectural的区别是Meltdown Attack的主要攻击点。这里的攻击知道CPU内部是如何工作的。\nCPU caches 接下来我将介绍Micro-Architectural的另一个部分，也就是缓存。我知道大家都知道CPU有cache，但是缓存或多或少应该是也透明的。\n让我画个图描述一下cache，因为我认为cache与Meltdown最相关。\n首先，你有CPU核，这是CPU的一部分，它会解析指令，它包含了寄存器，它有加法单元，除法单元等等。所以这是CPU的执行部分。\n当CPU核需要执行load/store指令时，CPU核会与内存系统通信。\n内存系统一些cache其中包含了数据的缓存。首先是L1 data cache，它或许有64KB，虽然不太大，但是它特别的快。\n如果你需要的数据在L1 cache中，只通过几个CPU cycle就可以将数据取回。L1 cache的结构包含了一些线路，每个线路持有了可能是64字节的数据。\n这些线路是个表单，它们通过虚拟内存地址索引。如果一个虚拟内存地址在cache中，并且cache为这个虚拟内存地址持有了数据，那么实际中可以认为L1 cache中也包含了来自对应于虚拟内存地址的PTE的权限。\nL1 cache是一个表单，当CPU核执行load指令时，首先硬件会检查L1 cache是否包含了匹配load指令的虚拟内存地址，如果有的话，CPU会直接将L1 cache中的数据返回，这样可以很快完成指令。\n如果不在L1 cache，那么数据位于物理内存中，所以现在我们需要物理内存地址，这里需要Translation Lookaside Buffer（TLB），TLB是PTE的缓存。\n现在我们会检查load指令中的虚拟内存地址是否包含在TLB中。如果不在TLB，我们就需要做大量的工作，我们需要从内存中读取相关的PTE。\n让我们假设TLB中包含了虚拟内存地址对应的物理内存Page地址，我们就可以获取到所需要的物理内存地址。通常来说会有一个更大的cache（L2 cache），它是由物理内存地址索引。\n现在通过TLB我们找到了物理内存地址，再通过L2 cache，我们有可能可以获取到数据。\n如果我们没有在L2 cache中找到物理内存地址对应的数据。我们需要将物理内存地址发送给RAM系统。这会花费很长的时间，当我们最终获得了数据时，我们可以将从RAM读取到的数据加入到L1和L2 cache中，最终将数据返回给CPU核。\n以上就是CPU的cache。\n如果L1 cache命中的话可能只要几个CPU cycle，L2 cache命中的话，可能要几十个CPU cycle，如果都没有命中最后需要从内存中读取那么会需要几百个CPU cycle。\n一个CPU cycle在一个2GHZ的CPU上花费0.5纳秒。所以拥有cache是极其有利的，如果没有cache的话，你将会牺牲掉几百倍的性能。所以cache对于性能来说是非常关键的。\n在Meltdown Attack的目标系统中，如果我们运行在用户空间，L1和L2 cache可以既包含用户数据，也包含内核数据。L2 cache可以包含内核数据因为它只是物理内存地址。\nL1 cache有点棘手，因为它是虚拟内存地址，当我们更换Page Table时，L1 cache的内容不再有效。因为更换Page Table意味着虚拟内存地址的意义变了，所以这时你需要清空L1 cache。不过实际中会有更多复杂的细节，可以使得你避免清空L1 cache。\n论文中描述的操作系统并没有在内核空间和用户空间之间切换的时候更换Page Table，因为两个空间的内存地址都映射在同一个Page Table中了。这意味着我们不必清空L1 cache，也意味着L1 cache会同时包含用户和内核数据，这使得系统调用更快。\n如果你执行系统调用，当系统调用返回时，L1 cache中还会有有用的用户数据，因为我们在这个过程中并没与更换Page Table。\n所以，当程序运行在用户空间时，L1 cache中也非常有可能有内核数据。L1 cache中的权限信息拷贝自TLB中的PTE，如果用户空间需要访问内核内存数据，尽管内核数据在L1 cache中，你也不允许使用它，如果使用的话会触发Page Fault。\n尽管Micro-Architectural的初衷是完全透明，实际中不可能做到，因为Micro-Architectural优化的意义在于提升性能，所以至少从性能的角度来说，它们是可见的。也就是说你可以看出来你的CPU是否有cache，因为如果没有的话，它会慢几百倍。\n除此之外，如果你能足够精确测量时间，那么在你执行一个load指令时，如果load在几个CPU cycle就返回，数据必然是在cache中，如果load在几百个CPU cycle返回，数据可能是从RAM中读取，如果你能达到10纳秒级别的测量精度，你会发现这里区别还是挺大的。\n所以从性能角度来说，Micro-Architectural绝对不是透明的。我们现在讨论的分支预测，cache这类功能至少通过时间是间接可见的。\n所以尽管Micro-Architectural设计的细节都是保密的，但是很多人对它都有强烈的兴趣，因为这影响了很多的性能。比如说编译器作者就知道很多Micro-Architectural的细节，因为很多编译器优化都基于人们对于CPU内部工作机制的猜测。\n实际中，CPU制造商发布的优化手册披露了一些基于Micro-Architectural的技巧，但是他们很少会介绍太多细节，肯定没有足够的细节来理解Meltdown是如何工作的。所以Micro-Architectural某种程度上说应该是透明的、隐藏的、不可见的，但同时很多人又知道一些随机细节。\n学生提问：L1 cache是每个CPU都有一份，L2 cache是共享的对吧？\nRobert教授：不同CPU厂商，甚至同一个厂商的不同型号CPU都有不同的cache结构。今天普遍的习惯稍微有点复杂，在一个多核CPU上，每一个CPU核都有一个L1 cache，它离CPU核很近，它很快但是很小。每个CPU核也还有一个大点的L2 cache。除此之外，通常还会有一个共享的L3 cache。\n另一种方式是所有的L2 cache结合起来，以方便所有的CPU共用L2 cache，这样我可以非常高速的访问我自己的L2 cache，但是又可以稍微慢的访问别的CPU的L2 cache，这样有效的cache会更大。\n所以通常你看到的要么是三级cache，或者是两级cache但是L2 cache是合并在一起的。典型场景下，L2和L3是物理内存地址索引，L1是虚拟内存地址索引。\n学生提问：拥有物理内存地址的缓存有什么意义？\nRobert教授：如果同一个数据被不同的虚拟内存地址索引，虚拟内存地址并不能帮助你更快的找到它。而L2 cache与虚拟内存地址无关，不管是什么样的虚拟内存地址，都会在L2 cache中有一条物理内存地址记录。\n学生提问：MMU和TLB这里位于哪个位置？\nRobert教授：我认为在实际中最重要的东西就是TLB，并且我认为它是与L1 cache并列的。如果你miss了L1 cache，你会查看TLB并获取物理内存地址。MMU并不是一个位于某个位置的单元，它是分布在整个CPU上的。\n学生提问：但是MMU不是硬件吗？\nRobert教授：是的，这里所有的东西都是硬件。CPU芯片有数十亿个晶体管，所以尽管是硬件，我们讨论的也是使用非常复杂的软件设计的非常复杂的硬件。所以CPU可以做非常复杂和高级的事情。所以是的，它是硬件，但是它并不简单直观。\n学生提问：Page Table的映射如果没有在TLB中命中的话，还是要走到内存来获取数据，对吧？\nRobert教授：从L2 cache的角度来说，TLB miss之后的查找Page Table就是访问物理内存，所以TLB需要从内存中加载一些内存页，因为这就是加载内存，这些内容可以很容易将Page Table的内容缓存在L2中。\n虚拟地址到物理地址的映射表主要存储在内存中。不过，为了提高查询效率，系统使用了一个称为转换后备缓冲区（Translation Lookaside Buffer, TLB）的缓存装置。\nTLB：\n存储最近使用的虚拟地址到物理地址的映射。 是一种小型的、高速的缓存，能加快地址转换过程。 内存：\n完整的虚拟地址到物理地址映射表（页表）存储在内存中。 当TLB未命中时，需访问内存中的页表来完成地址转换。 L1, L2, L3 缓存：\n主要用于缓存数据和指令，不存储地址映射表。 总结：映射表主要在内存中，TLB用于缓存一部分映射以提高查询速度\nFlush and Reload 为什么Cache与Meltdown相关呢？接下来我将讨论一下论文 中使用Cache的主要方法。\n论文中讨论了这种叫做Flush and Reload的技术，这个技术回答了一个问题：一段特定的代码是否使用了特定内存地址的数据？这个技术本身并不是一个直接的安全漏洞，因为它只能基于你有权限的内存地址工作。\n所以如果你是用户代码，你可以使用属于你的用户空间内存，并且你现在要调用一个你自己的函数，你可以使用Flush and Reload来知道你刚刚执行的函数是否使用了某个属于你自己的内存。\n你不能直接使用这种技术来获取其他进程的私有内存。进程之间有时候会共享内存，你还是可以访问这部分共享的内存。所以Flush and Reload回答了这个问题，特定的函数是否使用了特定内存地址？\n它的具体工作步骤如下：\n第一步，假设我们对地址X感兴趣，我们希望确保Cache中并没有包含位于X的内存数据。\n实际中，为了方便，Intel提供了一条指令，叫做clflush，它接收一个内存地址作为参数，并确保该内存地址不在任何cache中。\n这超级方便，不过即使CPU并没有提供这样的指令，实际中也有方法能够删除Cache中的数据，举个例子，如果你知道Cache有64KB，那么你load 64KB大小的随机内存数据，这些数据会被加载到Cache中，这时Cache中原本的数据会被冲走，因为Cache只有64KB大小。所以即使没有这个好用的指令，你仍然可以清空Cache中的所有数据。\n第二步，如果你对某段可能使用了内存地址X的代码感兴趣，你可以调用这个函数，先不管这个函数做了什么，或许它使用了内存地址X，或许没有。\n现在，你想要知道X是否在Cache中，如果是的话，因为在第一步清空了Cache，必然是因为第二步的函数中load了这个内存地址。所以你现在想要执行load，但是你更想知道load花费了多长时间，而且我们这里讨论的是纳秒级别的时间，比如5个纳秒或者100个纳秒，那么我们该怎样达到这种测量精度呢？\n这是个困难的任务。Intel CPU会提供指令来向你返回CPU cycle的数量，这被称为rdtsc。所以这里我们会执行rdtsc指令，它会返回CPU启动之后总共经过了多少个CPU cycle。如果是2GHZ的CPU，这意味着通过这个指令我们可以得到0.5纳秒的测量精度。\n现在我们会将内存地址X的数据加载到junk对象中。\n然后再通过rdtsc读取时间。如果两次读取时间的差是个位数，那么上一步的load指令走到了cache中，也就是第二步的函数中使用了内存地址X的数据。\n如果两次读取时间的差别超过100，这意味着内存地址X不在cache中，虽然这并不绝对，但是这可能代表了第二步的函数中并没有使用内存X的数据。因为函数中可能使用了内存地址X，然后又用了其他与X冲突的数据，导致内存地址X又被从cache中剔除了。\n但是对于简单的情况，如果两次时间差较大那么第二步的函数没有使用内存地址X，如果两次时间差较小那么第二步函数使用了内存地址X。\n现在还没有涉及到攻击，因为这里我们需要能够访问到内存地址X，所以这是我们可以访问的内存地址。\n以上就是有关Meltdown的前置知识。\nMeltdown Attack 接下来让我们回到Meltdown。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 char buf[8192] // L1 // the Flush of Flush+Reload clflush buf[0] clflush buf[4096] // L5 \u0026lt;some expensive instruction like divide\u0026gt; r1 = \u0026lt;a kernel virtual address\u0026gt; r2 = *r1 // L10 r2 = r2 \u0026amp; 1 // speculated r2 = r2 * 4096 // speculated r3 = buf[r2] // speculated \u0026lt;handle the page fault from \u0026#34;r2 = *r1\u0026#34;\u0026gt; // the Reload of Flush+Reload a = rdtsc // L18 r0 = buf[0] b = rdtsc r1 = buf[4096] // L21 c = rdtsc if b-a \u0026lt; c-b: low bit was probably a 1 // L24 这段代码比22.1里面的代码更加完整，这里是一个更完整的Meltdown攻击代码，这里我们增加了Flush and Reload代码。\n首先我们声明了一个buffer，现在我们只需要从内核中窃取1个bit的数据，我们会将这个bit乘以4096，所以我们希望下面的Flush and Reload要么看到buffer[0]在cache中，要么看到buffer[4096]在cache中。\n为什么要有这么的大的间隔？是因为硬件有预获取。如果你从内存加载一个数据，硬件极有可能会从内存中再加载相邻的几个数据到cache中。\n所以我们不能使用两个非常接近的内存地址，然后再来执行Flush and Reload，我们需要它们足够的远，这样即使有硬件的预获取，也不会造成困扰。所以这里我们将两个地址放到了两个内存Page中（注，一个内存Page 4096）。\n现在的Flush部分直接调用了clflush指令（代码第4第5行），来确保我们buffer中相关部分并没有在cache中。\n代码第7行或许并不必要，这里我们会创造时间差。我们将会在第10行执行load指令，它会load一个内核内存地址，所以它会产生Page Fault。但是我们期望能够在第10行指令Retired之前，也就是实际的产生Page Fault并取消这些指令效果之前，再预测执行（Speculative execution）几条指令。\n如果代码第10行在下面位置Retired，那么对我们来说就太早了。实际中我们需要代码第13行被预测执行，这样才能完成攻击。\n所以我们希望代码第10行的load指令尽可能晚的Retired，这样才能推迟Page Fault的产生和推迟取消预测执行指令的效果。因为我们知道一个指令只可能在它之前的所有指令都Retired之后，才有可能Retired。\n所以在代码第7行，我们可以假设存在一些非常费时的指令，它们需要很长时间才能完成。或许要从RAM加载一些数据，这会花费几百个CPU cycle；或许执行了除法，或者平方根等。\n这些指令花费了很多时间，并且很长时间都不会Retired，因此也导致代码第10行的load很长时间也不会Retired，并给第11到13行的代码时间来完成预测执行。\n现在假设我们已经有了内核的一个虚拟内存地址，并且要执行代码第10行。我们知道它会生成一个Page Fault，但是它只会在Retired的时候才会真正的生成Page Fault。我们设置好了使得它要过一会才Retired。\n因为代码第10行还没有Retired，并且在Intel CPU上，即使你没有内存地址的权限，数据也会在预测执行的指令中被返回。这样在第11行，CPU可以预测执行，并获取内核数据的第0个bit。第12行将其乘以4096。第13行是另一个load指令，load的内存地址是buffer加上r2寄存器的内容。\n我们知道这些指令的效果会被取消，因为第10行会产生Page Fault，所以对于r3寄存器的修改会被取消。但是尽管寄存器都不会受影响，代码第13行会导致来自于buffer的部分数据被加载到cache中。取决于内核数据的第0bit是0还是1，第13行会导致要么是buffer[0]，要么是buffer[4096]被加载到cache中。\n之后，尽管r2和r3的修改都被取消了，cache中的变化不会被取消，因为这涉及到Micro-Architectural，所以cache会被更新。\n第15行表示最终Page Fault还是会发生，并且我们需要从Page Fault中恢复。用户进程可以注册一个Page Fault Handler（注，详见Lec17），并且在Page Fault之后重新获得控制。论文还讨论了一些其他的方法使得发生Page Fault之后可以继续执行程序。\n现在我们需要做的就是弄清楚，是buffer[0]还是buffer[4096]被加载到了cache中。现在我们可以完成Flush and Reload中的Reload部分了。第18行获取当前的CPU时间，第19行load buffer[0]，第20行再次读取当前CPU时间，第21行load buffer[4096]，第22行再次读取当前CPU时间，第23行对比两个时间差。\n哪个时间差更短，就可以说明内核数据的bit0是0还是1。如果我们重复几百万次，我们可以扫描出所有的内核内存。\n学生提问：在这里例子中，如果b-a\u0026lt;c-b，是不是意味着buffer[0]在cache中？\nRobert教授：是的，你是对的。\n学生提问：在第9行之前，我们需要if语句吗？\nRobert教授：并不需要，22.2中的if语句是帮助我展示Speculative execution的合理理由：尽管CPU不知道if分支是否命中，它还是会继续执行。\n但是在这里，预测执行的核心是我们并不知道第10行的load会造成Page Fault，所以CPU会在第10行load之后继续预测执行。理论上，尽管这里的load可能会花费比较长的时间（例如数百个CPU cycle），但是它现在不会产生Page Fault，所以CPU会预测执行load之后的指令。如果load最终产生了Page Fault，CPU会回撤所有预测执行的效果。\n预测执行会在任何长时间执行的指令，且不论这个指令是否能成功时触发。例如除法，我们不知道是否除以0。一旦触发预测执行，所有之后的指令就会开始被预测执行。不管怎样，真正核心的预测执行从第10行开始，但是为了让攻击更有可能成功，我们需要确保预测执行从第7行开始。\n学生提问：在这个例子中，我们只读了一个bit，有没有一些其他的修改使得我们可以读取一整个寄存器的数据？\nRobert教授：有的，将这里的代码运行64次，每次获取1个bit。\n学生提问：为什么不能一次读取64bit呢？\nRobert教授：如果这样的话，buffer需要是2^64再乘以4096，我们可能没有足够的内存来一次读64bit。或许你可以一次读8个bit，然后buffer大小是256*4096。\n论文中有相关的，因为这里主要的时间在第17行到第24行，也就是Flush and Reload的Reload部分。如果一次读取一个字节，那么找出这个字节的所有bit，需要256次Reload，每次针对一个字节的可能值。\n如果一次只读取一个bit，那么每个bit只需要2次Reload。所以一次读取一个bit，那么读取一个字节只需要16次Reload，一次读取一个字节，那么需要256次Reload。所以论文中说一次只读取一个bit会更快，这看起来有点反直觉，但是又好像是对的。\n学生提问：这里的代码会运行在哪？会运行在特定的位置吗？\nRobert教授：这取决于你对于机器有什么样的权限，并且你想要窃取的数据在哪了。\n举个例子，你登录进了Athena（注，MIT的共享计算机系统），机器上还有几百个其他用户 ，然后你想要窃取某人的密码，并且你很有耐心。\n在几年前Athena运行的Linux版本会将内核内存映射到每一个用户进程的地址空间。那么你就可以使用Meltdown来一个bit一个bit的读取内核数据，其中包括了I/O buffer和network buffer。如果某人在输入密码，且你足够幸运和有耐心，你可以在内核内存中看见这个密码。\n实际中，内核可能会映射所有的物理内存，比如XV6就是这么做的，这意味着你或许可以使用Meltdown在一个分时共享的机器上，读取所有的物理内存，其中包括了所有其他进程的内存。\n这样我就可以看到其他人在文本编辑器的内容，或者任何我喜欢的内容。这是你可以在一个分时共享的机器上使用Meltdown的方法。其他的场景会不太一样。\n分时共享的机器并没有那么流行了，但是这里的杀手场景是云计算。如果你使用了云服务商，比如AWS，它会在同一个计算机上运行多个用户的业务，取决于AWS如何设置它的VMM或者容器系统，如果你购买了AWS的业务，那么你或许就可以窥探其他运行在同一个AWS机器上的用户软件的内存。我认为这是人们使用Meltdown攻击的方式。\n另一个可能有用的场景是，当你的浏览器在访问web时，你的浏览器其实运行了很多不被信任的代码，这些代码是各种网站提供的，或许是以插件的形式提供，或许是以javascript的形式提供。\n这些代码会被加载到浏览器，然后被编译并被运行。有可能当你在浏览网页的时候，你运行在浏览器中的代码会发起Meltdown攻击，而你丝毫不知道有一个网站在窃取你笔记本上的内容，但是我并不知道这里的细节。\n学生提问：有人演示过通过javascript或者WebAssembly发起攻击吗？\nRobert教授：我不知道。人们肯定担心过WebAssembly，但是我不知道通过它发起攻击是否可行。对于javascript我知道难点在于时间的测量，你不能向上面一样获取到纳秒级别的时间，所以你并不能使用Flush and Reload。或许一些更聪明的人可以想明白怎么做，但是我不知道。\n实际中Meltdown Attack并不总是能生效，具体的原因我认为论文作者并没有解释或者只是猜测了一下。如果你查看论文的最后一页，\n你可以看到Meltdown Attack从机器的内核中读取了一些数据，这些数据里面有一些XXXX，这些是没能获取任何数据的位置，也就是Meltdown Attack失败的位置。\n论文中的Meltdown Attack重试了很多很多次，因为在论文6.2还讨论了性能，说了在某些场景下，获取数据的速率只有10字节每秒，这意味着代码在那不停的尝试了数千次，最后终于获取到了数据，也就是说Flush and Reload表明了两个内存地址只有一个在Cache中。\n所以有一些无法解释的事情使得Meltdown会失败，从上图看，Meltdown Attack获取了一些数据，同时也有一些数据无法获得。\n据我所知，人们并不真的知道所有的成功条件和失败条件，最简单的可能是如果内核数据在L1 cache中，Meltdown能成功，如果内核数据不在L1 Cache中，Meltdown不能成功。如果内核数据不在L1 cache中，在预测执行时要涉及很多机制，很容易可以想到如果CPU还不确定是否需要这个数据，并不一定会完成所有的工作来将数据从RAM中加载过来。\n你可以发现实际中并没有这么简单，因为论文说到，有时候当重试很多次之后，最终还是能成功。所以这里有一些复杂的情况，或许在CPU内有抢占使得即使内核数据并不在Cache中，这里的攻击偶尔还是可以工作。\n论文的最后也值得阅读，因为它解释了一个真实的场景，比如说我们想要通过Meltdown窃取Firefox的密码管理器中的密码，你该怎么找出内存地址，以及一个攻击的完整流程，我的意思是由学院派而不是实际的黑客完成的一次完整的攻击流程。尽管如此，这里也包含了很多实用的细节。\nMeltdown Fix 我最后想讨论的是Meltdown的修复，你们实际已经接触了一些了。当论文 发表的时候，它获取了很多的关注。实际中还有另一篇论文，也是由这篇论文的部分作者参与完成，另一篇论文讨论了一种使用了CPU内一种叫做Spectre的不同的预测执行的不同攻击方法。这一对论文的同时出现让人非常兴奋。\n所以人们现在发现危害太大了，因为现在我们讨论的是操作系统的隔离性被破坏了。这里的技术破坏了Page Table的保护，这是我们用来实现用户和内核间隔离的技术，所以这是一个非常基础的攻击，或者至少以一种非常通用的方式破坏了安全性非常重要的一个部分。所以人们非常非常迫切的想要修复Meltdown。\n很多操作系统在这篇论文发表之后数周内就推出的一个快速修复，这是一个叫做KAISER，现在在Linux中被称为KPTI的技术（Kernel page-table isolation）。\n这里的想法很简单，也就是不将内核内存映射到用户的Page Table中，相应的就像XV6一样，在系统调用时切换Page Table。所以在用户空间时，Page Table只有用户内存地址的映射，如果执行了系统调用，会有类似于XV6中trampoline的机制，切换到拥有内核内存映射的另一个Page Table中，这样才能执行内核代码。\n这会导致Meltdown不能工作，因为现在你会切换Page Table，本来代表内核虚拟内存地址的r1寄存器不仅是没有权限，并且也没有意义了，因为现在的用户Page Table并没有包含对它的翻译，所以CPU并不知道该如何处理这个内存地址。\n现在这个虚拟内存地址不会存在于cache中，甚至都不会出现在TLB中。所以当在用户空间发起Meltdown Attack时，也就没有办法知道对应这个虚拟内存地址的数据是什么。这个虚拟内存地址并不是非法的，只是在用户空间没有意义了，这样会导致Meltdown Attack不能工作。\nKAISER的缺点是，系统调用的代价更高了，因为如果不做任何事情的话，切换Page Table会导致TLB被清空，因为现在TLB中的映射关系都是前一个Page Table的。同时也会导致L1 cache被清空，因为其中对应的虚拟内存地址对于新的Page Table也没有意义了。在一些机器上，切换Page Table会使得系统调用明显变慢。\n最近的CPU拥有叫做PCID（process-context identifiers）的技术，它可以帮助你在切换Page Table时避免清空Cache，尽管它还是要花费一些时间。\n如果你上网看的话，当时人们有很多顾虑，当时人们认为这种两个Page Table的方案是不可接受的慢。但是实际中这并不是一个严重的问题，你上网看的话就可以发现人们有对于工作负载的整体影响的评估，因为毕竟程序也不是一直在进出内核，这里的影响大概是5%，所以这并不是一个坏的主意。\n人们非常快的采用了这种方案，实际上在论文发表时，已经有内核采用了这种方案来抵御其他的攻击。\n除此之外，还有一个合理的硬件修复。我相信Intel在最近的处理器上已经添加了这个修复，AMD之前就已经有这个修复。\n这是Cache的结构，当指令从L1 cache中加载某个数据时，比如说我们想要窃取的内核数据，人们认为数据的权限标志位就在L1 cache中，所以CPU完全可以在获取数据的时候检查权限标志位。\n实际中，AMD CPU和最近的Intel CPU会在很早的时候检查权限标志位。如果检查不能通过，CPU不会返回数据到CPU核中。所以没有一个预测执行指令可以看到不该看到的数据。\n学生提问：为什么你觉得Intel会做这个呢？对我来说这里像是个讨论，我们应该为预测执行指令检查权限标志位吗？Intel的回答是不，为什么要检查呢？\nRobert教授：是的，为什么要检查呢？反正用户也看不到对应的数据。如果更早的做权限检查，会在CPU核和L1 cache之间增加几个数字电路门，而CPU核和L1 cache之间路径的性能对于机器来说重要的，如果你能在这节省一些数字电路门的话，这可以使得你的CPU节省几个cycle来从L1 cache获取数据，进而更快的运行程序。\n所以很容易可以想到如果过早的检查权限，会在电路上增加几个晶体管。因为毕竟所有的预测执行指令都会Retired，并不是说过早的检查权限就可以节省一些后续的工作，在指令Retired的时候还是要触发Page Fault。我这里只是猜测，这里做一些权限检测并不能带来什么优势。\n学生提问：既然Intel已经从CPU上修复了这个问题，有没有哪个内核计划取消KAISER来提升性能？\nRobert教授：我知道在很多内核上，这个是可选项，但是我并不完全清楚Intel修复的具体内容。我很确定他们有一些修复，但是具体内容我并不知道。\nFrans教授：我认为Linux中你可以查询哪些硬件修复已经存在，并根据返回要求Linux修改从软件对于硬件问题的规避。你可以在你的笔记本上运行一个Linux命令来查看它包含了哪些问题的修复，哪些问题已经在硬件中规避了。\nRobert教授：你是说如果CPU包含了修复的话，Linux实际会使用combined Page Table（注，也就是将内核内存映射到用户Page Table中）？\nFrans教授：是的，我99%相信是这样的，虽然我最近没有再看过了，但是我认为还是这样的。\n学生提问：人们是在干什么的时候发现这个的？\nRobert教授：当人们尝试入侵一个计算机的时候。谁知道人们真正想要干什么呢？论文是由学院派写的，或许他们在研究的时候发现了一些安全问题。\nFrans教授：我认为很长时间他们的一个驱动力是，他们想破解Address Space Layout Randomization，他们有一些更早的论文，看起来在这个领域有一些研究者。我认为最开始的时候，人们来自不同的领域。 就像Robert说过的，人们在这个领域工作了几十年来找到可以理解和攻击的Bug。\n学生提问：有多大的可能还存在另一种Meltdown？\nRobert教授：非常有可能。CPU制造商在几十年间向CPU增加了非常非常多酷炫的技术，以使得CPU运行的可以更快一些。人们之前并没有太担忧或者没有觉得这会是一个严重的安全问题。\n现在人们非常清楚这可能会是非常严重的安全问题，但是我们现在使用的CPU已经包含了30年的聪明思想，实际上在论文发表之前，已经存在很多基于Micro-Architectural的这一类攻击。我认为还需要一段时间才能把这一类问题完全消除。\nFrans教授：如果你查看过去两年的安全相关的会议，每个会议基本都有一个session是有关探索预测执行属性，来看看能不能发起一次攻击。\nRobert教授：或许这是一个更大的问题，是不是我们解决了有限的问题就没事了，又或者是上层设计方法出现问题了。这可能太过悲观了，但是你知道的，人们对于操作系统的隔离寄托了太多期望，可以非常合理的认为隔离可以工作。\n并且我们会在这种假设下设计类似于云计算，在浏览器中运行Javascript等等场景。但是现在这种假设实际并不成立，曾经人们认为操作系统的隔离性足够接近成立，但是这一整套基于Micro-Architectural的攻击使得这里的故事不再让人信服。\n学生提问：CPU设计者可以做到什么程度使得不使用Micro-Architectural又能保持高性能，同时也有很好的安全性？\nRobert教授：有些内容明显是可以修复的，比如这节课介绍的Meltdown Attack是可以被修复的，并且不会牺牲任何性能。对于一些其他的攻击，并不十分确定你可以在不损伤性能的前提下修复它们。\n有些问题隐藏的非常非常的深，现在有很多共享的场景，例如分时共享的计算机，云计算。假设在你的云主机上有一个磁盘驱动和一个网卡驱动，你或许可以仅仅通过监测别人的流量是怎么影响你的流量的，这里的流量包括了网络流量和磁盘流量，来获取同一个主机上的其他用户信息。\n我不知道这是否可行，但是对于很多东西，人们都能发现可以攻击的点。\n所以很多这里的Micro-Architectural带来的问题可以在不损伤性能的前提下清除掉，但是也或许不能。\n","date":"2024-10-07T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08122meltdown/19_hu15519103791573261584.jpg","permalink":"https://echudet.github.io/p/mit6s08122meltdown/","title":"MIT6S081：22Meltdown"},{"content":"Lecture 21 Networking 课前预习 论文速览 论文《Eliminating Receive Livelock in an Interrupt-driven Kernel》分为多个部分，每部分都有其核心内容，以下是各部分的详细总结：\n在引言（Introduction）部分首先介绍了接收活锁（receive livelock）问题。传统的中断机制适用于早期的流量控制系统，但在面对现代非流量控制协议（如多媒体和RPC客户端-服务器系统）时，这种机制容易导致活锁问题。接收活锁是指系统在处理中断时耗尽所有资源，却无法执行其他关键任务，从而使系统进程停滞。\n在激励应用（Motivating Applications）部分，作者提供了几种容易受到活锁影响的应用示例：\n基于主机的路由（Host-based Routing）：许多现代路由器在面对非流量控制的高负载流量时，容易陷入活锁状态。 被动网络监控（Passive Network Monitoring）：处于混杂模式的网络接口在监控流量时，易遭受大规模的数据包输入，从而面临活锁。 网络文件服务（Network File Services）：如NFS服务器，在处理大量RPC流量时，也可能受到活锁的影响。 在调度网络任务的需求（Requirements for Scheduling Network Tasks）部分谈到，为了避免活锁并确保系统性能，操作系统必须合理调度网络任务。该部分讨论了系统在面对不同负载时，如何保证吞吐量、延迟、抖动（延迟波动）的合理控制，同时确保公平分配CPU资源，以避免系统过载。\n在中断驱动的调度及其后果（Interrupt-driven Scheduling and Its Consequences）部分中，中断驱动系统的缺陷被详细讨论，包括：\n接收活锁：系统在处理中断时，忽略了其他任务，导致吞吐量降为零。 延迟增加：在高负载下，系统推迟了数据包的交付，延迟大幅增加。 传输饥饿：过多的输入处理会阻止数据包的传输，导致系统仅接收数据包，却无法发送。 在改进调度避免活锁（Avoiding Livelock Through Better Scheduling）部分中作者提出了一些避免活锁的方法：\n限制中断到达速率：当系统超载时，暂时禁用中断，以便系统处理更高层的任务。 轮询（Polling）机制：通过轮询机制代替中断，确保公平分配CPU资源，避免活锁。 避免抢占：防止系统在处理中断时被中断，这样可以降低活锁的风险。 在 BSD路由器中的活锁问题（Livelock in BSD-based Routers）部分中，作者通过实验展示了BSD系统中的活锁问题。在负载超过一定限度后，系统因处理中断耗尽资源，导致性能急剧下降。作者详细讨论了为何接收活锁会出现在这些系统中，并指出这些系统在面对高负载时会陷入活锁状态。\n在确保用户进程的进展（Guaranteeing Progress for User-level Processes）部分，作者讨论了如何在高网络负载下，仍然为用户级进程保留足够的CPU时间。提出了一些机制，如CPU周期限制，确保系统在处理中断时，不会完全忽略用户进程。\n相关工作（Related Work），该部分回顾了先前与轮询机制、中断驱动设计等相关的工作，讨论了现有技术如何减少中断开销，但这些技术仍不足以完全消除活锁问题。\n总结与结论（Summary and Conclusions），论文总结了避免活锁的关键点：\n活锁在中断驱动系统中是一个严重的问题，尤其是在高网络负载下。 通过引入新的调度策略（如中断限速、轮询机制和CPU时间管理），系统可以有效避免活锁，并维持良好的性能。 这些改进方法使得系统能够在高负载情况下，依然保持合理的响应速度和稳定的性能。\nfeedback 在实验中，作者发现当系统处理接收的数据包时，如果筛选队列（例如供 screend 使用的筛选队列）已满，那么继续接收数据包会导致系统资源浪费。这些数据包最终会在被完全处理前被丢弃，浪费了宝贵的处理时间。为了应对这一问题，作者在内核中添加了反馈机制：当检测到筛选队列已满时，系统会停止进一步的输入处理，直到队列有了足够的空间。\n具体来说，反馈机制会在系统中检测到特定队列（例如筛选队列）满时，立即暂停接收新的数据包，暂时禁用输入中断。系统会定期检查队列状态，当队列中的数据包处理到一定程度（比如达到队列容量的25%）时，重新允许输入中断恢复工作。这一机制确保了系统不会因过多的接收任务导致资源浪费，并且能够腾出时间处理现有的队列中的数据包。\n在实验中，作者选择的筛选队列大小为32个数据包，并在队列填满75%时停止输入处理，只有当队列数据量减少至25%时才重新启用输入。这种高、低水位线的策略能够帮助系统有效地管理资源并保持一定的吞吐量。队列反馈机制的主要作用包括：\n避免活锁：系统在高负载下避免无谓的数据包处理，当筛选队列满时直接暂停输入，确保资源集中处理现有数据包。 提高资源利用率：反馈机制确保了系统可以充分利用输入队列，在队列资源消耗完之前处理到位，从而减少数据包丢弃情况，提升整体处理效率。 稳定系统性能：通过合理地分配资源，该机制确保系统在高负载下依然能够以一定的速度处理和转发数据包，从而避免系统在过载情况下崩溃或陷入死循环。 在使用 screend 程序的实验中，通过引入反馈机制，系统在高负载时能够显著提升峰值吞吐量，避免了活锁问题。通过这些测试，作者还发现不同的配额设置（即每次轮询处理的包数量）对系统性能影响较大，小配额（如10-20个包）能有效避免活锁并保持系统稳定。\n总体来说，第6.6节的反馈机制通过实时监控队列状态，并在队列容量达到一定程度时暂停输入，实现了更高效的资源管理，确保系统在高负载下稳定运行并防止活锁现象。\n操作系统的网络功能 操作系统的网络功能在底层确实使用了类似上面提到的编程接口（API）来处理网络通信，但这些功能背后涉及操作系统中的网络协议栈、驱动程序和硬件接口等复杂的机制。操作系统通过提供一系列的系统调用或库函数接口，使应用程序可以方便地进行网络通信。\n这些接口可以分为以下几类：\n套接字接口（Socket API）\n这是操作系统最常见的网络编程接口，主要用于实现不同网络协议（如 TCP 和 UDP）的网络通信。大多数现代操作系统提供的网络功能都是基于套接字的。操作系统通过系统调用（如 socket()、connect() 等）为用户提供操作网络连接的接口。\n以下是常用的套接字接口：\nsocket()：创建一个套接字，指定协议族（如 IPv4、IPv6）和传输协议（如 TCP、UDP）。 bind()：将套接字与本地地址（IP 和端口）绑定，用于服务端监听特定的网络接口。 listen()：将套接字设置为被动监听模式，等待客户端连接。 accept()：服务端接受客户端连接，返回新的套接字用于通信。 connect()：客户端用来连接服务器的套接字。 send() 和 recv()：分别用于发送和接收数据。 sendto() 和 recvfrom()：用于无连接的 UDP 协议，直接向指定地址发送数据或从指定地址接收数据。 close()：关闭套接字，终止连接。 网络设备接口\n操作系统提供了对网络设备（如网卡、无线适配器）的抽象，允许应用程序通过接口访问这些设备。例如，通过系统调用和ioctl命令来控制设备状态，修改网络配置。\n常见的操作有：\nioctl()：控制网络接口，配置地址、网络掩码、路由表等。 ifconfig：在类 Unix 系统中，用于配置和管理网络接口的命令（或对应的 API）。 ethtool：查看和控制以太网设备的属性。 数据链路层接口\n一些操作系统允许开发人员直接操作数据链路层，通过特定的接口发送和接收原始的以太网帧（Raw Sockets）。这通常用于开发低级别的网络工具或网络协议的实现。\n接口如：\nAF_PACKET：Linux 中允许直接在链路层上发送/接收数据包的套接字类型，用于实现自定义协议。 网络配置接口\n操作系统还提供了一些接口，用于用户或应用程序动态配置网络，如修改 IP 地址、设置路由、配置防火墙等。\n常见的工具和接口：\nnetstat：查看网络连接和端口状态。 route：查看和修改系统的路由表。 ip：现代 Linux 系统中，用于配置 IP 地址、路由、防火墙等。 iptables：设置和管理 Linux 系统的防火墙规则。 异步 I/O 接口\n现代操作系统提供的网络接口通常支持异步 I/O，允许应用程序在等待网络事件时不阻塞。这对于高并发应用和事件驱动的网络程序非常重要。\n常用的异步接口：\nselect() 和 poll()：检查多个套接字的状态，确定哪些套接字可读、可写或发生错误。 epoll()（Linux） 和 kqueue()（BSD 系统）：更高效的事件通知机制，用于大规模并发连接。 异步 I/O 框架：如 Linux 的 AIO 或 Windows 的 IOCP，允许更高效的非阻塞 I/O 操作。 高级网络协议接口\n除了底层套接字接口，操作系统往往通过用户态库提供对更高级网络协议的支持，如：\nHTTP：通过操作系统提供的 API 或高级网络库（如 libcurl）支持高层协议。 TLS/SSL：如 OpenSSL，提供安全传输层的加密通信支持。 虚拟网络接口\n操作系统还可以提供虚拟网络设备，用于虚拟机网络通信、VPN 实现等，如：\nTUN/TAP 设备：Linux 中用于创建用户态虚拟网络接口的设备。TUN 用于三层网络（如 IP），TAP 用于二层网络（如以太网帧）。 网络管理接口\n现代操作系统通过管理接口提供动态的网络资源管理，应用程序可以通过这些接口调整网络行为，如 QoS、流量管理等。\n常见的网络管理接口包括：\nNetworkManager：Linux 上的网络管理服务，允许动态切换网络配置（Wi-Fi、有线网络等）。 Windows Sockets（Winsock）：Windows 操作系统上用于网络通信的接口。 防火墙和安全接口\n操作系统提供了用于设置网络防火墙和安全策略的接口。例如：\niptables（Linux）和 firewalld：用于定义网络防火墙规则。 Windows Firewall API：Windows 操作系统提供的防火墙控制接口。 示例代码（基于 C 语言的 POSIX 套接字）\n操作系统底层通过 C 语言的 POSIX 标准为程序员提供系统调用来访问网络功能。以下是一个基于 C 的简单 TCP 服务器示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; int main() { int server_fd, new_socket; struct sockaddr_in address; int opt = 1; int addrlen = sizeof(address); char buffer[1024] = {0}; // 创建套接字 server_fd = socket(AF_INET, SOCK_STREAM, 0); if (server_fd == 0) { perror(\u0026#34;socket failed\u0026#34;); exit(EXIT_FAILURE); } // 绑定地址和端口 address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons(8080); bind(server_fd, (struct sockaddr *)\u0026amp;address, sizeof(address)); // 监听 listen(server_fd, 3); printf(\u0026#34;Server is listening on port 8080\\n\u0026#34;); // 接受客户端连接 new_socket = accept(server_fd, (struct sockaddr *)\u0026amp;address, (socklen_t*)\u0026amp;addrlen); // 读取数据 read(new_socket, buffer, 1024); printf(\u0026#34;Received: %s\\n\u0026#34;, buffer); // 发送响应 char *response = \u0026#34;Hello from server\u0026#34;; send(new_socket, response, strlen(response), 0); // 关闭连接 close(new_socket); close(server_fd); return 0; } 总的来说，操作系统通过各种网络接口向用户提供网络功能，从底层的套接字系统调用到更高级的网络管理工具和库。应用程序通过这些接口可以实现网络通信、数据传输、网络设备配置等功能。\nLivelock Livelock 是一种计算机系统中常见的同步问题，类似于死锁（Deadlock），但有所区别。在 livelock 情况下，多个进程或线程虽然没有完全停止运行（也就是说它们仍然在“活动”中），但无法继续执行有意义的操作，因为它们彼此不断地改变状态或采取相互回避的行为，导致始终无法达到目标状态。二者区别如下：\n死锁（Deadlock） 活锁（Livelock） 进程或线程由于资源竞争或互相等待而完全停滞，不会再有任何进展。 进程或线程仍在运行，但因互相妥协或状态改变，始终无法完成预期的任务，表现为一种不断尝试和回退的循环。 举一个例子来形容livelock，即两个人同时尝试从狭窄的走廊中通过，并相互避让。人 A 往左，人 B 也往左。然后人 A 发现两人撞上了，决定往右，而人 B 也往右。他们就这样不断相互避让，但永远无法成功通过。\n计算机系统中的例子：在多线程编程中，如果两个线程使用自旋锁（spinlock），并不断尝试解除锁，但总是因为其他线程在做相同的事情而陷入循环，则可能形成 livelock。\n造成Livelock可能有多个原因：\n不恰当的资源释放策略：线程或进程在资源竞争中频繁地释放和请求资源。 优先级反转和调度问题：当线程的优先级动态变化，导致调度策略始终无法满足任务需求时。 不良的避免冲突策略：如多个线程在检测冲突后不断地相互回避，却无法找到合适的行动策略。 相应的，可以预防 Livelock的方法有如下：\n引入随机化：通过在冲突时引入随机等待或随机操作策略，可以避免相互的“镜像”行为。 限制重试次数：设定尝试的最大次数或时间限制，超过后采取其他方式或放弃操作。 引入优先级机制：在多线程竞争时，设置适当的优先级，以确保重要操作能够顺利完成。 要识别 Livelock，通常需要对系统的状态进行监控和分析，查看是否存在某些进程或线程在做无效的、重复的动作而没有实际进展。可以通过以下方法进行调试：\n日志分析：检查重复操作的模式。 CPU 占用率监控：看是否存在线程持续占用 CPU 但没有实际产出。 执行路径跟踪：追踪线程执行的具体路径，确认是否进入了一个无效循环。 Livelock 是一种由于资源协调不当而导致的“忙碌等待”现象，其表现为系统中某些进程或线程虽然在活动，但因重复的避让行为而无法完成有效的任务。与死锁不同，活锁需要更复杂的策略来处理，如引入随机化、调整优先级等。\nReceive Livelock Receive Livelock 是网络系统中的一种活锁现象，通常发生在高流量环境下，当系统过度忙于处理接收的数据包，但无法有效传递数据到更高层的应用或协议栈时。它本质上是网络设备或操作系统在网络包接收过程中被不断打断，导致没有足够时间处理或传递这些数据包，使得传输效率极低。\n在网络中，尤其是高流量下，网络接口会迅速收到大量数据包。由于每个接收的包都可能触发一个中断，CPU会频繁响应这些中断，导致以下问题：\n频繁的中断：系统一旦收到网络包就产生中断，迫使 CPU 停止当前任务去处理网络包，这会使应用层或协议栈无法获取足够的时间来处理这些包。 资源争用：由于系统忙于响应中断，几乎所有 CPU 资源都用于接收和调度数据包，使得系统其他任务无法执行，包括转发或处理这些接收的数据包。 处理延迟：不断的中断和无效处理会让系统无法在每次中断间完成一个完整的数据包处理流程，进而形成 livelock。 系统表现出接收很多包但没有真正处理的情况。例如：\n高 CPU 占用率：系统在处理中断，但 CPU 时间主要耗在接收上，而非有效处理。 低传输效率：虽然网络接口忙于接收，但传输的数据量不成比例，表现为低吞吐量。 应用层数据包少：应用程序无法接收到足够的有效数据，影响服务性能。 为了应对 receive livelock，可以采取以下措施：\n减少中断频率：采用“中断聚合”（Interrupt Coalescing）技术，将多个中断合并为一个，从而减少 CPU 处理中断的频率，让系统有时间有效处理数据包。 使用 NAPI（New API）技术：NAPI 是 Linux 内核中的一种中断处理机制，用于网络包的批量处理。它将接收从中断驱动切换到轮询驱动，当流量过高时，系统进入轮询模式，这样系统可以避免频繁中断，从而减轻 CPU 负担。 分布式处理：使用多核 CPU 并合理分配网络任务，让多个核心共同处理接收和转发数据包。 限流策略：控制接收的数据包速率，例如限制每秒接收的包数，避免超过系统处理能力。 为确认是否发生 receive livelock，可以通过以下步骤检测：\n监控 CPU 占用和中断频率：观察是否有单个 CPU 核心被高频中断占满。 流量分析：检查网络流量与应用层收到的有效数据包量是否存在显著差距。 应用层性能测试：通过对比不同网络负载下的应用性能，分析是否存在高负载下的明显性能下降。 Receive Livelock 是网络系统因高频中断而导致的活锁现象，会严重影响系统的数据处理和传输效率。通过减少中断、优化处理机制和引入流量控制策略，可以有效减轻其影响。\n正课部分 计算机网络概述 今天我想讨论一下Networking以及它与操作系统的关联。今天这节课的很多内容都与最后一个lab，也就是构建一个网卡驱动相关。\n在这节课，我们首先会大概看一下操作系统中网络相关的软件会有什么样的结构，之后我们会讨论今天的论文Livelock 。Livelock展示了在设计网络协议栈时可能会出现的有趣的陷阱。\n首先，让我通过画图来描述一下基本的网络场景。网络连接了不同的主机，这里的连接有两种方式：\n相近的主机连接在同一个网络中。例如有一个以太网设备，可能是交换机或者单纯的线缆，然后有一些主机连接到了这个以太网设备。这里的主机可能是笔记本电脑，服务器或者路由器。在设计网络相关软件的时候，通常会忽略直接连接了主机的网络设备。\n这里的网络设备可能只是一根线缆（几十年前就是通过线缆连接主机）；也可能是一个以太网交换机；也可能是wifi无线局域网设备（主机通过射频链路与网络设备相连），但是不管是哪种设备，这种直接连接的设备会在网络协议栈的底层被屏蔽掉。\n每个主机上会有不同的应用程序，或许其中一个主机有网络浏览器，另一个主机有HTTP server，它们需要通过这个局域网来相互通信。\n一个局域网的大小是有极限的。局域网（Local Area Network）通常简称为LAN。\n一个局域网需要能让其中的主机都能收到彼此发送的packet。有时，主机需要广播packet到局域网中的所有主机。当局域网中只有25甚至100个主机时，是没有问题的。但是你不能构建一个多于几百个主机的局域网。\n所以为了解决这个问题，大型网络是这样构建的。首先有多个独立的局域网，假设其中一个局域网是MIT，另一个局域网是Harvard，还有一个很远的局域网是Stanford，在这些局域网之间会有一些设备将它们连接在一起，这些设备通常是路由器Router。\n其中一个Router接入到了MIT的局域网，同时也接入到了Harvard的局域网。\n路由器是组成互联网的核心，路由器之间的链路，最终将多个局域网连接在了一起。\n现在MIT有一个主机需要与Stanford的一个主机通信，他们之间需要经过一系列的路由器，路由器之间的转发称为Routing。\n所以我们需要有一种方法让MIT的主机能够寻址到Stanford的主机，并且我们需要让连接了MIT的路由器能够在收到来自MIT的主机的packet的时候，能够知道这个packet是发送给Harvard的呢，还是发送给Stanford的。\n从网络协议的角度来说，局域网通信由以太网协议决定。而局域网之上的长距离网络通信由Internet Protocol协议决定。以上就是网络的概述。\n接下来我想介绍一下，在局域网和互联网上传递的packet有什么样的结构，之后再讨论在主机和路由器中的软件是如何处理这些packet。\n二层网络 \u0026mdash; Ethernet 让我从最底层开始，我们先来看一下一个以太网packet的结构是什么。当两个主机非常靠近时，或许是通过相同的线缆连接，或许连接在同一个wifi网络，或许连接到同一个以太网交换机。\n当局域网中的两个主机彼此间要通信时，最底层的协议是以太网协议。你可以认为Host1通过以太网将Frame发送给Host2。\nFrame是以太网中用来描述packet的单词，本质上这就是两个主机在以太网上传输的一个个的数据Byte。以太网协议会在Frame中放入足够的信息让主机能够识别彼此，并且识别这是不是发送给自己的Frame。\n每个以太网packet在最开始都有一个Header，其中包含了3个数据。Header之后才是payload数据。Header中的3个数据是：目的以太网地址，源以太网地址，以及packet的类型。\n每一个以太网地址都是48bit的数字，这个数字唯一识别了一个网卡。\npacket的类型会告诉接收端的主机该如何处理这个packet。接收端主机侧更高层级的网络协议会按照packet的类型检查并处理以太网packet中的payload。整个以太网packet，包括了48bit+48bit的以太网地址，16bit的类型，以及任意长度的payload这些都是通过线路传输。\n除此之外，虽然对于软件来说是不可见的，但是在packet的开头还有被硬件识别的表明packet起始的数据（注，Preamble + SFD），在packet的结束位置还有几个bit表明packet的结束（注，FCS）。packet的开头和结束的标志不会被系统内核所看到，其他的部分会从网卡送到系统内核。\n如果你们查看了这门课程的最后一个lab，你们可以发现我们提供的代码里面包括了一些新的文件，其中包括了kernel/net.h，这个文件中包含了大量不同网络协议的packet header的定义。\n上图中的代码包含了以太网协议的定义。我们提供的代码使用了这里结构体的定义来解析收到的以太网packet，进而获得目的地址和类型值（注，实际中只需要对收到的raw data指针强制类型转换成结构体指针就可以完成解析）。\n学生提问：硬件用来识别以太网packet的开头和结束的标志是不是类似于lab中的End of Packets？\nRobert教授：并不是的，EOP是帮助驱动和网卡之间通信的机制。\n这里的开头和结束的标志是在线缆中传输的电信号或者光信号，这些标志位通常在一个packet中是不可能出现的。以结束的FCS为例，它的值通常是packet header和payload的校验和，可以用来判断packet是否合法。\n有关以太网48bit地址，是为了给每一个制造出来的网卡分配一个唯一的ID，所以这里有大量的可用数字。\n这里48bit地址中，前24bit表示的是制造商，每个网卡制造商都有自己唯一的编号，并且会出现在前24bit中。后24bit是由网卡制造商提供的任意唯一数字，通常网卡制造商是递增的分配数字。\n所以，如果你从一个网卡制造商买了一批网卡，每个网卡都会被写入属于自己的地址，并且如果你查看这些地址，你可以发现，这批网卡的高24bit是一样的，而低24bit极有可能是一些连续的数字。\n虽然以太网地址是唯一的，但是出了局域网，它们对于定位目的主机的位置是没有帮助的。如果网络通信的目的主机在同一个局域网，那么目的主机会监听发给自己的地址的packet。\n但是如果网络通信发生在两个国家的主机之间，你需要使用一个不同的寻址方法，这就是IP地址的作用。\n在实际中，你可以使用tcpdump来查看以太网packet。这将会是lab的一部分。下图是tcpdump的一个输出：\ntcpdump输出了很多信息，其中包括：\n接收packet的时间 第一行的剩下部分是可读的packet的数据 接下来的3行是收到packet的16进制数 如果按照前面以太网header的格式，可以发现packet中：\n前48bit是一个广播地址，0xffffffffffff。广播地址是指packet需要发送给局域网中的所有主机。 之后的48bit是发送主机的以太网地址，我们并不能从这个地址发现什么，实际上这个地址是运行在QEMU下的XV6生成的地址，所以地址中的前24bit并不是网卡制造商的编号，而是QEMU编造的地址。 接下来的16bit是以太网packet的类型，这里的类型是0x0806，对应的协议是ARP。 剩下的部分是ARP packet的payload。 二/三层地址转换 \u0026mdash; ARP 下一个与以太网通信相关的协议是ARP。\n在以太网层面，每个主机都有一个以太网地址。但是为了能在互联网上通信，你需要有32bit的IP地址。\n为什么需要IP地址呢？因为IP地址有额外的含义。\nIP地址的高位bit包含了在整个互联网中，这个packet的目的地在哪。所以IP地址的高位bit对应的是网络号，虽然实际上要更复杂一些，但是你可以认为互联网上的每一个网络都有一个唯一的网络号。\n路由器会检查IP地址的高bit位，并决定将这个packet转发给互联网上的哪个路由器。IP地址的低bit位代表了在局域网中特定的主机。\n当一个经过互联网转发的packet到达了局域以太网，我们需要从32bit的IP地址，找到对应主机的48bit以太网地址。这里是通过一个动态解析协议完成的，也就是Address Resolution Protocol，ARP协议。\n当一个packet到达路由器并且需要转发给同一个以太网中的另一个主机，或者一个主机将packet发送给同一个以太网中的另一个主机时，发送方首先会在局域网中广播一个ARP packet，来表示任何拥有了这个32bit的IP地址的主机，请将你的48bit以太网地址返回过来。\n如果相应的主机存在并且开机了，它会向发送方发送一个ARP response packet。\n下图是一个ARP packet的格式：\n它会出现在一个以太网packet的payload中。所以你们看到的将会是这样的结构：首先是以太网header，它包含了48bit的目的以太网地址，48bit的源以太网地址，16bit的类型；之后的以太网的payload会是ARP packet，包含了上图的内容。\n接收到packet的主机通过查看以太网header中的16bit类型可以知道这是一个ARP packet。在ARP中类型值是0x0806。通过识别类型，接收到packet的主机就知道可以将这个packet发送给ARP协议处理代码。\n有关ARP packet的内容，包含了不少信息，但是基本上就是在说，现在有一个IP地址，我想将它转换成以太网地址，如果你拥有这个IP地址，请响应我。\n同样的，我们也可以通过tcpdump来查看这些packet。\n在网络的lab中，XV6会在QEMU模拟的环境下发送IP packet。所以你们可以看到在XV6和其他主机之间有ARP的交互。\n下图中第一个packet是我的主机想要知道XV6主机的以太网地址，第二个packet是XV6在收到了第一个packet之后，并意识到自己是IP地址的拥有者，然后返回response。\ntcpdump能够解析出ARP packet，并将数据打印在第一行。对应ARP packet的格式，在第一个packet中，10.0.2.2是SIP，10.0.2.15是DIP。在第二个packet中，52:54:00:12:34:56对应SHA。\n同时，我们也可以自己分析packet的原始数据。对于第一个packet：\n前14个字节是以太网header，包括了48bit目的以太网地址，48bit源以太网地址，16bit类型。 从后往前看，倒数4个字节是TIP，也就是发送方想要找出对应以太网地址的IP地址。每个字节对应了IP地址的一块，所以0a00 020f对应了IP地址10.0.2.15。 再向前数6个字节，是THA，也就是目的地的以太网地址，现在还不知道所以是全0。 再向前数4个字节是SIP，也就是发送方的IP地址，0a000202对应了IP地址10.0.2.2。 再向前数6个字节是SHA，也就是发送方的以太网地址。 剩下的8个字节表明了我们感兴趣的是以太网和IP地址格式。 第二个packet是第一个packet的响应。\n学生提问：ethernet header中已经包括了发送方的以太网地址，为什么ARP packet里面还要包含发送方的以太网地址？\nRobert教授：我并不清楚为什么ARP packet里面包含了这些数据，我认为如果你想的话是可以精简一下ARP packet。\n或许可以这么理解，ARP协议被设计成也可以用在其他非以太网的网络中，所以它被设计成独立且不依赖其他信息，所以ARP packet中包含了以太网地址。\n现在我们是在以太网中发送ARP packet，以太网packet也包含了以太网地址，所以，如果在以太网上运行ARP，这些信息是冗余的。\n但是如果在其他的网络上运行ARP，你或许需要这些信息，因为其他网络的packet中并没有包含以太网地址。\n学生提问：tcpdump中原始数据的右侧是什么内容？\nRobert教授：这些是原始数据对应的ASCII码，“.”对应了一个字节并没有相应的ASCII码，0x52对应了R，0x55对应了U。当我们发送的packet包含了ASCII字符时，这里的信息会更加有趣。\n我希望你们在刚刚的讨论中注意到这一点，网络协议和网络协议header是嵌套的。我们刚刚看到的是一个packet拥有了ethernet header和ethernet payload。\n在ethernet payload中，首先出现的是ARP header，对于ARP来说并没有的payload。\n但是在ethernet packet中还可以包含其他更复杂的结构，比如说ethernet payload中包含一个IP packet，IP packet中又包含了一个UDP packet，所以IP header之后是UDP header。\n如果在UDP中包含另一个协议，那么UDP payload中又可能包含其他的packet，例如DNS packet。\n所以发送packet的主机会按照这样的方式构建packet：DNS相关软件想要在UDP协议之上构建一个packet；UDP相关软件会将UDP header挂在DNS packet之前，并在IP协议之上构建另一个packet；IP相关的软件会将IP heade挂在UDP packet之前；最后Ethernet相关的软件会将Ethernet header挂在IP header之前。所以整个packet是在发送过程中逐渐构建起来的。\n类似的，当一个操作系统收到了一个packet，它会先解析第一个header并知道这是Ethernet，经过一些合法性检查之后，Ethernet header会被剥离，操作系统会解析下一个header。\n在Ethernet header中包含了一个类型字段，它表明了该如何解析下一个header。\n同样的在IP header中包含了一个protocol字段，它也表明了该如何解析下一个header。\n软件会解析每个header，做校验，剥离header，并得到下一个header。一直重复这个过程直到得到最后的数据。这就是嵌套的packet header。\nDNS UDP IP ETH 三层网络 \u0026mdash; Internet Ethernet header足够在一个局域网中将packet发送到一个host。\n如果你想在局域网发送一个IP packet，那么你可以使用ARP获得以太网地址。但是IP协议更加的通用，IP协议能帮助你向互联网上任意位置发送packet。下图是一个IP packet的header，你们可以在lab配套的代码中的net.h文件找到。\n如果IP packet是通过以太网传输，那么你可以看到，在一个以太网packet中，最开始是目的以太网地址，源以太网地址，以太网类型是0x0800，之后是IP header，最后是IP payload。\nPAYLOAD IP T=0x0800 S D 在一个packet发送到世界另一端的网络的过程中，IP header会被一直保留，而Ethernet header在离开本地的以太网之后会被剥离。\n或许packet在被路由的过程中，在每一跳（hop）会加上一个新的Ethernet header。但是IP header从源主机到目的主机的过程中会一直保留。\nIP header具有全局的意义，而Ethernet header只在单个局域网有意义。\n所以IP header必须包含足够的信息，这样才能将packet传输给互联网上遥远的另一端。对于我们来说，关键的信息是三个部分，目的IP地址（ip_dst），源IP地址（ip_src）和协议（ip_p）。\n目的IP地址是我们想要将packet送到的目的主机的IP地址。地址中的高bit位是网络号，它会帮助路由器完成路由。IP header中的协议字段会告诉目的主机如何处理IP payload。\n如果你们看到过MIT的IP地址，你们可以看到IP地址是18.x.x.x，虽然最近有些变化，但是在很长一段时间18是MIT的网络号。所以MIT的大部分主机的IP地址最高字节就是18。全世界的路由器在看到网络号18的时候，就知道应该将packet路由到离MIT更近的地方。\n接下来我们看一下包含了IP packet的tcpdump输出。\n因为这个IP packet是在以太网上传输，所以它包含了以太网header。呃……，实际上这个packet里面有点问题，我不太确定具体的原因是什么，但是Ethernet header中目的以太网地址不应该是全f，因为全f是广播地址，它会导致packet被发送到所有的主机上。\n一个真实网络中两个主机之间的packet，不可能出现这样的以太网地址。所以我提供的针对network lab的方案，在QEMU上运行有点问题。不管怎么样，我们可以看到以太网目的地址，以太网源地址，以及以太网类型0x0800。0x0800表明了Ethernet payload是一个IP packet。\nIP header的长度是20个字节，所以中括号内的是IP header，\n从后向前看：\n目的IP地址是0x0a000202，也就是10.0.2.2。 源IP地址是0x0a00020f，也就是10.0.2.15。 再向前有16bit的checksum，也就是0x3eae。 再向前一个字节是protocol，0x11对应的是10进制17，表明了下一层协议是UDP 其他的就是我们不太关心的一些字段了，例如packet的长度。 IP header中的protocol字段告诉了目的主机的网络协议栈，这个packet应该被UDP软件处理。\n四层网络 \u0026mdash; UDP IP header足够让一个packet传输到互联网上的任意一个主机，但是我们希望做的更好一些。每一个主机都运行了大量需要使用网络的应用程序，所以我们需要有一种方式能区分一个packet应该传递给目的主机的哪一个应用程序，而IP header明显不包含这种区分方式。\n有一些其他的协议完成了这里的区分工作，其中一个是TCP，它比较复杂，而另一个是UDP。TCP不仅帮助你将packet发送到了正确的应用程序，同时也包含了序列号等用来检测丢包并重传的功能，这样即使网络出现问题，数据也能完整有序的传输。\n相比之下，UDP就要简单的多，它以一种“尽力而为”的方式将packet发送到目的主机，除此之外不提供任何其他功能。\nUDP header中最关键的两个字段是sport源端口和dport目的端口。\n当你的应用程序需要发送或者接受packet，它会使用socket API，这包含了一系列的系统调用。一个进程可以使用socket API来表明应用程序对于特定目的端口的packet感兴趣。\n当应用程序调用这里的系统调用，操作系统会返回一个文件描述符。每当主机收到了一个目的端口匹配的packet，这个packet会出现在文件描述符中，之后应用程序就可以通过文件描述符读取packet。\n这里的端口分为两类，一类是常见的端口，例如53对应的是DNS服务的端口，如果你想向一个DNS server发请求，你可以发送一个UDP packet并且目的端口是53。\n除此之外，很多常见的服务都占用了特定的端口。\n除了常见端口，16bit数的剩下部分被用来作为匿名客户端的源端口。比如说，我想向一个DNS server的53端口发送一个packet，目的端口会是53，但是源端口会是一个本地随机选择的端口，这个随机端口会与本地的应用程序的socket关联。\n所以当DNS server向本地服务器发送一个回复packet，它会将请求中的源端口拷贝到回复packet的目的端口，再将回复packet发送回本地的服务器。本地服务器会使用这个端口来确定应该将packet发送给哪个应用程序。\n接下来我们看一下UDP packet的tcpdump输出。\n首先，我们同样会有一个以太网Header，以及20字节的IP header。IP header中的0x11表明这个packet的IP协议号是17，这样packet的接收主机就知道应该使用UDP软件来处理这个packet。\n接下来的8个字节是UDP header。\n这里的packet是由lab代码生成的packet，所以它并没有包含常见的端口，源端口是0x0700，目的端口是0x6403。\n第4-5个字节是长度，第6-7个字节是校验和。XV6的UDP软件并没有生成UDP的校验和。\nUDP header之后就是UDP的payload。在这个packet中，应用程序发送的是ASCII文本，所以我们可以从右边的ASCII码看到，内容是“a.message.from.xv6”。\n所以ASCII文本放在了一个UDP packet中，然后又放到了一个IP packet中，然后又放到了一个Ethernet packet中。最后发布到以太网上。\n学生提问：当你发送一个packet给一个主机，但是你又不知道它的以太网地址，这个packet是不是会被送到路由器，之后再由路由器来找到以太网地址？\nRobert教授：如果你发送packet到一个特定的IP地址，你的主机会先检查packet的目的IP地址来判断目的主机是否与你的主机在同一个局域网中。\n如果是的话，你的主机会直接使用ARP来将IP地址翻译成以太网地址，再将packet通过以太网送到目的主机。更多的场景是，我们将一个packet发送到互联网上某个主机。\n这时，你的主机会将packet发送到局域网上的路由器，路由器会检查packet的目的IP地址，并根据路由表选择下一个路由器，将packet转发给这个路由器。这样packet一跳一跳的在路由器之间转发，最终离目的主机越来越近。\n学生提问：对于packet的长度有限制吗？\nRobert教授：有的。这里有几个不同的限制，每一个底层的网络技术，例如以太网，都有能传输packet的上限。\n今天我们要讨论的论文基于以太网最大可传输的packet是1500字节。最新的以太网可以支持到9000或者10000字节的最大传输packet。\n为什么不支持传输无限长度的packet呢？这里有几个原因：\n发送无限长度的packet的时间可能要很长，期间线路上会有信号噪音和干扰，所以在发送packet的时候可能会收到损坏的bit位。\n基本上每一种网络技术都会在packet中带上某种校验和或者纠错码，但是校验和也好，纠错码也好，只能在一定长度的bit位内稳定的检测错误。\n如果packet长度增加，遗漏错误的可能性就越来越大。所以一个校验和的长度，例如16bit或者32bit，限制了传输packet的最大长度。\n另一个限制是，如果发送巨大的packet，传输路径上的路由器和主机需要准备大量的buffer来接收packet。\n这里的代价又比较高，因为较难管理一个可变长度的buffer，管理一个固定长度的buffer是最方便的。而固定长度的buffer要求packet的最大长度不会太大。\n所以，以太网有1500或者9000字节的最大packet限制。\n除此之外，所有的协议都有长度字段，例如UDP的长度字段是16bit。所以即使以太网支持传输更大的packet，协议本身对于数据长度也有限制。\n以上就是UDP的介绍。在lab的最后你们会通过实验提供的代码来向谷歌的DNS server发送一个查询，收到回复之后代码会打印输出。你们需要在设备驱动侧完成以太网数据的处理。\n网络协议栈（Network Stack） 与packet的协议和格式对应的是运行在主机上的网络协议栈。人们有各种各样的方式来组织网络软件，接下来我会介绍最典型的，并且至少我认为是最标准的组织方式。\n假设我们现在在运行Linux或者XV6，我们有一些应用程序比如浏览器，DNS服务器。这些应用程序使用socket API打开了socket layer的文件描述符。\nSocket layer是内核中的一层软件，它会维护一个表单来记录文件描述符和UDP/TCP端口号之间的关系。同时它也会为每个socket维护一个队列用来存储接收到的packet。\n我们在networking lab中提供的代码模板包含了一个非常原始的socket layer。\n在socket layer之下是UDP和TCP协议层。UDP软件几乎不做任何事情，它只是检查收到的packet，获取目的端口号，并将UDP payload传输给socket layer中对应的队列。\nTCP软件会复杂的多，它会维护每个TCP连接的状态，比如记录每个TCP连接的序列号，哪些packet没有被ACK，哪些packet需要重传。所以TCP的协议控制模块会记录大量的状态，但是UDP中不会记录任何状态。\nUDP和TCP通常被称为传输层。networking lab提供的代码中有一个简单的UDP层，但是没有TCP的代码。\n在TCP/UDP之下是IP层，IP层的软件通常很简单。虽然我不确定是在同一层还是下一层，与IP层在一起的还有ARP层。\n再往下的话，我们可以认为还会有一层以太网。但是通常并没有一个独立的以太网层。通常来说这个位置是一个或者多个网卡驱动，这些驱动与实际的网卡硬件交互。网卡硬件与局域网会有实际的连接。\n当一个packet从网络送达时，网卡会从网络中将packet接收住并传递给网卡驱动。网卡驱动会将packet向网络协议栈上层推送。\n在IP层，软件会检查并校验IP header，将其剥离，再把剩下的数据向上推送给UDP。UDP也会检查并校验UDP header，将其剥离，再把剩下的数据加入到socket layer中相应文件描述符对应的队列中。\n所以一个packet在被收到之后，会自底向上逐层解析并剥离header。当应用程序发送一个packet，会自顶向下逐层添加header，直到最底层packet再被传递给硬件网卡用来在网络中传输。所以内核中的网络软件通常都是被嵌套的协议所驱动。\n这里实际上我忘了一件重要的事情，在整个处理流程中都会有packet buffer。所以当收到了一个packet之后，它会被拷贝到一个packet buffer中，这个packet buffer会在网络协议栈中传递。\n通常在不同的协议层之间会有队列，比如在socket layer就有一个等待被应用程序处理的packet队列，这里的队列是一个linked-list。通常整个网络协议栈都会使用buffer分配器，buffer结构。\n在我们提供的networking lab代码中，buffer接口名叫MBUF。\n以上就是一个典型的网络协议栈的分层图。\nRing Buffer 对于今天的论文，了解packet的控制流程是如何工作的还是比较重要，这里的控制流程与前一节介绍的分层网络协议栈还不太一样。\n有关网络协议栈，通常会有多个独立的actor会处理packet，解析packet并生成输出。\n出于各种各样的原因，这些不同的actor之间是解耦的，这样它们可以并发的运行，并且连接不同的packet队列。这对于今天的论文来说，是非常重要的前提。\n现在我们有了一张网卡，有了一个系统内核。当网卡收到了一个packet，它会生成一个中断。系统内核中处理中断的程序会被触发，并从网卡中获取packet。\n因为我们不想现在就处理这个packet，中断处理程序通常会将packet挂在一个队列中并返回，packet稍后再由别的程序处理。所以中断处理程序这里只做了非常少的工作，也就是将packet从网卡中读出来，然后放置到队列中。\n在一个传统的网络协议栈中，我们之所以想要快速的将packet从网卡中读出并存放于软件队列中，是因为通常来说网卡中用来存储packet的内存都非常小，而在计算机的RAM中，会有GB级别的内存，所以计算机的内存要大得多。\n如果有大量的packet发送到网卡，网卡可能会没有足够的内存来存储packet，所以我们需要尽快将packet拷贝到计算机的内存中。\n之后，在一个独立的线程中，会有一个叫做IP processing thread的程序。它会读取内存中的packet队列，并决定如何处理每一个packet。其中一个可能是将packet向上传递给UDP，再向上传递给socket layer的某个队列中，最后等待某个应用程序来读取。\n通常来说，这里的向上传递实际上就是在同一个线程context下的函数调用。\n另一种可能就是，这个主机实际上是个路由器，packet从一个网卡进来，经过路由需要从另一个网卡出去。\n通过例如Linux操作系统构建路由器是非常常见的。如果你买一个wifi路由器，或者一个有线调制解调器，非常有可能里面运行的就是Linux系统，并且使用了Linux网络协议栈，因为Linux的协议栈实现了完整的路由协议。\n所以，如果IP process thread查看了packet的目的IP地址，并决定将packet从另一个网卡转发出去，它会将packet加入到针对发送网卡的发送队列中。\n通常来说网卡会有发送中断程序，当网卡发送了一个packet，并且准备好处理更多packet的时候，会触发一个中断。所以网卡的发送中断也很重要。\n在这个结构中，有一点非常重要，这里存在一些并发的组件，它们以不同的方式调度。中断处理程序由网卡的发送或者接受中断触发。\nIP processing thread就是一个内核线程。在一个处理器上，IP processing thread不能与中断处理程序同时运行，因为中断处理程序的优先级最高，不过在多核处理器上，并发度可能会更高。\n最后，应用程序要能够读取socket layer中的packet，应用程序又是另一个独立调度的组件。所有这些组件都会参与到CPU的调度中。\n缓存队列经常会被提到，在上图中，总共有3个队列。这里的队列的作用是，一个独立的组件会向队列中添加packet，其他的组件会从队列中读取packet。\n在网络系统中，这样的队列很常见，主要出于以下几个原因：\n其中一个原因是可以应对短暂的大流量。\n比如，IP processing thread只能以特定的速度处理packet，但是网卡可能会以快得多的速度处理packet。对于短暂的大流量，我们想要在某个位置存储这些packet，同时等待IP processing来处理它们，这是网卡的接收方向。\n在网卡的发送方向，我们可能需要在队列中存储大量的packet，这样网卡可以在空闲的时候一直发送packet。有的时候100%利用网卡的发送性能还是很重要的。\n第三个原因是，队列缓存可以帮助组件之间解耦。\n我们不会想要IP processing thread或者应用程序知道中断处理程序的具体实现。在一个传统的操作系统中，IP processing thread并不必须知道中断是什么时候发生，或者应用程序怎么运行的。\n学生提问：同一个网卡可以即是接收方又是发送方吗？\nRobert教授：可以的。比如说我的笔记本只有一个网卡连接到了wifi，packet会从一个网卡进入并发出。双网卡通常用在路由器中。\n比如说我家里的wifi路由器，它就有两张网卡，其中一个网卡连接到线缆并进一步连接到整个互联网，另一个网卡是wifi网卡。\n有很多服务器也有多个网卡，尤其是对于web服务器来说，会有一个网卡连接互联网，另一个网卡连接你的私有的敏感的数据库信息。两个网卡连接的是完全不同的网络。\n学生提问：所以多网卡的场景在于想要连接不同的网络？\nRobert教授：是的。如果你想要连接不同的网络，那么你需要有多块网卡。\n我想再讨论一下当packet送到网卡时，网卡会做什么操作？这与networking lab非常相关。\n对于一个网卡的结构，会有一根线缆连接到外面的世界。网卡会检查线缆上的电信号，并将电信号转换成packet。网卡会接入到一个主机上，主机会带有网卡的驱动软件。我们需要将网卡解码出来的packet传递给主机的内存，这样软件才能解析packet。\n网卡内有许多内置的内存，当packet到达时，网卡会将packet存在自己的缓存中，并向主机发送中断，所以网卡内部会有一个队列。而主机的驱动包含了一个循环，它会与网卡交互，并询问当前是否缓存了packet。\n如果是的话，主机的循环会逐字节的拷贝packet到主机的内存中，再将内存中的packet加到一个队列中。\n这是我们今天要看的论文中网卡的工作方式：网卡驱动会负责拷贝网卡内存中的数据到主机内存。这在30年前还是有意义的，但是今天通过驱动中的循环来从硬件拷贝数据是非常慢的行为。\n即使是在同一个计算机上，外设到CPU之间的距离也非常的长，所以它们之间的交互需要的时间比较长。所以人们现在不会这么设计高速接口了。\n接下来我将讨论一下E1000网卡的结构，这是你们在实验中要使用的网卡。\nE1000网卡会监听网线上的电信号，但是当收到packet的时候，网卡内部并没有太多的缓存，所以网卡会直接将packet拷贝到主机的内存中，而内存中的packet会等待驱动来读取自己。所以，网卡需要事先知道它应该将packet拷贝到主机内存中的哪个位置。\nE1000是这样工作的，主机上的软件会格式化好一个DMA ring，ring里面存储的是packet指针。所以，DMA ring就是一个数组，里面的每一个元素都是指向packet的指针。\n当位于主机的驱动初始化网卡的时候，它会分配一定数量，例如16个1500字节长度的packet buffer，然后再创建一个16个指针的数组。\n为什么叫ring呢？因为在这个数组中，如果用到了最后一个buffer，下一次又会使用第一个buffer。\n主机上的驱动软件会告诉网卡DMA ring在内存中的地址，这样网卡就可以将packet拷贝到内存中的对应位置。\n当网卡收到packet时，网卡还会记住当前应该在DMA ring的哪个位置并通过DMA将packet传输过去。\n传输完成之后，网卡会将内部的记录的指针指向DMA ring的下一个位置，这样就可以拷贝下一个packet。\n刚才说的都是接收packet，对应的是RX ring。类似的，驱动还会设置好发送buffer，也就是TX ring。驱动会将需要网卡传输的packet存储在 TX ring中，网卡也需要知道TX ring的地址。\n你们在networking lab中的主要工作就是写驱动来处理这些ring。\n你们在networking lab中的主要工作就是写驱动来处理这些ring。\n学生提问：E1000与生产环境的高性能场景使用的网卡有什么区别吗？\nRobert教授：E1000曾经是最优秀的网卡，没有之一，并且它也曾经使用在生产环境中，但这是很多年前的事了。现代的网卡更加的“智能”，但是我们这里介绍的DMA ring结构并没有太多的变化，现在你仍然可以发现网卡使用DMA来传输packet，内存中对应的位置是由ring buffer的位置决定。\n现代的网卡更加“智能”在以下几个方面：\nE1000只能与一个RX ring传输数据，而现代网卡可以与多个RX ring同时传输数据。\n比如说你可以告诉一张现代的网卡，将接受到的packet分别传输给21个RX ring，网卡会根据packet的内容，决定将packet送到哪个RX ring。\n人们在很多地方都使用了这个特性，比如说在主机上运行了多个虚拟机，你可以使用这个特性将虚拟机对应的packet送到虚拟机对应的RX ring中，这样虚拟机可以直接读取相应的RX ring。（注，也就是网卡多队列）\n现代网卡更加“智能”的体现是，它们会完成一些TCP的处理，最常见的就是校验和计算。（注，各种TCP offload）\n所以，现代的网卡有与E1000相似的地方，但是更加的“智能”。\n学生提问：在接下来的networking lab中，IP层和驱动之间没有队列，是吗？\nRobert教授：是的，lab中的网络栈已经被剥离到了最小，它比实际的网络协议栈简单的多\n学生提问：那这样的话，性能会不会很差？\nRobert教授：我不知道，我没有在实际环境中运行过这些代码。在写networking lab的代码时，我们没有关注过性能。大多数情况下，性能不是问题，lab中的代码可以完成一个网络协议栈95%的功能，例如处理多网卡，处理TCP。\n学生提问：为了让网卡能支持DMA，需要对硬件做一些修改吗？在E1000之前的网卡中，所有的数据传输都是通过CPU进行传输。\nRobert教授：我们在介绍E1000之前的网卡时，网卡并不能访问内存。我认为这里最重要的问题是，当网卡想要使用主机内存中的某个地址时，虚拟内存地址是如何翻译的。我不知道这里是如何工作的。\n网卡通过总线，并经过一些可编程芯片连接到了DRAM，我认为在现代的计算机中，你可以设置好地址翻译表，这样网卡可以使用虚拟内存地址，虚拟内存地址会由网卡和DRAM之间的硬件翻译，这对于一些场景还是很有价值的。\n另一方面，如果网卡需要读写一些内存地址，而内存数据现在正在CPU的cache中，那么意味着内存对应的最新数据位于CPU cache中，而不是在RAM。这种情况下，当网卡执行DMA时，我们希望网卡能读取CPU的cache而不是RAM。\n在Intel的机器上，有一些精心设计的机制可以确保当网卡需要从内存读取数据而最新的内存数据在CPU cache中时，CPU cache而不是RAM会返回数据。一些软件基于这种机制来获得高性能。对于写数据同样的也适用，网卡可以直接将数据写到CPU cache中，这样CPU可以非常快的读到数据。\n我们介绍的E1000的结构非常简单，但是实际中的网卡机制非常的复杂。\nReceive Livelock 接下来我们看一下今天的论文 。因为我们已经介绍了很多论文相关的背景知识，我们直接来看一下论文的图1。我们之后根据论文中的图来开展讨论。\n这张图是一个路由器的性能图。这是一个有两张网卡的路由器，它的工作是从一个网卡接收packet，再从另一个网卡送出 。\nX轴是接收速率，也就是接收端网卡的收到packet的速率。Y轴是发送速率，也就是观察到的发送端网卡发送packet的速率。\n我们关心的是实心圆对应的曲线，它先上升，再下降。所以即使还不知道任何上下文，看到这个图之后我们会问自己，为什么这个曲线先上升，再下降？曲线的转折点有什么特殊之处？是什么决定了曲线的上升斜率和下降斜率？即使不知道任何背景知识，我们还是可以问出这么多问题。\n首先，为什么这条曲线开始会上升？\n学生回答：在到达处理的瓶颈之前，路由器可以处理更多的接收方向的packet，也可以处理更多的发送发向的packet。\n完全正确，在出现错误之前，对于每个接收到的packet，路由器都可以转发出去。比如说当packet以2000pps的速度接收时，路由器直接将packet从输入网卡拷贝到输出网卡，所以路由器的发送速率与接收速率一样，都是2000pps，所以这里X轴与Y轴的值相等。这种状态一直保持，直到曲线到达转折点。\n那么为什么曲线不是一直上升的呢？\n学生回答：是不是因为中断不能被处理导致的？\nRobert教授：这个其实是为什么曲线会下降的原因。我这里的问题是为什么曲线在某个点之后就不再上升了。假设这里的设计是合理的，对于一个合理的系统，对应的曲线会一直上升吗？\n学生回答：我认为不会，就算系统能足够快的处理packet，对于足够多的packet，还是可能触发系统的瓶颈。\n是的，CPU的算力并不是无限的，CPU最多每秒执行一定数量的指令。\n对于每个packet，IP软件会查看packet的header，检查校验和，根据目的地址查找转发表等等，这个过程会消耗数百甚至数千条CPU指令时间来处理一个packet。所以，我们不能期望曲线能一直向上走，它必然会在某个位置停止向上。\n上面的图中，曲线在5000的位置就停止不再上升了，这告诉我们这台机器处理每个packet要消耗200微秒。所以，曲线的转折点隐含的包含了处理一个packet需要的时间信息。\n虽然这只是一个猜想，但是通常与真实的结果非常相近。或许我们可以修改软件使其更加的高效，我们可以优化到处理每个packet只需要150微秒，我们或许可以将曲线的转折点向上移一些，但是在到达了这台机器每秒能处理的packet数量的极限时，我们还是会到达曲线的转折点。\n除了CPU的性能，还有一些不是必然存在的瓶颈需要注意一下。最明显的一个就是网络的性能。如果你使用的网络只有10Mb/s，那么底层的网路硬件最多就能按照这个速率传输数据，这也有可能构成一个限制。\n所以也有可能是因为网络传输的速率决定了曲线的顶点是在5000pps这个位置。论文中并没有说明究竟是CPU还是网速是这里的限制因素，但是对于一个10Mb/s的网络，如果你传输小包的话，是可以达到10-15 Kpps，这实际上是网线的能达到的极限，而上图中转折点对应的5Kpps远小于10-15Kpps，所以几乎可以确定限制是来自CPU或者内存，而不是网络本身。\n在一个设计良好的路由器中，如果处理每个packet要200微秒，那么我们期望看到的是不论负载多高，路由器至少每秒能处理5000个packet。所以我们期望看到的曲线在5000pps之后是一条水平线，路由器每秒处理5000个packet，并丢弃掉其他的packet。\n但是我们实际拥有的曲线会更加的糟糕，当收到的packets超过5000pps时，成功转发的packets随着收到的packet的增多反而趋向于0。为什么曲线会下降呢？前面有同学已经提到了。\n论文作者给出的原因是，随着packet接收速率的增加，每个收到的packet都会生成一个中断，而这里的中断的代价非常高，因为中断涉及到CPU将一个packet从网卡拷贝到主机的内存中。如果我们知道packet将会以10K每秒的速率到达，并且我们知道我们不能处理这么多packet，那么我们可以期望的最好结果就是每秒转发5000个packet，并且丢弃5000个packet之外的其他packet。\n但是实际上，5000个packet之外的其他packet，每个都生成了一个昂贵的中断，收到的packet越多，生成的中断就越多。而中断有更高的优先级，所以每一个额外的packet都会消耗CPU时间，导致更少的CPU时间可以用来完成packet的转发。最后，100%的CPU时间都被消耗用来处理网卡的输入中断，CPU没有任何时间用来转发packet。\n这里曲线的下降被称为中断的Livelock，这是一个在很多系统中都会出现的现象。\n这里背后的原因是有两个独立的任务，比如这里的两个任务是输入中断和转发packet程序。由于调度的策略，输入中断的优先级更高，使得转发packet的任务可能分配不到任何CPU时间。\n几乎在任何需要处理输入的系统中，如果输入速率过高，都有可能出现Livelock。\nLivelock不仅会因为CPU耗尽而发生，也可能是其他原因，比如说网卡的DMA耗尽了RAM的处理时间，那么网卡占据了RAM导致CPU不能使用RAM。所以，即使你拥有大量的CPU空闲时间，还是有可能触发Livelock。不管怎样，这曲线的下降被称为Livelock。\n你或许会问，不能处理的packet最后怎么样了？我们回想一下网络协议软件的结构，网卡会通知网卡的接收中断，接收中断将packet拷贝到队列缓存中，之后会有一个线程处理队列缓存中的packet。\n所以packet会在队列缓存中丢失。队列缓存有一个最大的长度，至少RAM的大小是有限制大，但是队列缓存的大小会远小于RAM的大小。如果网卡的接收中断从网卡获得了一个packet，并且发现队列缓存的长度已经是最长了，接收中断程序会丢弃packet。\n如何解决Livelock 论文 作者对于Livelock提出了一种解决方法。这种解决方法的最直接结果就是，当packet的输入速率达到了5000pps，随着输入速率的增加，转发性能维持在5000pps。\n曲线后半部分的水平是一种完美的non-livelock性能曲线。之所以是水平的，是因为受CPU的限制，最多只能处理5000pps的转发。\n在这个解决方案中，还是存在处理packet的线程和中断处理程序。当网卡第一次触发中断时，会导致中断处理函数的运行。但是中断处理函数并不会从网卡拷贝packet，相应的，它会唤醒处理packet的线程，并且关闭网卡的中断，这样接下来就收不到任何中断了。\n处理packet的线程会有一个循环，在循环中它会检查并从网卡拉取几个packet，论文中我记得是最多拉取5个packet，之后再处理这些packet。\n所以现在处理packet的线程是从网卡读取packet，而不是从中断处理程序读取。如果网卡中没有等待处理的packet，那么处理线程会重新打开网卡中断，并进入sleep状态。\n因为最后打开了中断，当下一个packet到达时，中断处理程序会唤醒处理packet线程，线程会从sleep状态苏醒并继续处理packet。这就是论文介绍的解决Livelock的方法。\n这里的处理方式实际上是将中断模式（Interrupt Scheme）转变成了轮询模式（Polling Scheme）。\n在高负载的情况下，中断会被关闭，并且CPU会一直运行这里的循环中，不断读取packet并处理packet。因为中断被关闭了，CPU用来运行主线程的时间不会被中断占据。在低负载的情况下，中断会被打开，在收到packet之后，线程会被中断处理程序直接唤醒。\n学生提问：这里的循环会检查所有的设备吗？还是只会检查产生中断的设备？\nRobert教授：这是个好问题，如果存在多个网卡，我并不知道这里的循环会怎么工作。\n一个非常合理的设计是，packet处理线程需要记录每个网卡是在中断模式还是在轮询模式，然后只对轮询模式的网卡。。。等一下，因为中断处理程序现在不从网卡读取packet，所以线程中的循环可以直接检查所有网卡，如果网卡中有待处理的packet，就读取几个packet并处理。\n如果所有的网卡都没有待处理的packet，主循环会打开所有网卡的中断，并进入sleep状态。之后，任何一个网卡的中断都会唤醒packet处理线程。\n学生提问：当处理线程运行的时候，packet是如何进入到一个等待读取的队列中？我觉得网卡上只会有一个packet。\nRobert教授：最开始的时候，packet会在网卡自己的内存中按照队列形式缓存。而处理线程的主循环会询问每个网卡是否在自己的内存中有待处理的packet。如果有的话，主循环会在主机的RAM中申请缓存，再将packet数据从网卡中拷贝到RAM中的缓存，再处理packet。\n学生提问：所以一次可以拷贝多个packet？\nRobert教授：是的，我认为论文中说的是一次拷贝5个packet。即使有100packet在网卡中等待处理，一次也只会读取5个，这样可以避免阻塞输出。\n学生提问：但是这就要求提升网卡的内存容量了吧？\nRobert教授：Well，我不知道要多少内存容量。\n在Livelock曲线的转折点之前，都是靠中断来处理的。在转折点之前，如果网卡收到了一个packet，处理线程会立即被唤醒并读出packet。但是在转折点之后，处理线程就一直在轮询模式而不是中断模式。\n在转折点之后，肯定会有丢包，因为现在输入速率和输出速率之间是有差异的，而这个差异间的packet都被丢弃了。因为这些packet不论如何都会被丢弃，增加网卡的内存并不太能减少这里的丢包，所以不太确定网卡是否需要增加内存容量。\n在论文中，一次会读取最多5个packet，那么网卡必然需要存储5个packet的内存容量，但是更多的packet是否有好处就不太确定了。\n网卡上的buffer大小，对于短暂的高pps有帮助，这样可以保存好packet等处理线程来读取它们。但是我们这里并没有讨论短暂的overload，我们讨论的是持续的overload。所以增加网卡的buffer，并不是很有用。\n学生提问：当网卡中断被关闭了，网卡还能在自己的buffer上加入新的packet吗？\nRobert教授：可以的。网卡是自治的，不论中断是打开还是关闭，只要有一个packet到达了网卡，网卡都会将packet加入到自己的缓存队列中。\n当然不同的网卡设计可能非常不一样，但是在论文中网卡不会调用DMA，不会主动访问主机内存。如果网卡上内存都用光了，packet会被丢弃。\n所以，在这里的设计中，丢包发生在网卡内部。在一个overload的场景下，网卡中的队列总是满的，当再收到一个packet时，网卡会直接丢包，这样就不会浪费CPU时间。网卡可以在不消耗CPU时间的前提下直接丢包，是避免Livelock的直接原因。\n学生提问：有没有这种可能，CPU从网卡读取packet，但是处理线程内部的队列满了？\nRobert教授：当然。在其他地方肯定也有瓶颈，例如对于收到的packet，需要交给监听了socket的应用程序去处理，如果应用程序并没有以足够快的速度读取packet，相应的socket buffer会满，那么packet会在处理线程中丢包，而这也可能导致Livelock。\nLivelock发生的根本原因是我们浪费时间处理了一些最终会被丢弃的packet，这里的处理是徒劳。\n另一种发生Livelock的可能是，当负载增加时，我们可能会消耗100%的CPU时间在packet处理线程上，而留给应用程序的CPU时间为0，这时还是会发生Livelock。\n论文在第六节中有相应的介绍，如果一个packet将要被传输给本地的应用程序，网络线程会查看应用程序的socket buffer，如果socket buffer过满的话，网络线程会停止从网卡读取packet，直到socket buffer变小。\n这意味着网络线程会停止运行，并给应用程序机会运行并处理packet，所以如果你不够小心的话，你可能会在任何阶段都经历类似Livelock的问题。\n","date":"2024-10-06T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08121networking/bg_hu10059490865042218752.jpg","permalink":"https://echudet.github.io/p/mit6s08121networking/","title":"MIT6S081：21Networking"},{"content":"Lecture 20 Kernels and HLL 课前预习 论文速览 这篇论文的标题是《The Benefits and Costs of Writing a POSIX Kernel in a High-Level Language》，通过梳理论文的结构和内容，以下是根据各部分标题总结的主要内容：\n在引言 (Introduction)部分，作者探讨了在高性能操作系统内核中使用高级语言（如Go）的可行性，特别是与传统C语言的对比。他们构建了一个名为Biscuit的内核，该内核使用Go语言编写，支持POSIX系统调用子集，并通过实验评估其性能与安全性。主要目标是研究高级语言在编写内核时带来的性能开销、实现挑战、以及编程效率和安全性。\n在相关工作 (Related Work)部分，本节回顾了与高级语言编写操作系统内核相关的先前研究，包括早期基于高级语言的内核如Pilot和Lisp machine，以及现代的高级系统编程语言如Rust和Go。还探讨了内核中自动内存管理（垃圾回收）和其他内存分配方案的研究。\n在 动机 (Motivation)部分，这里讨论了选择C语言和高级语言的利弊。C语言在内核编写中被广泛使用，主要因为它的低级内存管理能力能够实现高性能。而高级语言（如Go）的优势则在于减少了编程复杂度和内存管理相关的错误，如缓冲区溢出和内存释放错误。\n在概述 (Overview)部分，这一部分详细介绍了Biscuit内核的结构和设计，包括Go运行时的修改、进程调度、虚拟内存、文件系统和网络栈的实现。Biscuit使用了Go语言的多核支持、并发机制（如Go的goroutines和channels）来处理系统调用和设备驱动。\n在垃圾回收 (Garbage Collection)部分，高级语言的垃圾回收机制是本研究的关键挑战之一。本节描述了Go语言的并行标记-清除垃圾回收器以及Biscuit如何配置垃圾回收来最小化对系统性能的影响。研究表明，在内核中使用垃圾回收虽然增加了一定的CPU开销，但通过合理的设计可以将其控制在可接受范围内。\n在避免堆耗尽 (Avoiding Heap Exhaustion)部分，Biscuit通过使用静态分析工具MAXLIVE来计算系统调用可能使用的最大内存量，以避免内核堆耗尽。每个系统调用在启动时会预留内存，如果没有足够的内存预留则会进入等待状态，直到释放出足够的内存。此机制使得Biscuit在处理内存分配时比传统的C语言内核更具鲁棒性。\n在 实现 (Implementation)部分，这里列举了Biscuit内核的具体实现细节，包括Go代码行数统计、系统调用支持情况（58个POSIX系统调用）、设备驱动的实现、内核启动过程以及内核的多核并发处理机制。还讨论了对Go运行时的修改，以及如何通过unsafe操作与硬件交互。\n在评估 (Evaluation)部分，本节通过实验数据展示了使用Go语言编写内核的性能代价（HLL tax）。评估了Biscuit在垃圾回收、堆内存大小、系统调用处理上的性能表现，并与Linux内核进行了直接比较。实验结果表明，虽然Biscuit在性能上不及Linux，但依然具有良好的性能表现，尤其是在多核并发处理上，Biscuit得益于Go语言的高效并发模型。\n在讨论与未来工作 (Discussion and Future Work)部分，作者对Biscuit和Go语言在内核开发中的优势和不足进行了总结。虽然Go语言降低了编程复杂度和内存管理的错误风险，但在性能上仍不如C语言。最后，作者指出，Go语言适用于那些强调安全性和快速原型开发的新内核项目，但对于现有的大型C语言内核，切换到Go的成本可能超过其收益。\n这篇论文通过Biscuit内核的案例，深入探讨了高级语言在操作系统内核开发中的可行性和局限性，并提供了定量的实验评估数据。\n处理器架构 x86-64 和 ARM 是两种主要的处理器架构（Instruction Set Architecture, ISA），它们决定了计算机硬件如何与软件进行交互。ISA 定义了处理器能执行的指令集，也就是说，软件和硬件如何沟通、数据如何被处理和存储。\n为了帮助你理解，我会给你列出几种常见的处理器架构，并简单解释它们的特点。这样你就可以看到 x86-64 和 ARM 是如何与其他架构相似或不同的。\nx86-64 (或 x64)\n主要特点：这是 x86 架构的 64 位扩展版本，由 Intel 和 AMD 开发。它广泛应用于桌面电脑、笔记本电脑和服务器。\n应用场景：Windows、Linux 和 macOS 系统广泛支持。大多数个人电脑和企业级服务器使用的就是 x86-64 处理器。\n厂商：Intel 和 AMD。\n优点：向后兼容 32 位 x86 程序，有强大的性能和广泛的软件兼容性。\n缺点：功耗较大，因此在移动设备中应用较少。\nARM (Advanced RISC Machines)\n主要特点：ARM 是一种基于 RISC（精简指令集计算，Reduced Instruction Set Computing）的架构，设计更加注重低功耗和高效能。ARM 处理器的指令集更小、更简单。\n应用场景：广泛应用于智能手机、平板电脑、嵌入式设备以及最近的 Apple M1/M2 处理器系列（用于 MacBook）。\n厂商：苹果（Apple）、高通（Qualcomm）、三星、NVIDIA 等使用 ARM 架构设计自己的处理器。\n优点：低功耗，非常适合电池驱动的设备，架构灵活，授权模式使得多家厂商可以开发自己的实现版本。\n缺点：历史上软件兼容性较弱，尤其是在桌面电脑上，但这一点正在逐渐改进。\nRISC-V\n主要特点：一个开源的 RISC 架构，它的设计简单且可扩展，任何公司或个人都可以免费使用并进行修改。\n应用场景：目前多用于学术界、研究项目、嵌入式系统以及一些创新应用中。\n厂商：SiFive、Microchip 等。\n优点：完全开放，便于创新和定制化，同时也更容易推动硬件开发的标准化。\n缺点：目前的软件生态系统不如 x86 和 ARM 成熟。\nPowerPC (Performance Optimization With Enhanced RISC – Performance Computing)\n主要特点：由 IBM、苹果和摩托罗拉联合开发，基于 RISC 架构。PowerPC 曾广泛应用于 Apple 早期的 Macintosh 电脑，后来逐渐淡出消费市场。\n应用场景：目前多应用于高性能计算和嵌入式设备，例如游戏机（如早期的 Xbox 和 PlayStation 3）、汽车电子系统等。\n厂商：IBM、Freescale 等。\n优点：适合高性能计算，具有较强的并行处理能力。\n缺点：已逐渐被 ARM 和 x86-64 超越，尤其是在消费市场中。\nMIPS (Microprocessor without Interlocked Pipeline Stages)\n主要特点：MIPS 是一种经典的 RISC 架构，历史悠久，简单易用，广泛应用于嵌入式系统中。\n应用场景：路由器、电视机顶盒、网络设备等。\n厂商：早期由 MIPS Technologies 开发，现在由 Wave Computing 维护。\n优点：设计简单，功耗低，非常适合嵌入式系统。\n缺点：在高性能市场中逐渐被其他架构取代。\nSPARC (Scalable Processor Architecture)\n主要特点：一种由 Sun Microsystems 开发的 RISC 架构，专为高性能服务器和工作站设计。 应用场景：多用于服务器和高性能计算环境。 厂商：最著名的厂商是 Oracle（曾经的 Sun Microsystems）。 优点：在其时代，适合并行处理和高负载环境。 缺点：随着 x86 和 ARM 的崛起，SPARC 逐渐被淘汰，市场份额越来越小。 Itanium (IA-64)\n主要特点：由 Intel 开发的 64 位架构，最初是为了取代 x86，但未能成功。\n应用场景：主要应用于高端服务器和一些专用的计算环境。\n厂商：Intel。\n优点：在特定高性能服务器应用中有一定优势。\n缺点：市场接受度较低，最终被 x86-64 和 ARM 架构压制。\n总的来说，这些架构可以分为两大类：\nCISC（复杂指令集计算）：如 x86-64，指令集复杂，但单指令功能强大。 RISC（精简指令集计算）：如 ARM、RISC-V、PowerPC，指令集较简单，但执行效率高。 x86-64 强调性能和向后兼容性，适合高性能设备；而 ARM 注重功耗和效率，成为移动设备和嵌入式设备的首选。每种架构都有其特定的应用场景，取决于性能需求、功耗要求和开发生态。\nGo 语言简介 Go 语言（通常称为 Golang）与 C 语言相比，具有一些显著的不同和优势。Go 是由 Google 开发的，设计目标是解决在现代多核计算和并发编程中的一些问题，并且它非常注重简洁、快速开发和高效执行。以下是对比 Go 和 C 语言的几个重要方面：\nC 语言 Go 语言 设计目标 C 诞生于 1970 年代，主要用于系统编程，特别是操作系统开发（如 UNIX 系统）。它是一种底层语言，直接操控硬件资源，具有很高的执行效率和灵活性。C 的设计强调简洁和可移植性，但编程时需要手动管理内存（如指针、malloc/free）。 Go 于 2009 年发布，设计目标是提高编程效率，特别是对于现代互联网时代的服务器和并发程序。Go 的核心设计理念是简化开发流程，提供更好的并发支持，并减少程序员因复杂性（如内存管理）带来的错误。 并发支持 虽然 C 能够通过多线程和库（如 pthreads）实现并发，但这些实现相对低级，需要程序员手动管理线程、锁和同步机制。这增加了编程复杂性和出错的可能性。 Go 具有内建的并发模型，使用所谓的 goroutine 和 channel 来实现高效的并发处理。Goroutine 是比线程更轻量的实体，Go 运行时负责调度和管理，开发者只需要专注于逻辑。使用 Go 编写并发程序比 C 要简单得多。 内存管理 C 要求程序员手动分配和释放内存（malloc 和 free），虽然这种直接控制提供了很高的灵活性和性能，但也容易引发错误，例如内存泄漏、空指针引用等问题。 Go 有自动的 垃圾回收（Garbage Collection） 机制，它会自动管理内存的分配和释放。这减少了开发者手动管理内存的负担，同时减少了内存相关的错误，虽然代价是稍微降低了一些性能。 编程复杂性 C 作为一种底层语言，接近于汇编语言，提供了极高的灵活性。你可以直接操作内存、硬件寄存器等，但这也带来了较高的复杂性。C 代码通常较为繁琐，调试和维护成本高。 Go 强调 简洁性和可读性，语言本身设计得非常简洁，提供了现代编程所需的内置工具，比如内置的并发支持、标准化的包管理和文档生成工具。与 C 相比，Go 代码往往更简洁，开发速度更快。 类型系统 C 是一种静态类型语言，但类型系统相对简单。C 支持结构体（structs）、枚举（enums）等，但没有内置的面向对象编程支持。 Go 也是静态类型语言，支持基本的数据类型、结构体、接口等。Go 并不直接支持面向对象编程，但通过结构体和接口，可以实现类似 OOP 的风格，避免了一些传统 OOP 语言（如 Java、C++）的复杂性。 生态系统和应用场景 C 是一种通用语言，几乎可以用于任何场景，特别是在系统编程、驱动程序、嵌入式系统和高性能计算等领域应用广泛。C 语言的生态系统非常成熟，有丰富的库和工具支持。 Go 的设计初衷是用于构建高并发、高性能的网络服务器和分布式系统。其生态系统非常适合现代 Web 开发、云计算和微服务架构。许多著名的项目和企业，如 Docker、Kubernetes、Uber、Dropbox 等，都是用 Go 开发的。 编译与性能 C 代码经过编译器优化后，可以生成非常高效的机器代码，因此 C 程序通常具有极高的性能。这使得 C 适用于性能要求极高的应用（如嵌入式系统、操作系统内核等）。 Go 也属于编译型语言，尽管在极端性能上不如 C，但在大多数应用场景中，其性能表现足够优越。Go 的编译器能够进行较多优化，编译速度也非常快。同时，Go 提供了跨平台编译工具，非常适合开发分布式系统和服务器。 错误处理 C 的错误处理依赖于返回值和全局变量（如 errno），这有时可能导致代码难以阅读和维护。 Go 的错误处理通过显式的错误返回值（error type）来实现，虽然这种方式有时显得繁琐，但它使得错误处理更加明确和可靠。Go 强调显式错误处理，这减少了程序中的潜在隐患。 总结\nC 语言：是一种底层、高性能、灵活的系统编程语言，非常适合需要对硬件进行精确控制的场景。 Go 语言：是一种面向现代网络开发、并发处理和分布式系统的高效语言。它简化了许多复杂的系统编程任务，同时保持了较高的执行性能。 如果正在开发需要直接控制硬件、实时性要求高的系统（如嵌入式开发、内核开发），C 是一个理想的选择。但如果在开发现代的 Web 服务、云计算平台或分布式系统，Go 会带来更高的开发效率和更轻松的并发处理。\n代码示例：使用 Go 创建一个简单的 Web 服务器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) // 定义一个结构体来表示 JSON 响应的格式 type Response struct { Message string `json:\u0026#34;message\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` } // 处理根路径请求的处理函数 func helloHandler(w http.ResponseWriter, r *http.Request) { response := Response{ Message: \u0026#34;Welcome to the Go Web Server!\u0026#34;, Timestamp: time.Now(), } // 设置响应的 Content-Type 为 application/json w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // 将结构体编码为 JSON 并返回给客户端 if err := json.NewEncoder(w).Encode(response); err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } } func main() { // 将 / 路由绑定到 helloHandler 函数 http.HandleFunc(\u0026#34;/\u0026#34;, helloHandler) // 启动 Web 服务器，监听 8080 端口 log.Println(\u0026#34;Starting server on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatalf(\u0026#34;Could not start server: %s\\n\u0026#34;, err.Error()) } } 代码说明：\nResponse 结构体：这是用于生成 JSON 响应的数据结构，包含一个 Message 字符串和 Timestamp 时间戳。 helloHandler 函数：这是处理根路径 / 的请求处理器。它创建一个响应结构体，并使用 Go 的 json.NewEncoder() 函数将其编码为 JSON 格式，然后发送给客户端。 http.HandleFunc(\u0026quot;/\u0026quot;, helloHandler)：将根路径 / 绑定到 helloHandler，当客户端访问根路径时，调用此函数。 http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)：启动 Web 服务器，监听 8080 端口。 如何运行：\n将上述代码保存为一个 .go 文件，例如 main.go。\n在命令行中导航到文件目录，并运行以下命令来编译和运行 Go 服务器：\n1 go run main.go 打开浏览器，访问http://localhost:8080，你会看到如下 JSON 响应：\n1 2 3 4 { \u0026#34;message\u0026#34;: \u0026#34;Welcome to the Go Web Server!\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-10-06T12:34:56.789123456Z\u0026#34; } 扩展思路：\n可以进一步扩展这个服务器，添加更多的路由（如 /about, /users），并处理不同的 HTTP 方法（如 GET, POST）。 也可以引入 Go 的 web 框架如 Gin 或 Echo，以简化路由处理和中间件的使用。 Biscuit Heap Issue Biscuit 的方法，虽然看似没有引入全新的技术，但它的确提供了一种相对简单且有效的解决内核堆内存分配问题的方法。它并不是传统方法的革命性替代，但在特定上下文中有其优势，尤其是通过静态分析预估系统调用所需的内存并进行提前分配，从而避免了许多传统内存管理问题，比如复杂的分配失败恢复和内存死锁。以下是对 Biscuit 方法的更深入分析，并与传统方法对比：\n静态分析与提前预留内存，Biscuit 的创新点在于它使用静态分析来预测每个系统调用可能需要的内存，并在系统调用执行之前预留足够的内存。这样，当系统调用开始时，它可以确保所有需要的内存已经就绪，从而避免了在执行过程中动态分配失败的风险。传统方法的对比：\n动态分配：许多操作系统采用动态分配策略，在内存不够时，可能导致分配失败、等待或触发 OOM Killer 之类的机制。Biscuit 的提前预留内存策略通过分析避免了这些复杂的分配失败处理逻辑。\n内存池：预分配内存池虽然可以提高分配效率，但并不能保证每次分配的内存量足够满足所有调用需求，仍然可能出现内存不足的情况。\n避免分配失败和恢复机制，传统的内存管理系统往往需要应对内存分配失败，这可能涉及到复杂的错误恢复机制。例如，如果系统调用在执行过程中遇到内存分配失败，可能需要回滚操作或进行繁琐的资源释放。Biscuit 的设计则通过预先分配内存避免了这一问题，简化了错误处理。传统方法的对比：\n恢复和回滚：处理内存分配失败通常涉及复杂的恢复机制，特别是在并发环境下，可能会遇到死锁或竞态条件的问题。Biscuit 通过在执行之前保证分配，避免了这种复杂性。\n等待内存释放：传统系统中，当内存不足时，进程可能会阻塞并等待内存释放，这可能会导致性能瓶颈或死锁风险。Biscuit 通过预先确定内存需求，减少了这些问题的发生。\n实现的简洁性，Biscuit 的方法虽然不复杂，但它依赖于一个关键前提：内核中每个系统调用的内存需求可以通过静态分析确定。这种设计思路虽然并不算高明或者复杂，但它的确很实用，尤其是在现代内核中，减少了内存分配失败的风险，并简化了内存管理代码。传统方法的对比：\n传统方法可能会依赖复杂的内存分配器和恢复机制，开发和维护成本较高。Biscuit 的设计则更加直接和简洁，减少了处理分配失败和恢复的需要。 局限性，尽管 Biscuit 通过静态分析来预先分配内存，这种方法并非适用于所有场景。某些复杂的系统调用可能无法准确预测内存需求，特别是在涉及到大量动态数据的情况下。此外，Biscuit 的设计需要在内存紧张的情况下小心处理，否则预分配的策略可能导致系统调用等待较长时间，影响性能。传统方法的优势：\n灵活性：传统的动态分配方法更灵活，能够适应不确定的内存需求。这对于复杂的系统调用或者高度动态的应用环境尤为重要。\n性能：Biscuit 的方法依赖于静态分析和预分配，这在某些情况下可能导致系统调用延迟，特别是当多个系统调用竞争有限的内存资源时。\n总的来说，从技术角度看，Biscuit 的方法并不是高度复杂或特别创新的，但它确实在内核设计中引入了一种相对简单、直接的方式来解决内存分配失败问题。它的优势在于简化了内存管理，避免了复杂的恢复逻辑和潜在的死锁问题。通过提前分析内存需求并进行预分配，它可以有效避免动态分配中常见的瓶颈和风险。\n尽管如此，它的局限性也很明显，尤其是在动态性较高或内存需求不可预测的场景中，这种方法可能会显得不够灵活。因此，Biscuit 的方法是一种针对特定问题的实用而非高明的解决方案。\n静态分析 “静态分析”是一个专业术语，广泛用于软件工程和编译器设计中。它指的是在不运行程序的情况下，通过分析源代码来检查程序的正确性、性能、潜在错误以及代码结构等。静态分析与动态分析（在程序运行时进行的分析）相对。静态分析可以应用在多个层面，以下是它通常会分析的几个方面：\n语法检查（Syntax Checking）\n分析内容：检查源代码是否符合编程语言的语法规则。例如，确保没有语法错误、未闭合的括号或语句等。\n工具：编译器的前端部分通常负责语法检查，如 gcc 在编译 C 代码时会做基础的语法检查。\n类型检查（Type Checking）\n分析内容：确保程序中的类型匹配正确，变量和函数的使用符合其声明的类型。例如，在静态类型语言（如 C、Go、Java）中，确保函数接收和返回的参数类型正确，避免类型转换错误。\n工具：大部分编译器都进行类型检查，像 Go 和 Rust 等现代编程语言有非常严格的类型检查机制。\n数据流分析（Data Flow Analysis）\n分析内容：追踪程序中的变量从定义到使用的路径，分析数据如何在程序中流动。这有助于发现未初始化变量、死代码、常量传播（优化机会）等问题。\n工具：一些优化编译器或代码静态分析工具如 Clang Static Analyzer、SonarQube 等可以执行这种分析。\n控制流分析（Control Flow Analysis）\n分析内容：检查程序中的控制结构（如条件分支、循环、函数调用）的执行路径，确保每个可能的路径都是有效的。它有助于发现潜在的无限循环、死锁或不安全的代码路径。\n工具：编译器和静态分析工具都可以执行控制流分析，用于优化和安全分析。\n内存分析（Memory Analysis）\n分析内容：分析代码中的内存分配、使用和释放，帮助检测内存泄漏、指针错误、越界访问等问题。内存分析也包括堆栈分配的正确性，检测未释放内存或者无效的内存访问。\n工具：Coverity、Cppcheck 等工具可以进行静态内存分析。\n程序依赖分析（Dependency Analysis）\n分析内容：检查函数、变量和模块之间的依赖关系，分析程序模块的耦合程度。这有助于识别出耦合过高的模块，优化代码的可维护性和解耦程度。\n工具：软件开发工具如 SonarQube 和 Understand 可以帮助进行程序依赖的分析。\n并发分析（Concurrency Analysis）\n分析内容：在有并发的程序中，静态分析可以帮助检测潜在的竞态条件、死锁和线程安全问题。通过分析不同线程对共享资源的访问方式，可以发现同步问题。\n工具：像 ThreadSanitizer 等静态分析工具专门用于检测并发相关问题。\n安全漏洞分析（Security Vulnerability Analysis）\n分析内容：静态分析可以用于发现安全漏洞，例如 SQL 注入、缓冲区溢出、未验证的输入、权限提升等。它通过扫描代码，寻找已知的安全模式或问题。\n工具：Fortify、Veracode、OWASP ZAP 等工具用于静态代码安全分析。\n资源使用分析（Resource Usage Analysis）\n分析内容：静态分析可以检测程序在运行过程中可能使用的资源，例如内存、CPU 时间、文件句柄等。在实时系统或嵌入式系统中，这类分析有助于确保程序不会耗尽资源。\n工具：实时系统开发中的专用静态分析工具如 Polyspace 可以用于这类分析。\n循环不变性与优化（Loop Invariants and Optimization）\n分析内容：在代码优化中，静态分析可以帮助发现循环中的不变条件，优化代码性能。例如，通过分析循环不变表达式，将其移出循环，以减少不必要的计算。\n工具：优化编译器（如 LLVM）会执行这种分析。\n论C 语言与 Go 在静态分析中的区别，虽然静态分析在多种编程语言中都可以执行，但 C 语言和 Go 语言由于各自的设计特点，分析的难度和效果有很大的差别：\nC 语言\n指针操作和内存管理：C 语言允许直接的指针操作和手动内存管理，导致内存分析的难度增加。分析工具很难准确追踪指针的生命周期或内存的动态分配。 灵活性：C 语言的灵活性让静态分析更难，因为它允许低级别的操作（如汇编嵌入、直接操作内存、动态类型转换等），这些特性让编译器和分析工具很难精确理解代码的意图。 较少的类型信息：C 语言的类型系统相对简单且宽松，类型检查也不如 Go 严格，增加了分析的复杂性。 Go 语言\n垃圾回收：Go 的自动垃圾回收系统减少了手动内存管理的复杂性，简化了静态内存分析。程序员不需要显式管理内存，分析工具可以更容易跟踪对象的分配和释放。 更强的类型系统：Go 语言的类型系统比 C 更严格，变量的类型和分配更容易被分析工具理解，从而提供更准确的静态分析。 并发模型：Go 的 goroutine 和 channel 使得并发的分析更加简单和明确，分析工具能够更好地理解并发程序的行为。 静态分析是一个强大的工具，广泛应用于软件开发的各个阶段。它可以帮助程序员发现潜在问题，优化代码，提升安全性。在 C 语言中，静态分析受到语言复杂性的限制，但在 Go 等现代语言中，由于其内存管理的简化和更严格的类型系统，静态分析变得更可行和有效。\nBiscuit 的静态分析 Biscuit 能够使用静态分析来预测每个系统调用所需的堆内存，这是因为它采用了Go 语言，而 Go 语言的一些特性使得这种静态分析更为可行和有效。相比之下，C 语言由于语言本身的特性和复杂性，静态分析的难度要大得多。\n静态分析是指在编译时对程序代码进行分析，而不是在运行时进行。它可以通过检查源代码中的函数、变量、内存分配等，来推断程序的行为、性能或者资源需求。对于 Biscuit 来说，静态分析的目标是推断出每个系统调用的最大堆内存使用量，从而在系统调用开始之前预留足够的内存，确保不会在运行时遇到分配失败的问题。\nGo 语言在设计上有一些特性使得静态分析比 C 语言更容易实现：\nGo 语言有内置的垃圾回收机制，开发者不需要手动管理内存分配和释放。这种自动内存管理简化了内存分配的逻辑，减少了复杂的内存管理模式（如手动释放、双重指针、内存池等），使得静态分析工具可以更容易地跟踪和预测内存的使用情况。\n在 C 语言中，手动管理内存意味着程序员可以随时随地调用 malloc() 或 free()，而且可能通过复杂的方式（如指针运算）操作内存。这种灵活性大大增加了静态分析的难度，因为分析工具很难准确判断内存的分配和释放行为。\nGo 的内存模型相对简单。它的堆内存分配通常是通过标准的内存分配器进行，且由于 Go 的类型系统是静态类型的，内存分配的大小在大多数情况下是可以确定的。这使得静态分析工具可以更容易地推断出每个函数的内存需求。\n相比之下，C 语言允许更复杂的内存操作，比如通过指针动态分配、操作和释放内存。这些操作的灵活性和复杂性，使得 C 程序的内存模型变得不可预测，尤其是在使用 malloc() 或 realloc() 等函数时，很难通过静态分析准确预估内存需求。\nGo 语言没有 C 语言那样广泛的指针操作。虽然 Go 也支持指针，但它在设计上限制了对裸指针的直接操作。指针的滥用是 C 语言中一个常见的静态分析障碍，因为指针可以指向任意内存位置，并且程序员可以随时通过指针修改内存。这使得 C 语言的静态分析工具很难跟踪内存的生命周期和使用情况。而 Go 的内存引用机制更加受控，这有助于静态分析工具做出更准确的推断。\nGo 有自己的轻量级并发模型（goroutine 和 channel），它相较于 C 语言中的多线程模型更为简洁和可控。虽然 C 可以通过 pthread 或其他库来实现多线程并发，但这些操作涉及锁、信号量等复杂的同步机制，这会进一步增加静态分析的复杂性。而 Go 语言通过自己的并发原语大大简化了并发操作，使得静态分析工具可以更容易推断 goroutine 的行为和内存使用情况。\nC 语言为什么难以实现静态分析\nC 语言中的手动内存管理（malloc() 和 free()）大大增加了静态分析的复杂性。C 程序员可以在任意位置、任意时间进行动态内存分配，并且释放的时间和方式也完全由程序员控制。这种灵活性使得分析工具很难判断每个系统调用或函数可能会分配多少内存，尤其是在多个模块或库函数之间复杂交互的情况下。\n指针是 C 语言的一个核心特性，允许开发者直接操作内存地址。尽管这种能力非常强大，但它给静态分析带来了极大的困难。通过指针操作，程序可以将内存分配、释放、传递给不同的函数，甚至可以修改完全不可预测的内存区域。因此，C 程序中的指针使得静态分析无法精确追踪内存的使用情况。\nC 的类型系统相对宽松，允许许多类型转换和操作。尤其是指针类型可以自由地转换为不同类型，进一步增加了内存分析的难度。分析工具很难判断内存对象的确切大小和类型，因此难以精确预测内存需求。\nC 语言中广泛使用的宏（#define）和内联函数也给静态分析增加了难度。宏可以改变代码的结构，并且由于它们在预处理阶段展开，静态分析工具在某些情况下难以处理宏的行为。这使得分析工具难以准确理解代码的实际执行路径和行为。\nC 程序员不仅可以使用 malloc() 进行堆分配，还可以使用 alloca() 在栈上动态分配内存，或通过静态分配全局变量。这些不同的分配方式大大增加了内存管理的复杂性，使得静态分析难以统一处理所有的内存分配场景。\n总的来说，Biscuit 能够使用静态分析来解决内核堆内存分配问题，主要得益于 Go 语言的简洁内存模型、自动垃圾回收、类型安全性以及受控的并发模型。这些特性使得 Go 的程序行为在编译时更加可预测，静态分析工具可以较容易地推断出每个系统调用的内存需求。\n相比之下，C 语言的手动内存管理、复杂指针操作和宽松的类型系统，使得静态分析要困难得多。C 语言的灵活性和低级控制带来了更多的自由度，但也增加了代码行为的不确定性，特别是在处理内存分配和管理方面，因此很难实现类似于 Biscuit 的静态分析内存分配策略。\nType-safe语言 Type-safe（类型安全）是一种编程语言特性，它确保数据类型的使用在程序中始终保持一致和正确。换句话说，类型安全的语言能够防止开发者在代码中进行不合法的类型操作，例如将字符串误用为整数、将浮点数错误地当作布尔值等。Go 语言被称为 Type-safe 语言，因为它在编译时严格检查类型，并且不允许不安全的类型转换。\nType-safe 简单来说，就是在编写程序时，不允许混淆不同数据类型的操作，确保代码中每一个变量的类型都在其生命周期内保持一致。比如：\n在 Type-safe 语言中，你不能将整数赋值给一个预期存储字符串的变量。 不能进行不安全的类型转换，比如将指针类型强制转换为整数类型（除非有显式的、明确的机制）。 Go 语言的类型安全性意味着：\n静态类型检查：在编译阶段，Go 会检查每个变量的类型是否正确使用。如果类型不匹配，程序将无法通过编译。例如，不能将 string 类型的值赋给 int 类型的变量。 强制类型转换：如果确实需要进行不同类型之间的转换，必须通过显式的类型转换。例如，int 和 float64 不能直接相互赋值，必须使用类型转换操作。 1 2 3 4 5 6 7 8 var i int = 10 var s string = \u0026#34;hello\u0026#34; // 编译时会报错，因为 i 是整数，而 s 是字符串 s = i // 错误：cannot use i (type int) as type string in assignment // 必须使用显式的转换 s = strconv.Itoa(i) // 正确：将 int 转为 string Type-safe 的作用\n减少错误： 类型安全机制能有效避免许多运行时错误。例如，在动态类型语言（如 Python）中，变量的类型检查发生在运行时，可能会因为类型不匹配导致程序崩溃。而在静态类型安全语言（如 Go、Rust、Java）中，类型错误会在编译时被发现，程序在运行之前就能被修正。 增强代码可读性与可维护性： 类型安全的代码有助于让开发者更明确地理解每个变量的意图和用法。类型明确定义，使得代码在维护和重构时减少了因类型不匹配而引发的潜在问题。 提高编译器优化的可能性： 类型安全的代码可以让编译器更容易优化，因为编译器能够提前了解每个变量的确切类型，从而进行更深入的性能优化。例如，编译器可以针对特定类型生成更高效的机器代码。 安全性： 类型安全性不仅防止程序崩溃，还能防止某些安全漏洞。例如，类型不安全的代码可能会引发缓冲区溢出、内存泄漏等问题，这些问题常常成为系统安全漏洞的来源。 随着软件系统的规模和复杂性不断增长，类型安全在现代软件开发中变得越来越重要，原因如下：\n复杂性增加： 现代软件系统往往非常复杂，涉及大量的模块、库、服务和并发操作。类型安全的语言可以帮助开发者管理复杂的代码结构，减少因数据类型错误带来的潜在错误。 并发和分布式系统： 在现代应用中，并发编程和分布式系统已成为常态。类型安全在这些场景下尤为重要，因为并发操作下的类型错误更难调试和排查。Go 等语言的类型安全机制使得并发编程更为安全和可靠。 大型团队协作： 大型团队中开发人员之间经常共享代码库。类型安全语言可以确保每个开发者按照一致的规则编写代码，减少因为类型误用导致的跨模块错误。这对于持续集成和快速迭代的开发模式来说至关重要。 防止运行时错误： 动态类型语言的一个缺点是，许多错误在运行时才会暴露出来，而这些错误可能导致系统崩溃、数据损坏或者严重的安全漏洞。类型安全语言通过在编译时检查这些错误，减少了系统在生产环境中出现问题的几率。 内存安全与安全漏洞防范： 类型安全与内存安全密切相关。在类型不安全的语言中，错误的类型转换可能导致访问非法内存地址，产生严重的安全漏洞（如缓冲区溢出）。现代软件开发对安全性要求极高，因此类型安全的语言更适合用于构建安全的系统。 许多现代编程语言都非常注重类型安全，例如：\nGo：通过静态类型和强类型系统，Go 确保类型错误在编译时被发现，并提供了简单但强大的类型转换机制。 Rust：Rust 不仅类型安全，还通过借用检查器（borrow checker）确保内存安全，防止常见的指针错误和数据竞争。 TypeScript：作为 JavaScript 的超集，TypeScript 引入了静态类型检查，弥补了 JavaScript 类型安全不足的问题。 Java：长期以来，Java 一直强调静态类型和类型安全，这使得 Java 在大型企业级应用中被广泛使用。 Type-safe 是一种编程语言的关键特性，确保了类型使用的一致性与安全性，减少了潜在错误。它的重要性在现代软件开发中日益提升，尤其是在处理复杂系统、并发操作和大规模协作时。Go 等现代编程语言通过类型安全机制提供了更高的可靠性和可维护性，帮助开发者在代码开发过程中减少错误，提升生产力和系统稳定性。\n用户空间的内核 \u0026ldquo;用户空间的内核\u0026rdquo; 和 \u0026ldquo;虚拟机上的内核\u0026rdquo; 虽然有一些相似的功能，但它们本质上是不同的概念。\n用户空间的内核（User-space Kernel） 在传统的操作系统架构中，内核（Kernel）运行在内核空间，它有访问系统硬件的最高权限，并管理进程调度、内存管理、设备驱动等功能。与此相对，用户应用程序则运行在用户空间，没有直接操作硬件的权限，必须通过系统调用与内核通信。\n用户空间的内核指的是一种特殊的设计，在这种设计中，内核的一部分或者类似于内核的功能在用户空间中运行，而不是在内核空间。这种用户空间的内核主要目的是隔离和安全，减少对主机操作系统内核的直接依赖，从而提高系统的安全性。例如：\ngVisor：gVisor 是一个用户空间内核，它模拟了大量的 Linux 系统 API，让容器可以像在标准 Linux 内核上运行一样运作，但它实际上并没有直接使用主机的 Linux 内核来运行这些容器。这种设计让容器运行时更加安全，因为它限制了容器对主机系统的访问。 关键区别：用户空间的内核运行在宿主操作系统的用户空间，它与宿主系统的内核隔离，通常使用系统调用与宿主内核通信。它的主要功能是提供一种额外的安全层，以限制和管理用户空间进程对系统资源的访问。\n虚拟机上的内核 虚拟机的内核则是另一个概念。在虚拟化环境中，虚拟机模拟了一个完整的物理计算机，这意味着虚拟机有自己的操作系统，这个操作系统有一个标准的内核，通常是 Linux 或 Windows 内核。\n在虚拟机上，虚拟机内核是运行在虚拟机操作系统中的内核。这个内核是完整的，管理虚拟机内部的所有系统资源和进程，就像它在真实硬件上运行一样。 虚拟化层（如 Hypervisor 或 KVM）在底层提供资源的抽象，它将物理硬件的资源分配给多个虚拟机。因此，虚拟机上的内核运行在虚拟化层上，而不是直接运行在真实的硬件上。 关键区别：虚拟机内核是模拟了一个完整的硬件环境，它运行在虚拟机的操作系统中，就像在实际硬件上运行一样；而用户空间的内核（如 gVisor）则是在宿主操作系统的用户空间中运行，它不是独立的操作系统内核，而是通过模拟部分系统功能为应用提供隔离和安全环境。\n相似性与差异\n相似性\n两者都提供了一种隔离机制，目的是增强安全性。 两者都与宿主操作系统隔离，不直接接触宿主内核。 差异\n用户空间内核是运行在用户空间的轻量级内核，它通常不是一个完整的操作系统，而是部分内核功能的实现，用于提供安全隔离（如 gVisor）。 虚拟机内核是运行在虚拟化环境中的完整操作系统内核，它模拟了整个硬件系统，因此是一个完整的操作系统。 虽然用户空间内核和虚拟机内核都有相似的隔离和安全目标，但它们的实现方式不同。用户空间内核（如 gVisor）通过在宿主操作系统上提供内核功能的模拟来隔离应用，而虚拟机内核则是虚拟化技术的一部分，它运行在模拟的硬件环境中，并管理整个虚拟机内部的操作系统和资源。\n死锁风险 在 Linux 内核中，系统调用（system calls）会分配内存给内核来处理各种任务（如创建进程、管理文件系统等）。当内核堆内存耗尽时，系统调用可能无法完成分配所需的内存。Linux 的策略是乐观分配，让系统调用继续进行，直到实际的内存分配失败为止。\n如果分配失败，系统可能会进入等待状态，给 OOM Killer（Out-Of-Memory Killer） 线程一个机会来寻找并终止某个消耗大量内存的进程，从而释放出一些内存供其他进程使用。\n问题在于，当一个线程正在分配内存时，它可能持有某些资源的锁（例如文件系统的锁、进程表的锁等）。如果该线程因内存分配失败而进入等待状态，而此时 OOM Killer 线程杀死了另一个持有锁的进程，可能会导致死锁情况的出现。以下是几个场景：\n情况 1：分配线程持有锁，等待内存\n线程 A 需要分配内存来完成某个系统调用，它已经持有某个锁（例如一个文件系统锁），但是由于内存不足，它进入等待状态。 OOM Killer 开始寻找消耗大量内存的进程，准备杀掉其中一个进程以释放内存。 线程 B 是 OOM Killer 选择的目标，但线程 B 也持有了另一个重要的内核锁（例如进程表锁），并且它正等待线程 A 释放锁。 此时就出现了经典的死锁：线程 A 等待内存释放，而线程 B 等待线程 A 释放锁，而线程 B 可能正被 OOM Killer 杀死。在这种情况下，由于线程 B 持有的锁没有释放，整个系统可能陷入死锁状态。 情况 2：OOM Killer 杀死持有锁的进程\n线程 C 正在执行某个操作，持有某个内核锁，并尝试分配内存。 内存不足，OOM Killer 选择线程 C 作为目标，并强制终止线程 C。 如果线程 C 在被杀死时没有机会释放它持有的锁，那么其他依赖于该锁的线程将永远无法获得该锁，可能导致整个系统的一部分冻结或陷入死锁。 Linux 内核采取了一些措施来减少死锁的风险，但并不能完全消除它。以下是几种常见的策略：\nOOM Killer 的谨慎选择 OOM Killer 通常会避免杀死关键系统进程或持有重要锁的进程。它通过一些启发式规则来选择那些可以安全终止的进程。例如，OOM Killer 会优先选择消耗大量内存但相对不重要的用户进程，而不是那些核心系统进程。\n无等待内存分配策略 在某些情况下，内核代码会被设计成不等待内存分配。如果分配失败，系统调用会立即返回错误，而不是进入无限制的等待状态。这减少了进程在持有锁时因内存不足而进入等待状态的机会，从而减少死锁风险。\n锁的分层与优先级顺序 为了避免锁的相互依赖导致的死锁，Linux 内核通常会设计锁的层级顺序。通过确保不同的锁以一致的顺序被获取，可以避免交叉锁定（即两个线程分别持有彼此需要的锁）的情况。\n分配超时和中断机制 在某些情况下，内核可能会设置一个超时，让内存分配的等待时间有限。如果超过这个时间限制，线程会被强制中断或返回错误，而不是无限期等待内存的释放。这也可以降低持锁线程进入死锁的可能性。\n示例 1：文件系统死锁。假设一个进程（线程 A）正在执行一个文件写入操作，它持有了文件系统的锁（fs_lock）。在这个操作过程中，它需要为某个数据结构分配内存，但此时系统内存不足，于是线程 A 进入等待状态。与此同时，OOM Killer 开始尝试释放内存，并选择了另一个进程（线程 B）作为目标进行杀死。但是线程 B 此时持有了进程表的锁（proc_lock），并且正等待 fs_lock 被释放。此时，fs_lock 被线程 A 持有，线程 A 又在等待内存释放，形成了死锁。\n示例 2：内存分配导致的死锁。假设有一个多线程应用程序，其中一个线程正在尝试分配内存，并持有内存管理相关的锁（如 memory_lock）。当它无法分配内存并进入等待状态时，OOM Killer 选择杀死另一个持有 memory_lock 的线程（因为这个线程消耗了大量的内存）。然而，当该线程被杀死后，由于它在被杀死时持有的 memory_lock 没有释放，导致其他线程无法再获得该锁，系统可能陷入死锁。\n总而言之，死锁的风险主要来自于进程在内存分配失败时持有锁进入等待状态，以及 OOM Killer 杀死持锁进程时导致的资源未释放问题。Linux 内核通过启发式选择进程、锁定顺序、无等待分配等机制来减少死锁发生的概率，但在极端情况下，死锁仍可能发生。\n这些问题表明，在内存管理和多线程编程中，并发、锁管理、内存分配相互交织，导致了一些复杂的挑战。\nruntime \u0026ldquo;runtime\u0026rdquo; 指的是一个程序执行时所依赖的运行时环境，它为程序提供了执行过程中所需的功能和支持。C 语言通常被认为没有复杂的运行时环境，因为它的设计非常接近硬件，程序运行时不需要依赖复杂的框架或机制来管理内存、线程、异常等。\n在编程语言中，运行时环境（runtime environment） 是指在程序执行过程中提供支持的一组库、工具和服务。这些服务可以包括：\n内存管理：如垃圾回收（GC）。 线程管理和调度：处理多线程并发的机制。 异常处理：如处理错误和异常的机制。 类型检查：某些语言可能在运行时进行类型检查或转换。 系统调用接口：为程序提供与操作系统交互的接口。 在一些编程语言中，运行时环境是相对复杂的，因为它不仅要管理内存，还要处理并发、线程同步、异常处理等问题。运行时环境通常伴随着一个虚拟机（如 Java 的 JVM）或某些底层库（如 Python 的解释器）来确保程序在不同环境下都能一致运行。\nC 语言的设计非常简洁，接近硬件层，它的程序不依赖复杂的运行时环境。C 语言提供了直接的内存管理（通过 malloc() 和 free()），而且程序员需要手动管理堆栈、内存分配和释放。它不像其他语言那样依赖自动垃圾回收器或虚拟机。C 语言的运行时环境非常轻量化，主要包括一些标准库函数的支持，但它不提供自动内存管理或复杂的系统抽象。\n相比之下，其他一些现代编程语言通常带有复杂的运行时。例如：\nJava：Java 依赖 Java 虚拟机（JVM） 作为其运行时环境。JVM 提供了垃圾回收、自动内存管理、线程调度、异常处理等功能。因此，Java 程序运行时依赖一个复杂的运行时环境来执行，这也是为什么它具有较好的跨平台能力。 Go：Go 语言虽然编译成机器代码，但它有一个复杂的运行时系统，主要用于管理**并发（goroutine）*和*垃圾回收。Go 的垃圾回收器负责自动管理内存的分配和回收，这使得 Go 程序员不需要手动释放内存，但也带来了一定的性能开销。 Python：Python 是解释型语言，依赖于 Python 解释器 来执行代码。解释器不仅处理代码的解释执行，还管理内存（如垃圾回收）、处理异常，并提供动态类型系统的支持。Python 的运行时环境非常复杂，包含了大量的高层机制。 某些现代编程语言通过复杂的运行时提供了很多高级特性，如：\n自动内存管理：例如 Java、Go、Python 等语言都有垃圾回收器（GC），自动回收不再使用的对象。这种功能可以让开发者不用担心内存泄漏问题，但同时它也增加了运行时的复杂性。 类型安全：一些语言（如 Java 和 Go）有严格的类型检查机制，运行时可能会进行类型检查，确保类型安全，这也需要额外的运行时支持。 并发管理：像 Go 和 Java 有内置的并发机制，Go 的 goroutine 和 Java 的线程管理都依赖于复杂的运行时来调度和管理并发任务。 异常处理：C 语言没有内建的异常处理机制，但像 Java、Python 等语言有复杂的异常处理系统，它们需要运行时来捕捉和处理异常，确保程序的健壮性。 在这段话中，\u0026ldquo;runtime\u0026rdquo; 指的是一个编程语言在程序执行时所依赖的环境支持。C 语言的运行时非常简单，因为它不依赖垃圾回收、虚拟机或复杂的系统抽象，程序员必须手动管理内存和系统资源。而其他语言，如 Java、Go、Python 等，往往依赖复杂的运行时来提供内存管理、并发支持、类型安全和异常处理等高级功能。现代社会中，这种复杂运行时让程序员开发时更加高效、安全，但也增加了一些运行时的开销和复杂性。\nFutex Futexes（fast userspace mutexes，快速用户空间互斥锁）是一种用于高效实现用户空间线程同步的机制，它允许线程在用户空间中完成大部分的互斥锁操作，只有在需要操作内核资源时才会进入内核。这种设计使得线程同步更加高效，因为它避免了频繁的系统调用开销。\nFutex 是一种结合了用户空间和内核空间的锁机制，用于解决多线程环境中的同步问题。它的核心思想是：大部分时间，线程同步操作可以在用户空间中完成，只有当发生竞争或资源不足时，才会通过 futex 机制进入内核进行处理。这种机制帮助减少不必要的上下文切换，提升了性能。主要作用包括：\n用户空间中的锁管理：Futex 允许多个线程在用户空间中通过简单的内存操作来实现锁的获取和释放。这意味着，当锁可用时，线程之间的锁竞争可以不进入内核，直接在用户空间解决，大大降低了系统调用的频率。 内核中的等待机制：当多个线程竞争一个资源时，可能会出现资源不可用的情况。在这种情况下，线程可以调用 futex 系统调用，将自己挂起到内核中的等待队列中，直到资源可用时被唤醒。这样，线程只会在锁被争抢时进入内核，减少了不必要的内核参与。 避免繁重的上下文切换：传统的锁机制（如互斥锁、条件变量等）可能会频繁调用内核进行线程调度，而 futex 在用户空间处理大部分锁竞争时，避免了频繁的内核调度，提升了性能。 Futex 和线程的关联主要体现在线程同步和锁竞争上。在多线程程序中，多个线程常常需要共享资源，而同步机制（如互斥锁、信号量）则是保证多个线程安全访问共享资源的手段。Futex 是一种更高效的线程同步原语，专门为优化线程同步性能而设计。具体关联如下：\n线程间的锁竞争：当多个线程尝试同时访问同一个资源时，它们会竞争获取锁。Futex 允许这种竞争在用户空间中解决，当锁不可用时，线程会进入内核的等待队列等待被唤醒。 线程的挂起和唤醒：当线程无法立即获得锁时，它可以通过 futex 系统调用主动进入内核，并将自己挂起，等待锁被释放。这种机制避免了线程在用户空间中自旋等待（一直忙等），节省了 CPU 资源。 内核支持的同步：Futex 作为用户空间和内核之间的桥梁，当用户空间的锁争抢进入复杂状态时，才会依赖内核进行线程的管理和调度。这种设计结合了用户空间的高效性和内核的强大调度能力。 Futex的典型使用场景有：\nPthread 实现中的基础：许多操作系统（如 Linux）中基于 POSIX 线程（Pthread）的实现都使用 futex 作为底层的同步原语。通过 futex，可以高效实现锁、条件变量等机制。 高性能多线程应用：在高性能多线程应用中，futex 允许程序减少进入内核的系统调用次数，从而提高并发性能，特别是在资源竞争较少的情况下。 总的来说，Futex 是一种设计用于高效线程同步的机制，它允许在用户空间处理大部分的锁操作，只有在锁被争抢或资源不足时才进入内核。这种机制减少了系统调用开销，提升了多线程程序的性能。与线程的关系在于，futex 提供了线程之间的锁竞争、挂起与唤醒机制，是实现高效线程同步的重要工具。在像 Biscuit 这样的系统中，futex 为内核支持的线程同步提供了基础，确保了高效的线程管理。\nFutex（Fast Userspace Mutex）和普通互斥锁（Mutex） Futex（Fast Userspace Mutex）和普通互斥锁（Mutex）都用于实现线程之间的同步和互斥，确保多个线程安全地访问共享资源。然而，它们的实现方式和效率存在差异。接下来，我将分别介绍 futex 和普通 mutex，并通过举例说明它们的使用场景及区别。\n普通互斥锁（Mutex）\n普通互斥锁 是一种最常见的线程同步原语，常用于保护共享资源，确保只有一个线程在特定时间内访问共享资源。它通常依赖内核进行线程调度和管理。其特点为：\n完全内核实现：每次加锁和解锁都会涉及系统调用，进入内核空间进行处理。\n开销较大：由于每次加锁和解锁都需要上下文切换到内核，因此当竞争不激烈时，性能较低。\n简单直接：开发者不需要关注底层实现细节，只需调用相应的加锁和解锁函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //代码举例 #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; pthread_mutex_t lock; void* thread_function(void* arg) { pthread_mutex_lock(\u0026amp;lock); // 加锁，进入临界区 printf(\u0026#34;Thread %d is accessing the critical section\\n\u0026#34;, *(int*)arg); pthread_mutex_unlock(\u0026amp;lock); // 解锁，离开临界区 return NULL; } int main() { pthread_mutex_init(\u0026amp;lock, NULL); // 初始化互斥锁 pthread_t thread1, thread2; int thread1_id = 1, thread2_id = 2; pthread_create(\u0026amp;thread1, NULL, thread_function, \u0026amp;thread1_id); pthread_create(\u0026amp;thread2, NULL, thread_function, \u0026amp;thread2_id); pthread_join(thread1, NULL); pthread_join(thread2, NULL); pthread_mutex_destroy(\u0026amp;lock); // 销毁互斥锁 return 0; } 代码解释：\n上面的代码使用 pthread_mutex_lock() 和 pthread_mutex_unlock() 来对共享资源进行加锁和解锁。\n每次加锁和解锁都会涉及系统调用，即进入内核来确保线程之间的互斥。这在资源竞争不激烈时，可能会造成不必要的性能开销。\nFutex（Fast Userspace Mutex）\nFutex 是一种更加高效的线程同步机制，结合了用户空间和内核空间的优势。大部分情况下，futex 操作只在用户空间中进行，当且仅当锁被争抢时，才需要进入内核。其特点为：\n大部分操作在用户空间完成：如果没有锁竞争，futex 的加锁和解锁操作可以完全在用户空间中完成，避免了系统调用。\n进入内核的条件：只有当锁被争抢时，才会进入内核来挂起等待中的线程，从而减少系统调用的频率。\n性能更高：由于减少了系统调用的次数，futex 在锁竞争较少时能够显著提高性能。\n假设我们在 Linux 上实现了一个 futex 锁机制，可以通过 futex() 系统调用来挂起和唤醒等待中的线程。下面是一个简化版的伪代码示例，展示 futex 的工作原理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;linux/futex.h\u0026gt; #include \u0026lt;syscall.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int futex_word = 0; void futex_wait(int *futex_addr, int expected) { syscall(SYS_futex, futex_addr, FUTEX_WAIT, expected, NULL, NULL, 0); } void futex_wake(int *futex_addr) { syscall(SYS_futex, futex_addr, FUTEX_WAKE, 1); } void lock() { while (__sync_lock_test_and_set(\u0026amp;futex_word, 1)) { // 进入内核等待 futex 被唤醒 futex_wait(\u0026amp;futex_word, 1); } } void unlock() { __sync_lock_release(\u0026amp;futex_word); // 释放锁 futex_wake(\u0026amp;futex_word); // 唤醒等待中的线程 } void* thread_function(void* arg) { lock(); printf(\u0026#34;Thread %d is accessing the critical section\\n\u0026#34;, *(int*)arg); unlock(); return NULL; } int main() { pthread_t thread1, thread2; int thread1_id = 1, thread2_id = 2; pthread_create(\u0026amp;thread1, NULL, thread_function, \u0026amp;thread1_id); pthread_create(\u0026amp;thread2, NULL, thread_function, \u0026amp;thread2_id); pthread_join(thread1, NULL); pthread_join(thread2, NULL); return 0; } 代码解释：\n这里使用 futex_wait() 和 futex_wake() 来模拟基于 futex 的线程同步。\nfutex_wait() 用于将线程挂起到内核的等待队列中，直到某个条件被满足（例如锁被释放），futex_wake() 用于唤醒等待中的线程。\n当锁没有被争抢时，lock() 函数通过原子操作（__sync_lock_test_and_set）直接在用户空间处理锁的获取，无需系统调用。\n只有当锁被争抢时，才会进入内核进行等待和唤醒操作。\nFutex 和 Mutex 的区别\nFutex 普通 mutex 实现机制 大部分操作发生在用户空间，只有在竞争发生时才进入内核，这显著降低了系统调用的次数，减少了开销。 所有加锁、解锁操作都依赖内核，需要频繁进行系统调用，这增加了上下文切换和开销。 性能 futex 在竞争少时性能更高，因为大部分锁操作都可以在用户空间完成。 普通 mutex 在竞争少时效率较低，因为每次锁操作都需要进入内核，带来额外的性能开销。 复杂性 普通 mutex 的使用相对简单，程序员不需要关心底层实现细节。 futex 的实现更复杂，因为它结合了用户空间和内核空间的操作，需要考虑在用户空间完成大部分操作的同时，在需要时进入内核。 应用场景对比 适用于高性能的多线程场景，特别是竞争不激烈时。比如在 Web 服务器、数据库系统中，futex 的高效锁管理可以大大提高并发性能。 适用于竞争较多或锁机制相对简单的场景。比如小规模的并发程序，在锁的竞争比较频繁时，普通 mutex 的开销与 futex 差异不大。 总结\n普通 mutex 是内核实现的锁机制，所有操作都通过系统调用，较为稳定但性能略低。 futex 是一种结合用户空间和内核的同步机制，减少了系统调用的次数，提高了性能，尤其适用于多线程高并发应用中。 Futex 和 xv6 中的睡眠锁（sleeping locks） Futex 和 xv6 中的睡眠锁（sleeping locks）确实有一些相似之处，特别是在它们如何处理线程等待和唤醒的机制上。但两者的实现细节、使用场景以及设计目标有所不同。接下来我会详细对比 futex 和 xv6 的睡眠锁，并解释它们的区别和相似之处。\nFutex（Fast Userspace Mutex）\nFutex 是 Linux 中的一种高效锁机制，允许线程在用户空间完成大部分的锁操作，只有在必要时才进入内核进行等待和唤醒。它的核心机制是将锁竞争和锁的实际管理分为两个阶段：\n用户空间操作：当线程尝试获取锁时，它在用户空间执行操作。如果锁是空闲的，直接获取锁，线程继续执行，无需进入内核。\n进入内核：如果锁被占用，线程进入内核，通过 futex() 系统调用将自己挂起，等待锁释放。当锁释放时，内核通过 futex_wake() 唤醒等待的线程，重新竞争锁。\n这种机制的设计主要为了提高锁竞争较少时的性能，减少频繁的系统调用。由于大部分锁操作在用户空间完成，它在无竞争情况下比完全依赖内核的锁（如传统的互斥锁）效率更高。\nxv6 中的睡眠锁（sleeping locks）\n在 xv6 中，睡眠锁是一种用于确保线程安全访问共享资源的机制，特别适用于长时间持有锁的场景。在 xv6 中，睡眠锁的机制如下：\n获取锁：当线程尝试获取睡眠锁时，如果锁已经被其他线程持有，那么该线程将进入睡眠，并被移出运行队列。这种睡眠通常是通过操作系统内核的调度器来实现的，调度器会挂起该线程，直到锁变为可用。\n唤醒：当持有锁的线程释放锁时，系统会唤醒等待该锁的线程，让它们重新进入调度队列，并在锁空闲时竞争获取。\n睡眠锁的设计理念是，避免忙等待（busy waiting）。在某些情况下，锁可能需要持有较长时间（例如，执行 I/O 操作），因此让线程进入睡眠状态可以节省 CPU 资源，而不是让它们一直自旋等待锁的释放。\nFutex 与 xv6 睡眠锁的相似之处\n等待时挂起线程：两者的核心相似之处在于，当锁不可用时，线程会被挂起，直到锁被释放。这样做的目的是避免忙等待，节省 CPU 资源。\n唤醒机制：当锁空闲时，两者都会唤醒等待的线程，让它们重新参与锁的竞争。\n避免忙等待：两者都旨在避免 CPU 资源浪费，使线程在无法获取锁时有效地让出 CPU。\nFutex 与 xv6 睡眠锁的区别\nFutex xv6 睡眠锁 实现细节 Futex 是基于用户空间和内核的混合机制。锁的获取和释放大部分是在用户空间完成的，只有在竞争激烈或资源不足时才会通过系统调用进入内核。这使得 futex 更加高效，尤其是在低竞争环境中，减少了频繁的内核调用。 xv6 中的睡眠锁则完全依赖内核调度。每当线程无法获取锁时，线程直接进入内核，被挂起到等待队列中，并依赖内核调度器唤醒。这是一种传统的、完全依赖内核的锁机制，没有 futex 那样的用户空间优化。 性能与使用场景 Futex 设计上更为高效，特别适合于多线程高并发场景，因为它避免了每次锁操作都进入内核。如果锁竞争较少，futex 的性能优势明显，因为它大部分时间在用户空间处理。 xv6 的睡眠锁则更加适合于较简单的多线程环境，尤其是长时间持有锁的场景。由于它完全依赖内核进行线程调度，在某些低并发场景下，这种机制是足够的，但在高并发或频繁的锁竞争情况下，性能可能不如 futex。 设计复杂度 Futex 的实现较复杂，因为它需要在用户空间和内核之间协同工作。需要处理用户空间锁竞争的同步，还要处理进入内核时的等待队列管理等操作。 xv6 的睡眠锁设计相对简单，完全依赖内核来进行调度。内核负责管理锁的获取、释放以及线程的睡眠和唤醒操作。 举例说明\n**Futex 示例：**当多个线程尝试获取同一个 futex 锁时，假设锁是空闲的，线程 A 可以直接在用户空间获取锁并执行。线程 B 尝试获取锁时发现锁已经被占用，于是它调用 futex_wait() 进入内核，等待线程 A 释放锁。当线程 A 释放锁时，它通过 futex_wake() 唤醒线程 B，线程 B 重新获得执行机会。\n**xv6 睡眠锁示例：**在 xv6 系统中，假设线程 A 持有了某个共享资源的睡眠锁，正在进行 I/O 操作。线程 B 想要访问同一个资源，但发现锁已被线程 A 占用，于是线程 B 被内核调度器挂起，进入睡眠状态。等到线程 A 完成操作并释放锁时，调度器会唤醒线程 B，线程 B 才能继续执行。\n总结\nFutex 是一种更复杂、更高效的锁机制，主要通过在用户空间解决大部分锁操作来减少系统调用，适用于高性能的多线程并发环境。\nxv6 的睡眠锁 则是一种相对简单的锁机制，完全依赖内核进行线程调度和唤醒，适用于较低并发或长时间持有锁的场景。\n尽管它们在挂起和唤醒线程的机制上有相似之处，但在性能优化、设计复杂度以及使用场景上有显著差异。\nFutex xv6 睡眠锁 用户态与内核态的配合 首先在用户态尝试获取锁，只有当用户态无法获取锁时才进入内核。这种设计大幅减少了用户线程与内核间的切换开销，从而提高了性能。 锁的获取和释放完全在内核中进行。所有与锁相关的操作都依赖于内核，无法在用户态直接操作。 锁状态和线程状态管理 锁的状态管理主要由用户态的锁变量（通常是一个整数）维护，内核只在必要时介入。线程在用户态时不需要依赖内核调度。 锁的状态和线程的状态管理都依赖内核，由内核调度器直接负责线程的挂起和唤醒。 性能与复杂度 Futex 设计上更加复杂，它的混合设计保证了在大多数情况下无需频繁进入内核，从而具有更好的性能表现。 xv6 睡眠锁 设计简单，适合教学与理解，但是每次锁竞争时都需要内核介入，性能上不如 futex 高效。 针对schedule的复习 在 xv6 操作系统中，系统中多个进程如何在多个 CPU 上运行、切换，并在内核态和用户态之间切换的机制。\n假设的场景：\n4 个 CPU：你有一个四核的系统。 3 个进程：系统当前有 3 个用户进程正在运行或准备运行。 在这种情况下，是否存在额外的进程在运行 \u0026ldquo;内核监管的内容\u0026rdquo;，或者这些进程是如何在用户态和内核态之间切换的。\n多进程与 CPU 调度\n在 xv6 中，CPU 的调度由操作系统的内核控制。假设有 3 个进程和 4 个 CPU 时，操作系统将决定这些进程如何分配到不同的 CPU 上运行。如果系统中只有 3 个进程，而有 4 个 CPU，那么其中一个 CPU 会空闲，因为没有足够的进程去占用每个 CPU。每个 CPU 在某个时刻只能运行一个进程。\n在多核环境下，xv6 的内核调度器会根据进程的状态（例如：就绪、运行、阻塞等）决定哪些进程在哪些 CPU 上运行。\n进程的用户态和内核态切换\n每个进程都会在需要时在 用户态 和 内核态 之间切换。这个切换发生在以下几种典型场景下：\n系统调用：当进程调用某些系统资源（如文件系统、网络、内存管理等），需要通过系统调用进入内核态。\n中断：如定时器中断、硬件中断等，进程会暂时进入内核态，处理完中断后再切换回用户态。\n进程调度：当操作系统决定切换运行进程时，当前进程会切换到内核态进行上下文切换，然后另一个进程会被调度并切换到用户态继续运行。\n在这些场景下，虽然同一个进程可以在用户态和内核态之间切换，但这些进程的执行仍然是在这 3 个用户进程中，没有额外的进程在运行专门的内核任务。\n内核与用户态的关系\n用户态进程：用户态的进程是我们平时所运行的应用程序，它们通过系统调用与内核交互。\n内核态：当进程需要访问系统资源或处理中断时，进程会从用户态切换到内核态。此时，虽然执行的是内核的代码，但它仍然是当前的进程在执行，只不过执行的是与内核相关的代码，而不是用户应用代码。\n因此，没有一个单独的进程在运行内核监管的内容。而是每个进程在需要时会进入内核态，执行系统调用或处理中断等任务，然后返回用户态。\n总的来说\n在 4 个 CPU 和 3 个进程的场景下，3 个进程会被内核调度到 CPU 上运行，其中一个 CPU 会空闲。 这些进程在执行时，会在用户态和内核态之间切换（例如通过系统调用或中断），但没有额外的进程专门负责运行内核任务。 每个进程的用户态和内核态的切换是由具体的事件触发的（系统调用、中断等），这些切换不会创建新进程，只是在现有进程中执行内核代码。 针对schedule的复习（2） 在 xv6 中，当 CPU 数量少于进程数时，确实会涉及到进程的调度，这由操作系统内核的**调度器（scheduler）**负责管理和执行。然而，这个调度过程并不会创建一个额外的\u0026quot;调度进程\u0026quot;来控制进程运行，而是由操作系统中的调度逻辑直接在内核态中执行。\n在 xv6 和大多数操作系统中，调度器（scheduler） 是内核中的一部分代码，它并不是一个单独的进程。调度器函数是内核态执行的代码，它负责选择下一个要运行的进程。每个 CPU 会有自己的调度器逻辑，它会检查当前有哪些进程可以运行，选择一个进程，然后将它调度到 CPU 上。\n当进程主动放弃 CPU（例如调用 sleep() 函数）或时间片用尽时，系统会进入内核态，调用调度器。调度器会在所有可以运行的进程中选择下一个要运行的进程。这个过程并不会创建一个新的调度进程，而是当前的进程（或者说当前 CPU）进入内核态，调用内核中的调度代码来进行调度。\n当 CPU 数量少于进程数时，调度器就会负责让每个进程“轮流”占用 CPU。这通常通过以下方式实现：\n时间片（time slice）：每个进程在 CPU 上运行一段时间（称为时间片），当时间片耗尽时，调度器会选择另一个就绪的进程来运行。 主动让出 CPU：进程可能在需要等待某些资源（如 I/O、信号等）时主动进入睡眠状态，调度器会选择另一个可运行的进程接替。 当时间片耗尽或进程需要进入睡眠时，CPU 进入内核态，由当前进程的上下文切换到调度器的上下文，调度器函数在内核态中执行，负责选择下一个进程运行。因此，在这个过程中，没有一个专门的调度进程，调度器逻辑是作为操作系统内核的一部分直接运行的。\n调度器通常在以下情况下被调用：\n时间片用尽：当一个进程在 CPU 上的时间片耗尽时，会发生时钟中断，当前进程进入内核态，调用调度器函数来选择下一个要运行的进程。 进程阻塞：当进程主动调用 sleep()、wait() 等函数时，它进入睡眠状态，并通过调度器选择其他可运行的进程。 进程退出：当进程完成并调用 exit() 时，调度器会选择下一个进程运行。 中断处理：在中断（例如 I/O 完成）期间，进程可能会从睡眠状态唤醒，这时调度器也可能被调用，选择唤醒的进程继续运行。 虽然调度器代码在内核态中执行，但它并不是由用户进程直接控制的。当用户进程运行时，它并不知道调度器的存在。调度过程完全由内核管理，是在操作系统内核态中运行的。换句话说，调度器逻辑是内核的一部分，而不是用户进程的一部分。具体来说：\n当前进程不控制调度：当一个进程的时间片耗尽时，时钟中断发生，进程会被强制切换到内核态，执行调度器逻辑。这个过程由内核自动控制，进程本身并不会主动去调用调度器。 调度是内核的行为：调度器在内核态中运行，切换不同的进程，不依赖任何一个用户进程去执行调度逻辑。 总而言之\n没有额外的调度进程：在 CPU 少于进程数时，不会有一个额外的进程专门执行调度任务。调度过程由操作系统的内核负责，调度器逻辑直接在内核态中执行。 调度逻辑由内核管理：每当需要切换进程时，当前进程会进入内核态，由调度器选择下一个要运行的进程。 CPU 轮换执行进程：调度器确保每个进程轮流占用 CPU，根据时间片或进程状态（如进程阻塞或退出）来决定何时切换进程。 因此，调度器函数是在内核中运行的，不是原来的进程自己运行的。每当系统需要调度时，当前进程会进入内核态，调度器逻辑会在内核中执行，并决定下一个要运行的进程。\nQ:什么叫做在内核中运行的，函数不是只有在运行在CPU上的进程时才能执行么\nA:你的问题涉及到内核态与用户态的概念，以及如何理解 \u0026ldquo;在内核中运行\u0026rdquo; 这一说法。让我详细解释一下。\n在操作系统中，进程的执行可以分为用户态（user mode）和内核态（kernel mode）。这是为了安全性和隔离，防止普通用户程序直接访问硬件或操作系统的核心资源。\n用户态：大部分应用程序在用户态运行，拥有受限的权限，不能直接访问硬件或者操作系统内核的资源。如果需要访问这些资源（比如进行文件读写、网络操作等），需要通过系统调用（system call）请求内核来完成。 内核态：当系统调用或中断发生时，进程进入内核态，执行操作系统的代码，直接与硬件交互。内核态拥有最高权限，可以执行任何操作。调度器（scheduler） 也是在内核态中运行的。 进程确实是运行在 CPU 上的。无论进程是在用户态还是内核态，最终都是由 CPU 执行代码。只不过，用户态的代码和内核态的代码是有所区别的。操作系统通过上下文切换来在不同进程之间切换，而 \u0026ldquo;内核中运行\u0026rdquo; 是指在内核态执行操作系统的代码，而不是运行用户的应用代码。举个例子：\n一个进程在用户态执行一个应用程序的代码，例如访问文件。此时它调用了 read() 系统调用。 这个 read() 系统调用触发进程进入内核态，进程现在开始执行内核代码（例如文件系统的操作）。 当文件读取操作完成后，进程会返回用户态，继续执行它的应用代码。 所以，在内核中运行，并不是指有一个独立的\u0026quot;内核进程\u0026quot;，而是指当前进程切换到了内核态，此时进程执行的是操作系统内核的代码，而不是用户态的代码。\n操作系统的调度器也是在内核态中运行的。当某个进程的时间片耗尽，或者它调用了阻塞操作（如等待 I/O），CPU 会触发一个中断（通常是时钟中断），使当前进程切换到内核态。此时，调度器代码会运行，它会：\n保存当前进程的状态（上下文）。 选择一个新的进程来运行（通过调度算法）。 切换 CPU 的上下文到新进程的上下文。 让 CPU 开始执行新进程的代码。 这个整个过程都是在内核态中发生的，执行的也是内核代码，但依然是通过当前 CPU 的资源运行的。\n当我们说某个函数 \u0026ldquo;在内核中运行\u0026rdquo; 时，实际意思是：\n当前的进程（或 CPU 上正在执行的任务）处于 内核态，执行的是操作系统内核提供的功能（例如文件系统、调度器等），而不是用户程序的代码。 在某个时刻，一个进程可能处于用户态执行用户代码，也可能处于内核态执行内核代码。这都是由同一个进程完成的，只不过切换了执行的权限和代码。 关键点是：内核态和用户态不是两种不同的进程或任务，而是同一个进程在不同的执行状态下执行不同的代码。\n在内核中运行是指进程切换到内核态，执行操作系统的代码，比如系统调用、调度器逻辑等。 调度器在内核态中执行，它负责在进程间进行上下文切换，让不同的进程共享 CPU 资源。调度器运行时，实际也是某个进程在 CPU 上执行内核态的调度逻辑。 没有独立的 \u0026ldquo;内核进程\u0026rdquo; 在执行这些操作，而是当前 CPU 或进程在必要时进入内核态执行系统级的代码。 所以，当你问 \u0026ldquo;函数不是只有在运行在 CPU 上的进程时才能执行么\u0026rdquo;，答案是是的，无论是内核态还是用户态，都是在进程执行时通过 CPU 来运行代码，只不过在内核态下，进程执行的是操作系统内核的功能，而不是用户代码。\n针对schedule的复习（3） 我们可以进一步解释一下进程切换和多核系统中的调度，特别是如何在内核态执行 schedule 以及如何确保不会重复调度已经在其他 CPU 上运行的进程。\n进程切换（上下文切换）\n当一个进程进入内核态并执行调度（schedule() 函数）时，操作系统会选择下一个要运行的进程，并进行上下文切换。上下文切换是将当前进程的状态（如寄存器、程序计数器、栈指针等）保存下来，然后加载下一个进程的状态，使得 CPU 可以继续执行新进程的任务。具体过程如下：\n当前进程的状态保存：当一个进程进入内核态并被调度器决定切换出去时，系统会保存当前进程的上下文，包括寄存器的值、程序计数器、栈指针等信息。这些信息会被保存在内核的数据结构中，以便稍后能够恢复这个进程的执行。 选择下一个进程：schedule() 函数会从就绪队列中选择下一个可以运行的进程。就绪队列（run queue）包含所有等待被调度执行的进程。调度器根据调度策略（如轮转、优先级等）选择下一个进程。 恢复下一个进程的上下文：调度器选定下一个进程后，会加载该进程的上下文，即恢复该进程的寄存器、程序计数器、栈指针等信息。然后，CPU 开始执行这个进程的代码。 退出内核态，回到用户态：完成上下文切换后，新的进程从内核态退出，重新进入用户态，继续它的执行，直到再次被调度器选中或需要进入内核态。 进程调度和多核系统\n在多核系统中，多个 CPU 可以同时运行不同的进程。为了避免同一个进程被分配到多个 CPU 上运行，调度器需要确保在一个 CPU 上选择的进程没有在其他 CPU 上执行。如何避免进程在多个 CPU 上运行？\nCPU 绑定和调度器协调：在多核系统中，调度器使用一个全局的数据结构来管理所有的就绪进程（通常称为全局就绪队列），或者每个 CPU 维护自己的就绪队列。无论采用哪种方式，调度器需要在进程状态中记录该进程当前是否已经在某个 CPU 上运行，防止同一个进程同时被调度到多个 CPU 上。 进程状态：每个进程都有状态，如运行中（running）、就绪（ready）、阻塞（blocked）等。当一个进程正在某个 CPU 上运行时，它的状态会被标记为“运行中”，这意味着其他 CPU 不会再次调度这个进程。 CPU 和进程的关系管理：调度器在分配进程时，会查询每个进程的状态。如果进程正在其他 CPU 上运行，它会跳过该进程并选择其他就绪的进程。这样可以确保每个进程只在一个 CPU 上执行。 调度器使用进程的状态和 CPU 数据结构来管理哪些进程正在运行、哪些进程可以被调度：\n运行队列（runqueue）：每个 CPU 通常有一个自己的运行队列，存放所有可以在该 CPU 上执行的进程。这个队列中可能会有一些进程等待执行。如果进程正在另一个 CPU 上运行，它不会被加入该 CPU 的运行队列。\n进程控制块（PCB）：每个进程都有一个数据结构，称为进程控制块（Process Control Block, PCB），其中包含进程的各种状态信息。每个 CPU 的调度器会检查进程的 PCB，判断该进程是否正在运行，是否可以调度。\n进程的状态与调度流程\n为了确保同一个进程不会在多个 CPU 上运行，操作系统通过以下几个步骤进行协调：\n检查进程状态：调度器在调用 schedule() 时，会检查进程的状态。只有那些处于就绪（ready）状态的进程会被考虑调度。如果进程已经处于运行中状态，则表示它正在其他 CPU 上运行，当前 CPU 不会选择它。 状态标记：一旦调度器决定将某个进程分配给 CPU，它会将该进程的状态标记为运行中，同时更新该进程的 PCB，记录该进程当前在哪个 CPU 上运行。这样，其他 CPU 的调度器在检查时，就会知道这个进程正在执行，不会重复调度。 CPU 核心的独立调度：每个 CPU 都有自己的调度器，负责决定该 CPU 上运行哪些进程。在某些多核系统中，调度器会有全局协调机制，确保多个 CPU 不会重复调度同一个进程。在其他系统中，调度器可以依赖每个 CPU 的独立运行队列来确保进程不会重复分配。 总而言之\n调度器在内核态运行：当一个进程进入内核态并调用 schedule() 函数时，调度器会选择下一个要运行的进程，并执行上下文切换。上下文切换是保存当前进程状态并加载下一个进程状态的过程。 多核调度中的协调：在多核系统中，调度器需要确保一个进程不会同时在多个 CPU 上运行。它通过进程的状态标记（如运行中、就绪）和全局或局部的运行队列来管理多个进程，并避免重复调度。 进程的状态和管理：每个进程有一个控制块（PCB）来记录它的状态和所运行的 CPU。调度器使用这些信息来选择哪些进程可以运行，并避免调度那些已经在其他 CPU 上运行的进程。 通过这些机制，操作系统确保在多核环境中，调度是协调一致的，避免同一个进程在多个 CPU 上同时运行。\n针对schedule的复习（4） 在多核系统中，某些数据结构是所有 CPU 共享的，这些数据使得各个 CPU 能够了解到其他 CPU 上的进程状态，从而避免重复调度同一个进程。\n共享的数据结构\n在操作系统中，进程的状态和调度信息通常存储在一些全局或者共享的数据结构中，以便所有 CPU 可以访问。以下是几个常见的共享数据结构：\n进程控制块（PCB，Process Control Block）\n每个进程都会有一个唯一的进程控制块（PCB），其中存储了该进程的状态信息，例如：\n进程当前的状态（如就绪、运行中、阻塞）。 当前进程在哪个 CPU 上运行。 寄存器值、程序计数器、栈指针等用于上下文切换的信息。 这些 PCB 是系统中的共享数据结构，各个 CPU 的调度器可以查看每个进程的 PCB 来判断它的状态。通过这些信息，CPU 能够了解当前进程是否已经在其他 CPU 上运行，从而避免重复调度。\n运行队列（Run Queue）：每个 CPU 可能会有自己的就绪队列，但在某些系统中，也可以有全局的运行队列（runqueue），该队列存储了所有处于就绪状态、等待被调度的进程。每当一个 CPU 需要调度进程时，它会查看这个共享的运行队列或它自己的局部运行队列，选择一个处于“就绪”状态且尚未运行的进程来执行。\n全局调度器信息：在某些操作系统中，存在一个全局调度器，它能够协调所有 CPU 的进程调度。在这种情况下，全局调度器可以直接管理各个 CPU 的负载，平衡进程在多个 CPU 上的分布。这种全局信息也可以让系统知道每个进程在哪个 CPU 上运行，并避免重复调度。\n进程状态的共享与同步：为了确保多个 CPU 能够正确地共享这些信息，并避免出现竞争条件（例如两个 CPU 同时调度同一个进程），系统通常采用锁机制或者原子操作来保护对这些共享数据的访问。\n原子操作：当需要改变进程的状态（例如从“就绪”变为“运行中”）时，系统会使用原子操作来更新进程的状态，确保状态更新是不可分割的，避免多个 CPU 同时修改状态导致冲突。\n锁机制：如果多个 CPU 需要同时访问共享数据（如全局运行队列），系统会使用锁来确保一次只有一个 CPU 能够修改共享数据。这可以防止竞争条件，但同时也会带来一些额外的性能开销。\n如何避免重复调度：通过共享的 PCB 和进程状态信息，每个 CPU 的调度器能够了解到当前有哪些进程正在运行，以及它们在哪些 CPU 上运行。这是如何避免重复调度的关键：\n在每次调度时，调度器会检查进程的状态。如果进程处于“运行中”状态，且已经在某个 CPU 上运行，则该调度器会跳过这个进程，选择其他处于“就绪”状态的进程。 通过这种方式，各个 CPU 能够同步了解进程的状态变化，确保同一个进程不会同时在多个 CPU 上运行。 总结\n进程的状态信息（如 PCB）和运行队列是所有 CPU 可以访问的共享数据结构。每个 CPU 的调度器通过访问这些共享数据来决定哪些进程可以运行，避免重复调度。\n操作系统使用原子操作和锁机制来保护对这些共享数据的访问，确保多个 CPU 在并行运行时能够正确更新和读取进程的状态。\n这种共享的数据结构确保了在多核系统中，调度器可以协调多个 CPU 之间的进程执行，避免同一个进程同时在多个 CPU 上运行。\n因此，进程状态的数据是可以被所有 CPU 共享的，这也是如何在多核系统中进行有效调度的核心机制。\nschedule的死锁 在操作系统的调度器（scheduler）实现中，锁机制是非常重要的，它用于保护共享资源的访问，防止多个 CPU 或多个线程同时修改共享数据，导致数据不一致。然而，锁的使用不当可能引发一系列问题，如死锁、优先级反转、锁竞争等。下面是调度器中使用锁时常见的一些易产生错误的地方，以及对应的解决办法：\n死锁（Deadlock）\n死锁发生在两个或多个进程相互等待对方持有的锁，导致所有相关进程都无法继续执行。调度器中可能需要对多个资源进行加锁，例如对进程控制块（PCB）、就绪队列、进程状态等进行加锁。在获取多个锁时，可能会出现锁的顺序不一致，进而引发死锁。解决办法有：\n锁定顺序一致性：确保所有锁的获取顺序是一致的。这意味着在不同的代码路径中，如果多个锁需要同时持有，锁的获取顺序必须始终保持一致。\n避免嵌套锁：尽量减少嵌套锁的使用。如果确实需要多个锁，使用分层的锁机制来确保不会形成循环等待。\n死锁检测和恢复：某些操作系统实现了死锁检测机制，通过检测锁的依赖关系，发现可能的死锁情况并强制中断相关进程。\n优先级反转（Priority Inversion）\n优先级反转发生在一个低优先级的进程持有了一个锁，而一个高优先级的进程需要等待这个锁释放。如果有一个中优先级的进程占据了 CPU，低优先级的进程无法运行并释放锁，那么高优先级的进程将被中优先级的进程“饿死”，导致系统性能下降。解决办法有：\n优先级继承（Priority Inheritance）：当高优先级的进程等待一个由低优先级进程持有的锁时，低优先级进程会临时继承高优先级，以加速锁的释放，避免优先级反转问题。这是一个常见的调度器优化技术。\n避免不必要的锁：确保只在必要时使用锁，尽可能缩短锁的持有时间，减少优先级反转的可能性。\n锁竞争和性能瓶颈\n在多核系统中，当多个 CPU 频繁访问同一个共享数据结构时，可能会导致大量的锁竞争。比如在调度器中，所有 CPU 都需要访问共享的运行队列或进程控制块，这会造成频繁的锁获取和释放，进而影响系统性能。解决办法有：\n锁分离和局部队列：通过将全局锁分离为多个锁，减少竞争。例如，每个 CPU 可以有自己的运行队列（local runqueue），避免所有 CPU 竞争访问同一个全局队列。\n读写锁（Read-Write Locks）：对于读多写少的共享数据结构，使用读写锁可以提升并发性能。多个 CPU 可以同时读取数据，而写操作则需要独占锁。\n减少锁的粒度：尽量减少锁的粒度，避免对大范围的操作进行锁定，只对真正需要保护的部分加锁。可以通过锁分割来减少锁的范围，例如针对不同的数据结构使用不同的锁。\n持锁时间过长\n如果进程在持有锁的情况下进行耗时较长的操作，其他等待锁的进程可能会被长时间阻塞，影响系统的并发性和响应能力。在调度器中，这个问题可能出现在当进程持有锁并且执行复杂的调度算法或处理其他任务时。解决办法有：\n短期持锁策略：确保锁的持有时间尽可能短。锁应仅用于保护关键的、最小粒度的代码区域。\n使用自旋锁或休眠锁的结合：在持锁时间较短的情况下可以使用自旋锁（spinlock），这样等待的进程不会进入睡眠状态，而是自旋等待锁释放。对于持锁时间较长的情况，使用休眠锁（sleeping lock）更为合适。\n抢占中断和锁\n如果进程在持有锁时被抢占或发生中断，且中断处理程序或新的被调度的进程也需要获取同一个锁，可能会造成系统的不可预测行为。解决办法有：\n禁用抢占：在某些关键区域（持有锁期间），可以短暂禁用抢占，确保进程不会在持有锁时被打断。\n中断安全的锁机制：确保使用的锁机制在中断上下文中是安全的，例如在自旋锁中禁用中断，防止锁被不安全地抢占。\n双重释放锁或未释放锁\n错误地释放同一个锁两次（双重释放锁）或者未释放锁（锁泄漏）是常见的编程错误。这在调度器的代码中尤其可能发生，尤其是在上下文切换、调度新进程时忘记释放锁。解决办法有：\n使用调试工具：可以使用一些调试工具（如 lockdep，用于 Linux 内核），它能动态检测锁的使用不当，提前发现问题。\n代码审计与测试：在锁操作的代码中进行严密的审查和测试，确保所有加锁的地方都能够正常释放锁，避免逻辑漏洞。\n饥饿问题（Starvation）\n如果调度器中某个低优先级的进程长时间无法获取锁（尤其是当锁被高优先级的进程频繁获取时），该进程可能会遭遇饥饿，即它的执行总是被推迟。解决办法有：\n公平锁（Fair Locking）：通过公平锁机制，确保锁的获取是公平的，按顺序来决定谁能获取锁，防止某些进程总是被饿死。例如使用队列锁或基于优先级的公平调度机制。\n限时锁等待：对等待锁的时间设置一个限制时间，如果超过这个时间还没有获取到锁，则可能选择放弃当前操作或采取其他补救措施。\n递归锁导致的逻辑混乱\n递归锁允许同一个线程多次获得同一把锁，但在调度器中，递归锁的使用如果不加以控制，会导致逻辑混乱或不可预测的行为，特别是在调度复杂的多线程环境中。解决办法有：\n避免使用递归锁：尽量避免在调度器中使用递归锁。如果确实需要嵌套锁，尽量使用普通锁，并对锁的获取和释放顺序进行严格控制。 总而言之，在操作系统的调度器中，使用锁来保护共享资源和数据结构是必不可少的，但错误的锁使用会导致死锁、性能下降、优先级反转等问题。为了解决这些问题，操作系统通常采取以下措施：\n确保锁的获取顺序一致，避免死锁。 使用优先级继承等机制解决优先级反转问题。 使用局部队列、读写锁等机制减少锁竞争。 确保锁的持有时间短，避免影响系统的并发性。 通过这些措施，可以有效地提高调度器的性能和可靠性。\nschedule的中断 在调度（schedule）和上下文切换（context switch）的关键时刻，如果发生中断，确实可能会造成系统状态的不一致或混乱。为此，操作系统（如 xv6 和 Linux）都有相应的机制来处理这种情况，确保调度和上下文切换时的中断不会导致问题。接下来，我们分别看看 xv6 和 Linux 是如何解决这一问题的。\nxv6 中的中断处理\n在 xv6 中，调度和上下文切换是非常重要且关键的操作。为了避免在这些关键时刻被中断打断，xv6 采用了一些简化但有效的机制来处理中断问题。\n在 schedule 和 context switch 等关键的调度场合，xv6 通过禁用中断来保证这些操作的原子性。具体来说，在调度器运行期间，系统会暂时禁用中断，确保不会有外部中断（如时钟中断、设备中断）在关键时刻打断调度过程。\n以下是 xv6 的具体做法：\n在进入调度器前禁用中断：当调度器决定切换进程时，会禁用当前 CPU 上的中断。这意味着在上下文切换完成之前，不会有任何中断能够发生。这确保了上下文切换的安全性。 完成上下文切换后重新启用中断：当上下文切换完成并且新的进程开始执行后，调度器会重新启用中断。这样，系统在调度完成后能够处理外部中断，但不会在关键时刻被打断。 xv6 上下文切换的步骤：\n禁用中断：调度器在切换进程时会禁用中断，确保切换过程不会被打断。 保存当前进程状态：保存当前正在运行进程的 CPU 状态到进程控制块（PCB）。 选择新的进程：从就绪队列中选出下一个要运行的进程。 加载新进程状态：将新进程的状态从 PCB 中恢复到 CPU。 重新启用中断：上下文切换完成后，恢复 CPU 上的中断。 通过在关键操作时禁用中断，xv6 避免了在调度和上下文切换中发生的潜在冲突和数据不一致问题。\nLinux 中的中断处理\nLinux 是一个复杂的多任务操作系统，相比 xv6，它处理调度和中断的机制更加复杂和优化。Linux 需要保证在调度和上下文切换时，能够正确处理中断，同时尽量减少性能开销。Linux 的解决方案主要依赖于更复杂的锁机制、抢占控制和中断屏蔽。\nLinux 使用抢占（preemption）机制来确保高优先级任务能够及时执行，但在某些关键区域，Linux 通过禁用抢占来确保安全的调度和上下文切换：\n禁用抢占：在进入调度器和执行上下文切换时，Linux 内核会临时禁用抢占（preemption）。这意味着当前 CPU 不会被更高优先级的任务打断，直到调度和上下文切换完成。 中断屏蔽：类似于 xv6，Linux 也可以在调度器运行和上下文切换期间临时屏蔽中断。通过屏蔽中断，Linux 确保不会在关键时刻打断调度和切换进程。 局部中断禁用：为了提高系统的实时响应能力，Linux 只禁用当前 CPU 的中断（局部中断），而不是禁用全局中断。这种设计保证了其他 CPU 在调度过程中可以继续处理中断和任务，而不会因为一个 CPU 的上下文切换而影响整个系统。 Linux 还依赖**各种锁（如自旋锁、互斥锁）**来保护关键的共享数据结构。调度器和上下文切换期间，操作系统必须访问和修改一些关键的数据结构，如进程控制块（PCB）、调度队列等。这些访问通常通过锁来保护，防止多个 CPU 同时访问同一个数据结构。\n自旋锁：在某些短暂的、快速完成的操作中，Linux 使用自旋锁。在持有自旋锁期间，CPU 不会进入睡眠状态，而是自旋等待锁的释放。对于上下文切换等需要快速完成的任务，自旋锁是一个合适的选择。 互斥锁：对于较长的操作，Linux 使用互斥锁来避免多个进程同时修改相同的数据结构。 Linux 上下文切换的步骤：\n禁用抢占：Linux 在上下文切换期间禁用抢占，防止其他进程在此过程中抢占 CPU。 保存当前进程状态：保存当前进程的状态，包括 CPU 寄存器、栈指针等到进程控制块（PCB）。 选择下一个进程：通过调度器算法（如 CFS，完全公平调度器），从就绪队列中选择下一个要运行的进程。 加载下一个进程状态：将下一个进程的上下文信息从 PCB 中恢复到 CPU。 重新启用抢占：上下文切换完成后，重新启用抢占和中断处理。 关键差异总结：xv6 vs Linux\n特性 xv6 Linux 中断处理 在调度和上下文切换期间禁用中断 在调度和上下文切换期间禁用局部中断和抢占，避免冲突 抢占机制 没有复杂的抢占机制 使用抢占控制，在必要时禁用抢占，允许实时调度 锁机制 锁机制简单，主要用于保护关键数据 复杂的锁机制，如自旋锁和互斥锁，用于保护内核数据 上下文切换 通过简单的状态保存和恢复进行切换 使用调度器算法和复杂的状态管理，支持多核和高并发 在调度（schedule）和上下文切换过程中，避免中断打断关键操作是非常重要的。xv6 通过简单地禁用中断来确保上下文切换的安全性，适合简单的单 CPU 或较少核心的系统。Linux 则更复杂，它通过禁用抢占、局部中断以及使用锁机制来处理多核系统中的复杂并发和中断问题，同时确保高效的系统响应能力。\nBiscuit 操作系统中断处理 Device Interrupt Handlers in Biscuit “A Biscuit device interrupt handler marks an associated device-driver goroutine as runnable and then returns”\n当设备（例如硬件设备）发出中断时，操作系统的**中断处理程序（interrupt handler）**会被触发。\n在 Biscuit 中，中断处理程序的主要作用是\n标记与该设备相关的设备驱动程序的 goroutine 为“可运行”（runnable）\nGoroutine 是 Go 语言中的轻量级线程。 Marking as runnable 意味着操作系统不会在中断处理程序中做大量工作，而是通知相关的 goroutine 可以运行，然后返回。中断处理程序不会长时间运行，也不会在中断处理过程中直接处理复杂的设备逻辑。 中断处理程序执行完这个标记操作后，立即返回，不会进行更多操作。这是一种常见的设计，可以提高中断响应效率，并减少由于中断处理程序长时间运行可能带来的问题（如性能降低或资源死锁）。\nWhy Do Interrupt Handlers Return Quickly? “Interrupt handlers cannot do much more without risk of deadlock, because the Go runtime does not turn off interrupts during sensitive operations such as goroutine context switch”\n这里提到，中断处理程序不能做太多事情，否则可能导致死锁问题。这是因为： Go 运行时不会在某些敏感操作期间关闭中断，例如在 goroutine 上下文切换 的时候。 如果中断处理程序在这些敏感时期执行复杂操作，可能会干扰 Go 运行时对 goroutine 的调度，从而引发死锁（即程序无法继续执行的状态）。 为了避免死锁，Biscuit 采用了一种保守的设计——中断处理程序只做简单的标记操作，然后快速返回，让真正的工作交给相应的 goroutine 来完成。 Handlers for System Calls and Faults “Handlers for system calls and faults from user space can execute any Go code”\n相较于设备中断处理程序，\n系统调用和用户态故障（faults）的处理程序可以执行任意的 Go 代码\n系统调用（system call）：指用户程序请求操作系统服务的调用（如读写文件、分配内存等）。Biscuit 中，系统调用处理程序是在与用户线程相关联的 goroutine 上执行的，因此它们不受设备中断处理那种约束。 Faults from user space：这些故障（如页面错误或非法内存访问）也会由相应的 handler 来处理，这些 handler 同样可以执行复杂的 Go 代码。 Execution Context for System Calls and Faults “Biscuit executes this code in the context of the goroutine that is associated with the current user thread”\n当处理系统调用或用户态故障时，Biscuit 在与当前用户线程（user thread）相关联的 goroutine 中执行处理代码。这意味着： 执行上下文是当前的 goroutine（这 goroutine 是与发起系统调用或触发故障的用户线程相关联的）。 由于 Go 运行时对 goroutine 的管理，Biscuit 可以在这些情况下执行任意的 Go 代码，而不必担心中断相关的死锁问题。 这段话解释了 Biscuit 操作系统如何处理中断和系统调用。具体来说：\n设备中断处理程序只标记相关设备驱动的 goroutine 为“可运行”，并迅速返回，以避免与 Go 运行时的敏感操作（如上下文切换）发生冲突，从而避免死锁。 与此相反，系统调用和用户态故障处理程序可以执行任意的 Go 代码，因为它们是在用户线程相关联的 goroutine 上执行的，不受设备中断处理程序那样的限制。 在 Biscuit 系统中，中断处理程序不能直接处理复杂的逻辑，尤其是涉及到 goroutine 的操作。主要原因在于 Go 运行时的敏感操作和死锁风险，具体解释如下：\nGo 运行时的特性： Go 运行时管理着 goroutine 的调度，这涉及到 goroutine 的上下文切换和状态管理。而上下文切换是一种敏感操作，这意味着它不能被中断打扰，因为打断这种操作可能导致调度状态的不一致，进而导致整个调度系统陷入混乱。 死锁的来源： 如果中断处理程序（interrupt handler）在这些敏感时期执行了复杂的操作，例如尝试操作一个资源或者等待某个锁的释放，很有可能与 Go 运行时本身的锁定机制发生冲突，产生 死锁。比如，假设某个中断在 Go 运行时正忙于调度 goroutine 时发生，如果中断处理程序尝试与调度器共享资源，就可能出现双方相互等待的情况——调度器等待中断处理完成，中断处理等待调度器释放锁，这就形成了死锁。 因此，Biscuit 系统选择了一种保守的设计方式，即中断处理程序 尽可能少地做事情，它只是标记一个与设备相关联的 goroutine 为“可运行”，然后迅速返回。这样一来，中断处理程序不会与 Go 运行时的调度逻辑产生直接冲突，避免了在上下文切换时可能的死锁。\n这里的中断不包括 trap。\n中断 (Interrupt)：通常由硬件设备触发，用来通知 CPU 需要执行某个设备的服务。这种中断通常是异步的（随时可能发生），比如网卡有新数据到达、硬盘请求完成等等。 Trap：一种特殊的中断，通常是由软件触发的，例如系统调用（system call）或程序发生异常（如非法指令或内存访问错误）。trap 由 CPU 执行指令时检测到条件后发出，可以看作是 同步中断。 Biscuit 中提到的设备中断处理程序是特指硬件中断，而 trap 处理（例如系统调用和用户态故障）被赋予了更多的自由，因为它们发生在 系统调用上下文中，可以在与用户线程相关联的 goroutine 上执行任意的 Go 代码，不涉及中断期间的敏感情况。因此，trap 并不受同样的限制，且其处理程序可以执行更复杂的逻辑。\n中断处理程序只做标记的方式非常类似于经典的 top-half 和 bottom-half 机制。\n在传统操作系统中，top-half 和 bottom-half 的设计是为了提高中断处理的效率，同时保证系统的稳定性和响应性。简单介绍它们：\nTop-Half： 是中断发生后由中断控制器立即触发的部分。 主要目的是快速处理紧急的任务，例如确认接收到的数据或者告诉设备继续传输。 保持尽可能短的执行时间，以便快速让出 CPU，避免阻塞其他中断。 Bottom-Half： 由 top-half 触发或标记，稍后由系统调度进行执行。 主要用于处理一些相对复杂且时间消耗大的任务，因为这些任务不需要立即响应。 这样可以减少中断关闭时间，提高系统整体的中断响应性能。 在 Biscuit 中，中断处理程序只做一个简单的标记操作，类似于 top-half 的行为，它不会在中断期间做大量工作。标记设备驱动程序的 goroutine 为“可运行”后，这部分工作会在系统调度后由这个 goroutine 来完成，这类似于 bottom-half 的工作方式——即，复杂的设备处理逻辑被推迟到一个更合适的时机进行处理。\n总结来说：\nBiscuit 系统的中断处理程序做标记并迅速返回，是为了避免在中断期间与 Go 运行时的调度机制产生冲突，从而防止死锁。 设备中断和 trap 不同，trap 的处理可以执行复杂的 Go 代码，因为它们是在安全的上下文中处理的。 这种设计非常类似 top-half 和 bottom-half 机制，将紧急、短暂的操作放在 top-half，复杂、耗时的工作放在 bottom-half，由底层调度来完成，从而提高系统响应效率和稳定性。 Biscuit速记 Biscuit 使用页表硬件实现按需分配内存、写时复制和按需文件映射。这些机制通过页面缺失处理来填充页表，提升内存使用效率。 Biscuit 通过紧凑记录连续的内存映射，减少管理开销，提高性能。 Biscuit 使用引用计数来跟踪物理页面的共享情况，确保物理内存的正确管理和回收。 Biscuit 操作系统设备中断处理程序只标记相关设备驱动的 goroutine 为“可运行”，并迅速返回，以避免与 Go 运行时的敏感操作（如上下文切换）发生冲突，从而避免死锁。 Biscuit操作系统与此相反，系统调用和用户态故障处理程序可以执行任意的 Go 代码，因为它们是在用户线程相关联的 goroutine 上执行的，不受设备中断处理程序那样的限制。 Biscuit 实现了从硬件到软件的完整网络通信堆栈： TCP/IP 协议栈：用 Go 语言实现了支持 TCP 和 IP 协议的网络协议栈，负责处理网络中的数据传输、连接管理和错误处理等功能。 网络硬件驱动：实现了支持 Intel PCI-Express 以太网网卡的驱动，使用 DMA 来加速数据传输，使用 MSI 中断来提高网络事件处理的效率。 POSIX 套接字接口：提供了标准化的系统调用接口，使得应用程序可以方便地通过熟悉的 POSIX 套接字 API 进行网络编程。 Biscuit 不支持交换或页出，这意味着它的内存管理相对简单，所有数据必须保留在物理内存中，无法使用磁盘来扩展内存空间。 Biscuit 不支持反向映射，因此它无法有效地回收映射的页面，在内存管理上有所限制。 Biscuit 缺乏许多现代操作系统的安全功能，包括用户管理、访问控制列表和地址空间随机化，这使得它在安全性方面不如其他操作系统健全。 Biscuit 的物理内存分配是通过一个独立的分配器进行的，不依赖于 Go 堆内存管理。 由于物理内存资源有限，页面分配可能会失败，因此内核代码需要检查分配结果，并在分配失败时采取措施（如返回错误）。 Biscuit 内核堆有固定大小，称为 M，用于管理系统调用的内存。 在每次系统调用执行前，内核必须为系统调用保留足够的内存（s），以保证调用能够成功运行。 即使堆内存紧张，Biscuit 会调用垃圾回收器回收超出部分的内存，确保系统调用可以继续执行。 保守估算：由于堆中存活数据的精确大小无法实时获取，Biscuit 使用三个计数器（g、c、n）来保守估算内存使用量，并通过 L = g + c + n 进行判断。 内存不足时的行为：如果内存不足，Biscuit 会唤醒 killer 线程，执行垃圾回收或杀死进程以释放内存，确保系统调用有足够的资源继续执行。 Biscuit 的预留内存机制通过确保每个系统调用在执行前已经预留了足够的内存，避免了系统调用在运行过程中遇到内存不足的问题。这消除了内存资源不足引发的阻塞或循环等待情况，因此系统调用在该机制下无需担心死锁。如果内存不足以启动系统调用，Biscuit 会通过垃圾回收或进程终止来释放资源，从而保证系统的正常运行。 静态分析工具 MAXLIVE Biscuit 操作系统如何通过静态分析工具 MAXLIVE 来确定每个系统调用所需的最大内存保留量（即 s）。这个过程涉及分析 Biscuit 内核代码和所使用的 Go 包，以识别系统调用在执行过程中可能需要的最大活动内存量。让我们详细解释其工作原理。\nMAXLIVE 工具的目标是通过静态分析，计算出每个系统调用需要保留的内存大小 s。这对于确保系统调用能够成功执行，并防止内存不足导致的错误至关重要。\n静态分析的核心挑战\n“The core challenge is detecting statically when allocated memory can no longer be live”\n核心挑战：主要难点在于静态分析中检测出已分配的内存何时不再存活（即不再被使用），这是确定 s 的关键。许多系统调用会为短暂的用途分配内存（如临时变量或数据缓冲区），在某些时候这些内存不再需要，因此可以释放。但静态分析需要精确判断这些时机，确保计算出的 s 是足够保守但不过度的。 其他挑战： 循环：系统调用中的循环可能具有非固定的边界，导致内存分配量难以确定。静态分析必须能够处理这种不确定性，确保保守估计 s。 后台活动：有些内核活动与特定的系统调用无关（如后台任务），但它们仍然可能消耗内存。静态分析工具需要考虑这些活动对内存的影响。 通过事件驱动结构简化分析\n“We address these challenges by exploiting the characteristic event-handler-style structure of most kernel code”\nBiscuit 内核代码大多是事件驱动的，系统调用和内核任务通常只进行少量的工作，然后返回或进入空闲状态。这种结构化的、分阶段的工作模式使得内存的生命周期更加清晰，从而简化了静态分析的复杂度。 系统调用的结构化实现：大多数系统调用都是按照特定的顺序执行，分配内存、使用内存，然后释放内存。由于系统调用在执行过程中通常不涉及复杂的递归或长时间的内存保留，静态分析可以利用这些规律更准确地计算 s。 修改内核代码以适应静态分析\n“We are willing to change the kernel code to make it amenable to the reservation approach”\n为了适应内存保留方法并简化静态分析，开发者对内核代码进行了调整。特别是，他们避免了递归等复杂的内存管理模式，改写了一些函数，使代码结构更加适合分析工具的静态检测。 修改 Go 标准库：为了使静态分析工具更准确，Biscuit 还对 Go 的一些标准包进行了修改（如 time 和 fmt 包）。这些包可能涉及到复杂的内存分配模式或内存保留，修改它们可以让静态分析工具更容易理解这些包的内存行为，并正确估计内存使用量。 如何进行静态分析\nMAXLIVE 的静态分析流程大致如下：\n分析内存分配点：工具会扫描代码中的内存分配点（如变量声明、动态分配等），并跟踪这些内存分配的生命周期。 检测内存释放点：工具需要确定何时分配的内存不再使用，找到内存的释放点。 估算最大活跃内存：基于事件驱动的代码结构，工具能够估算每个系统调用在执行过程中同时活跃的最大内存量 s。 处理循环和递归：对于循环和递归，工具使用保守估计的方法，确保即使在复杂情况下，也能计算出一个安全的 s。 总的来说，MAXLIVE 是一种静态分析工具，用于计算每个系统调用执行过程中所需的最大内存保留量 s。它通过分析 Biscuit 内核代码和 Go 包，利用内核代码的事件驱动结构来简化分析过程。遇到循环、递归和后台任务等复杂情况时，工具采用保守估计。为了确保工具能够准确分析内存行为，开发者还修改了部分内核代码和 Go 标准包。这种分析方法确保了在系统调用执行时有足够的内存可用，避免了内存不足导致的系统调用失败。\nMAXLIVE 的基本操作原理 分析调用图（Call Graph）\n“MAXLIVE examines the call graph (using Go’s ssa and callgraph packages) to detect all allocations a system call may perform”\n调用图：调用图是一个描述程序中函数调用关系的图形结构。通过分析调用图，MAXLIVE 能够了解系统调用在执行过程中可能触发的所有函数调用，并在这些函数中检测内存分配行为。 Go 的 SSA 和 callgraph 包：MAXLIVE 使用 Go 的 SSA（静态单赋值，Static Single Assignment） 和 callgraph 包 来构建和分析系统调用的调用图。这帮助它检测出系统调用期间的所有潜在内存分配点。 逃逸和指针分析\n“It uses escape and pointer analysis (Go’s pointer package) to detect when an allocation does not “escape” above a certain point in the call graph”\n逃逸分析（escape analysis）和指针分析（pointer analysis）是静态分析的重要技术，MAXLIVE 使用 Go 的 pointer 包 来实现这些功能： 逃逸分析：用于确定内存分配是否会在调用图中的某个函数之外继续存在。如果分配的内存不会逃逸（即仅在局部函数中使用并在该函数返回时失效），那么可以认为内存在函数返回时已经死亡。 指针分析：用于追踪指针引用，确保分析可以精确地确定内存的使用情况。 逃逸的意义：如果某段内存分配不会逃逸出当前函数或调用点，MAXLIVE 就能够判断这块内存在函数返回时已经死亡，并不需要计入最大内存保留量 s。 特殊处理的内存分配类型\nMAXLIVE 对一些特定类型的内存分配进行了特殊处理：\na. go 关键字（创建 goroutine）\n“go (which creates a goroutine) is treated as an escaping allocation of the maximum kernel stack size”\ngoroutine 创建：在 Go 语言中，go 关键字用于启动一个新的 goroutine，这会创建一个独立的内核栈。MAXLIVE 将这种分配视为逃逸分配，因为新的 goroutine 会在不同的执行上下文中运行。 内核栈保留：当一个 goroutine 创建时，必须为其保留足够的内存，就像它是一个新的系统调用一样。因此，MAXLIVE 在分析 go 关键字时，会考虑它的最大栈空间大小并将其计入内存保留。 b. defer 关键字\n“defer is a non-escaping allocation, but is not represented by an object in the SSA so MAXLIVE specifically considers it an allocation”\ndefer：Go 中的 defer 关键字用于推迟执行某些操作，通常是在函数返回之前执行的清理操作。虽然 defer 不涉及逃逸分配（它的生命周期限定在函数内部），但它并不会在 SSA 中明确表示为对象。因此，MAXLIVE 需要特殊处理 defer，并将其视为一种分配行为。 c. maps 和 slices\n“Every insertion into a map or slice could double its allocated size; MAXLIVE generally doesn’t know the old size, so it cannot predict how much memory would be allocated”\nmaps 和 slices：在 Go 语言中，maps 和 slices 是动态数据结构，它们在需要扩展时可能会分配更多的内存。每次插入到 map 或 slice 时，分配的内存可能会翻倍。 问题：MAXLIVE 无法准确预测 map 和 slice 的初始大小或当前已分配的内存大小，因此它无法直接估计每次插入或扩展会消耗多少内存。 解决方案：为了解决这个问题，Biscuit 的代码中手动添加了 70 个注解，明确声明 maps 和 slices 的最大可能大小。通过这些注解，MAXLIVE 能够在静态分析时更精确地估算这些数据结构的内存消耗。 总的来说\nMAXLIVE 静态分析工具 通过分析调用图来检测系统调用期间的所有内存分配，并使用逃逸分析和指针分析来判断哪些分配在函数返回时会失效（不再存活）。 特殊处理：MAXLIVE 对某些内存分配行为进行了特殊处理，如 go 关键字启动的 goroutine、defer 操作，以及动态数据结构 maps 和 slices。对于后者，由于无法准确估算内存增长，Biscuit 的代码中添加了注解，帮助 MAXLIVE 估计最大可能的内存分配量。 这一静态分析工具的目的是为了准确计算系统调用需要保留的内存 s，从而确保系统调用能够在有限的内存中成功执行。 Biscuit 系统在应对 内存耗尽 问题时的局限性 内存耗尽与垃圾回收：Biscuit的设计要求垃圾收集器在内存几乎耗尽的情况下也能成功运行。然而，Go的垃圾收集器在运行时，可能需要额外分配内存，尤其是用于追踪未处理指针的工作栈（work stack）。这是垃圾回收的一个潜在问题：即使内存紧张，垃圾收集器仍可能需要分配内存来继续进行扫描。 解决方案的潜在改进：虽然目前Biscuit还没有实现解决这一问题的机制，但理论上它可以通过检测工作栈是否已满来应对这个问题。当工作栈满了时，可以回退到使用“标记位图”（mark bitmap）作为工作栈，通过扫描已经被标记的对象，并寻找那些包含未标记指针的对象。这种策略虽然可以确保垃圾收集完成，但效率会较低。好在这种情况预计比较少见，因为工作栈缓冲区可以事先以较低的成本进行预分配。在实验中，垃圾收集器最多只会为工作栈分配 0.8% 的堆内存。 内存碎片问题：Go的垃圾收集器不会移动对象（即不进行内存压缩），这意味着它不能减少内存碎片。因此，即便系统有足够的空闲内存，如果这些内存被分割成了太小的片段，仍然无法满足较大块的内存分配需求。 解决碎片问题的潜在方法：为了解决碎片问题，系统应该在每个系统调用期间为不同大小类别的对象计算其最大存活量（MAXLIVE），以确保在调用时能预留足够的内存。但目前的实现还没有做到这一点。 Biscuit 内核 的实现细节 内核的编程语言： Biscuit 内核几乎全部使用 Go 语言编写，有 27,583 行 Go 代码，1,546 行汇编代码，没有使用 C 语言。这个点强调了 Biscuit 的与众不同之处：大部分内核是用 Go 这种高级语言实现的，而不是通常的 C。 系统调用的支持： Biscuit 提供了 58 个系统调用（如图 4 所示），这些系统调用是操作系统与应用程序交互的重要接口。 它还具备一定的 POSIX 兼容性，这意味着它可以运行一些现有的服务器程序，比如 NGINX 和 Redis，这表明它有能力支持主流的网络服务应用程序。 设备驱动的实现： Biscuit 包含了针对 AHCI SATA 磁盘控制器 和 Intel 82599 系列的以太网控制器（比如 X540 10G 网卡）的驱动程序。 这两个驱动程序使用 DMA（直接内存访问） 来加速数据传输。驱动程序使用 Go 语言的 unsafe.Pointer 来访问设备寄存器和硬件定义的内存结构（如 DMA 描述符）。Go 的 atomic 包则用来控制访问顺序，以确保多线程环境下的内存一致性。 如果 Go 支持某种 内存屏障（memory fence），代码会更加简洁。内存屏障是一种用于保证内存访问顺序的机制，对于硬件寄存器的安全访问非常重要。 Go 中的 \u0026ldquo;unsafe\u0026rdquo; 使用： Biscuit 使用了 90 处 Go 中的 \u0026ldquo;unsafe\u0026rdquo; 例程（不包括 Go 运行时本身的使用）。这些 \u0026ldquo;unsafe\u0026rdquo; 操作通常是为了直接访问底层硬件，而这些操作无法通过 Go 的安全内存模型来完成。 这些不安全操作包括：解析和格式化网络数据包、在物理页号与指针之间进行转换、读写用户内存以及访问硬件寄存器等。这类操作在操作系统中是必须的，因为内核需要直接与硬件交互，而这些操作无法完全通过 Go 语言的安全机制来实现。 内存屏障 (Memory Fence) 内存屏障是一种用于在多核或多线程环境中保证内存访问顺序的机制。它确保某些特定的内存操作在内存屏障之前或之后的顺序是被严格遵守的。通常，在现代处理器中，出于性能优化的考虑，内存读写操作可以被乱序执行。但在某些场景下，乱序可能导致数据一致性问题，尤其是在多线程或多核程序中。因此，需要使用内存屏障来确保对内存的访问按照预期顺序发生。\n假设有两个线程 Thread A 和 Thread B，它们共享一个变量 x 和一个标志位 flag。Thread A 会先写入变量 x，然后设置标志位 flag，而 Thread B 会根据 flag 的值来判断是否读取 x。\nThread A:\n1 2 3 x = 42; // 写入数据 memory_fence(); // 内存屏障 flag = 1; // 设置标志位 Thread B:\n1 2 3 4 if (flag == 1) { // 确保读取的是最新的 x 值 printf(\u0026#34;%d\u0026#34;, x); } 在 Thread A 中，如果没有内存屏障，处理器可能会为了优化性能，先设置 flag 然后再写入 x，这就会导致 Thread B 在读取 x 时得不到最新的值。通过插入内存屏障，可以确保写入 x 的操作在设置 flag 之前执行，从而保证数据的一致性。\nDMA（直接内存访问，Direct Memory Access） DMA（Direct Memory Access） 是一种让设备直接与内存交互而无需经过 CPU 的技术。通常情况下，设备与内存之间的数据传输需要通过 CPU 进行调度，这样会占用大量 CPU 的时间。而使用 DMA，设备可以自己控制数据传输，CPU 只需启动传输，然后继续处理其他任务，等数据传输完成后再处理通知。\n假设有一个计算机系统，连接了一个硬盘驱动器和一个内存模块。如果使用传统方式，数据从硬盘读入内存需要经过 CPU，CPU 需要把每个字节的数据从硬盘拷贝到内存，这样 CPU 的资源会被大量占用。\n而如果采用 DMA，硬盘驱动器可以直接与内存通信，数据从硬盘被直接传输到内存，而不需要 CPU 逐字节进行拷贝。CPU 只需要在传输开始时配置 DMA 控制器，并在传输结束后收到通知。这样可以大大提高系统性能，因为 CPU 可以在传输数据时继续执行其他任务。\n例子中的流程：\nCPU 初始化 DMA 控制器，告诉它将硬盘上的数据块直接传输到内存的某个区域。 DMA 控制器直接从硬盘读取数据，并写入内存，CPU 不再干预。 传输完成后，DMA 控制器向 CPU 发送中断信号，通知 CPU 数据已写入内存。 这种方式极大提高了数据传输效率，特别适合大规模数据传输，比如磁盘到内存的拷贝，网络数据包的处理等。\nBiscuit 与 Go 运行时环境 的协同工作 对 Go 运行时的修改： 记录 goroutine 的内存分配：Biscuit 修改了 Go 运行时来记录每个 goroutine 的堆内存分配量（heap reservations）。这是为了监控各个 goroutine 在内核中的内存使用情况。 检测设备处理器 goroutine：Biscuit 的改动还允许检查是否有等待运行的设备处理 goroutine，以便及时处理设备请求。 增加默认栈大小：Biscuit 将 Go 运行时的默认栈大小从 2KB 增加到了 8KB，以避免在处理一些常见的系统调用时频繁触发栈扩展操作。这样可以提高系统调用的效率。 与 Go 运行时保持兼容的部分： Go 运行时在持有锁时不关闭中断：Go 运行时在持有锁或操作 goroutine 私有状态时不会关闭中断。这意味着内核中断不能直接与持有锁的代码同时操作，以避免潜在的死锁问题。 Biscuit 的解决方案：Biscuit 采取了一个较简单的策略：当处理设备中断时，中断处理器并不直接操作设备，而只是设置一个标志，表示应该唤醒对应的设备处理 goroutine。这样，实际的设备处理会由 goroutine 完成，避免了死锁的风险。 处理定时器中断和上下文切换： 定时器中断与 goroutine 上下文切换：Biscuit 的定时器中断处理器无法直接强制 goroutine 进行上下文切换（context switch），因为 Go 运行时可能正处于切换上下文的中间状态。这种情况下，如果强制切换可能会导致不一致或错误的状态。 使用 Go 的抢占机制：Biscuit 依赖 Go 编译器插入的抢占检查（pre-emption checks）来完成对内核 goroutine 的上下文切换。Go 编译器会在生成的代码中插入这些检查点，以确保 goroutine 可以被适时抢占。 用户态的上下文切换：对于从用户态发出的定时器中断，Biscuit 可以强制上下文切换，因为用户态与内核态的上下文切换是可以安全进行的。 Biscuit 通过修改 Go 运行时以跟踪内存分配和调整栈大小，确保内核 goroutine 的高效运行。为了避免中断与锁竞争导致的死锁，Biscuit 采用了一种间接的方式处理设备中断。对于上下文切换，它依赖 Go 的抢占机制来确保 goroutine 的正常调度。\nBiscuit 内核与 Go 运行时 的集成方式 Goroutine 调度和上下文切换：\n调度决策在运行时环境中：Biscuit 不直接控制 goroutine 的调度决策，因为这些决策是由 Go 运行时管理的。这意味着 Biscuit 继承了 Go 运行时的调度策略，而无法对其进行修改或优化。\n没有切换进程页表：由于 goroutine 调度和上下文切换是由 Go 运行时处理的，Biscuit 并不会在 goroutine 切换时切换每个进程的页表。因此，Biscuit 的系统调用代码不能直接使用用户空间地址。相反，Biscuit 需要显式地将用户的虚拟地址转换为物理地址，并检查相应的页权限。\n必要时切换页表：在从内核态切换到用户态之前，Biscuit 会在必要时切换页表，以确保内核和用户态之间的内存隔离。\n垃圾回收相关的改动：\n关闭垃圾收集器专用线程：为了减少由于垃圾回收导致的延迟，Biscuit 关闭了专用的垃圾收集器线程。这样做的目的是让应用线程与垃圾收集线程不再竞争 CPU 资源，避免性能下降。\n根标记提供分配信用：在垃圾回收的标记阶段，Biscuit 改进了根标记（root marking）操作，给予内存分配线程一定的“分配信用”，以免某个线程在执行标记时遇到过多的根标记工作，导致延迟。\n减少标记阶段大对象的分块大小：Biscuit 将大对象在标记过程中分割的块大小从 128kB 减小到 10kB。这可以让标记工作更平滑地进行，减少大对象标记时的开销。\n内核性能优化：\n使用大页映射内核代码：Biscuit 内核的文本段（代码部分）使用了**大页（large pages）**映射，这样可以减少 iTLB（指令翻译后备缓冲器）的缺失次数，从而提高指令读取的效率。\n使用每个 CPU 独立的网卡传输队列：Biscuit 对每个 CPU 使用独立的网卡（NIC）传输队列，从而提高网络传输的效率，避免多个 CPU 共享传输队列带来的竞争。\n无读锁的数据结构：在一些性能关键的代码中（例如目录缓存和 TCP 轮询），Biscuit 使用了无读锁的数据结构。无读锁的结构在并发读操作时性能更好，因为多个线程可以同时读取而不需要锁的开销。\nGo 对优化的影响：\nGo 并未妨碍优化：总体上，Biscuit 的开发者发现，虽然 Go 是一种高级语言，但它并没有显著妨碍内核性能优化的实现。通过合理的设计和技巧，Biscuit 仍然能够实现许多标准的内核优化。 Biscuit 内核在处理调度、垃圾回收以及性能优化时的策略依赖 Go 运行时管理调度和上下文切换，并在某些方面进行了调整以适应内核需求，如手动进行地址转换和页表切换。在垃圾回收方面，Biscuit 通过关闭专用线程和优化根标记来减少回收延迟。性能优化方面，Biscuit 实现了大页映射、无锁数据结构等标准优化措施，并且 Go 语言并未阻碍这些优化的实现。\n无读锁结构（lock-free read structure） 无读锁结构（lock-free read structure）是一种在并发编程中使用的数据结构，它允许多个线程或进程同时进行读取操作而不需要加锁。这种设计可以显著提高读取操作的并发性能，因为读操作之间不会互相阻塞或等待，从而减少了锁的开销。无读锁结构通常通过使用某种形式的原子操作或版本控制来确保数据一致性。\n在传统的读写锁设计中，读操作和写操作都会使用锁来保护共享数据的访问。一旦一个线程获取了读锁，其他线程可以同时获取读锁进行读取，但写操作则需要等待所有读操作完成。而无读锁结构通过设计，允许读操作在不加锁的情况下执行，通常通过以下几种机制来实现：\n原子操作：通过原子操作（如 compare-and-swap）来确保写操作不会影响读操作的一致性。 版本控制：读操作在读取数据时会检查一个版本号，如果写操作修改了数据，它会更新版本号。读操作在完成后检查是否版本号一致，从而判断数据是否被修改。 例子 1：无锁哈希表（Lock-free Hash Table）。在无锁哈希表中，多个线程可以同时读取数据，而不需要锁住整个哈希表。写操作（插入或删除键值对）可以通过原子操作完成，比如使用 CAS（Compare-And-Swap） 来更新哈希表中的指针。例如，当一个线程想要从无锁哈希表中读取数据时，它只需要找到哈希槽并读取其中的数据。如果在读的过程中有其他线程在写数据，写线程会通过原子操作来更新哈希表指针。读线程可以在读取后检查版本号，确保数据一致。\n例子 2：RCU (Read-Copy-Update)。RCU 是一种常见的无读锁技术，用于提高读操作的效率。它的工作方式是这样的：\n当进行写操作时，RCU 会创建数据的一个副本。修改不会直接应用到现有数据，而是应用到副本上。 读操作仍然读取原来的数据，不受写操作的影响。 写操作完成后，将新的副本替换为旧的数据，所有后续的读操作会看到新的数据。 RCU 的一个典型应用是 Linux 内核中的 目录缓存（dentry cache）。多个线程可以同时读取文件系统的目录信息，而不需要锁定整个缓存，写操作则通过版本控制机制来进行同步。\n例子 3：无锁队列（Lock-free Queue）。无锁队列是一种队列数据结构，它允许多个线程在无锁的情况下并发进行入队和出队操作。无锁队列通常通过原子操作来确保并发安全。最经典的例子是 Michael \u0026amp; Scott 无锁队列：\n入队操作：入队线程会通过 CAS 原子操作来更新队列的尾部指针，将新节点插入队列的末尾。 出队操作：出队线程也通过 CAS 操作更新队列的头部指针，从队列头部取出数据。 由于入队和出队是独立的、无锁的，多个线程可以同时执行这些操作，而不会相互阻塞。\n总而言之，无读锁结构允许多个线程在不需要加锁的情况下并发进行读取操作，从而提高了读取性能和系统的整体并发性。常见的实现方式有基于原子操作的无锁队列、无锁哈希表，以及使用版本控制机制的 RCU。通过这些机制，可以减少锁的争用，提高系统在高并发场景下的性能。\n什么是poll 在操作系统中，poll 是一种用于监控文件描述符（file descriptors）状态的系统调用，通常用于网络编程和 I/O 操作。它允许程序同时等待多个文件描述符的事件，如可读、可写或发生错误。\npoll 系统调用的主要作用是让程序可以同时监视多个 I/O 通道，而不需要为每个 I/O 通道分别等待。这在多路复用（multiplexing）I/O 中非常重要，特别是在处理并发网络连接时。程序员可以用 poll 来检查哪些文件描述符已经准备好进行读、写操作或是否有错误发生，然后对那些准备好的文件描述符进行处理。\npoll 通常用于网络服务器，特别是那些需要同时处理多个连接的服务器程序。例如，服务器会监听多个客户端的网络套接字，通过 poll 确定哪个套接字有数据可读，哪个套接字可以发送数据，或者哪个套接字发生了错误。其工作原理为：\n程序向操作系统传递一个文件描述符列表，每个文件描述符都关联了需要监听的事件（如可读、可写、错误等）。 poll 将阻塞进程，直到其中至少有一个文件描述符准备好处理（或指定的超时时间到达）。 一旦返回，程序可以根据 poll 返回的结果，处理对应的文件描述符。 假设你有多个客户端连接到服务器，服务器可以使用 poll 来检查哪些客户端有新的数据要读取：\n1 2 3 4 5 6 7 8 9 10 11 12 struct pollfd fds[2]; // 创建两个文件描述符的数组 fds[0].fd = socket_fd; // 监听的套接字文件描述符 fds[0].events = POLLIN; // 监听可读事件 fds[1].fd = another_fd; // 另一个文件描述符 fds[1].events = POLLOUT; // 监听可写事件 int ret = poll(fds, 2, timeout); // 调用 poll，等待事件 if (fds[0].revents \u0026amp; POLLIN) { // 如果 fds[0] 的文件描述符准备好读取数据 read(socket_fd, buffer, length); } poll 通过提供一种机制，让程序可以同时监视多个 I/O 通道的状态，而不会被阻塞在单个通道上。这对于处理高并发的网络应用程序至关重要。\nGo 语言的高级语言特性（HLL）对 Biscuit 内核性能的影响 性能成本分析： 这部分分析了 Go 的高级语言特性在 Biscuit 内核中的性能成本，并展示了三个应用的基准测试结果（见图 7）。 Tput（吞吐量）列：显示应用每秒处理的请求数量，表示系统处理任务的速度。 Kernel time（内核时间）列：显示应用在内核中执行的时间比例（而不是在用户空间）。结果表明，这些基准测试主要由内核活动主导，并且在测试中所有四个 CPU 核心都被 100% 占用。 内核堆内存和垃圾回收： 应用程序在运行时会在 Biscuit 内核堆中产生18 到 48 MB 的活动数据。 它们快速分配了大量的临时对象，导致每次基准测试运行中触发了数十次垃圾回收（GC）。 这些垃圾回收占用了系统1% 到 3% 的总 CPU 时间。 编译器生成的代码分析： Prologue cycles（函数前置代码）：这一项衡量由编译器生成的代码在每个函数开始时消耗的时间，用于检查栈是否需要扩展，以及垃圾收集器是否需要暂停整个系统。这部分代码是所有高级语言特性中最耗时的。 WB cycles（写屏障周期）：用于处理对象在并发垃圾回收期间被修改时，编译器生成的写屏障代码的执行时间。 运行时安全检查成本： Safety cycles（安全周期）：报告了用于运行时检查的成本，如 nil 指针检查、数组和切片边界检查、除零检查、以及动态类型转换检查。这些检查是由编译器自动生成的，贯穿整个代码。 通过一个工具分析这些检查在 Biscuit 二进制文件中的位置，并与 CPU 时间消耗交叉引用，以确定它们的成本。 内存分配时间： Alloc cycles（分配周期）：衡量 Go 内存分配器花费的时间，这包括检查自由列表以满足分配请求，但不包括并发垃圾回收的工作。虽然内存分配并不是高级语言独有的任务，但一些 C 语言编写的内核使用定制的分配器来优化分配过程。 结果总结： 基于图 7 的数据显示，函数前置代码（Prologue cycles） 是使用高级语言特性时最昂贵的部分。垃圾回收成本虽然明显，但并不是最大的性能瓶颈。 另一方面，随着活动数据量的增加，垃圾回收的成本会增长，这在 §8.6 进行了进一步讨论。同时，函数前置代码的开销似乎有可能通过优化进一步减少。 虽然Go 语言高级特性对 Biscuit 内核性能的影响，尤其是函数前置代码、垃圾回收、写屏障和安全检查的性能开销带来了一定的性能损耗，但其中大部分问题可以通过进一步优化来减轻。\n正课部分 C语言实现操作系统的优劣势 今天我们要讨论使用高级编程语言实现操作系统的优点和代价。今天课程的内容主要是我们之前写的一篇论文 ，论文作者包括了Robert和我，以及一个主要作者Cody Cutler，他是这门课程的一个TA。\n我通常不会讨论我们自己写的论文，但是因为这篇论文与课程内容相关，所以它在这。今天我们会用一些PPT，而不是在白板上讲解这节课。\n这篇论文的起因是这样一个问题：你应该用什么样的编程语言来实现操作系统内核？\n这个问题很多同学都问过，可能是因为你发现了在操作系统中有Bug，然后你会想，如果我使用一种其他的编程语言，或许我就不会有这些Bug了，所以这是一个经常出现的问题。\n虽然这也是操作系统社区里的一个热烈争论的问题，但是并没有很多事实来支撑这里的讨论。在课程结束的时候，我们其实也不能对这个问题有个干脆的回答。\n但是至少这篇论文贡献了一些数据使得你可以对应该使用什么样的编程语言来实现内核这个话题，有一些更深入的讨论。这是这篇论文的出发点。\n为了能回答上面的问题，我们使用了一个带自动内存管理的编程语言写了一个新的内核，所以现在内核带了Garbage Collector，你不用调用free来释放内存，这样就可以避免一类Bug。\nGC是高级编程语言通常都带有的属性，我们选择了一种带GC的编程语言，并且实现过程中我们遵循了传统的monolithic UNIX风格，这样我们才能做出公平的对比。\n实际上，从某些角度你可以认为我们创建的是类似于XV6的内核，但是拥有了更多的特性和更高的性能。你知道的，XV6中里存在各种O(n^2)算法和O(n)的查找，如果你想要写一个高性能的操作系统，你不能有这样的实现。\n以上就是论文的起因，以及我们创建Biscuit（也就是上面提到的操作系统）的原因，我们想要回答上面的问题，或者至少给出一些启发。\n今天这节课首先我要讨论一些通用的背景，之后我们会深入到Biscuit的细节中。\n现在很多操作系统都是用C实现的，你知道的，XV6是用C写的，一些更流行的运行在你的电脑、手机的操作系统，例如Windows，Linux，以及各种形式的BSD都是用C写的。\n为什么它们都是用C实现的呢？\n首先C提供了大量的控制能力，从我们的实验中你可以看到，C可以完全控制内存分配和释放 C语言几乎没有隐藏的代码，你几乎可以在阅读C代码的时候想象到对应的RISC-V机器指令是什么 通过C可以有直接内存访问能力，你可以读写PTE的bit位或者是设备的寄存器 使用C会有极少的依赖，因为你不需要一个大的程序运行时。你几乎可以直接在硬件上运行C程序。你们可以在XV6启动过程中看到这一点， 只通过几行汇编代码，你就可以运行C代码 以上就是C代码的优点，也是我们喜欢C语言的原因。但是C语言也有一些缺点。\n在过去几十年已经证明了，很难写出安全的C代码。\n这里存在各种各样的Bug，首先是最著名的buffer overrun，比如说数组越界，撑爆了Stack等等。其次是use-after-free bugs，你可能会释放一些仍然在使用的内存，之后其他人又修改了这部分内存。第三，当线程共享内存时，很难决定内存是否可以被释放。\n其中一些Bug在XV6中已经出现，其他的一些不太常见。因为XV6很少有动态内存分配，几乎所有的东西都是预分配的，所以共享内存Bug很少出现，但是buffer overrun和use-after-free bugs的确出现过。\nCVEs一个跟踪所有的安全漏洞的组织，如果你查看他们的网站，你可以发现，在2017年有40个Linux Bugs可以让攻击者完全接管机器。\n很明显，这些都是非常严重的Bugs，这些Bug是由buffer overrun和一些其他memory-safety bug引起。这就太糟糕了，如果你用C写代码，就很难能够完全正确运行。\n当然，我可以肯定你们在之前的实验中都见过了这些Bug，之前在课程论坛上的一些问题涉及了use-after-free Bug。特别是在copy-on-write lab中，这些问题出现了好几次。\n高级编程语言实现操作系统的优劣势 高级编程语言吸引人的一个原因是它提供了memory-safety，所以上一节中CVEs提到的所有Bugs，都将不再存在。\n要么当它们发生时程序运行时会检查数组是否越界，如果越界了就panic；要么高级编程语言不允许你写出引起Bug的代码，所以这些问题完全不可能出现。\n当然，高级编程语言还有一些其他的优点：\n首先是Type safety，类型安全 通过GC实现了自动的内存管理，所以free更容易了，你都不用去考虑它，GC会为你完成所有的内存释放工作 对并发更友好 有更好的抽象，接口和类等面向对象的语法使得你可以写出更加模块化的代码 高级编程语言有这么多优势，你不禁会想它有哪些缺点呢？为什么XV6或者Linux没有用Java，Golang，Python来写？\n这里的原因是高级编程语言通常有更差的性能。高级编程语言通常都有一些额外的代价，这被称为High Level Language Tax。\n比如说在索引一个数组元素时检查数据边界，比如说检查空指针，比如说类型转换。 除此之外，GC也不是没有代价的，需要花费一些时间来跟踪哪些对象可以被释放。 除了性能之外，高级编程语言与内核编程本身不兼容。\n比如说高级编程语言没有直接访问内存的能力，因为这从原则上违反了Type safety。 高级编程语言不能集成汇编语言，而在内核中的一些场景你总是需要一些汇编程序，比如说两个线程的context switching，或者系统启动 编程语言本身支持的并发与内核需要的并发并不一致，比如我们在调度线程的时候，一个线程会将锁传递给另一个线程。一些并发管理模式在用户程序中不太常见，但是在内核中会出现。 今天论文的目标是能够测量出高级编程语言的优劣势，并从safety，programmability和性能损失角度，探索使用高级编程语言而不是C语言实现内核的效果。\n当然，为了做到这一点，你需要在一个产品级的内核上做实验，而不是在XV6上。\nXV6现在是由C语言写的很慢的内核，如果你用Golang也写了个很慢的内核，这不能说明C还是Golang更快，这只能说明XV6很慢。所以，你会想要在一个为高性能而设计的内核中完成这里的测量。\n很奇怪之前并没有一个论文完成了这里的测量。有很多论文研究了在用户程序中高级编程语言的优劣势，但是你知道的，内核与用户程序还是很不一样的，比如内核中需要有更小心的内存管理，内核中的并发或许会略有不同。\n所以，现在我们想要在内核中而不是用户程序中完成分析，而我们并没有找到之前的任何论文真正做了这个工作。\n之前的确有很多内核是用高级编程语言写的，这里有很长的历史，甚至可以回溯到最早的计算机中。\n但是最近的一些基于高级编程语言的内核并不是为了评估High Level Language Tax，而是为了探索新的内核设计和新的内核架构，所以这些内核并没有在保持结构相同的同时，直接对比C语言内核。只有保持系统结构相同，你才可以真正的关注语言本身，而不是一些其他的问题。\n为什么没有很多论文做这样的对比测试？一个原因可能是这里的工作有点棘手。\n如果你想得到正确的结果，你需要与产品级别的C内核进行对比，例如Linux，Windows等等。同时，你也需要构建一个产品级别的内核。\n很明显，这对于一个小的团队来说很难，因为有许多许多的Linux开发人员日复一日做了许多许多的更新才创造了Linux，所以很难用高级编程语言实现同样的功能并构建同样的内核，所以我们这里会构建一个功能稍微少的系统内核。\n所以我们能做到的最好情况是：\n用高级编程语言构建内核 保留与Linux中最重要的部分对等的功能 优化性能使得其与Linux基本接近，即使这里的功能与Linux并不完全一致，但是我们至少可以将它们拉到一个范围内 最后我们就可以测量高级编程语言的优劣 当然，这种方法的风险在于我们构建的内核与Linux还是略有不同，它不会与Linux完全一样，所以在得出结论时需要非常小心。\n这就是为什么不能对论文提出的问题（注，也就是应该使用什么样的编程语言实现操作系统）给出一个十分清晰的答案的原因。尽管如此，我们还是可以期望更深入的了解这个问题，而不是完全不知道它的内容。\n以上就是论文的背景，以及为什么很少有人会做同样的工作的原因。\n高级编程语言选择 \u0026mdash; Golang 接下来我们看一下对比方法，图中左边是Biscuit，这是一个我们为了论文 专门用Golang写的内核，它以大概类似的方式提供了Linux中系统调用的子集。Biscuit和Linux的系统调用有相同的参数和相同的调用方式。\n并且我们在内核之上运行的是相同的应用程序，这里的应用程序是NGINX，这是一个web server，这里我们将相同的应用程序分别运行在Biscuit和Linux之上，应用程序会执行相同的系统调用，并传入完全相同的参数，Biscuit和Linux都会完成涉及系统调用的相同操作。\n之后，我们就可以研究高级编程语言内核和Linux之间的区别，并讨论优劣势是什么。以上就是对比方法的核心。\n因为Linux和Biscuit并不完全一样，它们会有一些差异，所以我们花费了大量的时间来使得这里的对比尽可能得公平。\n有很多同学可能会问，这里会使用什么样的高级编程语言呢？基于以下原因，我们选用了Golang。\n这是一个静态编译的编程语言，和Python不同，这里没有解释器。我们喜欢静态编译的语言的原因是编译语言性能通常更好，实际上Go编译器就非常好，所以基本上来说这是一种高性能编程语言。 另外，Golang被设计成适合系统编程，而内核就是一种系统编程所以Golang也符合这里的场景。例如： Golang非常容易调用汇编代码，或者其他的外部代码 Golang能很好地支持并发 Golang非常的灵活 另一个原因是Golang带有Garbage Collector。使用高级编程语言的一个优点就是你不需要管理内存，而GC是内存管理的核心。 在我们开始写论文的时候，Rust并不十分流行，并且也不是十分成熟和稳定。\n但是如果你现在再做相同的事情，你或许会想要用Rust来实现。因为Rust也是为系统编程而设计，它有一个小的运行时，它能生成好的代码。不过Rust相比Golang还有一个缺点，Rust认为高性能程序不能有GC，所以Rust不带GC。\n实际上Rust的类型系统以一种非常聪明且有趣的方式实现，所以GC对于Rust并不是必须的。\n这里涉及到一个有趣的问题：通过高级编程语言实现内核时，GC的代价到底有多少？而Rust通过不使用GC而跳过了这个问题。\n学生提问：如果我们这里使用Rust而不是Golang来实现高级编程语言内核，通过一定的优化有没有可能达到比C内核更高的性能？\nFrans教授：因为我们没有做过这样的实验，所以我就猜一下。我觉得不会有比C内核更高的性能，但是基本在同一个范围内。因为C是如此的底层，你可以假设你在Rust做的工作，都可以在C中完成。\n这里有一个问题，并且在这节课最后我们会再次回顾这个问题。\n我们想要使用高级编程语言内核的部分原因是为了避免一类特定的Bug，那么你可以问自己的一个问题的是，你们在实验中遇到的Bug，是否可以通过使用高级编程语言来避免？\n我肯定你可以回想起一些Bug，它们耗费了你很多的时间，很多精力，现在你可以问自己，如果实验中的XV6是使用某种高级编程语言实现的，你的生活会不会更轻松一些？你是否能有更多时间做一些其他的事情。\n让我们记住这个问题，并在这节课结束的时候再看这个问题。\nBiscuit 接下来我将对Biscuit稍作介绍，包括了Biscuit是如何工作的，以及在实现中遇到的问题。其中有些问题是预期内的，有些问题不在预期之内。\n就像Linux和XV6一样，Biscuit是经典的monolithic kernel。\n所以它也有用户空间和内核空间，用户空间程序可能是你的编译器gcc，或者论文中主要用到的webserver。\n这里用户空间程序主要用C实现，尽管原则上它可以是任何编程语言实现的，但是因为这里只是性能测试，我们这里统一选用的是C版本的应用程序。\n大部分用户程序都是多线程的，所以不像在XV6中每个用户程序只有一个线程，在Biscuit中支持用户空间的多线程。\n基本上，对于每个用户空间线程，都有一个对应的位于内核的内核线程，这些内核线程是用Golang实现的，在Golang里面被称为goroutine。你可以认为goroutine就是普通的线程，就像XV6内核里的线程一样。\n区别在于，XV6中线程是由内核实现的，而这里的goroutine是由Go runtime提供。\n所以Go runtime调度了goroutine，Go runtime支持sleep/wakeup/conditional variable和同步机制以及许多其他特性，所以这些特性可以直接使用而不需要Biscuit再实现一遍。\nBiscuit中的Go runtime直接运行在硬件上，稍后我将介绍更多这部分内容，但是你现在可以认为当机器启动之后，就会启动Go runtime。这里会稍微复杂，因为Go runtime通常是作为用户空间程序运行在用户空间，并且依赖内核提供服务，比如说为自己的heap向内核申请内存。\n所以Biscuit提供了一个中间层，使得即使Go runtime运行在裸机之上，它也认为自己运行在操作系统之上，这样才能让Go runtime启动起来。\nBiscuit内核本身与XV6非常相似，除了它更加的复杂，性能更高。它有虚拟内存系统可以实现mmap，有更高性能的文件系统，有一些设备驱动，比如磁盘驱动，以及网络协议栈。\n所以Biscuit比XV6更加完整，它有58个系统调用，而XV6只有大概18-19个系统调用；它有28000行代码，而XV6我认为只有少于10000行代码。所以Biscuit有更多的功能。\n学生提问：这里的接口与XV6类似对吧，所以进程需要存数据在寄存器中，进程也会调用ECALL。\nFrans教授：我稍后会再做介绍，但是这里完全相同。\n以上是Biscuit的特性，有些我已经提到过了。\n首先它支持多核CPU。Golang对于并发有很好的支持，所以Biscuit也支持多核CPU。类似的，XV6却只对多核CPU有有限的支持。所以在这里，我们相比XV6有更好的同步协调机制。 它支持用户空间多线程，而XV6并没有。 它有一个相比XV6更高性能的Journaled File System（注，Journaled就是指log，可以实现Crash Recovery）。如果你还记得EXT3论文，它与EXT3的Journaled File System有点类似。 它有在合理范围内较为复杂的虚拟内存系统，使用了VMAs并且可以支持mmap和各种功能。 它有一个完整的TCP/IP栈，可以与其他的服务器通过互联网连接在一起。 它还有两个高性能的驱动，一个是Intel的10Gb网卡，以及一个非常复杂的磁盘驱动AHCI，这比virtIO磁盘驱动要复杂的多。 Biscuit支持的用户程序中：\n每个用户程序都有属于自己的Page Table。 用户空间和内核空间的内存是由硬件隔离的，也就是通过PTE的User/Kernel bit来区分。 每个用户线程都有一个对应的内核线程，这样当用户线程执行系统调用时，程序会在对应的内核线程上运行。如果系统调用阻塞了，那么同一个用户地址空间的另一个线程会被内核调度起来。 如之前提到的，内核线程是由Go runtime提供的goroutine实现的。如果你曾经用Golang写过用户空间程序，其中你使用go关键字创建了一个goroutine，这个goroutine就是Biscuit内核用来实现内核线程的goroutine。 来看一下系统调用。就像刚刚的问题一样，这里的系统调用工作方式与XV6基本一致：\n用户线程将参数保存在寄存器中，通过一些小的库函数来使用系统调用接口。 之后用户线程执行SYSENTER。现在Biscuit运行在x86而不是RISC处理器上，所以进入到系统内核的指令与RISC-V上略有不同。 但是基本与RISC-V类似，控制权现在传给了内核线程。 最后内核线程执行系统调用，并通过SYSEXIT返回到用户空间。 所以这里基本与XV6一致，这里也会构建trapframe和其他所有的内容。\n学生提问：我认为Golang更希望你使用channel而不是锁，所以这里在实现的时候会通过channel取代之前需要锁的场景吗？\nFrans教授：这是个好问题，我会稍后看这个问题，接下来我们有几页PPT会介绍我们在Biscuit中使用了Golang的什么特性，但是我们并没有使用太多的channel，大部分时候我们用的就是锁和conditional variable。\n所以某种程度上来说Biscuit与XV6的代码很像，而并没有使用channel。我们在文件系统中尝试过使用channel，但是结果并不好，相应的性能很差，所以我们切换回与XV6或者Linux类似的同步机制。\n在实现Biscuit的时候有一些挑战：\n首先，我们需要让Go runtime运行在裸机之上。\n我们希望对于runtime不做任何修改或者尽可能少的修改，这样当Go发布了新的runtime，我们就可以直接使用。在我们开发Biscuit这几年，我们升级了Go runtime好几次，所以Go runtime直接运行在裸机之上是件好事。\n并且实际上也没有非常困难。Golang的设计都非常小心的不去依赖操作系统，因为Golang想要运行在多个操作系统之上，所以它并没有依赖太多的操作系统特性，我们只需要仿真所需要的特性。\n大部分这里的特性是为了让Go runtime能够运行起来，一旦启动之后，就不太需要这些特性了。\n我们需要安排goroutine去运行不同的应用程序。\n通常在Go程序中，只有一个应用程序，而这里我们要用goroutine去运行不同的用户应用程序，这些不同的用户应用程序需要使用不同的Page Table。\n这里困难的点在于，Biscuit并不控制调度器，因为我们使用的是未经修改过的Go runtime，我们使用的是Go runtime调度器，所以在调度器中我们没法切换Page Table。\nBiscuit采用与XV6类似的方式，它会在内核空间和用户空间之间切换时更新Page Table。所以当进入和退出内核时，我们会切换Page Table。\n这意味着像XV6一样，当你需要在用户空间和内核空间之间拷贝数据时，你需要使用copy-in和copy-out函数，这个函数在XV6中也有，它们基本上就是通过软件完成Page Table的翻译工作。\n另一个挑战就是设备驱动，Golang通常运行在用户空间，所以它并不能从硬件收到中断。\n但是现在我们在裸机上使用它，所以它现在会收到中断，比如说定时器中断，网卡中断，磁盘驱动中断等等，我们需要处理这些中断。然而在Golang里面并没有一个概念说是在持有锁的时候关闭中断，因为中断并不会出现在应用程序中，所以我们在实现设备驱动的时候要稍微小心。\n我们采取的措施是在设备驱动中不做任何事情，我们不会考虑锁，我们不会分配任何内存，我们唯一做的事情是向一个非中断程序发送一个标志，之后唤醒一个goroutine来处理中断。\n在那个goroutine中，你可以使用各种各样想要的Golang特性，因为它并没有运行在中断的context中，它只是运行在一个普通goroutine的context中。\n前三个挑战我们完全预料到了，我们知道在创造Biscuit的时候需要处理它们，而最难的一个挑战却不在我们的预料之中。这就是heap耗尽的问题。所以接下来我将讨论一下heap耗尽问题，它是什么，它怎么发生的，以及我们怎么解决的？\nHeap exhaustion 让我们讨论一下Heap耗尽问题，我不会像论文 一样深入讨论，但是至少会演示问题是什么。\n假设蓝色的矩形是内核，内核会有一个heap，它会从其中申请动态内存。\n在XV6中，我们并没有这样一个heap，我们在内核中没有内存分配器，所有内存都是静态分配的。但是任何其他的内核中，都会有heap，所以你在内核中可以调用malloc和free。可能通过heap分配的对象有socket对象，文件描述符对象和进程对象。\n所以，我们在XV6中静态分配的所有结构体，例如struct proc，struct fd，在正常的内核中都是动态分配的。所以当你打开一个新的文件描述符时，内核会通过heap分配一个文件描述符对象。\n这里的问题是，你可以运行很多个应用程序，它们会打开很多个文件描述符，拥有很多个socket，它们会逐渐填满heap。\n在某个时间点，heap会被填满，这时没有额外的空间可以用来分配一个新的对象。\n如果这时应用程序需要打开一个新的文件描述符，或者调用了fork使得内核想要在heap中分配一个新的proc结构体，heap中没有了空间。这时你该怎么办呢？\n这是一个不太常见的常见问题，但是如果你使劲用你的电脑，你或许会遇到所有内存都被使用了的情况，你的heap满了，并且没有进程调用free，因为它们都还在运行且想分配到更多的内存。\n所有的内核都会遇到这个问题，不管是C内核也好，Biscuit也好，任何内核都需要解决这个问题。\n之所以这个问题对于我们来说是个严重的问题，是因为在很多内核中，你可以对malloc返回错误，实际上，XV6就是这么做的。\n但是在Go runtime中，当你调用new来分配一个Go对象，并没有error condition，new总是可以成功。让我们来讨论一些可以解决这里问题的方法。\n第一种方法我们在XV6中见过。如果XV6不能找到一个空闲的block cache来保存disk block，它会直接panic。这明显不是一个理想的解决方案。这并不是一个实际的解决方案，所以我们称之为strawman。\n另一个strawman方法是，当你在申请一块新的内存时，你会调用alloc或者new来分配内存，你实际上可以在内存分配器中进行等待。这实际上也不是一个好的方案，原因是你可能会有死锁。\n假设内核有把大锁，当你调用malloc，因为没有空闲内存你会在内存分配器中等待，那么这时其他进程都不能运行了。因为当下一个进程想要释放一些内存时，但是因为死锁也不能释放。\n对于内核中有大锁的情况，这里明显有问题，但是即使你的锁很小，也很容易陷入到这种情况：在内存分配器中等待的进程持有了其他进程需要释放内存的锁，这就会导致死锁的问题。\n下一个strawman方法是，如果没有内存了就返回空指针，你检查如果是空指针就直接失败，这被称为bail out。\n但是bail out并不是那么直观，进程或许已经申请了一些内存，那么你需要删除它们，你或许做了一部分磁盘操作，比如说你在一个多步的文件系统操作中间，你只做了其中的一部分，你需要回退。所以实际中非常难做对。\n当研究这部分，并尝试解决这个问题，Linux使用了前面两种方法，但是两种方法都有问题。实际中，内核开发人员很难将这里弄清楚。\n如果你对这个问题和相关的讨论感兴趣，可以Google搜索“too small to fail ”，会有一篇小的文章讨论释放内存，在内存分配器中等待的复杂性。\n对于Biscuit来说，strawman 2解决方案不可能实施，因为new不会fail，它总是能成功。除此之外，这里的方案都不理想，所以我们需要有一种更好的方法。\nHeap exhaustion solution Biscuit的解决方案非常直观，当应用程序执行系统调用，例如read，fork时，在系统调用的最开始，跳转到内核之前，它会先调用reserve函数，reserve函数会保留足够的内存以运行系统调用。\n所以reserve会保留足够这个系统调用使用的空闲内存，以使得系统调用总是能成功。所以一旦系统调用被执行，且保留了足够的内存，那么它就可以一直运行而不会有内存耗尽和heap exhaustion的问题。\n如果reserve函数执行时没有足够的内存，那么程序会在这里等待。\n因为现在在系统调用的最开始，系统调用现在还没有持有任何的锁，也没有持有任何的资源，所以在这里等待完全没有问题，也不会有死锁的风险。当程序在等待的时候，内核可以撤回cache并尝试在heap增加空闲空间，比如说kill一个进程来迫使释放一些内存。\n一旦内存够用了，并且内核决定说是可以满足需要保留的内存，之后内核会让系统调用继续运行，然后执行系统调用需要的操作。在最后，当系统调用完成的时候，所有之前保留的内存都返回到池子中，这样后续的系统调用可以继续使用。\n这个方案中有一些很好的特性：\n在内核中没有检查。你不需要检查内存分配是否会失败，在我们的例子中这尤其得好，因为在Golang中内存分配不可能会失败。 这里没有error handling代码。 这里没有死锁的可能，因为你在最开始还没有持有锁的时候，就避免了程序继续执行。 当然，现在的问题是如何实现reserve函数，你如何计算运行一个系统调用会需要多少内存？\n你保留的内存数量是重要的，你可以为每个系统调用保留一半的内存或者一些其他夸张的内存数量。但是这意味着你限制了可以并发执行的系统调用的个数，所以你这里尽量精确地计算一个系统调用的内存边界。\n这里的解决方法是使用了高级编程语言的特性。Golang实际上非常容易做静态分析，Go runtime和Go生态里面有很多包可以用来分析代码，我们使用这些包来计算系统调用所需要的内存。\n所以你可以想象，如果你有一个read系统调用，我们可以通过系统调用的函数调用图查看比如函数f调用函数g调用函数h等等等等。我们可以做的是弄清楚这里调用的最大深度，对于最大的深度，计算这里每个函数需要的内存是多少。\n比如说函数f调用了new，因为这是一个高级编程语言，我们知道new的对象类型，所以我们可以计算对象的大小。我们将所有的new所需要的内存加起来，得到了一个总和S，这就是这个调用图（或者说系统调用）任何时间可能需要的最大内存。\n实际中并没有这么简单，会有点棘手。因为函数h可能会申请了一些内存，然后再回传给函数g。所以当h返回时，g会得到h申请的一些内存。这被称为escaping，内存从h函数escape到了函数g。\n存在一些标准算法来完成这里的escape分析，以决定哪些变量escape到了函数调用者。当发生escape时，任何由函数h申请的内存并且还在函数g中存活，我们需要将它加到函数g的内存计数中，最后加到S中。\n学生提问：某些函数会根据不同的工作负载申请不同量的内存，那么在计算函数消耗的内存时，会计算最差的情况吗？\nFrans教授：是的。这里的工具会计算最深函数调用时最大可能使用的内存量。所以它会计算出每个系统调用可能使用的最多内存，虽然实际中系统调用可能只会使用少的多的内存。\n但是保险起见，我们会为最坏情况做准备。一些系统调用内的for循环依赖于传给系统调用的参数，所以你不能静态地分析出内存边界是什么。所以在一些场景下，我们会标注代码并规定好这是这个循环最大循环次数，并根据这个数字计算内存总量S。\n类似的，如果有你有递归调用的函数，谁知道会递归多少次呢？或许也取决于一个动态变量或者系统调用的参数。实际中，我们在Biscuit中做了特殊处理以避免递归函数调用。所以最后，我们才可能完成这里的内存分析。\n所以，这里的内存分析不是没有代价的，也不是完全自动的。\n这花费了Cody（论文一作）好几天检查代码，检查所有的循环并标注代码。还有一些其他的Golang特有的问题需要处理，例如，向Slice添加元素可能会使内存使用量扩大一倍，所以我们也给Slice标注了最大的容量。\n但是所有这些工作都是可完成的，在花费了几天时间之后，使用这里的内存分析工具，你可以得到对于系统调用使用的最大内存量的合理评估。以上基本就是Biscuit如何解决heap exhaustion问题。\n学生提问：这里的静态内存分析工具，如果不是用来构建内核，它们通常会用来干嘛？\nFrans教授：Go编译器内部使用它来完成各种各样的优化，并分析得出最优的编译方式。这里正好编译器使用了一个包，我们也可以使用同样的包。在后面你还可以看到，我们还将它用于一些其他特性，有这么一个包非常的方便。\nEvaluation: HLL benefits Biscuit的实现与其他内核，例如XV6，非常相似，除了Biscuit比XV6性能要高的多。Biscuit采用了很多Linux内核的优化和聪明的设计：\n我们对于内核文本采用了大页，以避免TLB的代价。 我们有针对每个CPU的网卡队列，这样可以避免CPU核之间同步。 我们有RCU实现了不需要读锁的Directory Cache。 \u0026hellip;\u0026hellip; 通常为了高性能而做的优化，编程语言并不会成为阻碍。Golang并没有成为阻碍这些优化实现的因素。这些优化之前是在C和Linux中实现，我们现在只是在Golang中又实现它们。在实现这些优化时有很多的工作，但是这些工作与编程语言本身无关。\n今天论文 的出发点就是了解用高级编程语言实现操作系统的收益和代价。所以我们将分两部分来评估，首先是收益，其次是代价。\n有关高级编程语言，我们要回答三个问题：\n首先，我们有没有作弊？或许我们避免使用了所有Golang提供的高级编程语言中代价较高的功能。 其次，高级编程语言是否有简化Biscuit代码？ 最后，高级编程语言是否能阻止前面提到的内核漏洞？ 首先，我们有没有使用高级编程语言的特性？我们会对比一下Biscuit与其他两个大的Golang项目在使用语言特性上是否类似，这样我们才可以说我们的内核以类似的方式利用了相同的语言特性。\n这里我们使用了相同的静态分析工具来分析两个大的Golang项目，它们都有超过100万行代码，其中一个项目是Go runtime以及包含的所有包，另一个是一个叫做Moby的系统。\n之后我们画出了一些高级语言特性在每1000行代码中的使用量。图中X轴是语言特性：\nallocation对应于new maps就是hashtable slice是动态数组 channel是同步的工具，如你所见我们用的很少，Go runtine和Moby也用的很少 很明显我们最喜欢的特性就是函数返回多个值 我们使用了Closure（闭包） 我们稍微使用了defer 我们使用了Interface 使用了Type assertion来以一种类型安全的方式将一个类型转换成另一个类型 同时我们也import了很多包，Biscuit内核是由很多个包构建出来的，而不是一个大的单一的程序 如你所见，有些特性Biscuit用的比Go runtime和moby更少，有些特性Biscuit用的更多，这里没有很明显的区别。所以从这张图中可以得出的主要结论是：Biscuit使用了Golang提供的高级编程语言特性，而不是为了得到好的性能而避开使用它们。\n学生提问：你这里是怎么统计的？是不是使用了静态分析工具？\nFrans教授：是的，这里使用的就是静态分析工具。通过写一个小程序利用静态分析工具来查看这些项目的每一行代码，并记录对应的特性是什么，这样就能统计这些特性的使用数量。\n第二个问题有点主观，高级编程语言有没有简化Biscuit代码？笼统的说我认为有的，我这里会讨论一两个例子。\n使用Garbage allocation是极好的，你可以回想XV6，当你调用exit时，有大量的结构化数据需要被释放回给内核，这样后面的进程才能使用。\n如果使用Garbage Collector这里的工作着实容易，Garbage Collector会完成这里的所有工作，你基本不用做任何事情。如果你从地址空间申请了一段内存，对应这段内存的VMA会自动被GC释放，所以这里可以简化代码。\n如之前所说的，函数返回多个值对于代码风格很好。闭包很好，map也很好。XV6中很多地方通过线性扫描查找数据，但是如果你有map和hashtable作为可以直接使用的对象，那么你就不用线性扫描了。\n你可以直接使用map，runtime会高效地为你实现相应的功能。所以直观上的感受是，你可以得到更简单的代码。\n但是前面只是定性的评估，下面会介绍一些更具体的例子。当有大量的并发线程，且线程有共享的数据时，GC如何起作用的。\n这里有个最简单的例子。假设你申请了一些动态的对象，比如说buffer，你fork一个线程来处理这个buffer，原线程也会处理同一个buffer。\n当两个线程都完成了工作，buffer需要被释放，这样内存才可以被后面的内核代码使用。这在C语言里面有点难协调，因为你需要有某种方式来决定buffer不再被使用。\n如果你使用GC，那么就没什么好决定的，因为当两个线程都处理完buffer之后，没有线程会指向那个buffer。\nGC会从线程栈开始追踪，并且在任何线程栈中都找不到buffer，因此GC会在稍后某个时间释放内存。所以在一个带GC的编程语言中，你完全不必考虑这个问题。\n在C中你可以这样解决这个问题，为对象增加引用计数，引用计数需要被锁或者一些原子性操作保护，当引用计数到达0时，你可以释放内存。\n实际中锁加上引用计数代价稍微有点高。如果你想要高性能，并且并发可以扩展到CPU核数，这可能会是个瓶颈，我们在后面介绍RCU的时候会看这部分。\n所以，如果你想要高性能，好的并发能力，人们倾向于不给读数据加锁。\n在实际中，我们会使得读数据至少是不需要锁的，这样你就不需要付出额外的代价。\n上面是我们在Golang中的实现，我们有个get函数，它会读取并返回链表的头结点。这里就没有使用锁，而是使用了atomic_load，它会读取头结点，但是又不需要锁。\n后面的pop函数使用了锁。这种风格在Linux内核中非常常见，写数据需要加锁，读数据不用加锁。这里pop函数会从链表中弹出头结点，这样你就可以重用头结点对应的内存。\n在C中实现这种风格会有点困难，因为有可能当你释放头结点内存时，其他并发的线程正好读取到了头结点的指针。这样当你做完atomic_store，你不能释放指针内容，因为有可能有另一个线程的指针指向了这部分内容。如果你在这里释放了指针内容，你有可能会有use-after-free Bug。\n我们在这门课程的最后一节课会看到，Linux内核对这个问题有一种非常聪明的解决办法，被称为Read-Copy-Update或者是RCU。它的工作就是推迟释放内存，直到确定指针不再被使用，并且它有一种非常聪明的方案来决定什么时候可以安全释放内存。\n但是这个方案有各种各样的限制，程序员需要在RCU关键区域内遵守各种规则。比如说你不能在RCU关键区域sleep，也不能切换线程。\n所以尽管实际中Linux内核非常成功的使用了RCU，但是RCU还是有点容易出错，并且需要小心编程来使得它能正确工作。在带有GC的编程语言，例如Golang，这就不是问题了，因为GC会决定某个对象不再被使用，只有这时才释放它。\n所以现在对于编程人员来说没有限制了，所有的限制都被GC考虑了。这是一种带有GC的编程语言的明显优势。\n接下来看看CVEs Bugs，这在前面提到过（注，20.1）。\n我们手动的检查了所有的CVEs Bug，并尝试确定Golang是否修复了问题。\n第一行代表我们不能弄清楚这些Bug的结果是什么，它会怎么展现，我们知道如何修复这些问题，但是我们不能确定Golang是否能避免这些问题。 有很多逻辑Bug，可以认为Golang会有与C相同的Bug，所以结果是相同的 接下来是40个memory-safety Bugs，包括了use-after-free，double-free，out-of-bound。其中8个直接消失了，因为GC考虑了内存释放，32个会产生panic，比如说数组越界。当然panic并不好，因为内核会崩溃，但是或许要比直接的安全漏洞更好。所以在这40个Bug中，高级编程语言有帮到我们。 以上就是使用高级编程语言实现内核的优势，接下来讨论一些代价，也就是High Level Language Tax。\nEvaluation: HLL performance cost(1) 以上是6个问题，我应该不会全部介绍，因为我想在课程的最后留些时间来回顾我们在本节课开始时提出的问题。\n以上就是测试环境，Biscuit运行在裸机之上，所以我们的测试是在物理服务器而不是QEMU之上。我们使用了三个应用程序来做性能测试，它们分别是，Webserver，K/V store，Mail server benchmark。\n这三个应用程序都会给内核一定的压力，它们会执行系统调用，内核会做大量的工作。你可以看到，大部分CPU时间都在内核中。\n首先的问题是，Biscuit是否是一个工业质量的内核？我们将前面的三个应用程序分别运行在Linux和Biscuit上，并进行对比。\n在Linux中，我们会关闭所有Biscuit不提供的功能，比如Page Table隔离，repoline等等很多功能，这样的对比才会尽可能的公平。有些特性会比较难关闭，但是我们会尽量将它们关闭。\n之后我们会测试吞吐量，如你所见Biscuit总是会比Linux更慢，mailbench可能差10%，nginx和redis差10%到15%。这里的数字并不是绝对的，因为两个系统并不完全一样。但是可以看出两个系统基本在同一个范围内，而不是差个2倍或者10倍。\n接下来我们会分析代码，并找到高级编程语言额外的CPU cycle消耗。我们会找到：\n哪些CPU cycle是GC使用的， 哪些是函数调用的Prologue使用的。Golang会为函数调用做一些额外的工作来确保Stack足够大，这样就不会遇到Out-of-Stack的问题 Write barrier是GC用来跟踪不同空间的指针的方法 Safety cycles是用在数组边界检查，空指针检查上的CPU cycles 通过测试上面的应用程序，可以得到测量结果。\n3%的执行时间用在了GC cycles中，这里我稍后会介绍为什么这很少。同时这也可以说明GC是在运行的，我们并不是用了一大块内存而没有使用GC 令人奇怪的是，Prologue占有的CPU时间最多，这基本上跟我们用来检查kernel Stack或者goroutine Stack是否需要增加的方案有关，这里或许更容易降低一些 Write barrier使用的时间很少 2%-3%的CPU时间用在了Safety cycles中 这些数据都很好，High Level Language Tax并不是那么的大。\n当然GC的占比可能会更高，因为它完全取决于heap大小和存活对象的数量，GC会跟踪所有的存活对象，并决定哪些对象已经不被使用。\n如果有大量的存活对象，GC也需要跟踪更多的对象。所以这里的CPU时间完全与存活对象的数量相关。\n所以我们做了一些其他的实验。我们创建了大量的存活对象，大概有200万个vnode，可以认为这是200万个inode。然后修改heap的headroom，也就是GC可以使用的空闲内存数量，最后再测量GC的代价。\n上图就是测量结果，存活对象占了640MB内存，我们在不同内存大小下运行测试。第一次测试时，有320MB空闲内存，是存活对象内存的一半，这时Golang有非常严重的overhead，大概是34%，GC因为没有足够的headroom需要运行很多额外的程序。\n如果空闲内存是存活对象的2倍，那么GC的overhead就没有那么疯狂，只有9%。所以，为了保持GC的overhead在10%以内，物理内存大小需要是heap大小的三倍。\n学生提问：什么是write barrier？是设置权限吗？\nFrans教授：你还记得Lec17的内容吗？当GC在运行的时候，需要检查指针是否在from空间，如果在from空间你需要拷贝它到to空间。write barrier是非常类似的功能，它的想法是一样的，你需要检查指针看它是否在你需要运行GC的区域内。\n学生提问：当存活对象的内存大于空闲内存的时候，GC该怎么工作呢？\nFrans教授：你买一些内存，vnode会使用一些内存，然后还剩下320MB空闲内存。当应用程序申请更多内存时，首先会从空闲内存中申请，直到空闲内存也用光了。\n与此同时，GC也在运行。所以我们刚刚的测试中是在3个不同配置下运行，在最后一个配置中，空闲内存是存活对象占用内存的两倍。这意味着GC有大量的headroom来与应用程序并行的运行，如果有大量的headroom，GC的overhead就没那么高了，只有10%左右，而不是34%。在第一个配置中，总共是640+320MB内存，而不是只有320MB内存。\nEvaluation: HLL performance cost(2) 接下来稍微介绍GC pause。\nGo的GC是一个带有短暂pause的并发GC，它在一段非常短的时间内停止程序运行来执行write barrier，之后再恢复应用程序的运行，同时GC也会完成自己的工作。\nGo的GC也是递增的，就像我们在Lec17中介绍过的一样，每次调用new都会做一些GC的工作。所以每次GC做一些工作的时候，应用程序都会有一些延时，这就是代价。\n所以我们做了一些测试，我们找了个应用程序并测试了最大的pause时间。也就是由于GC可能导致应用程序最大的停止时间。\n最大的单个pause时间是115微秒，也就是在web server中，因为使用了TCP stack，TCP Connection table中很大一部分需要被标记（注，GC的一部分工作是标记对象），这花费了115微秒。\n一个HTTP请求最大的pause时间是582微秒，所以当一个请求走到一个机器，最多会有总共582微秒延时来执行这个请求。而超过100微秒的pause发生的非常非常少，只有少于0.3%。\n如果你尝试达成某种SLA，其中要求的最长请求处理时间很短，那么582微秒就很严重。\n但是如果你查看Google论文，The Tail at Scale ，其中介绍有关一个请求最长可以有多长处理时间，他们讨论的都是几毫秒或者几十毫秒这个量级。所以Biscuit拥有最大pause时间是582微秒还在预算之内，虽然不理想，但是也不会很夸张。\n这表明了，Golang的设计人员把GC实现的太好了。并且我们在做Biscuit项目的时候发现，每次我们升级Go runtime，新的runtime都会带一个更好的GC，相应的GC pause时间也会变得更小。\n之前在Linux和Biscuit之间的对比并不真正的公平，因为Biscuit和Linux实现的是不同的功能。\n所以我们做了一个额外的测试，我们写了两个完全相同的内核，一个用C实现，另一个用Golang实现。这两个内核实现了完全相同的东西，并且我们会查看汇编代码以检查区别在哪。可能会有一些区别，因为Golang会做一些安全检查，但是对于基本功能来说，汇编代码是一样的。\n以上是有关测试的一部分，通过pipe来回传输一个字节。我们查看内核中有关将一个字节从pipe的一端传到另一端的代码。Go里面是1.2K行代码，C里面是1.8K行代码。\n这里没有内存分配和GC，所以这里只有语言上的差异。我们还查看了两种实现语言中花费最多时间的10个地方，这样我们才能确保两种语言实现的代码尽可能的接近。\n之后我们查看了每秒可以完成的操作数，如你可见Golang要慢15%。如果你查看Golang的Prologue和safety-check，这些指令是C代码所没有的，这些指令占了16%，这与更慢的处理速度匹配的上。\n所以这里的主要结论是Golang是更慢，但并不是非常夸张的慢，Golang还是非常有竞争力的。并且这与我们早些时候做的Biscuit和Linux对比结果一致。\n第6个问题跳过。\nShould one use HLL for a new kernel? 最后我想讨论我们在最开始问过的一个问题，你应该在一个新内核中使用高级编程语言吗?\n与其直接回答这个问题，我在这页有一些我们的结论和一些考虑。或许你们该回退一步，并问自己，你们更喜欢哪种方式？\n你们是喜欢像在实验中用C写XV6，还是喜欢使用类似Golang的高级编程语言。更具体的说，你们更想避免哪类Bug？或许在这节课的过程中想想你们遇到过什么Bug？我想听听你们的体验，你们是怎么想的？切换到高级编程语言会不会改变你们的体验？\n一些学生介绍自己的体验，有说C好的，有说C不好的\n当然，我们不会将XV6改成Golang或者任何高级编程语言。具体原因刚刚一些同学已经提到了，Golang还是隐藏了太多细节，这门课的意义在于理解系统调用接口到CPU之间的所有内容。\n举个例子，Golang隐藏了线程，我们并不想隐藏线程，我们想要向你解释线程是如何实现的。所以接下几年，这门课程还是会使用C语言。\n但是如果你要实现一个新的内核，并且目标不是教育你的学生有关内核的知识，目标是写一个安全的高性能内核。你可以从我们的研究中得出一些结论：\n如果性能真的至关重要，比如说你不能牺牲15%的性能，那么你应该使用C。 如果你想最小化内存使用，你也应该使用C。 如果安全更加重要，那么应该选择高级编程语言。 或许在很多场景下，性能不是那么重要，那么使用高级编程语言实现内核是非常合理的选择。 Cody、Robert和我在实现这个项目的过程中学到的一件事情是，任何一种编程语言就是编程语言，你可以用它来实现内核，实现应用程序，它并不会阻止你做什么事情。\n学生提问：我很好奇你们是怎么实现的Biscuit，你们直接在硬件上运行的Go runtime，具体是怎么启动的？\nFrans教授：这里有一层中间层设置好了足够的硬件资源，这样当Go runtime为heap请求内存时，我们就可以响应。这是Go runtime依赖的一个主要内容。\n学生提问：我知道你们实现了一些Go runtime会调用的接口，因为你们现在自己在实现内核，所以没有现成的接口可以使用。你们是全用汇编实现的这些接口吗？还是说有些还是用Golang实现，然后只在必要的时候用汇编？\nFrans教授：这就是Biscuit中1500行汇编代码的原因，它会准备好一切并运行Go runtime。有一些我们可以用C来实现，但是我们不想这么做，我们不想使用任何C代码，所以我们用汇编来实现。并且很多场景也要求用汇编，因为这些场景位于启动程序。\n我们的确写了一些Go代码运行在程序启动的最开始，这些Go代码要非常小心，并且不做内存分配。我们尽可能的用Golang实现了，我需要查看代码才能具体回答你的问题，你也可以查看git repo。\n学生提问：我有个不相关的问题，Golang是怎么实现的goroutine，使得它可以运行成百上千个goroutine，因为你不可能运行成百上千个线程，对吧？\nFrans教授：运行线程的主要问题是需要分配Stack，而Go runtime会递增的申请Stack，并在goroutine运行时动态的增加Stack。\n这就是Prologue代码的作用。当你执行函数调用时，如果没有足够的Stack空间，Go runtime会动态的增加Stack。而在线程实现中，申请线程空间会是一种更重的方法，举个例子在Linux中，对应的内核线程也会被创建。\n学生提问：goroutine的调度是完全在用户空间完成的吗？\nFrans教授：大部分都在用户空间完成。Go runtime会申请m个内核线程，在这之上才实现的的Go routine。所有的Go routine会共享这些内核线程。人们也通过C/C++实现了类似的东西。\n学生提问：C是一个编译型语言，所以它可以直接变成汇编或者机器语言，它可以直接运行在CPU上，所以对于XV6来说就不用加中间层代码。但是我理解Golang也是一种编译型语言，所以它也会变成汇编语言，那么为什么还要中间层（位于机器和Go runtime之间）？XV6有这样的中间层吗？为什么有一些事情不能直接编译后运行在CPU上？\nFrans教授：好问题。Go runtime提供了各种你在XV6中运行C时所没有的功能。\nGo runtime提供了线程，提供了调度器，提供了hashtable，提供了GC。举个例子，为了支持GC，需要一个heap来申请内存，通常是向底层的操作系统来申请内存作为heap。这里说的中间层Go runtime需要用来完成工作的相应功能（比如说响应内存申请）。\n学生提问：我们不能直接将runtime编译到机器代码吗？\nFrans教授：Runtime会被编译到机器码，但是当你运行Go代码时，有一部分程序是要提前运行的，这部分程序需要在那。\n即使C也有一个小的runtime，比如printf就是C runtime的中间层的一部分，或者字符串处理也是C runtime的一部分，它们也会被编译。C runtime有一些函数，但是这个runtime是如此之小，不像Go runtime需要支持许多Go程序所依赖的功能。\n学生提问：看起来这里的中间层像是一个mini的系统层，它执行了一些底层的系统功能。\nFrans教授：是的，或许一种理解中间层的方法是，XV6也有一个非常非常小的中间层。当它启动的时候，它做的第一件事情是分配一些Stack这样你才能调用C的main函数。\n你可以认为这一小段代码是针对XV6的中间层。一旦你执行了这些指令，你就在C代码中了，然后一切都能愉快的运行。Go runtime的中间层稍微要大一些，因为有一些功能需要被设置好，之后Go runtime才能愉快的运行。\n","date":"2024-10-05T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08120kernels-and-hll/bg_hu11905089816377866855.jpg","permalink":"https://echudet.github.io/p/mit6s08120kernels-and-hll/","title":"MIT6S081：20Kernels and HLL"},{"content":"Lecture 19 Virtual Machines 课前预习 论文速览 这篇论文《Dune: Safe User-level Access to Privileged CPU Features》通过以下几个部分展开。\n在摘要 (Abstract)部分，提到Dune系统为应用程序提供安全、直接的硬件特权访问（如页表、特权模式等），并展示了其在64位x86 Linux系统上的实现。文中介绍了Dune在沙盒、权限分离以及垃圾回收器中的应用，展示了其性能优势。\n在引言 (Introduction)部分，介绍了许多应用程序需要访问“内核级”硬件功能的原因，但这些功能通常限制在内核空间。传统的解决方法（如修改内核或使用虚拟机）存在种种弊端，而Dune通过虚拟化硬件为用户程序提供了安全访问硬件功能的新方式。\n在虚拟化和硬件支持 (Virtualization and Hardware)部分，详细描述了Intel的VT-x扩展如何支持虚拟化，包括VMX root和VMX non-root模式。Dune利用这些硬件特性为用户程序提供对特权硬件（如异常处理、虚拟内存、特权模式等）的访问。\n在Dune的内核支持 (Kernel Support for Dune)部分，讨论了Dune核心模块的架构、威胁模型及其与传统虚拟机管理程序（VMM）的区别。Dune通过一个轻量级内核模块来管理虚拟化硬件，并使用EPT（扩展页表）来确保内存隔离，同时提供对内存管理、硬件访问及系统调用的支持。\n在用户模式环境 (User-mode Environment)部分，介绍了Dune提供的用户模式库libDune，帮助用户程序管理特权硬件功能，包括页表管理、异常处理等。讨论了如何从普通进程过渡到Dune模式，及其对现有Linux程序的影响。\n在应用 (Applications)部分，提到Dune的三个应用场景：沙盒、权限分离、垃圾回收器。在每个场景下，Dune简化了实现并提供了显著的性能提升。例如，在沙盒应用中，Dune能够有效隔离不可信代码，同时保证较低的性能开销。\n在 评估 (Evaluation)部分，通过对系统调用、页错误处理、页表访问等操作的基准测试，量化了Dune的性能开销。并对其在不同应用场景中的加速效果进行了分析，显示了Dune在内存密集型应用中的优势。\n在对硬件的反思 (Reflections on Hardware)部分，讨论了在开发Dune过程中遇到的硬件限制，提出了一些硬件改进建议，尤其是在EPT和VMX过渡开销方面。这部分对硬件开发者更有意义。\n在 相关工作 (Related Work)部分，介绍了Dune与其它操作系统设计（如Exokernel、SPIN、Wedge等）的关系，并探讨了Dune相较于其他虚拟化和安全系统的独特优势。\n在结论 (Conclusion)部分，总结了Dune为用户程序提供访问特权硬件的新方法，提升了性能并简化了实现。作者还提出Dune的潜在扩展方向，如在不同硬件架构上的应用。\n阅读重点：\n摘要部分可以快速了解论文的整体内容。 引言和虚拟化和硬件支持部分能够帮助理解Dune的核心设计动机和硬件背景，适合第一次接触Dune的读者。 对于实际应用和性能评估，应用和评估部分是阅读的重点，尤其是对系统性能和应用场景感兴趣的读者。 虚拟化硬件 虚拟化硬件是一种通过硬件支持实现系统虚拟化的技术，它允许多个操作系统或用户进程共享同一台物理机器的硬件资源，同时确保每个操作系统或进程的独立性和安全性。这种技术通常通过 CPU 和内存的扩展来支持虚拟化操作，从而使虚拟机（Virtual Machine, VM）可以像独立物理机器一样运行。英特尔的 VT-x 和 AMD 的 SVM（Secure Virtual Machine）是两种主流的虚拟化硬件技术。\n虚拟化的早期实现主要通过软件来模拟硬件（如仿真 CPU 指令和内存访问），这种方式的性能开销较大。现代 CPU 引入了硬件虚拟化支持，例如 Intel 的 VT-x 和 AMD 的 SVM，使虚拟化操作更加高效。虚拟化硬件使得大部分虚拟机的操作能够直接在硬件上执行，从而减少了虚拟机管理程序（VMM）或操作系统内核的干预，这提高了虚拟化的性能并降低了开销。\n虚拟化硬件（如 Intel VT-x）通过将 CPU 划分为两种模式：\nVMX Root 模式：通常用于运行虚拟机管理程序（VMM），类似于操作系统内核的特权模式。 VMX Non-root 模式：限制了 CPU 的操作权限，主要用于运行虚拟机中的客体操作系统（Guest OS）。 硬件通过 VM entry 和 VM exit 来管理不同模式的切换。例如，当 VMM 发出 VMLAUNCH 指令时，CPU 进入 VMX Non-root 模式，运行虚拟机。当虚拟机需要 VMM 处理一些操作（如系统调用或页表管理），硬件会触发 VM exit，将控制权返回给 VMM。这种机制使得虚拟机可以高效地执行大部分指令，而无需 VMM 的干预。\n在论文《Dune: Safe User-level Access to Privileged CPU Features》中，Dune 系统利用虚拟化硬件（Intel VT-x）为用户进程提供对特权硬件功能（如页表、异常处理、特权模式等）的访问。Dune 中的用户进程运行在 VMX Non-root 模式下，这使得它们可以直接访问 CPU 的特权级操作，而不需要内核直接参与。这种机制不仅提高了系统性能，还保持了安全隔离。\n基于 xv6 和 Linux 的示例：\n在 xv6 中：xv6 是一个简单的教学操作系统，所有用户进程运行在用户模式下，只有内核才能运行在特权模式（ring 0）下。如果要通过虚拟化硬件将特权模式的一些功能暴露给用户程序，xv6 将需要借助类似 VT-x 这样的虚拟化技术。通过将用户进程放入 VMX Non-root 模式，xv6 可以在不破坏其进程隔离模型的前提下，让用户进程访问页表或执行硬件异常处理。 在 Linux 中：Linux 的设计更加复杂，虚拟化硬件可以通过虚拟机（如 KVM）让多个 Linux 内核在同一台物理机上运行。虚拟机使用硬件虚拟化技术，通过 VMM 来管理内存和 CPU 资源。在 Dune 的设计中，它直接通过虚拟化硬件将某些内核特权功能（如页表管理、系统调用拦截）暴露给用户进程，从而避免了复杂的内核修改。这对于高性能应用（如垃圾回收、权限隔离等）具有极大的好处。 虚拟化本身并不是新技术，最早可追溯到 1960 年代的 IBM 大型机。然而，现代硬件虚拟化（如 Intel VT-x 和 AMD SVM）是 2000 年代后期发展起来的技术，专为提高虚拟机性能而设计。\n虚拟化硬件技术是指虚拟机的指令大部分直接在真实硬件上执行，但在受控的环境中，不能直接影响到主机系统的资源和数据。这种技术通过使用虚拟化硬件（如 Intel VT-x），将虚拟机的执行限制在隔离的虚拟机监控程序（VMM）内，保证虚拟机对主机系统的干扰最小化。\n虚拟化硬件通过多种机制保证安全性，防止虚拟机中的恶意代码（如病毒）攻击主机系统的真实数据：\n隔离机制：虚拟化硬件通过扩展页表（Extended Page Table, EPT）等技术，确保虚拟机只能访问被分配给它的内存区域，而不能直接访问主机系统的物理内存。即使在虚拟机中执行恶意指令，硬件会通过 EPT 确保这些指令不能访问到不属于虚拟机的地址空间。 VM Exit：虚拟机运行时，如果尝试执行某些特权操作（如系统调用），CPU 会触发 VM Exit，将控制权转交给虚拟机监控程序（VMM）。这样，VMM 可以拦截并检查这些特权操作，确保安全性。 硬件虚拟化的防护：Intel 的 VT-x 和 AMD 的 SVM（Secure Virtual Machine）都提供了特定的硬件支持，确保虚拟机与物理机之间的安全隔离。任何尝试逃逸虚拟机环境的操作都会被硬件机制检测并阻止。 扩展页表（EPT）和地址空间隔离：虚拟化硬件通过 EPT 使虚拟机的物理地址与真实物理地址隔离。即便是恶意软件试图操作内存，也只能操作虚拟机自己的地址空间，而不能访问主机系统的数据。 因此，主机系统的内存数据是“锁定”的，虚拟机无法直接接触到。如果有恶意代码试图窃取数据或发起攻击，硬件将中止操作并返回给 VMM。\nQEMU 是一种广泛使用的开源虚拟机仿真器和虚拟化工具，它支持多种虚拟化方式，包括全系统仿真（full system emulation）**和**硬件辅助虚拟化（hardware-assisted virtualization）。当 QEMU 与 KVM（Kernel-based Virtual Machine）结合使用时，虚拟机的执行速度大幅提升，因为 KVM 可以利用硬件虚拟化特性（如 Intel VT-x 或 AMD SVM）将虚拟机的指令直接运行在物理 CPU 上。\n在 MIT 6.828 课程中使用的 xv6 操作系统是在 QEMU 中运行的。QEMU 在这个环境中既可以作为一个纯仿真器（即不依赖硬件虚拟化），也可以在支持的主机上与 KVM 一起工作，利用硬件虚拟化功能。这取决于你如何配置 QEMU。通常在课程中的 xv6 是在纯软件仿真环境中运行的，但如果启用了 KVM，则 QEMU 会利用硬件虚拟化功能加速虚拟机的运行。\n的确，安全攻击一直是虚拟化技术的一个潜在问题，特别是通过虚拟机逃逸（VM Escape）攻击主机系统的风险。虚拟化硬件通过严格的权限管理和隔离机制防止这种攻击。尽管硬件虚拟化技术在设计上已经考虑了安全问题，但历史上也出现过一些硬件漏洞（如 Intel 的 Meltdown 和 Spectre），这使得虚拟化系统需要通过额外的软件补丁和配置来增强安全性。\n总结：\n虚拟化硬件通过页表隔离和硬件中断等机制确保虚拟机不能干涉主机系统的真实数据。 QEMU 支持硬件虚拟化，可以利用 KVM 加速虚拟机的执行。 虽然硬件虚拟化设计上保证了安全性，但历史上也出现过一些需要防范的硬件漏洞。 特权硬件 在 Dune 系统中，特权硬件功能是指那些通常只有操作系统内核才有权限访问和控制的 CPU 功能。这些功能包括：\n异常处理（Exceptions）：处理 CPU 中断和异常（如页面错误、除零等）。 虚拟内存（Virtual Memory）：直接访问和管理页表，控制虚拟地址到物理地址的映射。 权限模式（Privilege Modes）：包括 CPU 中的权限环（如 ring 0 和 ring 3），用于区分内核态和用户态的操作权限。 其他硬件功能：如控制寄存器、TLB（转换后备缓冲区）等，用于内存管理和性能优化。 与特权硬件功能不同，不特权的硬件功能是用户程序可以直接访问的 CPU 功能。这些功能通常限于普通计算操作和基本的内存访问，例如：\n算术运算：加减乘除等 CPU 指令。 基本内存访问：访问分配给该进程的用户空间内存。 I/O 操作：通过操作系统的系统调用接口进行文件读写和网络操作等。 这些功能是受限的，用户态程序无法直接访问内核态所需的特权硬件资源，如页表管理和系统调用向量。\nDune 系统的设计初衷是通过虚拟化硬件技术（Intel VT-x）为用户进程提供对特权硬件功能的直接访问，目的是让用户程序能够更高效地进行某些操作，而不需要频繁地切换到内核态。这带来了几个显著的好处：\n提高性能：用户程序可以直接处理诸如页面错误等异常，减少了传统的系统调用和上下文切换的开销。例如，Dune 能够将页面错误的处理速度提升四倍以上。 灵活的内存管理：Dune 允许用户程序直接操作页表，控制地址翻译和访问权限。这对于实现自定义的垃圾回收、数据压缩等内存密集型操作非常有用。 权限分离和沙盒：通过暴露权限模式（如 ring 0 和 ring 3），Dune 可以有效地隔离不可信代码，提供类似操作系统内核的安全防护机制，但不需要进行复杂的内核修改【5†source】。 直接将特权交给用户进程会带来巨大的安全风险，比如：\n攻击面扩大：恶意程序可能会利用这些特权功能执行恶意操作，例如篡改内核数据或干扰其他进程。 系统不稳定：一旦用户进程获得了特权功能，可能会误用，导致系统崩溃或不稳定。 Dune 的设计是通过虚拟化硬件（Intel VT-x）提供了一种受控的方式，让用户进程能够安全访问特权硬件，同时避免直接干预到主机系统。这种机制主要通过以下几个方面确保安全：\n隔离机制：通过虚拟化模式，用户进程运行在 VMX non-root 模式下，主机操作系统运行在 VMX root 模式。即使用户进程具有特权硬件访问权，它们依旧在一个受限制的虚拟化环境中，无法直接访问主机系统的资源。 VM Exit 机制：每当用户进程尝试进行高风险的操作（如访问特定的硬件资源），硬件会触发 VM Exit，返回控制权给虚拟机管理程序（VMM），确保操作的合法性。 Dune和传统虚拟机管理程序（VMM）的差异 VMM和Dune的主要不同点：\nVMM Dune 环境模拟 传统的虚拟机管理程序模拟一个完整的机器环境，支持运行一个完整的客操作系统。这种模拟需要支持各种物理硬件接口，并且通常通过复杂的方法来提供虚拟硬件，通过这种方式，可以运行未修改过的操作系统。 Dune暴露的是进程环境，而非机器环境。因此，Dune不能支持一个完整的客操作系统，但由于只需处理进程环境，Dune更加轻量和灵活。 Hypercalls Hypercalls用于支持半虚拟化技术，这种技术要求对客操作系统进行修改，以便使用一些更有效率且更容易虚拟化的接口。例如，VMM可能提供一个hypercall来注册虚拟网络设备的中断处理程序。 Hypercalls机制调用的是正常的Linux系统调用。例如，Dune中的hypercall允许通过调用标准的操作系统服务，例如在TCP socket上进行读取操作。 硬件接口的模拟 为了支持未修改的客操作系统，VMM需要模拟各种物理硬件接口（例如虚拟显卡）。 只提供可以直接访问的硬件功能，不能直接访问的情况下，会退回到操作系统上。例如，对于显卡接口，Dune直接使用正常的操作系统显示服务。 状态保存与恢复 必须保存和恢复所有必要的状态以支持一个完整的客操作系统。 由于暴露的只是进程级别的硬件接口，Dune限制了客体与主机状态之间的差异，从而减少了执行VM entries和VM exits的开销。 地址空间 为每个虚拟机提供一个独立的地址空间，模拟平面的物理内存。 EPT（扩展页表）反映的是进程地址空间，这样内存布局可以是稀疏的，而且当两个进程映射同一个内存段时，内存可以一致地共享。 Hypercalls hypercalls的目的是让客操作系统调用虚拟机管理程序，以获取某些特定的虚拟化服务 hypercalls直接调用Linux系统调用，这大大简化了虚拟化层的设计，因为它不需要提供专门的虚拟化服务接口，直接利用操作系统已有的服务即可 状态保存 VMM需要保存和恢复完整的机器状态，以便可以中断和重新启动操作系统及其运行的应用程序。 Dune不需要保存大量的状态，是因为它只是提供一个进程环境，而该进程环境和主机操作系统的环境非常接近。Dune进程始终在主机操作系统中运行，其状态保存和恢复主要由操作系统本身管理，而不是由虚拟化层管理。 硬件模拟 VMM必须模拟完整的硬件层，以支持未修改的客操作系统，这包括CPU、内存、设备等。 Dune只模拟了硬件和操作系统之间的那一层，对于那些无法直接访问的硬件功能，直接调用主机操作系统的服务即可。 关于Hypercalls：\n大多数VMM采用Hypercall来支持半虚拟化。在这种情况下，客操作系统被修改以通过Hypercall与VMM通信。这些Hypercall是专门的虚拟化接口，通常由VMM来处理，处理时往往涉及复杂的步骤，因为VMM需要模拟硬件。 假设我们有一个虚拟网络设备。客操作系统需要注册一个中断处理程序来处理虚拟设备的中断。在传统VMM中，客操作系统通过Hypercall告诉VMM：“我需要注册一个中断处理程序。” VMM接收到这个Hypercall后，会更新其内部的数据结构，并确保当虚拟设备产生中断时，中断处理程序将由客操作系统正确处理。\n在Dune中，Hypercall直接调用主机操作系统（比如Linux）的系统调用。这意味着Dune不需要维护复杂的虚拟化接口，借助操作系统已有的机制即可实现功能。 同样是虚拟网络设备，在Dune中，进程不直接与Dune层通信以注册中断处理程序，而是通过标准的Linux系统调用来完成该操作。例如，进程可能使用read系统调用从一个TCP socket读取数据。实际上，这种调用通过Hypercall直接由Linux内核来处理，而不需要Dune进行复杂的虚拟化处理。\n关于状态保存：\n在传统的VMM中，每个虚拟机（包括其运行的客操作系统和应用程序）都有独立的计算状态，这包括CPU寄存器、内存状态、I/O设备状态等。当发生VM entry（进入虚拟机）或VM exit（退出虚拟机）时，VMM必须保存和恢复所有这些状态，以确保虚拟机的运行不受干扰。这是一项复杂且耗时的任务，因为状态信息非常多。 假设虚拟机正在执行某个操作，中途发生了VM exit（可能是由于某个中断）。在这种情况下，VMM必须保存一些寄存器、内存和I/O设备的状态，以便下一次虚拟机恢复运行时，能够无缝地继续之前的操作。\nDune不需要保存和恢复大量状态，部分原因是它处理的是进程层面而非机器层面的状态。进程的状态由操作系统（比如Linux）管理，包括上下文切换等操作。因为：\nDune进程是主机操作系统的一部分，从操作系统的角度来看，它是一个普通进程。 当发生切换时，Linux已经处理了大部分的状态保存和恢复工作，Dune只需极少的额外处理。 假设一个Dune进程需要执行IO操作。当进程被调度时，Linux内核会自动保存当前进程的状态（如寄存器、内存页表等），并恢复下一个进程的状态。这与标准的进程上下文切换相同，不需要Dune进行额外的状态管理。\n关于硬件模拟：\n传统的VMM需要完整地模拟物理硬件层，以支持未修改的客操作系统。这意味着VMM必须提供虚拟CPU、虚拟内存管理单元、虚拟设备（如网络、显卡等）等。\n对于虚拟显卡，VMM需要模拟显卡的各种寄存器和状态，让客操作系统认为它在与一个真实的显卡打交道。这涉及到高复杂性的模拟和处理。\nDune暴露的是硬件和操作系统之间的一层接口。它不完全模拟硬件，只是提供直接访问一些硬件功能的接口，而对于无法直接访问的功能，依赖于主机操作系统。\n在图形显示方面，Dune不会提供虚拟显卡，而是让进程使用操作系统的显示服务（例如通过Unix域套接字和共享内存访问X服务器）。这样，图形显示的复杂性被主机操作系统处理，Dune不需要额外的处理。\n总结来说，Dune尝试通过暴露进程级的虚拟环境来简化虚拟化的复杂性，这样做不仅使其更加轻量和灵活，而且能够充分利用主机操作系统已提供的服务来实现功能。\nDune 技术 Dune技术确实不同于传统虚拟机的虚拟化，它提供的不是完整的虚拟机，而是一种更轻量级的、针对线程级别的虚拟化，但是它仍然利用了虚拟化硬件的能力，这就是为什么你在论文中看到虚拟机相关的术语，如“VM entry”和“VM exit”。\nDune的核心思想是利用硬件虚拟化技术（如Intel VT-x）为用户态的进程或线程提供对底层硬件资源（如内存、CPU寄存器等）的更直接访问，而无需引入完整的虚拟机（即没有必要模拟整个操作系统）。它让用户态的程序在某种程度上获得与内核态程序类似的硬件访问权限，同时通过虚拟化技术保持对系统安全的控制。\n简单来说，Dune并没有像传统的虚拟机那样完全模拟一台机器，而是借助硬件虚拟化特性，让用户进程可以更高效地访问一些低级的硬件功能，通常只需一部分特权。因此，Dune更像是线程级别的虚拟化，但使用了虚拟机技术来实现这些特权访问。虚拟机与线程的区别:\n虚拟机（VM）：通常模拟的是整个物理机器，包括CPU、内存、存储、网络接口等完整的硬件设备。虚拟机运行的是一个完整的操作系统，并且这个操作系统可以管理多个线程或进程。因此，虚拟机不仅仅是一个线程，它包含了一个完整的执行环境。 线程：线程是操作系统中更轻量级的执行单位，它是进程的一部分，多个线程共享同一个进程的内存空间。每个线程只表示一个执行路径，而没有独立的硬件环境。Dune技术关注的是如何给线程提供某些底层硬件的直接访问权限。 Dune并不创建完整的虚拟机，但它使用了虚拟化硬件中的一些概念，比如：\nVMX Root 和 Non-Root模式：这些概念本质上属于硬件虚拟化管理，用来控制哪些操作系统级别的操作可以被虚拟化进程访问。Dune利用这套机制，让用户态进程进入一种“受控”的环境，能够直接访问某些硬件资源（比如页表操作），但同时保持一定的隔离。 VM entry/exit：尽管Dune不是传统虚拟机，但它通过类似虚拟机的方式进行状态切换。当用户态线程需要使用特权操作时，可能会触发类似“VM exit”的动作，切换到内核态或者让VMM（虚拟机监控器）进行处理。 在Dune的上下文中，虚拟机不是指一个完整的虚拟操作系统环境，而是指Dune利用虚拟化硬件为线程创建的一种“受控环境”，让用户态进程可以安全地使用某些特权指令。\n在传统的虚拟机系统中，**虚拟机（VM）**不是单一的线程，而是一个独立的计算环境，通常运行在虚拟机监控器（VMM）之上，它可以包含多个线程和进程。但是在Dune这样的技术中，虚拟机的概念被简化了，它更像是给某个用户线程提供了一种增强的硬件访问权限。从这个角度看，虚拟机可以被视为一个特权线程的运行环境，但它仍然与完整的虚拟机概念不同。\n总而言之\nDune提供的虚拟化是线程级别的，让用户态线程能够利用硬件虚拟化访问底层资源。 虽然Dune使用了虚拟化硬件的概念（如VMX模式和VM exit等），但它并不创建完整的虚拟机，而是通过类似虚拟机的机制来管理线程对硬件的访问。 虚拟机和线程的区别在于虚拟机模拟的是完整的操作系统环境，而线程只是单一的执行路径。在Dune中，虚拟机的作用类似于为用户态线程提供特权硬件访问的受控环境。 Dune的系统调用拦截 在Dune系统中，用户进程执行系统调用的方式确实与传统操作系统有所不同。这种设计主要是为了提供灵活性和安全性，特别是为了实现系统调用插装（interposition）和防止不可信代码直接访问内核。对于这一点，Dune系统采取了一些特殊的机制来处理系统调用。\n在传统的操作系统中，系统调用是通过陷入内核（trap to the kernel）来执行的。也就是说，当用户进程执行系统调用时，它会从用户态切换到内核态，并由内核处理这个系统调用。然而，在Dune系统中，系统调用会通过一个特殊的机制——系统调用插装（interposition）——来处理，这使得系统调用可以在用户空间被拦截和处理。\nDune系统利用了虚拟化技术，使用户态代码能够有更多的控制权，但同时确保系统安全性。具体来说，当用户进程执行系统调用时，可以采取以下几个步骤：\n系统调用捕获： 当一个Dune用户进程执行系统调用时，这个调用会被虚拟化系统（如虚拟机监控器或Dune层）捕获。这并不是直接陷入内核，而是通过Dune层进行拦截。 自定义处理： Dune层会检查该系统调用，并执行自定义处理逻辑。这个处理逻辑是用户定义的，可以用来： 审计和记录系统调用行为。 替换或修改系统调用的功能。 防止不可信代码执行某些潜在危险的系统调用。 条件性传递到内核： 根据自定义处理的结果，Dune层可以决定是否将系统调用传递给内核。如果需要，系统调用会以传统方式传给内核处理。如果不需要，Dune层可以直接返回结果，阻止系统调用到达内核。 用户进程在Dune系统中并不会直接包括内核的系统调用代码，但是它会通过Dune层的拦截机制进行处理。以下是其中几个可能的机制：\n基于虚拟化的拦截： Dune利用虚拟化硬件（如Intel VT-x）实现系统调用的拦截和处理。这样即使在用户态，进程也能执行一些类似于内核态的操作，但这些操作是在受控的环境下完成的。 共享地址空间： 为了实现更好的性能和灵活性，Dune可能会在用户态与内核态之间共享某些数据结构（例如页表）。这样，Dune可以避免频繁的上下文切换。 中间层代码： Dune可能包括一些用户态的库或运行时（runtime），这些库或运行时会拦截系统调用并将其传递给Dune层进行处理。Dune层为这些库提供了一套API，用户态进程通过这些API与Dune进行交互。 总而言之\nDune系统中的用户进程并不会直接包含内核的系统调用代码。相反，当用户进程执行系统调用时，系统调用会被Dune层捕获和处理。 系统调用捕获机制允许Dune进行系统调用插装、修改或阻止，从而增加系统的安全性和灵活性。 系统调用最终的处理可以通过自定义逻辑决定是否传递给内核，提供了对系统调用行为的细粒度控制。 总的来说，Dune系统通过虚拟化技术提供了一种新颖的、灵活的系统调用捕获和处理机制，使得用户进程可以在不陷入内核的情况下高效且安全地执行系统调用。这大大扩展了用户进程在用户态下的能力，同时通过严格的控制防止了安全问题。\nDune用户态执行指令 Dune确实可以被视为一种增强的缓存和拦截机制，它允许某些特定类型的指令或系统调用在用户态下直接执行，而无需陷入内核。这种机制通过虚拟化技术赋予用户态进程额外的权限，从而实现更高效的操作。\nDune如何在用户态执行指令。Dune系统依赖于硬件虚拟化技术（如Intel VT-x）以及一层特殊的中间层，这层中间层给予用户态一些改进后能够安全执行的权限。以下是Dune系统如何在用户态执行指令的几个核心机制：\n虚拟化技术的使用。Dune利用虚拟化技术，在用户态程序和底层硬件之间插入一层，用来管理和控制特权指令的执行。在Intel VT-x的支持下，Dune可以使用户态程序进入一种称为**VMX非根模式（VMX Non-Root Mode）**的状态，这种状态下可以执行原本需要内核态权限的特权操作。\n特权指令的捕获和处理。当用户态进程执行特权指令时，例如修改页表或执行特权系统调用，Dune层会通过虚拟机控制结构（VMCS）进行捕获。具体步骤如下：\n指令捕获：Dune会预先配置哪些指令或事件需要捕获（如页表访问、某些特殊的CPU指令等）。\n检查与执行：捕获后，Dune会检查这些指令，如果可以在用户态执行就直接执行，否则会传递给内核进行处理。\n回传结果：执行结果或状态更新会通过Dune层传回给用户进程。\n安全性和隔离措施。尽管Dune赋予了用户态进程更多的权限，但它也采取了一些安全措施来确保系统的整体安全性：\n内存隔离：通过虚拟化技术，如扩展页表（EPT），Dune确保进程只能访问它被授权的内存区域。\n特权管理：利用硬件虚拟化特性，Dune限制了用户态程序对内核态资源的直接访问，防止其对系统造成不安全影响。\nDune赋予的权限\n增强的用户态权限。通常情况下，用户态的权限是非常有限的，但通过Dune，用户态进程能够访问和管理一些本来只有内核态才能使用的资源和指令。这是通过虚拟化技术，使得用户态进程可以在安全的前提下执行部分特权操作。\n特权模式运行。在Dune环境下，用户态进程运行在一种特定模式下，这种模式被称作VMX非根模式，它允许：\n页面表管理：用户态进程可以直接修改页表，管理自己的虚拟地址空间，而无需每次都陷入内核。\n特权指令执行：某些以前需要切换到内核才能执行的指令可以直接在用户态执行，例如某种寄存器操作。\n执行优化：通过减少内核和用户态之间的上下文切换，Dune能够显著提高系统调用的性能。\n总结\nDune的设计目的是增强用户态程序的执行权限，但同时保持系统的安全性和隔离性。\n通过虚拟化技术（如Intel VT-x），Dune让用户态进程在一种受控的环境下运行，有些特权指令可以直接在用户态执行，而不用陷入内核。\n特权管理和隔离确保即使赋予用户态更多权限，系统仍然是安全的，不会因为用户进程的错误或恶意操作而受到破坏。\n总的来说，Dune的关键在于利用虚拟化硬件提供的一种隔离机制，使用户态进程能够安全、高效地执行一些原本需要内核态处理的操作，从而提高系统的整体性能。\nDune模式和页表 当用户进程进入Dune模式时，它确实会涉及到某些与页表相关的操作，但这些操作与传统的用户态到内核态的切换有所不同。以下是详细的解释：\n用户进程进入Dune模式和页表相关的操作\n进入Dune模式。当用户进程进入Dune模式，它并不会像传统的系统调用那样完全切换到内核态，而是进入一种特定的虚拟化模式（VMX Non-Root Mode）。在这种模式下，用户进程仍然运行在用户态，但它有更多的权限可以直接访问某些硬件资源和执行特权操作。\n页表管理\nEPT（扩展页表）：Dune模式下的存储管理通常依赖于扩展页表（Extended Page Table，EPT）。EPT是硬件虚拟化的一部分，它允许虚拟机监控器（VMM）管理内存映射，而不需要操作系统频繁切换页表。通过EPT，Dune可以为用户进程提供一种“伪内核态”的权限，让其直接管理和访问自己的内存空间，而不必每次都陷入内核。\n直接页表修改：Dune模式下，用户进程可以直接修改自己的页表，这是通过虚拟化硬件的支持实现的。在这种模式下，用户进程可以安全地进行页表操作，而不需要每次都进行上下文切换。\n用户进程进入内核态的页表切换\n用户态到内核态的切换。在传统的操作系统架构中，当用户进程需要进入内核态（例如执行系统调用），通常会涉及到一些上下文切换，这包括页表的切换。\n页表切换：当用户进程进入内核态时，操作系统可能会切换到一个不同的页表。这是因为内核态和用户态可能有不同的地址空间，需要通过不同的页表来管理。\n上下文切换：这包括保存当前用户态的CPU寄存器状态、切换到内核态的寄存器状态、更新栈指针等。然后内核会根据系统调用的参数和当前状态执行相应的内核逻辑。\n为什么需要页表切换\n地址空间隔离：用户态进程和内核态拥有不同的地址空间，使用不同的页表会提供更好的隔离和安全性，防止用户进程直接访问内核内存区域。\n权限管理：内核态需要访问所有系统资源，而用户态的访问权限较为有限。通过页表切换，内核可以获得全面的内存访问权限。\nDune与传统内核态的区别\n权限管理\n传统内核态：权限管理严格，用户态和内核态通过页表进行严格隔离，任何特权操作都需要通过陷入内核完成。\nDune模式：通过硬件支持的虚拟化技术，Dune在提供类似内核态权限的同时减少了上下文切换的开销，提升了操作效率。\n虚拟化硬件支持\n传统内核态：依赖于操作系统和CPU架构的基本特性进行上下文切换和权限隔离。\nDune模式：使用虚拟化硬件（如Intel VT-x）的高级特性，通过扩展页表（EPT）等机制实现更高效的权限管理和访问控制。\n总结\n用户进程进入Dune模式时，虽然会涉及到一些与页表相关的操作，但这些操作通过虚拟化硬件进行管理，使得切换过程更加高效。\n传统的用户态到内核态的切换通常需要页表切换和上下文切换，以保证地址空间的隔离和权限管理。\nDune模式通过虚拟化硬件提供了一种高效的替代方案，使用户进程在保留安全性的同时能够直接执行某些特权指令，减少了上下文切换的开销。\nEPT format incompatibility问题 Dune 解决 EPT（扩展页表）和标准 x86 页表格式不兼容的问题的方式如下。以下是详细步骤、每步的主题和执行地点：\n初始化空的 EPT（扩展页表）\n主题：EPT 初始配置 执行地点：Dune 模块初始化阶段 在 Dune 中，首先创建一个空的 EPT，表示此时并没有任何地址转换信息可用。这是因为 EPT 的格式与标准的 x86 页表格式不同，不能直接用于内核页表。\n在访问缺失的 EPT 条目时触发 EPT Fault\n主题：VM exit 处理 执行地点：EPT Fault 处理程序 每当访问缺失的 EPT 条目时，会触发一个 EPT Fault（这是一种 VM exit）。此时处理程序会捕获这个 Fault。\n向内核查询进程内存映射\n主题：查询内存映射 执行地点：EPT Fault 处理程序内部 当 EPT Fault 发生时，Dune 的处理程序会查询 Linux 内核，获取当前被访问的虚拟地址的内存映射和权限信息。此时需要向内核页面故障处理程序查询该地址的地址转换和权限。\n手动更新 EPT 以反映内核页表的映射\n主题：更新 EPT 执行地点：EPT Fault 处理程序内部 根据从内核查询到的地址转换信息，Dune 的处理程序会手动创建一个新的 EPT 条目。这个新条目反映了目标地址的正确物理地址和权限。该条目随后会**入到 EPT 中，以便处理故障的内存访问请求。\n处理地址区间的卸载（unmap）\n主题：地址区间管理 执行地点：MMU 通知链中的处理程序 系统运行过程中，有些地址区间可能会被卸载（unmap）。Dune 通过挂接到 MMU 通知链（这与 KVM 的做法类似）来处理这些事件。\n当某个地址被卸载（unmapped）时，Dune 接收到一个事件通知。\nDune 然后会逐出（evict）受影响的 EPT 条目，并且在适当的 Linux 页面结构中设置脏位（dirty bits）。\n支持页面访问信息和脏状态\n主题：页面状态维护 执行地点：Dune 模块中的 MMU 通知链处理程序中 为了支持内核的换页（swapping）和写回磁盘操作，内核需要页面的访问信息和脏状态。Dune 通过在 MMU 通知链中获取这些信息并适时地更新 EPT 及相应的页表条目来实现这一点。例如，当发现一个页面需要写回磁盘时，会将其在内核页表中的脏位标记为已设置。\n整体流程总结\n初始化：创建空的 EPT。 EPT Fault 处理：捕获 EPT Fault。 查询内核映射：向内核查询当前地址的映射关系。 更新 EPT：根据内核信息手动更新 EPT 条目。 处理 Unmap：使用 MMU 通知链处理地址区间的卸载。 页面状态维护：通过 MMU 通知链确保页面访问和脏状态信息的正确性。 这些步骤确保了 Dune 能够在不直接共享页表的情况下，尽可能反映进程在标准 Linux 操作系统下的地址空间。这一过程克服了 EPT 和标准 x86 页表格式不兼容的问题，并且在处理地址转换和权限时保持高效和灵活。\nEPT Fault是VM Exit VM Exit 是指从虚拟机模式退出到虚拟机管理程序（VMM）或主机操作系统的过程。当虚拟机遇到某些特定的事件或情况时，处理无法继续在虚拟机模式下进行，需要切换到VMM模式来处理，这时候会发生VM Exit。\nEPT（扩展页表）Fault 是由于虚拟机访问一个未被映射到物理内存的虚拟地址时发生的。当这样的情况发生时，虚拟机不能自行处理这个访问请求，需要主机的VMM处理，比如更新页表、加载数据等。这就需要退出到VMM模式，即触发一个VM Exit。\n为了更详细地理解过程，我们可以假设有一个运行在Dune环境下的进程，该进程尝试访问一个尚未映射的虚拟地址。假设前提为：\nDune进程在使用虚拟地址0xCAFEBABE进行读操作。 内核页表中关于此地址的信息尚未被加载到EPT中。 具体步骤\n初始化空的EPT 主题：EPT初始配置 地点：Dune模块初始化阶段 操作：Dune开始时创建一个空的EPT，表示尚无地址转换信息可用。 触发一个EPT Fault和VM Exit 主题：VM exit处理 地点：EPT Fault处理程序 操作：Dune进程尝试访问地址0xCAFEBABE。由于EPT中没有此地址的映射，发生了EPT Fault，导致一个VM Exit。 查询内核映射 主题：查询内存映射 地点：EPT Fault处理程序内部 操作：VM Exit引发Dune的Fault处理程序通过页表查询内核，找到0xCAFEBABE对应的物理地址和权限。假设内核返回此地址映射到物理地址0x12345000，并且权限为读（R），写（W），执行（X）。 手动更新EPT 主题：更新EPT 地点：EPT Fault处理程序内部 操作：根据内核信息，Dune在EPT中创建一个新的条目，将虚拟地址0xCAFEBABE映射到物理地址0x12345000，并设置适当的权限（RWX）。 Dune进程继续执行 主题：继续进程执行 地点：Dune进程 操作：EPT更新后，Dune进程能够继续执行对地址0xCAFEBABE的访问请求，且这次不会触发VM Exit。 处理地址区间的卸载\n假设后来某些原因导致地址区间0xCAFEBABE需要被卸载：\n地址区间unmap事件 主题：地址区间管理 地点：MMU通知链的处理程序 操作：Dune挂接到内核的MMU通知链，接收到地址0xCAFEBABE被unmap的通知。 清除EPT条目并更新内核页表 主题：页面状态维护 地点：Dune模块中的MMU通知链处理程序中 操作：Dune清除受影响的EPT条目，并在Linux页表中相应设置脏位（如果需要）。 Dune通过捕获VM Exit和EPT Fault来处理内存映射，并在必要时更新EPT条目。相对于传统的VMM，它并不完整模拟硬件环境，而是利用内核提供的接口和通知机制（例如MMU通知链）来实现内存管理。这使得Dune更加轻量和高效，能够在尽可能少的开销下提供虚拟化支持。\nTSC（时间戳计数器） TSC（Time Stamp Counter）是在CPU上的一个64位寄存器，用于记录自处理器重启以来经过的时钟周期数。程序可以使用rdtsc指令读取TSC的值来进行高精度的时间测量。由于TSC是与处理器的实际运行时钟直接相关的计数器，读取它不会引入额外的延时，因此非常适合性能分析等场景。然而，在虚拟化环境中，如果VMM对时间进行干预，TSC值可能会变得不太可靠，这就是为什么大多数VMM选择虚拟化TSC，以确保客操作系统获得一致的时间读数。\n例如，在高性能计算或需要精确时间测量的应用环境中，直接访问TSC可以提供非常高的时间测量精度。然而，在虚拟化环境中，如果VMM对TSC时间进行干预，时间测量的精度和一致性可能会受到影响。\nDune处理信号（signals） Dune在处理信号（signals）时，使用了一种与传统方法不同的机制。\n对于硬件页面故障（page faults），Dune完全利用硬件机制，基本上取代了软件信号（如SIGSEGV）的角色。 而对于其他类型的信号（如SIGINT），Dune模块通过向进程注入伪硬件中断（fake hardware interrupts）来处理这些信号。这种机制不仅高效，而且在处理特权模式时也能正确组合。 Dune在处理信号（signals）的具体解释：\n硬件页面故障和SIGSEGV 硬件页面故障：当进程访问没有映射的内存页时，硬件会产生一个页面故障。这种机制本身已经可以处理很多情况，比如尝试访问未授权内存、非法内存访问等。 SIGSEGV替代：Dune利用硬件页面故障来处理SIGSEGV（Segmentation Fault，段错误）的情况，不再需要额外的软件信号处理，因为硬件页面故障已经涵盖了这些角色。 其他信号（如SIGINT）的处理 注入伪硬件中断：对于其他类型的信号（如SIGINT），Dune会通过向进程内注入伪硬件中断来处理。当进程收到SIGINT信号时，Dune模块不直接处理信号，而是伪造一个硬件中断，这样进程在下次执行时会被中断处理器打断。 高效性：这种方式高效，因为硬件中断的处理机制通常比软件信号处理更直接，并且利用现有的硬件支持。 特权模式的正确组合 特权模式：特权模式指的是处理器的运行级别，通常包括用户态（ring 3）和内核态（ring 0）。 自动切换特权模式：例如，一个用户进程在运行某些不可信的代码时，可以被置于更低的特权级别（ring 3）。当发生硬件中断时，处理器会自动从ring 3切换到ring 0，确保中断处理和信号处理的安全性。 安全性的增强：这种机制确保即使在处理不可信代码时，硬件会自动进行特权级别的切换，从而保障系统的整体安全性。 假设一个用户进程正在运行，并且某些部分的代码由于不可信而在特权级别ring 3运行。在运行过程中：\n进程访问了一个未映射的内存地址，产生了页面故障。Dune利用硬件页面故障来处理这个访问，并不触发SIGSEGV软件信号。 用户按下Ctrl+C，发送SIGINT信号以中断进程。Dune模块接收到这个信号后，注入一个伪硬件中断到进程中。 由于硬件中断，处理器会自动从ring 3切换到ring 0安全地处理这个中断，并确保系统的安全性。 这种方法不仅高效，而且在不同特权级别之间正确切换，增强了系统的安全性。\nVPID的实现与工作原理 虚拟处理器标识符（VPIDs）提高了系统效率，特别是减少了TLB（翻译后备缓冲区）失效的开销。通过为每个Dune进程分配一个唯一的TLB标签，VPIDs使得在进行hypercalls和上下文切换时，不再需要频繁地刷新TLB。\nTLB是一个高速缓存，用于存储虚拟地址到物理地址的映射。它的主要作用是加速虚拟内存的地址转换。如果TLB中的条目发生变化（例如由于上下文切换到另一个进程），则需要进行TLB失效操作，以确保地址转换正确。\n每次TLB失效都意味着需要重新加载虚拟地址到物理地址的映射，这会带来显著的性能开销。尤其是当上下文切换频繁时，TLB失效会对系统性能产生较大影响。\nVPID（Virtual Processor Identifier）是在处理器虚拟化扩展中引入的一项技术，用于减少或消除TLB失效操作。\n为每个Dune进程分配唯一的TLB标签： 传统做法：没有VPID时，每次上下文切换必须使TLB失效，以确保新进程不会使用旧进程的地址映射。 VPID做法：有了VPID，每个Dune进程都有一个唯一的标识符，这个标识符也存在于TLB条目中。 TLB条目的标签： TLB条目：每个TLB条目不仅包含虚拟地址和物理地址的映射，还会包含对应的VPID。 有效性检查：在上下文切换时，处理器可以根据VPID快速判断当前TLB条目是否对新进程仍然有效。如果VPID与当前进程的标识符不匹配，说明该条目属于其他进程，应忽略。 如何具体实现VPID呢？\n分配VPID： 初始化：每个Dune进程启动时，Dune模块会从处理器的VPID池中分配一个唯一的VPID。 使用VPID： TLB条目包含VPID：当一个Dune进程运行时，处理器在生成TLB条目时会包含该进程的VPID。 上下文切换：上下文切换到另一个进程时，系统切换VPID而不必立即失效所有的TLB条目。 判断有效性： TLB查找：在进行地址转换时，处理器不仅检查虚拟地址，还会检查条目中的VPID是否与当前进程的VPID匹配。 有效TLB命中：如果匹配，说明当前TLB条目有效，可以直接使用，避免了重新加载地址映射。 假设有两个Dune进程A和B，分别分配了VPID 1和VPID 2。\n进程A运行 处理器在TLB中存储了若干条目，VPID为1，这些条目映射A进程的地址。 切换到进程B 处理器收到上下文切换请求，将当前的VPID设置为2，但不失效TLB条目。 TLB操作 处理器在进行地址转换时，如果发现TLB条目中的VPID不等于2，则忽略这些条目。 只有VPID等于2的条目（即进程B的映射）才会被使用。 通过这种方式，Dune在进行hypercalls和上下文切换时，不再需要频繁地使TLB失效，从而大大提高了系统的效率。VPID的使用确保了每个进程的TLB条目是独立的，进程间互不干扰。\nVMCALL与SYSCALL SYSCALL是x86架构中的一个指令，用于从用户态快速切换到内核态执行系统调用。它的主要用途是调用操作系统提供的服务，例如文件操作、进程管理等。这种切换通常伴随着上下文切换和权限级别的变化。\nVMCALL是虚拟化扩展中的一条指令，用于从虚拟机内部向虚拟机管理程序（VMM）发出请求。这种请求通常用于管理虚拟机资源或者操作VMM提供的虚拟化服务。\n为什么VMCALL比SYSCALL更快？在解释用VMCALL代替SYSCALL时，涉及一些虚拟化机制的细节：\n拦截处理与开销： SYSCALL：传统的SYSCALL指令需要从用户态切到内核态，这涉及到特权级别的转换和上下文保存，可能会带来一定的开销。 VMCALL：在虚拟化环境中，VMCALL用于虚拟机和虚拟机管理程序之间的通信。Dune可以利用这个机制来快速处理特权操作，而不需要进行多余的特权级别转换。 直接操作与简化路径： SYSCALL路径：SYSCALL路径可能涉及到复杂的系统调用处理逻辑，以及潜在的上下文切换和寄存器保存/恢复操作。 VMCALL路径：VMCALL路径可以更直接，因为它已经被设计用于虚拟环境中的高效通讯，大部分情况下不需要进行复杂的操作。 Dune的特殊优化： Dune环境：在Dune中，用户进程可以直接使用一些特权硬件功能（通过Dune提供的接口），利用VMCALL可以避免SYSCALL路径的复杂性。 libDune中的优化：修改后的libc通过VMCALL指令直接与Dune进行交互，可以减少系统调用的开销，从而获得性能提升。 假设有一个用户程序需要调用一个系统服务，比如获取当前时间：\n传统的SYSCALL路径： 用户程序发出SYSCALL指令。 处理器切换到内核态，保存当前用户态上下文。 内核处理系统调用并返回结果。 处理器切换回用户态，恢复上下文。 改用VMCALL路径（在Dune中）： 用户程序发出VMCALL指令。 处理器直接交由Dune处理，可能跳过一些特权级别转换和复杂的系统调用路径逻辑。 Dune通过已经设置好的接口直接处理请求，提高执行效率。 正是由于这些优化，使得在虚拟化环境中，使用VMCALL指令相比于传统的SYSCALL指令，可以获得轻微的性能提升。这样，Dune可以更加高效地实现其特权操作，充分利用虚拟化环境下的硬件性能。\n进程切换到Dune模式的启动流程 将一个进程切换到Dune模式的启动流程。这类似于启动一个操作系统（OS）。具体步骤如下：\n创建有效的页表 要求：在切换到Dune模式之前，必须提供一个有效的页表。 原因：简单的身份映射（identity mapping，虚拟地址直接映射到相同的物理地址）是不够的，因为需要考虑到EPT（扩展页表）的压缩布局。这个压缩布局意味着虚拟地址空间在EPT中的表现方式可能有所不同，需要一个特定的页表来匹配这种布局。 调用Dune入口ioctl 过程：一旦页表创建完成，通过传递页表根（page table root）作为参数来调用Dune entry ioctl。这个系统调用是用来通知Dune模块开始切换模式。 操作：Dune模块接收到ioctl调用后，会切换进程到Dune模式，并开始执行代码，使用提供的页表根作为初始的%CR3寄存器值。 libDune进一步配置环境 配置特权寄存器：libDune会配置一些特权寄存器来建立一个合理的操作环境。 加载GDT：全局描述符表（GDT）被加载，用于提供基本的平面分段。 加载IDT：中断描述符表（IDT）被加载，用于捕获硬件异常。 设置TSS中的独立栈：任务状态段（TSS）中设置了一个独立的栈，用于处理双重故障（double faults）。 配置GS段基址：配置GS段的基址，以便轻松访问每个线程的数据。 为什么不能直接映射页表？简单的身份映射无法满足Dune需求的原因有以下几点：\nEPT的压缩布局： 在Dune环境中，EPT所使用的物理内存布局可能不同于传统的直接映射。这种压缩布局允许Dune更高效地管理内存，但也意味着需要特定的页表来正确映射这些地址。 地址空间一致性： 尽管目标是让进程在转换前后的地址保持一致，但需要确保这些地址能够在EPT的压缩布局中有效映射。直接身份映射可能无法在压缩布局中找到合适的位置。 在切换到Dune模式和配置操作环境期间，以下寄存器会发生变化：\n%CR3寄存器： 这个寄存器包含页表根地址，切换到Dune模式后，%CR3会被设置为提供的页表根。 特权寄存器： GDT（全局描述符表基址寄存器）：加载一个新的GDT来提供基本平面分段。 IDT（中断描述符表基址寄存器）：加载一个新的IDT来捕获硬件异常。 TSS（任务状态段）： 设置一个独立的栈用于处理双重故障。如果发生双重故障，处理器会使用TSS中的栈。 段寄存器： GS段寄存器：配置GS段的基址，以便访问线程本地存储（TLS）数据。这个访问是为了让每个线程能够拥有独立的数据区，减少线程之间的干扰。 将进程切入Dune模式需要考虑到许多细节，以确保环境的正确性和高效性。从提供合适的页表到配置各种特权寄存器，每一步都至关重要。这种复杂的启动过程确保了Dune能够提供一个强大且高效的虚拟化环境。\n传递页表根（page table root）作为参数在将进程切换到Dune模式中的作用非常重要，这是因为页表根决定了整个虚拟地址空间的映射方式。以下是更详细的解释：\n页表根通常指页表的顶层指针，它在 x86 架构中存储在 %CR3 寄存器中。这个指针指向一个多级页表的起点，页表用于管理虚拟内存地址到物理内存地址的映射。页表根的主要作用为：\n初始化内存映射环境： 作用：页表根指向了多级页表的起点，通过这些页表，系统可以将虚拟地址转换为物理地址。这是虚拟内存管理的核心。 重要性：在切换到Dune模式时，传递页表根作为参数可以确保虚拟地址能够正确映射到物理地址，从而使进程能够继续无缝运行。 保证地址空间一致性： 作用：传递页表根使得Dune可以使用调用方提供的内存映射，从而保证进程在切换到Dune模式前后的地址空间保持一致。 重要性：地址空间的一致性对于进程的正常运行至关重要。否则，进程可能会在切换后找不到所需的内存，导致崩溃或异常行为。 支持EPT压缩布局： 作用：Dune利用EPT（扩展页表）对内存进行管理，这可能涉及到内存的压缩布局。传递特定的页表根可以确保内存映射符合Dune的需求。 重要性：简单身份映射无法适应EPT的压缩布局，必须使用特定的页表根来匹配这些布局，以确保虚拟地址在物理内存中的合理分布。 具体过程\n创建页表： 在将进程切换到Dune模式之前，系统或libDune会生成并设置一个新的页表，这个页表按照Dune环境的需求进行映射和配置。 传递页表根： 当调用Dune entry ioctl时，新创建的页表根作为参数传递给Dune模块。 更新%CR3寄存器： Dune模块接收到页表根后，会将其设置为%CR3寄存器的值。这样，处理器在进行地址转换时，就会根据这个新的页表来进行虚拟地址到物理地址的映射。 环境配置： 之后，libDune会继续配置其他特权寄存器和必要的系统资源，例如加载GDT、IDT，设置TSS栈和GS段基址等，以完成虚拟化环境的初始化。 假设我们有一个进程，其虚拟内存地址需要映射到特定的物理地址：\n原始页表：原来的页表根指向某个起始地址P，包含对虚拟地址V1到物理地址P1的映射，以及V2到P2的映射。 新页表：在切换到Dune模式之前，生成一个新页表根指向地址Q，这个新页表符合Dune的EPT布局，并且依然包含对V1到P1，V2到P2的映射。 当进程切换到Dune模式并传递新页表根之后：\n%CR3寄存器会被更新为新页表根Q。 处理器将使用新页表来进行地址转换，确保虚拟地址V1和V2依然能够正确地映射到P1和P2，从而确保进程的正常运行。 传递页表根作为参数在切换到Dune模式中至关重要，因为它确保了虚拟地址到物理地址的正确映射，保证地址空间一致性，并适应Dune的EPT布局需求。这个步骤有效地初始化了内存管理环境，使得系统能够顺利过渡到虚拟化状态。\n为什么直接使用旧的页表根并不可行，以及为什么在Dune模式下需要新页表根。关键差异和原因：\nEPT和标准页表的格式差异\n标准页表：这是操作系统使用的页表，用于管理虚拟地址到物理地址的映射。\nEPT（扩展页表）：这是硬件虚拟化中的一个关键组件，尤其在Intel VT-x技术中使用。EPT具有不同的格式和特点，不完全兼容常规的页表格式。\nEPT压缩布局\n地址空间压缩：Dune模式可能对某些虚拟地址进行压缩布局，以适应硬件虚拟化需求。如果直接使用旧的页表根，无法利用这些压缩布局的优化，可能会导致内存映射无效或效率低下。\n专用页表结构：为了支持这些优化，Dune需要一个特别设计的页表结构，这些结构与常规页表可能完全不同。\n内存保护和隔离\n安全性：Dune需要确保进程在虚拟化和非虚拟化模式之间的切换过程中，内存保护和隔离的一致性。采用新页表根可以防止在转换过程中出现安全漏洞或内存访问违规。\n特权寄存器设置：不同模式下，特权寄存器的配置方式不同，直接使用旧页表根可能无法正确初始化这些寄存器。\n通过实际操作的解析，理解为什么需要新页表根会更为直观。\n初始化新页表根： 生成新页表：在进入Dune模式之前，系统会生成一个符合Dune需求的页表，它考虑到EPT的特殊布局和格式。\n设置映射：根据旧页表的信息，将必要的虚拟地址到物理地址的映射配置到新页表中，但格式和布局可能不同。\n调用初始化IOCTL： 传递新页表根：通过IOCTL调用，把新页表根传递给Dune模块。\n更新%CR3寄存器：Dune模块将新页表根设置为%CR3寄存器的值，使处理器在进入Dune模式后使用新页表进行地址转换。\n配置其他特权资源： 加载GDT、IDT：在初始化过程中，libDune会加载全局描述符表（GDT）和中断描述符表（IDT），这些操作依赖于新页表的正确配置。\n设置TSS栈和GS段基址：这些配置同样需要依赖新的页表布局，以确保处理器可以正确访问所需的内存区域。\n假设我们有以下内存布局需求：\n虚拟地址V1映射到物理地址P1。 虚拟地址V2映射到物理地址P2。 原始页表的情形：\n页表根P：指向的页表中，V1 -\u0026gt; P1，V2 -\u0026gt; P2。 新页表的情形（Dune的需求下）：\n页表根Q：由于EPT的格式和压缩布局要求，这个新页表的结构可能如下：\n新的页表具有压缩特性，在同样完成V1 -\u0026gt; P1，V2 -\u0026gt; P2映射的同时，其方式更加优化和特定化，适应Dune环境。\n尽管旧页表和新页表在内容上可能看起来相似（例如映射相同的虚拟地址到物理地址），但它们的格式、结构和布局可能存在显著差异，这些差异对于支持Dune模式下的高效虚拟化至关重要。直接使用旧的页表根无法满足这些需求，因此必须提供一个新的页表根来确保Dune模式下的正确性和性能。\nsandbox防御ELF header 攻击 ELF文件是Linux系统的常见可执行文件格式，包含了程序的头部（header），该头部定义了二进制文件的各种结构和加载方式。不良设计或恶意构造的ELF头部可能会试图利用ELF加载器中的漏洞，从而实现代码注入或其他攻击。\n为了防御ELF头部攻击，Dune的沙箱采用了双重ELF加载器策略：\n初始ELF加载器（Minimal ELF Loader）： 角色：负责加载沙箱运行时环境。 实现：这个加载器是libDune的一部分，只支持静态二进制文件的加载。 安全性：由于只支持静态二进制文件，其复杂度和攻击面都较小，减少了被恶意ELF头部利用的风险。 二次ELF加载器（ld-linux.so）： 角色：在沙箱内加载不可信的二进制文件。 实现：使用Linux中的ld-linux.so，它是一个经过广泛使用和信任的ELF加载器。 安全性：ld-linux.so已经是Linux系统中不可或缺且受到信任的组件，经过严格的安全审查和优化。 双重ELF加载器策略防御的详细流程如下\n加载沙箱运行时环境： 步骤：内核启动沙箱进程，并通过libDune中的初始ELF加载器载入一个独立的、最小化的沙箱运行时环境。 复杂度：因为这个加载器只支持静态二进制文件，功能简单，减少了被攻击的可能性。 加载二次ELF加载器： 步骤：沙箱运行时环境使用初始ELF加载器，把ld-linux.so（作为二次ELF加载器的一部分）加载到一个不可信的环境中。 信任：使用ld-linux.so是因为它是Linux系统的一部分，经过广泛使用和安全验证。 加载不可信二进制文件： 步骤：ld-linux.so运行在沙箱内的用户态中，从不可信的环境中加载和执行不可信的二进制文件。 防御：因为ld-linux.so在用户态（ring 3）运行，即使不可信的二进制文件包含恶意的ELF头部，其攻击范围也仅限于用户态，无法直接影响内核或沙箱的运行时环境。这限制了其破坏力。 Dune沙箱通过采用双重ELF加载器机制，有效地将潜在的ELF头部攻击风险最小化。初始ELF加载器简单且只有有限的功能，使其难以成为攻击目标。而二次ELF加载器ld-linux.so在用户态加载不可信二进制文件，即使有攻击发生，也局限在用户态范围内，避免影响沙箱的核心安全性。\nELF头部攻击的实现和原理\nELF（Executable and Linkable Format）文件是Linux和其他类Unix操作系统中常见的可执行文件格式。ELF头部包含了关于文件布局和加载信息的元数据，例如程序头表、节头表、入口点地址等。恶意构造的ELF头部可以尝试利用ELF加载器中的漏洞，从而实现代码注入或其他攻击。\nELF头部攻击通常利用以下几个方面：\n格式解析漏洞： 描述：ELF加载器在解析头部信息时出现的漏洞。例如，缓冲区溢出、整数溢出、未做边界检查等。 影响：攻击者可以通过构造特别的头部数据，使加载器在解析时发生溢出或崩溃，从而执行任意代码或读取不应访问的内存区域。 高级加载器功能滥用： 描述：ELF格式支持复杂的加载和链接功能，如动态链接、内存映射、重定位等。攻击者可以利用这些功能，通过精心设计的头部信息，实现加载器预期之外的行为。 影响：例如，构造特定的段表和节表，使加载器跳到恶意代码的地址处执行。 以下是几个可能的ELF头部攻击实现方法：\n缓冲区溢出： 原理：通过构造异常长的字段值，诱使加载器在处理这些字段时发生缓冲区溢出。常见的目标字段包括节名字、节表项、程序头表项等。 示例：将段表或节表的大小字段设置得超大，加载器试图读取或处理超大数据量时发生溢出，导致恶意代码被执行。 整数溢出： 原理：利用加载器在处理ELF头部时的整数溢出漏洞。通过设置特定的字段值，诱使加载器在计算内存位置或大小时发生溢出，从而覆盖关键数据。 示例：将某个偏移字段设置为接近整数上限，使加载器在加法操作时发生溢出，导致访问不该访问的内存地址。 段表和节表操控： 原理：通过操纵段表和节表中的条目，使加载器加载和执行恶意代码段。 示例：将PT_LOAD段的虚拟地址设为指向恶意代码的位置，加载器在加载段时会将恶意代码映射到进程地址空间并执行。 重定位表攻击： 原理：ELF文件中包含重定位表，用于动态重定位符号地址。攻击者可以构造恶意重定位条目，导致加载器跳转到恶意代码。 示例：在重定位表中添加条目，使特定的符号地址被重定向到攻击者控制的内存区域。 为了防止上述攻击，常见的防御措施包括：\n硬化加载器： 边界检查：认真检查每个字段的边界，不允许超出预期范围。 格式验证：严格遵循ELF规范，验证每个部分的格式和一致性。 运行时防护： 地址空间布局随机化（ASLR）：随机化内存地址布局，使攻击者难以预测内存位置。 栈保护（Stack Canaries）：在栈变量前后添加特定的哨兵值，防止栈溢出攻击。 最小化加载器功能： 静态二进制支持：如Dune中的初始ELF加载器，仅支持静态二进制文件，减少复杂功能的使用。 双层加载策略：分离初始加载器和高级加载器，减少初始加载器的攻击面，并在不可信环境中运行高级加载器，如ld-linux.so。 以下是一个简单的ELF头部溢出攻击示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ccodepreviewrun// 伪代码示例，实际情况可能更复杂 // 攻击者构造的恶意ELF头部 struct elf_header { // 正常的ELF头部 char e_ident[16]; // ELF标识 uint16_t e_type; // 文件类型 uint16_t e_machine; // 机器类型 uint32_t e_version; // 版本 uint64_t e_entry; // 入口点 uint64_t e_phoff; // 程序头偏移 uint64_t e_shoff; // 节头偏移 uint32_t e_flags; // 标志 uint16_t e_ehsize; // ELF头大小 uint16_t e_phentsize; // 程序头项大小 uint16_t e_phnum; // 程序头项数目 uint16_t e_shentsize; // 节头项大小 uint16_t e_shnum; // 节头项数目 uint16_t e_shstrndx; // 字符串节索引 // 溢出部分 char overflow[1024]; // 超长字段，企图溢出缓冲区 uint64_t malicious_code; // 恶意代码地点 }; 攻击者可以通过修改上述字段，例如设置虚拟地址段为指向malicious_code部分，使加载器在解析阶段受溢出影响，将控制权转移到恶意代码。\nELF头部攻击通过构造恶意头部数据，利用加载器中的漏洞，实现代码注入或其他恶意操作。通过严谨的加载器设计和运行时保护机制，可以有效防御这类攻击。Dune采用双重ELF加载器策略，通过最小化初始加载器功能和使用可信的高级加载器，大大降低了头部攻击的风险。\n分离初始加载器和高级加载器的概念\n初始加载器（Minimal ELF Loader）\n功能有限：只支持加载静态（static）二进制文件，具有最低限度的功能。 目标：主要用于加载高级加载器，而不直接加载不可信的二进制文件。 典型例子：libDune中的最小ELF加载器。 高级加载器（Secondary ELF Loader）\n功能全面：可以处理动态链接（dynamic linking）、重定位（relocation）等复杂操作。 目标：在不可信环境中加载和执行不可信的二进制文件。 典型例子：Linux中常用的ld-linux.so。 分离加载策略的流程\n初始加载器加载沙箱运行时环境： 加载并初始化一个非常精简和受信任的沙箱运行时环境，以便在安全和受限的环境中执行后续操作。 初始加载器加载高级加载器： 使用初始加载器将高级加载器（如ld-linux.so）加载到沙箱的用户态环境中。此时，初始加载器的责任就完成了。 高级加载器加载不可信的二进制文件： 在沙箱的用户态环境中，高级加载器开始加载和执行不可信的二进制文件。\n即使不可信的二进制文件包含恶意头部，高级加载器运行在用户态，攻击面较小，只能影响沙箱内部而不影响整个系统。\n设想以下场景：一个不可信的二进制文件untrusted_app需要在Dune沙箱中运行。\n初始化和配置 创建沙箱：内核启动一个新的进程，进入Dune模式，创建一个隔离的沙箱运行时环境。\n初始加载器：libDune的最小ELF加载器被加载到沙箱中。\n1 2 3 4 5 6 7 8 9 // 初始加载器的简化伪代码 void initial_loader(const char* secondary_loader_path) { // 加载静态二进制文件 load_static_binary(\u0026#34;/path/to/sandbox_runtime\u0026#34;); // 加载二级加载器 load_static_binary(secondary_loader_path); // 跳转执行二级加载器 jump_to_entry_point(secondary_loader_path); } 初始加载器加载高级加载器 路径：假设高级加载器的路径是/lib64/ld-linux-x86-64.so.2。\n加载操作：初始加载器将ld-linux.so加载到沙箱中，并跳转到它的入口点。\n1 2 3 4 5 6 7 8 9 10 11 // 继续初始加载器的伪代码 void initial_loader(const char* secondary_loader_path) { // 加载静态二进制文件 load_static_binary(\u0026#34;/path/to/sandbox_runtime\u0026#34;); // 加载二级加载器 load_static_binary(secondary_loader_path); // 跳转执行二级加载器 jump_to_entry_point(secondary_loader_path); } initial_loader(\u0026#34;/lib64/ld-linux-x86-64.so.2\u0026#34;); 高级加载器加载不可信二进制文件 启动路径：在ld-linux.so加载并初始化后，它开始处理不可信的二进制文件untrusted_app。\n内存映射：ld-linux.so处理ELF头部，并映射不可信二进制文件中的各个段。\n执行：完成加载后，ld-linux.so将控制权转交给untrusted_app的入口点，开始执行不可信代码。\n1 2 # 使用ld-linux.so加载并执行不可信二进制文件 /lib64/ld-linux-x86-64.so.2 /path/to/untrusted_app 安全保障 用户态执行：整个加载和执行过程在沙箱的用户态（ring 3）进行，限制了不可信代码的权限和影响范围。\n高级加载器的安全性：由于ld-linux.so是经过大量使用和安全审查的组件，其安全性较高，减少了被恶意头部利用的可能。\n总结\n通过分离初始加载器和高级加载器的加载过程，Dune能够有效地抵御ELF头部攻击：\n初始加载器功能有限，只负责加载高级加载器，并且设计简单，减少攻击面。 高级加载器运行在用户态，并且经过充分验证和使用，在加载不可信二进制文件时，能最大限度地减少安全风险。 这种分离策略不仅提高了加载过程的安全性，还确保了即使不可信代码包含恶意头部，攻击的范围和影响也被限制在沙箱环境内。\n初始加载器确实在限定和设置高级加载器的运行环境方面起到了关键作用。如果没有初始加载器，直接加载高级加载器可能会带来一系列问题和安全隐患。让我们详细探讨这种分离策略的设计理由和必要性。\n初始加载器的主要作用\n环境初始化： 安全的执行环境：初始加载器负责创建一个受限制的、安全的执行环境，确保不可信代码在隔离的沙箱中运行。加载高级加载器之前，初始化必要的安全机制。 基础设施设置：例如，配置页表、加载全局描述符表（GDT）、中断描述符表（IDT），以及设置任务状态段（TSS）中的栈等。这些操作需要在特权模式下（ring 0）完成，是高级加载器无法直接处理的。 功能简化，减少攻击面： 只加载静态二进制：初始加载器只支持加载功能受限的静态二进制文件，功能简单，减少了潜在漏洞的数量和复杂度。 有限的攻击面：由于功能的简化，初始加载器的代码攻击面相对较小，不容易被恶意利用。 上下文切换和执行控制： 受控地启动高级加载器：初始加载器负责安全地加载和启动高级加载器，在确保环境安全的前提下，转移执行控制权。 配置用户态环境：在加载高级加载器之前，初始加载器可以设置用户态的特定配置，确保高级加载器在一个预期和受控的环境中运行。 如果没有初始加载器的后果\n缺乏安全保障： 直接暴露高级加载器：如果直接加载高级加载器（如ld-linux.so），任何环境初始化和安全配置都无法保证，加载过程可能会暴露在潜在的安全威胁中。 特权操作：高级加载器一般在用户态运行，缺乏执行特权操作的能力，如设置页表和配置特权寄存器，这些操作对于创建安全的执行环境至关重要。 复杂度增加： 处理复杂性：直接暴露高级加载器需要它处理非常复杂的环境初始化和安全设置，增加了代码复杂性，也增加了潜在漏洞的数量。 安全审计难度：代码复杂性和多样化功能使得对加载器进行全面安全审计变得更加困难，增加了被攻击的风险。 加载路径不受控制： 潜在恶意干扰：未经初始加载器预先配置和限制的加载路径可能被恶意代码干扰，导致加载不可信的或未签名的代码片段。 通过对加载流程的具体示例，我们可以更好地理解初始加载器的作用：\n步骤1：内核启动并加载初始加载器\n进入Dune模式：内核启动一个新的进程，并将其切入Dune模式，初始化隔离环境。 加载最小ELF加载器：内核通过libDune加载一个功能受限的、只支持静态二进制的初始加载器。 1 2 3 4 5 6 7 // 伪代码示例：内核启动的部分 void kernel_startup() { // 切换到Dune模式，初始化安全环境 enter_dune_mode(); // 加载并执行初始加载器 load_initial_loader(\u0026#34;/path/to/minimal_loader\u0026#34;); } 步骤2：初始加载器加载和设置高级加载器\n加载高级加载器：初始化基本环境后，初始加载器将ld-linux.so加载到受限的用户态环境中。 1 2 3 4 5 6 7 8 9 10 // 初始加载器的伪代码 void initial_loader(const char* secondary_loader_path) { // 设置基本的安全环境，如页表、GDT、IDT、TSS等 setup_basic_environment(); // 加载高级加载器 load_static_binary(secondary_loader_path); // 转移控制权到高级加载器 jump_to_entry_point(secondary_loader_path); } initial_loader(\u0026#34;/lib64/ld-linux-x86-64.so.2\u0026#34;); 步骤3：高级加载器加载不可信二进制文件\n用户态执行：在安全的用户态环境中，高级加载器ld-linux.so负责加载不可信的二进制文件，并执行它们。 防御机制生效：即使不可信二进制文件包含恶意ELF头部，攻击面仅限于用户态，无法影响内核或特权环境。 1 2 # 使用高级加载器加载并执行不可信二进制文件 /lib64/ld-linux-x86-64.so.2 /path/to/untrusted_app 通过以上解释和示例可以看到，初始加载器在设置和限制高级加载器的运行环境方面起到了关键作用。没有初始加载器的附加保护，高级加载器的环境将无法控制，容易受到恶意代码的攻击和干扰。初始加载器确保了安全的环境初始化和基本配置，从而为高级加载器提供了一个安全和受控的执行环境。这种分离策略不仅提高了加载过程的安全性，还限制了恶意代码的攻击范围，确保整个系统的稳定和安全。\nMachine Abstraction “使用虚拟化硬件提供进程抽象，而不是机器抽象”，实际上在描述虚拟化技术的不同层次和方式。为了更好地理解其中的概念，我们需要先弄清楚“进程抽象（process abstraction）”和“机器抽象（machine abstraction）”的含义。\nMachine Abstraction（机器抽象）\n“机器抽象”指的是通过虚拟化技术，将一台物理计算机抽象成多个虚拟机，每个虚拟机看起来像是一台独立的物理计算机。这种虚拟化是以“机器”为核心的，每个虚拟机都有独立的硬件资源（如CPU、内存、存储等），并运行一个完整的操作系统。用户或应用程序看到的是一个完整的虚拟机器。\n这种虚拟化技术的典型例子是虚拟机管理程序（Hypervisor），比如VMware、Xen或者KVM。每个虚拟机都有自己的操作系统，并且能运行不同的应用程序，就像使用一台真实的物理机器一样。\nProcess Abstraction（进程抽象）\n“进程抽象”则不同，它更关注虚拟化的粒度。在这种情况下，虚拟化提供的是进程级别的隔离，而不是虚拟一整台机器。它不再是模拟出一整台虚拟的计算机，而是提供一个隔离的、用于运行单个进程或应用的环境。这种进程级别的虚拟化通常比机器抽象更加轻量。\n**容器（Containers）**就是一种常见的进程级别的虚拟化方式，比如Docker、Kubernetes等。与虚拟机不同，容器不需要一个完整的操作系统实例，而是共享主机的内核资源，同时每个容器运行自己的应用程序或进程。这样可以大大提高资源利用率，并减少开销，因为不需要为每个应用程序运行一个完整的操作系统。\n解释\n这句话的意思是，文章所指的虚拟化技术是以“进程抽象”为核心，而不是传统的“机器抽象”。也就是说，虚拟化硬件并不是为每个应用创建一个独立的虚拟机，而是为每个应用或进程提供一个隔离的、虚拟化的运行环境。这样的虚拟化方式更加轻量化，更接近容器化技术，比如Docker或其他类似的技术。\n总结来说，虚拟机技术（machine abstraction）模拟的是一整台虚拟机器，而进程级别的虚拟化（process abstraction）则是在操作系统的级别进行隔离，为单个应用提供环境。\n虚拟化硬件可以被配置成避免为虚拟机保存和恢复某些硬件状态 当你运行一个虚拟机时，底层硬件（CPU、内存、I/O设备等）必须模拟或管理整个虚拟机的运行状态。这些状态包括：\nCPU寄存器：比如通用寄存器、指令指针、栈指针等。 内存状态：虚拟机当前使用的内存页面。 I/O设备状态：磁盘、网络接口等外设的状态。 其他硬件资源的状态：如中断控制器状态等。 每当虚拟机从一个任务切换到另一个任务时，或者从“主机”系统切换到“客户”虚拟机时，虚拟化系统必须保存这些硬件状态，并在需要时进行恢复。这被称为上下文切换（context switch），这是非常重要且资源密集的工作，尤其在处理虚拟机时，因为虚拟机模拟了整台计算机的硬件，所以每次切换时要保存和恢复的状态较多。\n保存和恢复完整的硬件状态代价很高。每次切换虚拟机时，虚拟化管理程序（如Hypervisor）都需要保存虚拟机的所有硬件状态，并在切换回来时重新恢复。这会导致以下几个问题：\n性能开销大：保存和恢复所有的硬件状态需要大量的时间和计算资源，尤其是在虚拟机数量较多时，频繁的上下文切换会导致系统性能下降。 复杂性增加：完整的硬件状态管理非常复杂，特别是在涉及多个硬件设备和资源时。 因此，避免或减少这种状态保存和恢复可以显著提高虚拟化的效率，特别是在处理较轻量级的虚拟化工作负载时，比如容器化进程（process abstraction），它们并不需要管理完整的硬件状态。\n要避免保存和恢复虚拟机所需的多个硬件状态，虚拟化硬件和管理程序可以通过以下方式进行优化：\n更轻量的虚拟化层次：像容器这样的进程级虚拟化不需要模拟整个操作系统和硬件，而是直接使用主机操作系统的内核，因此不需要保存和恢复大量的硬件状态。相比虚拟机，容器技术只需要隔离进程和网络等基本资源，从而减少了硬件状态的依赖。 硬件辅助虚拟化：现代CPU架构（如Intel的VT-x和AMD的AMD-V）提供了硬件辅助虚拟化技术，允许某些硬件状态直接在CPU层面管理，减少了虚拟化管理程序需要手动保存和恢复的状态。这样做可以减少上下文切换的负担，提升性能。 减少硬件仿真：通过简化虚拟化环境中的硬件仿真（如只提供虚拟机必要的部分硬件接口），虚拟化软件可以避免保存不必要的硬件状态，专注于管理核心资源（如CPU和内存），避免设备和外设等不常用资源的状态切换。 这句话的核心意思是：通过优化虚拟化硬件配置和管理方式，可以减少或避免对某些不必要硬件状态的保存和恢复。这种优化方式尤其适用于较轻量的虚拟化场景，比如容器，而不适用于完整虚拟机的模拟。通过减少状态保存和恢复的工作量，虚拟化系统可以显著提高性能，尤其是在频繁的上下文切换中。\nVMM 这些段落讨论了虚拟化技术中的几个关键概念，特别是VMX（Intel的虚拟机扩展）如何管理虚拟机和虚拟机管理程序（VMM）的交互。让我逐一解释并总结这些概念。\nVMM（Virtual Machine Monitor，虚拟机监控器）是虚拟化技术的核心部分，有时也称为Hypervisor。它的主要任务是管理虚拟机的执行，确保每个虚拟机能够独立运行，同时控制虚拟机与物理硬件的交互。\nVMM的作用：\n管理虚拟机的生命周期（创建、运行、暂停、销毁）。 隔离和保护每个虚拟机的资源，防止它们互相干扰。 控制虚拟机对硬件的访问，确保虚拟机不会直接操作底层硬件。 VMX Root Mode vs VMX Non-Root Mode\nVMX Root Mode：这是VMM运行的模式。虚拟化硬件在这个模式下给VMM完全控制权，可以管理虚拟机和底层物理硬件。VMM在VMX root mode下运行，并在需要管理虚拟机时切换到VMX non-root mode。 VMX Non-Root Mode：这是虚拟机（或Guest）运行的模式。在这个模式下，虚拟机以一种“受控”的方式运行，它并不直接访问硬件，而是通过VMM的控制。虚拟机的一些操作可能会触发VM exit，将控制权交还给VMM。 因此，VMX Root Mode 是VMM的运行模式，而VMX Non-Root Mode 是虚拟机的运行模式。\n这些模式与进程的关系\n在虚拟化环境中，虚拟机（或“guest”）其实相当于一个运行在VMM控制下的“操作系统”。这不同于普通的用户进程。普通的用户进程是运行在操作系统上的程序，而虚拟机则是运行在虚拟机管理程序上的操作系统。因此，它的虚拟化级别更深，与进程并不是同一个概念。 Guest 通常指的是虚拟机，而不是用户进程。在虚拟化环境中，guest 可能是运行在虚拟机中的一个完整的操作系统或多个进程。 VM entry 和 VM exit\nVM entry：当VMM执行 VMLAUNCH 或 VMRESUME 指令时，硬件会自动进行VM entry，这意味着CPU从VMX root mode切换到VMX non-root mode，开始运行虚拟机（guest）。 VM exit：当需要VMM采取某种操作时（比如虚拟机执行了某些敏感指令），硬件会触发VM exit，将CPU从VMX non-root mode切换回VMX root mode，并跳转到VMM的入口点。 在这些切换过程中，硬件自动保存和恢复大部分的架构状态，这些状态被存储在内存中的数据结构中，称为VM控制结构（VMCS）。\nVMCS 和硬件状态\nVMCS（Virtual Machine Control Structure）是一个特殊的数据结构，存储了虚拟机和VMM之间上下文切换时的状态信息。它包括：\n虚拟机的架构状态（如CPU寄存器等）。 各种配置参数，这些参数让VMM能够控制虚拟机的执行行为。 通过配置VMCS，VMM可以控制哪些硬件暴露给虚拟机。例如，VMM可以配置让虚拟机执行 HLT 指令时触发VM exit，也可以允许虚拟机直接暂停CPU。\n虚拟内存和EPT（Extended Page Table）\n虚拟内存是虚拟化过程中最复杂的硬件特性之一。一个简单的解决方案是让虚拟机直接访问页表根寄存器（%CR3），这会给虚拟机完全的控制权，允许它配置页表来访问任意物理内存地址，甚至包括VMM自身的内存，这显然是不安全的。\n为了解决这个问题，VT-x 提供了扩展页表（EPT）机制，它增加了一个额外的地址翻译层。EPT由VMM管理，能够确保虚拟机在访问虚拟内存时仍然受到内存隔离的约束，防止其访问不该访问的物理内存。AMD的SVM（Secure Virtual Machine）技术有类似的机制，叫做嵌套页表（NPT。\n总结\nVMM 是虚拟机监控器，管理虚拟机与物理硬件的交互。 VMX Root Mode 是VMM运行的模式，VMX Non-Root Mode 是虚拟机运行的模式。 VMCS 是存储虚拟机状态和配置的结构，允许VMM控制虚拟机的执行。 虚拟内存管理 通过硬件辅助技术（如EPT/NPT）实现，确保虚拟机不能访问不该访问的物理内存。 VM entry和VM exit 在Dune系统中，VM entry和VM exit是两个重要的过程，用来管理用户态进程和虚拟化层之间的权限切换。以下是它们分别在什么情况下执行以及一些具体的示例。\nVM Entry：进入Dune模式\nVM Entry是指从传统的用户态或内核态“进入”到Dune提供的虚拟化环境（VMX Non-Root Mode）。在这个过程中，系统会保存当前的状态并切换到Dune模式，使得用户进程可以利用Dune提供的增强权限执行特权操作。\nVM Entry通常在用户进程需要利用Dune的增强权限操作时执行。以下是一些典型的例子：\n进程启动： 当一个用户进程启动并被设置为运行在Dune模式下时，Dune系统会执行VM Entry，进入虚拟环境。此时，用户态进程获取到更多的硬件访问权限，如页表管理和特权指令的执行权限。\n示例：启动一个需要直接管理内存页面的高性能计算应用，该应用在启动时经过Dune配置可以直接操作页表而无需频繁陷入内核。\n特定特权操作： 某些操作需要Dune模式下的权限才能执行，如直接访问硬件寄存器或执行特别的系统调用。此时，需要通过VM Entry进入Dune模式。\n示例：一个用户态网络堆栈需要直接访问网络设备的寄存器以实现高效的数据包处理。为了实现这一点，在启动网络堆栈时执行VM Entry。\nVM Exit：退出Dune模式\nVM Exit是指从Dune模式“退出”到普通用户态或内核态。在这个过程中，虚拟化层会捕获某些特权操作或事件，将控制权交还给Dune的管理层或内核进行处理。\nVM Exit通常在用户进程执行了某些特权操作或事件，这些操作不能直接在Dune模式下完成或需要更高权限的内核处理时触发。以下是一些典型的例子：\n敏感指令执行： 当在Dune模式下执行某些受控的特权指令时，这些指令会触发VM Exit，将控制权交还给Dune的管理层或内核。\n示例：一个用户进程试图修改某些受保护的寄存器或执行不允许的特权指令时，硬件会触发VM Exit，将控制权交回给Dune管理层处理。\n系统事件： 某些系统事件如中断、异常或I/O操作可能无法完全在用户态处理，需要内核介入，此时会触发VM Exit。\n示例：处理I/O操作或设备中断时，由于这些操作需要内核的直接参与处理，它们会触发VM Exit，并将控制权交还给内核。\n请求Dune管理层处理： 用户进程可以显式请求退出Dune模式并交给Dune管理层处理某些操作。\n示例：用户进程通过执行特殊指令（如VMCALL）主动请求退出Dune模式以便执行某些需要完整内核权限的操作。\n总结\nVM Entry是在用户进程进入Dune模式以利用虚拟化提供的增强特权时执行的，例如进程启动时需要直接操作硬件或管理内存页面。 VM Exit是在用户进程执行特权操作或系统事件时，Dune模式需要交还控制权给管理层或内核进行处理时执行的，例如敏感指令执行、系统中断或I/O操作。 通过这些机制，Dune系统能够在保证安全性的前提下大幅提升用户态进程执行特权操作的效率，同时确保必要时能够安全地退出到内核或进行更多权限的操作。\nVM Exit，将控制权交还给内核 VM Exit 是指从虚拟化环境（在 Dune 系统里，即 VMX Non-Root Mode）退出，将控制权交还给虚拟机监控器（VMM）或内核。这一过程涉及一系列复杂的状态切换和资源管理操作，以确保系统能正确处理和恢复。以下是这一过程中具体的变化和步骤：\nVMX Non-Root Mode 到 VMX Root Mode 的切换\n保存虚拟机状态。当触发 VM Exit 时，硬件会自动保存当前虚拟机（Guest）的状态到 VMCS（Virtual Machine Control Structure）。这些状态信息包括：\nCPU 寄存器状态：通用寄存器、程序计数器、栈指针、状态寄存器等。\n控制寄存器和系统寄存器：CR0, CR3, CR4, EFER 等。\n访存相关寄存器：如 base address 寄存器。\n恢复 VMM 或内核的状态。随后，硬件会恢复 VM Entry 前的寄存器和状态，包括：\nVMM 或内核的 CPU 状态：恢复至 VM Entry 时保存的状态。\n指令指针和栈指针：指向处理 VM Exit 的位置。\nVM Exit 的原因和处理逻辑\n识别 VM Exit 原因。硬件会在 VMCS 中记录导致 VM Exit 的具体原因，VMM 或内核可以通过以下信息确定处理逻辑：\n退出原因码（Exit Reason Code）：如 CPUID 指令、执行外部中断、I/O 读写等。\n退出指令信息：包括导致 VM Exit 的指令地址、指令执行上下文等。\n权限管理和安全检查。在恢复 VMM 或内核状态后，VMM 或内核会对 VM Exit 事件做一系列检查和权限管理。确保虚拟机未执行未授权的操作或尝试访问受保护的资源：\n系统调用检查：确认系统调用是否合法。\nI/O 操作检查：确保不会扰乱底层硬件或操作系统状态。\n内存访问控制：通过 EPT 和页表确保虚拟机只能访问允许的地址空间。\n上下文切换和状态恢复\n上下文切换。VMM 或内核需要进行上下文切换，以便处理虚拟机引发的事件。通常包括：\n内核栈切换：切换到内核的栈空间。\n控制寄存器切换：切换控制寄存器以适应内核态或VMM的执行环境。\n内存管理单元（MMU）配置：恢复内核或 VMM 的页表状态，以便正确管理内存访问。\n状态恢复。在处理完 VM Exit 事件后，VMM 或内核可能需要重新进入虚拟机，继续其执行。这需要调用 VM Entry 过程：\n恢复虚拟机状态：从 VMCS 恢复虚拟机的寄存器和控制寄存器状态。\n更新控制寄存器：CR0, CR3, CR4, EFER 等需要更新为虚拟机状态。\n重新加载内存页面映射：确保 EPT 和页表正确映射虚拟机的地址空间。\n事件处理和恢复执行\n事件处理。具体的 VM Exit 事件可能涉及各种不同的处理逻辑，以下是一些常见的情况：\n外部中断处理：内核处理硬件中断，可能涉及调度其他任务或更新系统状态。\nI/O 操作：VMM 或内核处理与存储、网络、显示等设备的交互。\n敏感指令执行：VMM 可能模拟并处理虚拟机不能直接执行的特权指令。\n恢复执行。完成对 VM Exit 事件的处理后，VMM 或内核可能选择回到虚拟机继续其执行。此时需要重新执行 VM Entry：\n恢复虚拟机状态：重新加载 VMCS 中保存的虚拟机状态。\n重新进入虚拟机执行：执行 VMLAUNCH 或 VMRESUME 指令，进入 VMX Non-Root Mode，继续虚拟机程序的执行。\n在 Dune 系统以及其他基于硬件虚拟化的系统中，VM Exit 将控制权交还给内核或 VMM 触发一系列复杂的状态保存和恢复操作。这些操作包括保存虚拟机状态、识别和处理 VM Exit 的原因、上下文切换和权限管理、以及在处理完事件后重新进入虚拟机执行。通过这些机制，Dune 可以在用户态下高效执行特权操作，同时在需要时安全地交还控制权给内核或 VMM，以确保系统的稳定和安全。\n权限分离（Privilege Separation） 权限分离（Privilege Separation）是一种安全技术，通过将应用程序或系统的不同部分分离开来，并为不同部分分配不同的权限，以尽量减少安全漏洞的影响。如果某一部分被攻击或被利用，只会影响该部分，而不会波及整个系统。例如，一个具有高权限的进程可以分离出一个低权限的进程来处理不信任的数据，这样即使低权限的进程被攻破，高权限的进程依然安全。\nSthread 是一种在 Wedge 系统中引入的轻量级进程，设计用于提供类似于操作系统进程的隔离，同时具有类似于线程的高性能。其主要用途包括：\n隔离风险代码：将可能存在风险的代码运行在 sthread 中，以减少因漏洞攻击而带来的影响。 用户请求隔离：在如网络服务器这类应用中，每个用户请求可以在一个单独的 sthread 中运行，保证不同用户间的隔离，防止一个用户的攻击影响其他用户。 Sthread 的回收利用确实有点类似于线程池的思想。线程池技术通过预先创建一组线程，并在需要时重复使用这些线程，从而避免频繁创建和销毁线程的开销。Sthread 回收利用的原理是：\n在第一次创建 sthread 时，进行检查点保存。 当 sthread 完成任务并退出时，将其恢复到初始状态，这样在下次需要创建 sthread 时，可以直接从这个恢复的状态开始，而无需重新分配资源和初始化。 这样做减少了 sthread 创建和销毁的成本提高了系统的整体性能。 sthread 回收时的脏位 \u0026ldquo;脏位\u0026rdquo;（dirty bit）是一种用于管理内存页面状态的机制，但需要明确的是，脏位本身不会直接进行内存恢复，而是作为一个标记来帮助实现恢复过程。以下是具体的工作原理：\n脏位的作用：在内存管理中，脏位用来标识一个内存页面是否被修改过。每当一个页面被写操作修改，硬件会将该页面的脏位设为1，表示该页面已被修改。 ssthread的回收：在ssthread回收利用过程中，我们需要将ssthread的内存恢复到其初始状态，以便重新利用。这时，脏位的状态可以帮助识别哪些页面在ssthread运行期间被修改过。 如何实现内存恢复： 初始化检查点：在ssthread第一次创建时，保存其完整的干净状态，包括所有内存页面的内容。 运行期间标记修改：通过硬件的脏位机制，实时监控哪些页面在ssthread的执行过程中被修改。 恢复过程：当ssthread完成任务并准备回收时，检查所有被标记为“脏”的页面，然后将这些页面恢复到初始检查点保存的状态。 通过这种方法，我们不需要每次都全面检查和恢复所有内存页面，仅需关注那些实际发生修改的页面，从而大大提高了恢复效率。\n总结一下，脏位是一个标记，告诉系统哪些页面被修改了。它本身并不会实际“恢复”内存，而是在恢复过程中起到导引作用，使系统知道哪些页面需要恢复，大大优化了内存恢复的操作效率。\n影子页表（Shadow Page Table） 影子页表（Shadow Page Table）在计算机系统特别是虚拟化环境中扮演了关键角色。它主要应用于虚拟机管理程序（Hypervisor）或者虚拟机监控器（VMM）中，用于有效地管理虚拟机的内存以及确保访客操作系统和主机操作系统之间的内存隔离与映射的效率。其地位：\n中介角色：在虚拟化环境中，Shadow Page Table充当虚拟机的虚拟页表和宿主机的物理页表之间的中介。虚拟机运行时，每次访问内存时，都会先查找虚拟页面，然后由影子页表将其映射到宿主机的物理页面。 高效的内存管理：通过维护影子页表，虚拟机管理程序能够有效管理和隔离不同虚拟机的内存空间，确保各虚拟机互不干扰。 性能优化：由于影子页表可以批量管理页表更新和TLB刷新，它能显著提高虚拟机内存操作的性能，减少因频繁的页表更新导致的性能开销。 影子页表由一系列的数据结构构成，这些数据结构用于保存虚拟内存地址到物理内存地址的映射信息。具体来说，影子页表通常包含以下部分：\n页目录和页表页：包含页目录（Page Directory）和页表页（Page Table Entry）结构，用于存储虚拟到物理地址映射。 TLB管理：涉及管理和刷新TLB（Translation Lookaside Buffer），确保在影子页表更新时，TLB中的缓存条目也能得到及时更新。 访问控制信息：包含与访问权限和保护策略相关的信息，以确保安全性和内存隔离。 假设有一个虚拟化环境，其中运行一个虚拟机，该虚拟机需要访问虚拟内存地址0xABCD1234，这个访问过程涉及以下步骤：\n虚拟机访问：虚拟机尝试访问其虚拟地址空间中的地址0xABCD1234。 初步映射：虚拟机内有自己的页表，假设虚拟机页表指出虚拟地址0xABCD1234映射到虚拟机的“物理”地址0x101234。 影子页表查找：虚拟机管理程序查找影子页表，假设影子页表指示虚拟机的“物理”地址0x101234实际上映射到宿主机的物理地址0x12345678。 物理访问：最后，虚拟机管理程序访问宿主机的物理内存地址0x12345678，从而完成虚拟地址到物理地址的转换。 在这个过程中，影子页表起到了关键的桥接作用，不仅确保了虚拟机对内存的访问安全和隔离，同时也确保了内存操作的高效性。\n影子页表在虚拟内存管理中扮演了重要的角色，提供高效、安全和隔离的内存访问手段。它有效地桥接虚拟地址和物理地址，使得虚拟机能够像访问自己的物理内存一样访问宿主机内存，从而实现无缝的内存虚拟化。\n正课部分 Why Virtual Machine 今天讨论的话题是虚拟机。今天的内容包含三个部分:\n第一个部分是Trap and Emulate，这部分会介绍如何在RISC-V或者QEMU上构建属于自己的Virtual Machine Monitor（注，有些场合也称为Hypervisor）。 第二部分会描述最近在硬件上对于虚拟化的支持。 最后是讨论一下今天的论文 ，它使用了第二部分中硬件上的支持。 首先什么是虚拟机？你可以认为这是对于计算机的一种模拟，这种模拟足够能运行一个操作系统。\nQEMU可以认为是虚拟机的一个例子（注，QEMU应该是属于VMM/Hypervisor）。\n在架构的最底层，位于硬件之上存在一个Virtual Machine Monitor（VMM），它取代了标准的操作系统内核。\nVMM的工作是模拟多个计算机用来运行Guest操作系统。\nVMM往上一层，如果对比一个操作系统的架构应该是用户空间，但是现在是叫做Guest空间。\n所以在今天的架构图里面，上面是Guest空间，下面是Host空间（注，也就是上面运行Guest操作系统，下面运行VMM）。\n在Guest空间，会有一个或者多个Guest操作系统内核，或许其中一个是Linux kernel。\n这里的Linux kernel会觉得自己就是个普通的内核，并在自己之上还运行一堆用户进程，例如VI，C Compiler。\n我们或许还有另一个Guest运行了Windows操作系统，同时也包含了Windows用户进程。\n所以，在Host空间运行的是VMM，在Guest空间运行的是普通的操作系统。\n除此之外，在Guest空间又可以分为Guest Supervisor Mode，也就是Guest操作系统内核运行的模式，和Guest User Mode。\nVMM的主要目的是提供对计算机的模拟，这样你可以不做修改就启动普通的Linux，普通的Windows系统，并运行在虚拟机内，并且不用担心任何奇怪的事情发生。\n所以，VMM必须要能够完全按照实际硬件的行为来模拟Guest Supervisor Mode和Guest User Mode，尽管实际上不可能完全一样，我们之后会讨论VMM对于这两种模式的模拟。\n那么人们为什么会想要使用虚拟机呢？实际中有很多原因使得人们会在一个计算机上运行多个相互独立的操作系统。\n在一个大公司里面，你需要大量的服务，例如DNS，Firewall等等，但是每个服务并没有使用太多的资源，所以单独为这些服务购买物理机器有点浪费，但是将这些低强度的服务以虚拟机的形式运行在一个物理机上可以节省时间和资金。\n虚拟机在云计算中使用的也非常广泛。\n云厂商，例如AWS，不想直接出借物理服务器给用户，因为这很难管理。它们想向用户出借的是可以随意确定不同规格的服务器。\n或许有两个用户在一台物理服务器上，但是他们并没有太使用计算机，这样AWS可以继续向同一个物理服务器上加入第三或者第四个用户。\n这样可以不使用额外的资金而获得更高的收益。所以，虚拟机提供了额外的灵活性，这里借助的技术是：将操作系统内核从之前的内核空间上移至用户空间，并在操作系统内核之下增加新的一层（注，也就是虚拟机的内核是运行在宿主机的用户空间，虚拟机的内核通过新增加的一层VMM来对接底层硬件）以提供这里的灵活性。\n还有一些其他的原因会使得人们使用虚拟机。\n第一个是开发内核，这就是为什么我们在课程中一直使用QEMU。\n能够在虚拟环境而不是一个真实的计算机运行XV6，使得这门课程对于你们和我们来说都要方便的多。\n同时对于调试也更容易，因为相比在物理计算机上运行XV6，在QEMU提供的虚拟机环境中运行可以更容易的提供gdb的访问权限。\n最后一个人们使用虚拟机的原因是，通过新增的VMM提供的抽象可以实现更多的功能。\n例如，你可以为整个操作系统和其中的用户进程做一个快照，并在磁盘中保存下来。\n稍后再恢复快照，并将操作系统和其中的用户进程恢复成做快照时的状态。这可以增加运行的可靠性，或者用来调试，或者用来拷贝虚拟机的镜像并运行多次。\n除此之外，还可以将一个Guest操作系统迁移到另一个计算机上。\n如果你在一个物理计算机上运行了一个Guest操作系统，现在需要关闭并替换该物理计算机，你可以在不干扰虚拟机运行的前提下，将它迁移到另一个物理计算机，这样你就可以安全的关闭第一个物理计算机。\n以上就是人们喜欢使用虚拟机的原因。虚拟机实际上应用的非常非常广泛，并且它也有着很长的历史。虚拟机最早出现在1960年代，经过了一段时间的开发才变得非常流行且易用。\n对于这们课程来说，我们之所以要学习虚拟机是因为VMM提供了对于操作系统的一种不同视角。\n在操作系统的架构中，内核之上提供的封装单元（注，视频中说的是container，但是container还有容器的意思，所以这里说成是封装单元）是我们熟悉的进程，内核管理的是多个用户进程。\n而在VMM的架构中，VMM之上提供的封装单元是对计算机的模拟。VMM的架构使得我们可以从另一个角度重新审视我们讨论过的内容，例如内存分配，线程调度等等，这或许可以给我们一些新的思路并带回到传统的操作系统内核中。\n所以，在虚拟机场景下，大部分的开发设计研究工作，从传统的内核移到了VMM。某种程度上来说，传统操作系统内核的内容下移了一层到了VMM。\n今天课程的第一部分我将会讨论如何实现我们自己的虚拟机。\n这里假设我们要模拟的是RISC-V，并运行针对RISC-V设计的操作系统，例如XV6。\n我们的目的是让运行在Guest中的代码完全不能区分自己是运行在一个虚拟机还是物理机中，因为我们希望能在虚拟机中运行任何操作系统，甚至是你没有听说过的操作系统，这意味着对于任何操作系统的行为包括使用硬件的方式，虚拟机都必须提供提供对于硬件的完全相同的模拟，这样任何在真实硬件上能工作的代码，也同样能在虚拟机中工作。\n除了不希望Guest能够发现自己是否运行在虚拟机中，我们也不希望Guest可以从虚拟机中逃逸。很多时候人们使用虚拟机是因为它为不被信任的软件甚至对于不被信任的操作系统提供了严格的隔离。\n假设你是Amazon，并且你出售云服务，通常是你的客户提供了运行在虚拟机内的操作系统和应用程序，所以有可能你的客户运行的不是普通的Linux而是一个特殊的修改过的Linux，并且会试图突破虚拟机的限制来访问其他用户的虚拟机或者访问Amazon用来实现虚拟机隔离的VMM。\n所以Guest不能从虚拟机中逃逸还挺重要的。Guest可以通过VMM使用内存，但是不能使用不属于自己的内存。类似的，Guest也不应该在没有权限的时候访问存储设备或者网卡。\n所以这里我们会想要非常严格的隔离。\n虚拟机在很多方面比普通的Linux进程提供了更加严格的隔离。Linux进程经常可以相互交互，它们可以杀掉别的进程，它们可以读写相同的文件，或者通过pipe进行通信。\n但是在一个普通的虚拟机中，所有这些都不被允许。运行在同一个计算机上的不同虚拟机，彼此之间是通过VMM完全隔离的。\n所以出于安全性考虑人们喜欢使用虚拟机，这是一种可以运行未被信任软件的方式，同时又不用担心bug和恶意攻击。\n前面已经指出了虚拟机的目标是提供一种对于物理服务器的完全准确的模拟。但是实际中出于性能的考虑，这个目标很难达到。\n你将会看到运行在Guest中的Linux与VMM之间会相互交互，所以实际中Linux可以发现自己是否运行在VMM之上。\n出于效率的考虑，在VMM允许的前提下，Linux某些时候知道自己正在与VMM交互，以获得对于设备的高速访问权限。\n但这是一种被仔细控制的例外，实现虚拟机的大致策略还是完全准确的模拟物理服务器。\nTrap-and-Emulate \u0026mdash; Trap 我们该如何构建我们自己的VMM呢？\n一种实现方式是完全通过软件来实现，你可以想象写一个类似QEMU的软件，这个软件读取包含了XV6内核指令的文件，查看每一条指令并模拟RISC-V的状态，这里的状态包括了通过软件模拟32个寄存器。你的软件读取每条指令，确定指令类型，再将指令应用到通过软件模拟的32个寄存器和控制寄存器中。\n实际中有的方案就是这么做的，虽然说考虑到细节还需要做很多工作，但是这种方案从概念上来说很简单直观。\n但是纯软件解析的虚拟机方案应用的并不广泛，因为它们很慢。\n如果你按照这种方式实现虚拟机，那么Guest应用程序的运行速度将远低于运行在硬件上，因为你的VMM在解析每一条Guest指令的时候，都可能要转换成几十条实际的机器指令，所以这个方案中的Guest的运行速度比一个真实的计算机要慢几个数量级。\n在云计算中，这种实现方式非常不实用。所以人们并不会通过软件解析来在生产环境中构建虚拟机。\n相应的，一种广泛使用的策略是在真实的CPU上运行Guest指令。\n所以如果我们要在VMM之上运行XV6，我们需要先将XV6的指令加载到内存中，之后再跳转到XV6的第一条指令，这样你的计算机硬件就能直接运行XV6的指令。当然，这要求你的计算机拥有XV6期望的处理器（注，也就是RISC-V）。\n但是实际中你又不能直接这么做，因为当你的Guest操作系统执行了一个privileged指令（注，也就是在普通操作系统中只能在kernel mode中执行的指令，详见3.4）之后，就会出现问题。\n现在我们在虚拟机里面运行了操作系统内核，而内核会执行需要privileged权限指令，比如说加载一个新的Page Table到RISC-V的SATP寄存器中，而这时就会出现问题。\n前面说过，我们将Guest kernel按照一个Linux中的普通用户进程来运行，所以Guest kernel现在运行在User mode，而在User mode加载SATP寄存器是个非法的操作，这会导致我们的程序（注，也就是虚拟机）crash。\n但是如果我们蠢到将Guest kernel运行在宿主机的Supervisor mode（注，也就是kernel mode），那么我们的Guest kernel不仅能够修改真实的Page Table，同时也可以从虚拟机中逃逸，因为它现在可以控制PTE（Page Table Entry）的内容，并且读写任意的内存内容。所以我们不能直接简单的在真实的CPU上运行Guest kernel。\n相应的，这里会使用一些技巧。\n首先将Guest kernel运行在宿主机的User mode，这是最基本的策略。\n这意味着，当我们自己写了一个VMM，然后通过VMM启动了一个XV6系统，VMM会将XV6的kernel指令加载到内存的某处，再设置好合适的Page Table使得XV6看起来自己的内存是从地址0开始向高地址走。\n之后VMM会使用trap或者sret指令（注，详见6.8）来跳转到位于User mode的Guest操作系统的第一条指令，这样不论拥有多少条指令，Guest操作系统就可以一直执行下去。\n一旦Guest操作系统需要使用privileged指令，因为它当前运行在User mode而不是Supervisor mode，会使得它触发trap并走回到我们的VMM中（注，在一个正常操作系统中，如果在User mode执行privileged指令，会通过trap走到内核，但是现在VMM替代了内核），之后我们就可以获得控制权。\n所以当Guest操作系统尝试修改SATP寄存器，RISC-V处理器会通过trap走回到我们的VMM中，之后我们的VMM就可以获得控制权。\n并且我们的VMM也可以查看是什么指令引起的trap，并做适当的处理。这里核心的点在于Guest操作系统并没有实际的设置SATP寄存器。\n学生提问：VMM改如何截获Guest操作系统的指令？它应该要设置好一个trap handler对吧，但这不是一个拥有privileged权限的进程才能做的事情吗？而VMM又是个宿主机上的用户程序，是吧？\nRobert教授：我这里假设VMM运行在Supervisor mode。所以在这里的图中，VMM就是宿主机的kernel。\n这里我们不是启动类似Linux的操作系统，而是启动VMM（注，类似VMware的ESXi）。VMM以privileged权限运行，并拥有硬件的完整控制权限，这样我们就可以在VMM里面设置各种硬件寄存器。有一些VMM就是这么运行的，你在硬件上启动它们，并且只有VMM运行在Supervisor mode。\n实际上还有很多很多其他的虚拟机方案，比如说在硬件上启动Linux，之后要么Linux自带一个VMM，要么通过可加载的内核模块将VMM加载至Linux内核中，这样VMM可以在Linux内核中以Supervisor mode运行。今天我们要讨论的论文就是采用后者。\n这里主要的点在于，我们自己写的可信赖的VMM运行在Supervisor mode，而我们将不可信赖的Guest kernel运行在User mode，通过一系列的处理使得Guest kernel看起来好像自己是运行在Supervisor mode。\n在RISC-V上，如果在User mode尝试运行任何一个需要Supervisor权限的指令都会触发trap。\n这里需要Supervisor权限的指令并不包括与Page Table相关的指令，我们稍后会介绍相关的内容。\n所以每当Guest操作系统尝试执行类似于读取SCAUSE寄存器，读写STVEC寄存器，都会触发一个trap，并走到VMM，之后我们就可以获得控制权。\nTrap-and-Emulate \u0026mdash; Emulate VMM会为每一个Guest维护一套虚拟状态信息。所以VMM里面会维护虚拟的STVEC寄存器，虚拟的SEPC寄存器以及其他所有的privileged寄存器。\n当Guest操作系统运行指令需要读取某个privileged寄存器时，首先会通过trap走到VMM，因为在用户空间读取privileged寄存器是非法的。之后VMM会检查这条指令并发现这是一个比如说读取SEPC寄存器的指令，之后VMM会模拟这条指令，并将自己维护的虚拟SEPC寄存器，拷贝到trapframe的用户寄存器中。\n（注，有关trapframe详见Lec06，这里假设Guest操作系统通过类似“sread a0, sepc”的指令想要将spec读取到用户寄存器a0）\n之后，VMM会将trapframe中保存的用户寄存器拷贝回真正的用户寄存器，通过sret指令，使得Guest从trap中返回。\n这时，用户寄存器a0里面保存的就是SEPC寄存器的值了，之后Guest操作系统会继续执行指令。最终，Guest读到了VMM替自己保管的虚拟SEPC寄存器。\n学生提问：VMM是怎么区分不同的Guest？\nRobert教授：VMM会为每个Guest保存一份虚拟状态信息，然后它就像XV6知道是哪个进程一样，VMM也知道是哪个Guest通过trap走到VMM的。XV6有一个针对每个CPU的变量表明当前运行的是哪个进程，类似的VMM也有一个针对每个CPU的变量表明当前是哪个虚拟机在运行，进而查看对应的虚拟状态信息。\n学生提问：VMM可以给一个Guest分配多个CPU核吗？\nRobert教授：稍微复杂点的VMM都可以实现。\n学生提问：在实际的硬件中会有对应寄存器，那么为什么我们不直接使用硬件中的寄存器，而是使用虚拟的寄存器？\nRobert教授：这里的原因是，VMM需要使用真实的寄存器。\n举个例子，想象一下SCAUSE寄存器，当Guest操作系统尝试做任何privileged操作时（注，也就是读写privileged寄存器），会发生trap。\n硬件会将硬件中真实的SCAUSE寄存器设置成引起trap的原因，这里的原因是因为权限不够。\n但是假设Guest操作系统只是从Guest用户进程执行了一个系统调用，Guest操作系统需要看到SCAUSE的值是系统调用。\n也就是说Guest操作系统在自己的trap handler中处理来自Guest用户进程的系统调用时，需要SCAUSE的值表明是系统调用。\n而实际的SCAUSE寄存器的值却表明是因为指令违反了privilege规则才走到的trap。通常情况下，VMM需要看到真实寄存器的值，而Guest操作系统需要能看到符合自己视角的寄存器的值。\n（注，在Guest操作系统中，可能有两种情况会触发trap，一种是Guest用户空间进程的系统调用，也就是正常操作系统中正常的trap流程，另一种是Guest内核空间读取privileged寄存器时，因为Guest内核空间实际上也是在宿主机的用户空间，导致这是个非法操作并触发trap。Robert这边举的例子的流程应该是这样，Guest用户进程执行系统调用，在这一个瞬间SCAUSE寄存器的值是ECALL，也就是8，详见6.6。但是稍后在Guest系统内核的trap handler中需要读取SCAUSE的值，以确定在Guest中引起trap的原因，但是这就触发了第二种trap，SCAUSE的值会变成Illegal Access。我们不能让Guest系统内核看到这个值，所以VMM这里将它变成ECALL并返回。）\n在这种虚拟机的实现中，Guest整个运行在用户空间，任何时候它想要执行需要privilege权限的指令时，会通过trap走到VMM，VMM可以模拟这些指令。\n这种实现风格叫做Trap and Emulate。\n你可以完全通过软件实现这种VMM，也就是说你可以只通过修改软件就将XV6变成一个可以运行在RISC-V上的VMM，然后再在之上运行XV6虚拟机。当然，与常规的XV6一样，VMM需要运行在Supervisor mode。\n所有以S开头的寄存器，也就是所有的Supervisor控制寄存器都必须保存在虚拟状态信息中。\n同时还有一些信息并不能直接通过这些控制寄存器体现，但是又必须保存在这个虚拟状态信息中。其中一个信息就是mode。\nVMM需要知道虚拟机是运行在Guest user mode还是Guest Supervisor mode。\n例如，Guest中的用户代码尝试执行privileged指令，比如读取SCAUSE寄存器，这也会导致trap并走到VMM。但是这种情况下VMM不应该模拟指令并返回，因为这并不是一个User mode中的合法指令。所以VMM需要跟踪Guest当前是运行在User mode还是Supervisor mode，所以在虚拟状态信息里面也会保存mode。\nVMM怎么知道Guest当前的mode呢？\n当Guest从Supervisor mode返回到User mode时会执行sret指令，而sret指令又是一个privileged指令，所以会通过trap走到VMM，进而VMM可以看到Guest正在执行sret指令，并将自己维护的mode从Supervisor变到User。\n虚拟状态信息中保存的另外一个信息是hartid，它代表了CPU核的编号。\n即使通过privileged指令，也不能直接获取这个信息，VMM需要跟踪当前模拟的是哪个CPU。\n实际中，在不同类型的CPU上实现Trap and Emulate虚拟机会有不同的难度。\n不过RISC-V特别适合实现Trap and Emulate虚拟机，因为RISC-V的设计人员在设计指令集的时候就考虑了Trap and Emulate虚拟机的需求。\n举个例子，设计人员确保了每个在Supervisor mode下才能执行的privileged指令，如果在User mode执行都会触发trap。你可以通过这种机制来确保VMM针对Guest中的每个privileged指令，都能看到一个trap。\n学生提问：Guest操作系统内核中会实际运行任何东西吗？还是说它总是会通过trap走到VMM？\nRobert教授：如果你只是执行一个ADD指令，这条指令会直接在硬件上以硬件速度执行。如果你执行一个普通的函数调用，代码的执行也没有任何特殊的地方。所有User代码中合法的指令，以及内核代码中的non-priviledged指令，都是直接以全速在硬件上执行。\n学生提问：在Guest操作系统中是不是也有类似的User mode和Kernel mode？\nRobert教授：有的。Guest操作系统就是一个未被修改的普通操作系统，所以我们在Guest中运行的就是Linux内核或者XV6内核。\n而XV6内核知道自己运行在Supervisor mode，从代码的角度来说，内核代码会认为自己运行在Supervisor mode，并执行各种privileged指令，并期望这些指令能工作。\n当Guest操作系统执行sret指令时，它也知道自己将要进入到User空间。不过在宿主机上，Guest操作系统是运行在User mode，VMM也确保了这里能正常工作。\n但是从Guest角度来说，自己的内核看起来像是运行在Supervisor mode，自己的用户程序看起来像是运行在User mode。\n所以，当Guest执行sret指令从Supervisor mode进入到User mode，因为sret是privileged指令，会通过trap进入到VMM。\nVMM会更新虚拟状态信息中的mode为User mode，尽管当前的真实mode还是Supervisor mode，因为我们还在执行VMM中的代码。\n在VMM从trap中返回之前，VMM会将真实的SEPC寄存器设置成自己保存在虚拟状态信息中的虚拟SEPC寄存器。\n因为当VMM使用自己的sret指令返回到Guest时，它需要将真实的程序计数器设置成Guest操作系统想要的程序计数器值（注，因为稍后Guest代码会在硬件上执行，因此依赖硬件上的程序计数器）。\n所以在一个非常短的时间内，真实的SEPC寄存器与虚拟的SEPC寄存器值是一样的。\n同时，当VMM返回到虚拟机时，还需要切换Page table，这个我们稍后会介绍。\nGuest中的用户代码，如果是普通的指令，就直接在硬件上执行。\n当Guest中的用户代码需要执行系统调用时，会通过执行ECALL指令（注，详见6.3，6.4）触发trap，而这个trap会走到VMM中（注，因为ECALL也是个privileged指令）。\nVMM可以发现当前在虚拟状态信息中记录的mode是User mode，并且发现当前执行的指令是ECALL，之后VMM会更新虚拟状态信息以模拟一个真实的系统调用的trap状态。\n比如说，它将设置虚拟的SEPC为ECALL指令所在的程序地址（注，执行sret指令时，会将程序计数器的值设置为SEPC寄存器的值。这样，当Guest执行sret指令时，可以从虚拟的SEPC中读到正确的值）；\n将虚拟的mode更新成Supervisor；\n将虚拟的SCAUSE设置为系统调用；\n将真实的SEPC设置成虚拟的STVEC寄存器；\n注，STVEC保存的是trap函数的地址，将真实的SEPC设置成STVEC这样当VMM执行sret指令返回到Guest时，可以返回到Guest的trap handler。Guest执行系统调用以为自己通过trap走到了Guest内核，但是实际上却走到了VMM，这时VMM需要做一些处理，让Guest以及之后Guest的所有privileged指令都看起来好像是Guest真的走到了Guest内核\n之后调用sret指令跳转到Guest操作系统的trap handler，也就是STVEC指向的地址。\nTrap-and-Emulate \u0026mdash; Page Table 有关Trap and Emulate的实现还有两个重要的部分，一个是Page Table，另一个是外部设备。\nPage Table包含了两个部分，第一个部分是Guest操作系统在很多时候会修改SATP寄存器（注，SATP寄存器是物理内存中包含了Page Table的地址，详见4.3），当然这会变成一个trap走到VMM，之后VMM可以接管。\n但是我们不想让VMM只是简单的替Guest设置真实的SATP寄存器，因为这样的话Guest就可以访问任意的内存地址，而不只是VMM分配给它的内存地址，所以我们不能让Guest操作系统简单的设置SATP寄存器。\n但是我们的确又需要为SATP寄存器做点什么，因为我们需要让Guest操作系统觉得Page Table被更新了。\n此外，当Guest上的软件运行了load或者store指令时，或者获取程序指令来执行时，我们需要数据或者指令来自于内存的正确位置，也就是Guest操作系统认为其PTE指向的内存位置。\n所以当Guest设置SATP寄存器时，真实的过程是，我们不能直接使用Guest操作系统的Page Table，VMM会生成一个新的Page Table来模拟Guest操作系统想要的Page Table。\n所以现在的Page Table翻译过程略微有点不一样，首先是Guest kernel包含了Page Table，但是这里是将Guest中的虚拟内存地址（注，下图中gva）映射到了Guest的物理内存地址（注，下图中gpa）。\nGuest物理地址是VMM分配给Guest的地址空间，例如32GB。并且VMM会告诉Guest这段内存地址从0开始，并一直上涨到32GB。\n但是在真实硬件上，这部分内存并不是连续的。所以我们不能直接使用Guest物理地址，因为它们不对应真实的物理内存地址。\n相应的，VMM会为每个虚拟机维护一个映射表，将Guest物理内存地址映射到真实的物理内存地址，我们称之为主机物理内存地址（注，上图中的hpa）。\n这个映射表与Page Table类似，对于每个VMM分配给Guest的Guest物理内存Page，都有一条记录表明真实的物理内存Page是什么。\n当Guest向SATP寄存器写了一个新的Page Table时，在对应的trap handler中，VMM会创建一个Shadow Page Table，Shadow Page Table的地址将会是VMM向真实SATP寄存器写入的值。\nShadow Page Table由上面两个Page Table组合而成，所以它将gva映射到了hpa。Shadow Page Table是这么构建的：\n从Guest Page Table中取出每一条记录，查看gpa。 使用VMM中的映射关系，将gpa翻译成hpa。 再将gva和hpa存放于Shadow Page Table。 在创建完之后，VMM会将Shadow Page Table设置到真实的SATP寄存器中，再返回到Guest内核中。\n注，这样的效果是，Guest里面看到的Page Table就是一个正常的Page Table，而Guest通过SATP寄存器指向的Page Table，将虚拟内存地址翻译得到的又是真实的物理内存地址\n所以，Guest kernel认为自己使用的是一个正常的Page Table，但是实际的硬件使用的是Shadow Page Table。\n这种方式可以阻止Guest从被允许使用的内存中逃逸。Shadow Page Table只能包含VMM分配给虚拟机的主机物理内存地址。\nGuest不能向Page Table写入任何VMM未分配给Guest的内存地址。这是VMM实现隔离的一个关键部分。\n学生提问：如果Guest操作系统想要为一个进程创建一个新的Page Table，会发生什么呢？\nRobert教授：Guest会完全按照Linux或者XV6的行为来执行。首先是格式化Page Table Entries以构造一个Page Table。\n之后执行指令将Page Table的地址写入到SATP寄存器，这就是Guest操作系统的行为。\n但是它又不能设置实际的SATP寄存器，因为这是一个privileged操作，所以设置SATP寄存器会触发trap并走到VMM。\nVMM会查看trap对应的指令，并发现Guest要尝试设置SATP寄存器，之后VMM会创建一个新的Shadow Page Table。\nVMM会查看Guest尝试要设置的Page Table的每一条记录，通过gpa-\u0026gt;hpa的映射关系，将gva和hpa的对应关系翻译出来。\n如果Guest尝试使用一个不被允许的物理地址，VMM会生成一个真实的Page Fault。之后VMM会将Shadow Page Table设置到真实的SATP寄存器中，并返回到Guest中。\nShadow Page Table是实现VMM时一个比较麻烦的地方。\n除了设置SATP寄存器，Guest操作系统还有另一种方式可以与Page Table进行交互。\nXV6有时候会直接修改属于自己的Page Table Entry，或者读取PTE中的dirty bit。如果你读了RISC-V的文档，你可以发现在RISC-V上，如果软件更改了PTE，RISC-V不会做任何事情。\n如果你修改了PTE，RISC-V并不承诺可以立即观察到对于PTE的修改，在修改那一瞬间，你完全是不知道PTE被修改了。\n注，这里主要对比的是privileged指令，因为如果在用户空间执行了privileged指令，会立刻触发trap，而这里修改PTE不会有任何的额外的动作\n相应的，文档是这么说的，如果你修改PTE并且希望MMU可以看到这个改动，你需要执行sfence.vma指令，这个指令会使得硬件注意到你对Page Table的修改。\n所以如果你要自己写一个VMM，你在RISC-V上的VMM会完全忽略Guest对于PTE的修改，但是你知道Guest在修改完PTE之后将会执行sfence.vma指令，并且这是一个privileged指令，因为它以s开头，所以这条指令会通过trap走到VMM，VMM就可以知道sfence.vma指令被执行了。\n之后VMM会重新扫描Guest的当前Page Table，查找更新了的Page Table Entry。\n如果修改合法的话，VMM会将修改体现在Shadow Page Table中，并执行真实的sfence.vma指令来使得真实的硬件注意到Shadow Page Table的改动。最后再会返回到Guest操作系统中。\n学生提问：所以MMU只使用了一个Page Table，也就是Shadow Page Table，对吧？这里并没有使用EPT（Extended Page Table），对吧？\nRobert教授：这里还没有EPT。\n学生提问：所以Guest认为它自己有一个Page Table，也就是gva-\u0026gt;gpa，但是这里并没有做任何的翻译工作。VMM通过两个映射关系构建了属于自己的Page Table。\nRobert教授：是的。这里澄清一下，EPT是一种非常不一样的虚拟机实现方式，并且需要硬件的支持。我们这里假设除了对privileged指令触发trap以外，不需要使用任何特殊的硬件支持来构建一个虚拟机。\n学生提问：这里会弄乱direct mapping吗？\nRobert教授：这里不会有direct map。Guest会认为自己有一个direct mapping，但这只是在虚拟的世界里的一个direct mapping，在真实的机器上这不是direct mapping。但是这没有关系，因为我们这里欺骗了Guest使得看起来像是direct mapping。\n学生提问：我们刚刚说过性能的损耗，如果我们使用VMM，对于这里的trap机制看起来也会有大量的性能损耗。\nRobert教授：是的。如果你的操作系统执行了大量的privileged指令，那么你也会有大量的trap，这会对性能有大的损耗。\n这里的损耗是现代硬件增加对虚拟机支持的动机。今天要讨论的论文使用的就是现在硬件对于虚拟机的支持，Intel和AMD在硬件上支持更加有效的trap，或者说对于虚拟机方案，会有少得多的trap。\n所以是的，性能很重要。但是上面介绍的方案，人们也使用了很多年，它能工作并且也很成功，尽管它会慢的多，但是还没有慢到让人们讨厌的程度，人们其实很喜欢这个方案。\nTrap-and-Emulate \u0026mdash; Devices 接下来我们来看Trap and Emulate的最后一个部分，也就是虚拟机的外部设备。\n外部设备是指，一个普通的操作系统期望能有一个磁盘用来存储文件系统，或者是期望有一个网卡，甚至对于XV6来说期望有一个UART设备来与console交互，或者期望有一张声卡，一个显卡，键盘鼠标等等各种各样的东西。所以我们我们的虚拟机方案，需要能够至少使得Guest认为所有它需要的外部设备是存在的。\n这里人们通常会使用三种策略。\n第一种是，模拟一些需要用到的并且使用非常广泛的设备，例如磁盘。\n也就是说，Guest并不是拥有一个真正的磁盘设备，只是VMM使得与Guest交互的磁盘看起来好像真的存在一样。\n这里的实现方式是，Guest操作系统仍然会像与真实硬件设备交互一样，通过Memory Map控制寄存器与设备进行交互。\n通常来说，操作系统会假设硬件已经将自己的控制寄存器映射到了内核地址空间的某个地址上。\n在VMM中不会映射这些内存地址对应的Page，相应的会将这些Page设置成无效。这样当Guest操作系统尝试使用UART或者其他硬件时，一访问这些地址就会通过trap走到VMM。\nVMM查看指令并发现Guest正在尝试在UART发送字符或者从磁盘中读取数据。VMM中会对磁盘或者串口设备有一些模拟，通过这些模拟，VMM知道如何响应Guest的指令，之后再恢复Guest的执行。\n这就是我们之前基于QEMU介绍XV6时，QEMU实现UART的方式。\n在之前的介绍中，并没有UART硬件的存在，但是QEMU模拟了一个UART来使得XV6正常工作。\n这是一种常见的实现方式，但是这种方式可能会非常的低效，因为每一次Guest与外设硬件的交互，都会触发一个trap。但是对于一些低速场景，这种方式工作的较好。如果你的目标就是能启动操作系统并使得它们完全不知道自己运行在虚拟机上，你只能使用这种策略。\n在现代的世界中，操作系统在最底层是知道自己运行在虚拟机之上的。\n所以第二种策略是提供虚拟设备，而不是模拟一个真实的设备。\n通过在VMM中构建特殊的设备接口，可以使得Guest中的设备驱动与VMM内支持的设备进行高效交互。\n现在的Guest设备驱动中可能没有Memory Mapped寄存器了，但是相应的在内存中会有一个命令队列，Guest操作系统将读写设备的命令写到队列中。\n在XV6中也使用了一个这种方式的设备驱动，在XV6的virtio_disk.c文件中，你可以看到一个设备驱动尝试与QEMU实现的虚拟磁盘设备交互。\n在这个驱动里面要么只使用了很少的，要么没有使用Memory Mapped寄存器，所以它基本不依赖trap，相应的它在内存中格式化了一个命令队列。\n之后QEMU会从内存中读取这些命令，但是并不会将它们应用到磁盘中，而是将它们应用到一个文件，对于XV6来说就是fs.image。\n这种方式比直接模拟硬件设备性能要更高，因为你可以在VMM中设计设备接口使得并不需要太多的trap。\n第三个策略是对于真实设备的pass-through，这里典型的例子就是网卡。\n现代的网卡具备硬件的支持，可以与VMM运行的多个Guest操作系统交互。\n你可以配置你的网卡，使得它表现的就像多个独立的子网卡，每个Guest操作系统拥有其中一个子网卡。\n经过VMM的配置，Guest操作系统可以直接与它在网卡上那一部分子网卡进行交互，并且效率非常的高。所以这是现代的高性能方法。\n在这种方式中，Guest操作系统驱动可以知道它们正在与这种特别的网卡交互。\n以上就是实现外部设备的各种策略。\n我认为在实现一个VMM时，主要的困难就在于构建外部设备和设备驱动，并使得它们能正确的与Guest操作系统配合工作。这里或许是实现VMM的主要工作，尤其是当你使用第一种策略时。\n学生提问：我并没有太理解策略一emulation和策略二virtual device的区别。\nRobert教授：它们是类似的。可以这么想，如果你启动了一个完全不知道虚拟机的操作系统，它或许包含了很多磁盘驱动，但是所有的驱动都是为真实硬件提供的。\n如果你想要在虚拟机中启动这样一个操作系统，你需要选择其中一种真实的硬件，并且以一种非常准确的方式来模拟该硬件。\n这种方式并没有问题，只是大部分情况下硬件接口并没有考虑Trap and Emulate VMM下的性能。所以真实的设备驱动需要你频繁的读写它的控制寄存器，而VMM需要为每一次写控制寄存器都获取控制权，因为它需要模拟真实的硬件。\n这意味着每一次写控制寄存器都会触发一次trap走到VMM，并消耗数百个CPU cycles。所以策略一非常的慢且低效。\n策略二并没有卑微地模仿真实的设备，某些设计人员提出了一种设备驱动，这种设备驱动并不对接任何真实的硬件设备，而是只对接由VMM实现的虚拟设备。\n这种驱动设计的并不需要很多trap，并且这种驱动与对应的虚拟设备是解耦的，并不需要立即的交互。\n从功能层面上来说，使用策略一的话，你可以启动任何操作系统，使用策略二的话，如果你想要使用虚拟设备，你只能启动知道虚拟设备的操作系统。\n实际中，策略二是一种标准，并且很多虚拟机的实现方案都能提供。虽然我们并没有在除了QEMU以外的其他场景测试过，XV6中的virtio_disk.c稍作修改或许也可以在其他虚拟机方案上运行。\n学生提问：所以对于每一种主板，取决于不同的磁盘，编译XV6都需要不同的磁盘驱动，是吗？\nRobert教授：是的。我认为或许你可以买到支持virtio_disk驱动的真实硬件，但是大部分的磁盘硬件还不支持这个驱动，这时你需要为真实的硬件实现一种新的驱动。\n策略一：Emulation（设备模拟）\nEmulation 的核心思想是：让虚拟机（Guest）以为它正在与一个真实的物理设备交互，即使实际情况是虚拟机管理器（VMM）在模拟这个设备。\n工作原理：\nGuest 操作系统中的设备驱动程序是为真实硬件编写的，因此它期望设备具备物理设备的接口，比如内存映射的寄存器、IO 端口等。 VMM 需要模拟一个真实的硬件设备，包括这些设备的寄存器、控制信号、状态信息等。当 Guest 操作系统访问这些设备时，它会使用与真实设备相同的方式（例如读写寄存器、内存映射地址）进行交互。 VMM 拦截这些访问，并通过软件模拟真实设备的行为。比如，Guest 操作系统写入磁盘设备的控制寄存器，VMM 会模拟相应的寄存器操作，并将数据写入一个虚拟磁盘文件。 设备交互：\n由于 VMM 拦截 Guest 操作系统对外部设备的访问，每次交互都会引发trap，从而将控制权交还给 VMM，这一过程通常很耗时，因为每个寄存器的读写都需要切换到 VMM 中进行模拟。 性能问题：\n由于每次设备交互都需要 trap 进入 VMM 执行，trap 和 emulation 的频率很高，特别是在对设备进行大量小块数据读写时，性能会受到显著影响。这种方式适用于那些低速或不常被访问的设备（如 UART），但对于高速设备（如磁盘）效率较低。 总结：\nEmulation 适用于那些不需要虚拟化支持的操作系统，因为它们完全不需要知道自己运行在虚拟机中。通过模拟一个真实的硬件设备，VMM 可以让任何 Guest 操作系统与设备交互，几乎不需要修改操作系统的设备驱动。 缺点是每次设备交互都需要频繁 trap，性能较差。 策略二：Virtual Devices（虚拟设备）\n虚拟设备的关键思想是：直接为虚拟机提供一组虚拟化的设备接口，而不是模拟真实硬件设备。操作系统中的设备驱动程序是专门为虚拟设备编写的，而不是为真实硬件设计的。\n工作原理：\n在这种模式下，Guest 操作系统和 VMM 之间的设备交互方式是专门为虚拟环境设计的，跳过了模拟真实硬件的开销。 虚拟设备与 Guest 操作系统之间的交互不再依赖传统的硬件控制寄存器，而是通过共享内存或命令队列来交换数据。例如，Guest 操作系统将操作命令写入内存中的命令队列，VMM 直接读取该队列并执行相应的操作（如读写虚拟磁盘）。 设备交互：\n由于不再需要模拟真实硬件的寄存器和控制信号，虚拟设备不依赖 trap。这种方法允许 Guest 操作系统通过高速、低延迟的方式与 VMM 中的虚拟设备交互。例如，Guest 直接将命令写入共享内存，而 VMM 通过轮询或通知机制来处理这些命令。 VMM 可以设计更高效的交互机制，使 Guest 操作系统与虚拟设备之间的通信更为流畅，减少了频繁 trap 带来的性能开销。 性能优势：\n由于虚拟设备的设计不需要频繁的 trap，交互效率显著提高，尤其是对于需要快速读写的大量数据的设备（如虚拟磁盘、网卡等）。 这种方法通过优化内存交互，减少了 VMM 和 Guest 之间的上下文切换，从而提升了性能。 总结：\nVirtual Devices 是专为虚拟化环境设计的设备接口，只有知道虚拟化存在的操作系统才能使用它。因为虚拟设备不再试图模拟真实硬件，而是使用完全不同的接口，因此需要 Guest 操作系统的设备驱动支持这种虚拟设备。 优点是性能较好，因为减少了 trap 的频率，交互更加高效。缺点是只能在支持虚拟设备的操作系统上运行（比如支持 virtio 的操作系统）。 策略一与策略二的主要区别\n交互方式： 策略一模拟真实的硬件设备，Guest 操作系统按照与物理设备相同的方式进行设备交互，通过 trap 触发 VMM 进行处理。 策略二直接提供虚拟设备接口，Guest 操作系统通过优化的方式（如命令队列）与 VMM 进行交互，减少了 trap 频率。 性能： 策略一频繁的 trap 操作导致性能较差，尤其在处理高速设备时效率低下。 策略二通过虚拟设备设计，减少了 trap 的需求，性能显著提高。 操作系统的适配： 策略一对 Guest 操作系统没有特别要求，它可以运行任何不知晓虚拟化存在的操作系统，因为 VMM 模拟了物理设备。 策略二则需要 Guest 操作系统具备与虚拟设备兼容的驱动程序，比如 virtio 驱动。它不能伪装成真实设备，因此只有知道这些虚拟设备的操作系统才能正确与其交互。 为什么虚拟设备不能伪装成真实设备？\n虚拟设备和真实设备之间的差异在于它们的接口方式。虚拟设备并没有尝试去完全模拟物理设备的寄存器和操作方式，而是设计了更加高效的交互机制。如果虚拟设备伪装成真实设备，等于是走回了策略一的老路，需要模拟寄存器、控制信号等，增加了 trap 的频率，导致性能下降。因此，虚拟设备选择不伪装成真实设备，而是要求操作系统通过新的接口与它们交互，从而提升性能。\n总结：\n策略一：Emulation 模拟真实硬件，可以支持不知晓虚拟化的操作系统，但性能较差。 策略二：Virtual Devices 通过新的接口设计提供更高效的交互，但需要操作系统支持虚拟设备驱动。 策略一：设备模拟（Emulation）\n假设你有一个运行在虚拟机上的操作系统，这个操作系统（Guest）认为它有一个物理磁盘。Guest 操作系统上的磁盘驱动程序是为传统的硬件磁盘设计的，它会按照与物理磁盘交互的方式来访问磁盘。例如，操作系统会通过访问特定的寄存器来向磁盘发出读写命令。寄存器通常是内存映射的，操作系统通过向这些寄存器写入地址或指令来控制磁盘。\n在设备模拟模式下（策略一），VMM 会模拟一个硬件磁盘的行为。当 Guest 操作系统试图通过这些寄存器向磁盘发出命令时，由于这些寄存器并不实际存在，VMM 会捕获这些访问（触发trap），然后通过软件来模拟磁盘设备的行为。VMM 在后台可能将 Guest 系统的读写请求映射到一个虚拟磁盘文件中（例如 QEMU 使用的 .qcow2 或 .img 文件）。所以，从 Guest 的角度看，它好像在和一个真实的磁盘设备交互，但实际上是 VMM 模拟了整个交互过程。\n例子：\nGuest 操作系统向磁盘寄存器写入“读取块 1000”指令。 这个指令触发了一个trap，进入 VMM。 VMM 读取了这条指令，模拟磁盘的行为，并从虚拟磁盘文件中提取块 1000 的数据。 VMM 返回数据，并恢复 Guest 的执行。 这个过程每次需要访问寄存器都需要经过 VMM，性能较低，因为每次访问都涉及大量的 CPU 上下文切换。\n策略二：虚拟设备（Virtual Devices）\n在策略二中，磁盘设备的交互方式被重新设计，Guest 操作系统并不再使用标准的硬件寄存器来访问磁盘，而是使用虚拟化的磁盘接口。最常见的虚拟设备方案是Virtio，它是为虚拟机设计的高效设备接口。\n使用虚拟设备时，Guest 操作系统需要使用支持 Virtio 的磁盘驱动程序。这种驱动不再通过寄存器控制磁盘，而是通过共享内存中的命令队列来传递命令。VMM 和 Guest 共享一个内存区域，Guest 操作系统将所有的磁盘操作指令（例如读、写命令）写入到这个内存队列中，VMM 直接从内存中读取这些命令并处理它们。\n例子：\nGuest 操作系统通过 Virtio 磁盘驱动向共享内存的命令队列中写入“读取块 1000”指令。 VMM 监控这个队列，发现有新的读取请求。 VMM 直接从虚拟磁盘文件中提取块 1000 的数据，并将数据写回到共享内存中的结果缓冲区。 Guest 操作系统从共享内存中读取数据。 由于没有频繁的 trap，VMM 不需要每次都进入内核模式来模拟设备行为，这大幅提升了性能。\n具体场景中的区别\n假设你在虚拟化环境中运行一个高性能数据库服务器，这个服务器频繁地向磁盘读写数据：\n使用设备模拟（策略一）：每次数据库服务器向磁盘发起写操作，VMM 都会模拟磁盘寄存器的访问。由于每次都需要 trap，数据库的性能会受到很大影响，尤其是磁盘 IO 操作较为频繁时，频繁的上下文切换会消耗大量 CPU 资源。 使用虚拟设备（策略二）：Virtio 磁盘驱动通过高效的内存共享方式将读写命令发送给 VMM，VMM 直接从命令队列中读取指令，执行完命令后直接将数据写入内存。这种方式减少了大量的 trap 操作，大大提升了数据库的 IO 性能。 硬件对虚拟机的支持 接下来我将讨论硬件对于虚拟机的支持，这里特指的就是Intel的VT-x。为什么Intel和其他的硬件厂商会为虚拟机提供直接的硬件支持呢？\n首先虚拟机应用的非常广泛，硬件厂商的大量客户都在使用虚拟机\n其次，我们刚刚描述的Trap and Emulate虚拟机方案中，经常会涉及到大量高成本的trap，所以这种方案性能并不特别好。\n第三个原因或许就没那么有趣了。\nRISC-V非常适合Trap and Emulate虚拟机方案，但是Intel的x86处理器的一些具体实现使得它可以支持虚拟化，但是又没那么容易。\n所以Intel也有动力来修复这里的问题，因为它的很多客户想要在x86上运行VMM。\n这里硬件上的支持，是为了让人们能够更容易地构建运行更快的虚拟机。它已经存在了10年左右了，并且现在在构建虚拟机时使用的非常非常广泛。\n在Trap and Emulate方案中，VMM会为每个Guest在软件中保存一份虚拟状态信息，而现在，这些虚拟状态信息会保存在硬件中。\n这样Guest中的软件可以直接执行privileged指令来修改保存在硬件中的虚拟寄存器，而不是通过trap走到VMM来修改VMM中保存在软件中的虚拟寄存器。\n所以这里的目标是Guest可以在不触发trap的前提下，执行privileged指令。\n我们还是有一个VMM在内核空间，并且Guest运行在用户空间。\n当我们使用这种新的硬件支持的方案时，我们的VMM会使用真实的控制寄存器，而当VMM通知硬件切换到Guest mode时，硬件里还会有一套完全独立，专门为Guest mode下使用的虚拟控制寄存器。\n在Guest mode下可以直接读写控制寄存器，但是读写的是寄存器保存在硬件中的拷贝，而不是真实的寄存器。\n硬件会对Guest操作系统的行为做一些额外的操作，以确保Guest不会滥用这些寄存器并从虚拟机中逃逸。\n在这种硬件支持的虚拟机方案中，存在一些技术术语，至少Intel是这么叫的，Guest mode被称为non-root mode，Host mode中会使用真实的寄存器，被称为root mode。\n所以，硬件中保存的寄存器的拷贝，或者叫做虚拟寄存器是为了在non-root mode下使用，真实寄存器是为了在root mode下使用。\n现在，当我们运行在Guest kernel时，可以在不触发任何trap的前提下执行任何privileged指令。\n比如说如果想读写STVEC寄存器，硬件允许我们直接读写STVEC寄存器的non-root拷贝。\n这样，privileged指令可以全速运行，而不用通过trap走到VMM。这对于需要触发大量trap的代码，可以运行的快得多。\n现在当VMM想要创建一个新的虚拟机时，VMM需要配置硬件。\n在VMM的内存中，通过一个结构体与VT-x硬件进行交互。这个结构体称为VMCS（注，Intel的术语，全称是Virtual Machine Control Structure）。\n当VMM要创建一个新的虚拟机时，它会先在内存中创建这样一个结构体，并填入一些配置信息和所有寄存器的初始值，之后VMM会告诉VT-x硬件说我想要运行一个新的虚拟机，并且虚拟机的初始状态存在于VMCS中。\nIntel通过一些新增的指令来实现这里的交互。\nVMLAUNCH，这条指令会创建一个新的虚拟机。你可以将一个VMCS结构体的地址作为参数传给这条指令，再开始运行Guest kernel。 VMRESUME。在某些时候，Guest kernel会通过trap走到VMM，然后需要VMM中需要通过执行VMRESUME指令恢复代码运行至Guest kernel。 VMCALL，这条新指令在non-root模式下使用，它会使得代码从non-root mode中退出，并通过trap走到VMM。 通过硬件的支持，Guest现在可以在不触发trap的前提下，直接执行普通的privileged指令。\n但是还是有一些原因需要让代码执行从Guest进入到VMM中，其中一个原因是调用VMCALL指令，另一个原因是设备中断，例如定时器中断会使得代码执行从non-root模式通过trap走到VMM。\n所以通常情况下设备驱动还是会使得Guest通过trap走回到VMM。这表示着Guest操作系统不能持续占有CPU，每一次触发定时器中断，VMM都会获取控制权。\n如果有多个Guest同时运行，它们可以通过定时器中断来分时共享CPU（注，类似于线程通过定时器中断分时共享CPU一样）。\nVT-x机制中的另外一大部分是对于Page Table的支持。\n当我们在Guest中运行操作系统时，我们仍然需要使用Page Table。\n首先Guest kernel还是需要属于自己的Page Table，并且会想要能够加载CR3寄存器，这是Intel中类似于SATP的寄存器。VT-x使得Guest可以加载任何想要的值到CR3寄存器，进而设置Page Table。\n而硬件也会执行Guest的这些指令，这很好，因为现在Guest kernel可以在不用通过trap走到VMM再来加载Page Table。\n但是我们也不能让Guest任意的修改它的Page Table，因为如果这样的话，Guest就可以读写任意的内存地址。\n所以VT-x的方案中，还存在另一个重要的寄存器：EPT（Extended Page Table）。EPT会指向一个Page Table。当VMM启动一个Guest kernel时，VMM会为Guest kernel设置好EPT，并告诉硬件这个EPT是为了即将运行的虚拟机准备的。\n之后，当计算机上的MMU在翻译Guest的虚拟内存地址时，它会先根据Guest设置好的Page Table，将Guest虚拟地址（gva）翻译到Guest 物理地址（gha）。\n之后再通过EPT，将Guest物理地址（gha）翻译成主机物理地址（hpa）。硬件会为每一个Guest的每一个内存地址都自动完成这里的两次翻译。EPT使得VMM可以控制Guest可以使用哪些内存地址。\nGuest可以非常高效的设置任何想要的Page Table，因为它现在可以直接执行privileged指令。但是Guest能够使用的内存地址仍然被EPT所限制，而EPT由VMM所配置，所以Guest只能使用VMM允许其使用的物理内存Page（注，EPT类似于19.4中的Shadow Page Table）。\n学生提问：我对于硬件中保存的虚拟寄存器有问题，如果你有两个CPU核，然后你想要运行两个虚拟机，你会得到多少虚拟寄存器？\nRobert教授：每一个CPU核都有一套独立的VT-x硬件。\n所以每一个CPU核都有属于自己的32个通用寄存器，属于自己的真实的控制寄存器，属于自己的用在Guest mode下的虚拟控制寄存器，属于自己的EPT，所以你可以在两个CPU核上运行两个不同的虚拟机，它们不会共用任何寄存器，每个CPU核都有属于自己的寄存器。\n学生提问：那也需要一个新的VMM吗？\nRobert教授：VMM可以像一个普通的操作系统一样。XV6可以支持多个进程，并且为每个进程配备一个proc结构体。\n而我们的VMM也会为每个虚拟机配备一个vm结构体，用来跟踪Guest的信息。\n并且，如我之前所说的，如果你只有一个CPU核，但是有3个Guest，可以通过定时器中断结合VMM在3个Guest之间切换。\nDune: Safe User-level Access to Privileged CPU Features 今天要讨论的论文 利用了上一节介绍的硬件对于虚拟机的支持，但是却将其用作其他的用途，这是这篇论文的有趣之处，它利用了这种完全是为了虚拟机而设计的硬件，但是却用来做一些与虚拟机完全无关的事情。\n从一个全局的视角来看这篇论文的内容，它想要实现的是普通的进程。\n所以现在我们的场景是在一个Linux而不是VMM中，但是我们又用到了硬件中的VT-x。\n我们将会在Linux中加载Dune可加载模块，所以Dune作为kernel的一部分运行在Supervisor mode（注，又或者叫做kernel mode），除此之外，内核的大部分还是原本的Linux。\n因为这里运行的是Linux进程，所以我们期望Dune可以支持进程，以及包括系统调用在内的各种Linux进程可以做的事情。\n不过现在我们想要使用VT-x硬件来使得普通的Linux进程可以做一些额外的事情。\nDune会运行一些进程，或者说允许一个进程切换到Dune模式，这意味着，之前的进程只是被Page Table保护和隔离，现在这个进程完全被VT-x机制隔离开了。\n现在进程有了一套完整的虚拟控制寄存器，例如CR3寄存器，并且这些进程可以运行在non-root Supervisor mode，所以它可以在VT-x管理的虚拟状态信息上直接执行所有的privileged指令。\n基于上面的描述，Dune管理的进程可以通过属于自己的CR3寄存器，设置好属于自己的Page Table。\n当然Dune也会控制属于这个进程的EPT，EPT会被设置的只包含这个进程相关的内存Page。\n所以进程可以向CR3寄存器写入任意的Page Table地址，但是因为MMU会在翻译完正常的Page Table之后再将地址送到EPT去翻译，所以进程不能从分配给它的内存中逃逸。\n所以进程并不能修改其他进程或者kernel的内存，它只是有了一种更灵活的设置自己内存的方式。\nDune管理的进程也可以拥有Guest Supervisor mode和Guest User mode，就像一个小的虚拟机一样，并且可以保护运行在Supervisor mode的代码，不受运行在User mode的代码影响。\n论文中提到了可以基于Dune做的两件事情：\n首先，Dune能够在硬件层面支持进程同时拥有Guest Supervisor mode和Guest User mode，这样进程可以在自己的User mode中运行未被信任的插件代码。\n这里的主进程或许是一个网页浏览器，你可以为浏览器下载并运行各种各样的插件，或许是一个新的视频解码器，一个新的广告拦截插件等等。\n但是我们并不能完全信任这个插件，所以我们希望能够在权限受控的前提下运行它。虽然一个普通的Linux也可以达到这个目的，但是会比较麻烦。\n通过Dune，我们可以在Guest User mode下运行插件，同时让网页浏览器运行在进程的Guest Supervisor mode下。因为现在可以修改CR3寄存器，所以可以为Guest User mode配置一个不同的Page Table。\n这样，即使插件是恶意的，进程也可以安全的运行这里的未被信任的插件代码，因为插件代码现在不能任意的读写主浏览器的内存，只能访问网页浏览器指定的某些内存Page。\n进程的Guest User代码可能会执行系统调用，但是这些系统调用会通过trap走到进程的Guest Supervisor mode，而不是Linux内核，所以这里的插件代码或许会认为自己调用了fork/read/write等系统调用，但是实际上这里尝试运行的系统调用通过trap走到了进程对应的网页浏览器，而网页浏览器可以做任意的事情，它可以选择执行或者不执行系统调用。\n所以现在网页浏览器对于插件代码有了完全的控制能力。\n公平的说，这里提到的隔离效果可以通过Linux中一些非常不一样的技术来实现，但是Dune通过使用VT-x硬件，为你可以提供一个特别优雅且有效的实现方式。\n进程可以做的另一个事情是：通过Dune，进程的垃圾回收（Garbage Collect，GC）变得更快了。\n在这个场景中，没有了Guest Supervisor mode和Guest User mode。\n假设我们在运行任意一种带有GC的编程语言，比如说Java或者Python。\nGC可能会很慢，并且本身有着非常非常多的技术可以使得GC变快。许多GC都会扫描并找到位于内存中仍然在使用的对象，扫描会从寄存器中保存的对象指针开始，依次找到所有正在使用对象的所有指针。\n如果在扫描之后没能找到某个对象，那说明这个对象不被任何指针引用，那么它就可以被释放了。\n许多GC会同时在主程序的一个线程中运行，所以GC会从寄存器中保存的指针开始，根据指针之间的树或者图的关系，扫描一个个的对象。\n但是因为GC与程序本身是并行的在运行，所以程序可能会修改GC已经扫描过的对象，这很糟糕，因为这样的话，GC在扫描完成之后确定的要释放和不能释放的对象清单可能就不再正确了。\nDune使用了Page Table Entry中的一个特性来帮助GC检测这样的修改。\nDune管理的进程首先会设置好由VT-x提供的虚拟CR3寄存器，指向属于自己的Page Table，其中的PTE都是有效的。\n每一条PTE的dirty位，表明对于对应的Page存在写操作。所以如果程序在GC的过程中修改了某些对象，那么对应PTE的dirty位会被设置为1。\n当GC查找完所有的对象之后，它会查看所有PTE中的dirty位，找到包含了可能修改过的对象的内存Page，然后再重新扫描这些对象。\n实际中，获取PTE dirty位的过程在普通的Linux中既困难又慢，我甚至都不确定Linux是否支持这个操作，在一些其他操作系统中你可以通过系统调用来查询PTE的dirty位。\n但是如果你使用Dune和 VT-x，进程可以很快的使用普通的load和store指令获取PTE，进而获取dirty位。所以这里，Dune使得某些需要频繁触发GC的程序明显变得更快。\n学生提问：如果Guest User mode中的插件程序想要运行自己的GC会怎样？\nRobert教授：现在我们使用了Dune，并且有一个进程是被Dune管理的。\n这个进程通过VT-x实现了Supervisor mode和User mode，我们在User mode运行了一个插件，并且插件也是由带GC的编程语言写的，所以它有属于自己的Page Table，并且其中的PTE也包含了dirty位。\n但是刚刚说的GC加速在这不能工作，因为Dune会将插件运行在Guest User mode，而就像普通的User mode一样，Guest User mode不允许使用CR3寄存器。\n所以在Guest User mode，我们不能快速的访问PTE的dirty位。只有在Guest Supervisor mode，才能通过CR3寄存器访问Page Table。所以，并不能同时使用以上Dune提供的两种功能。\n学生提问：如果某人基于Dune写了个浏览器，那么对于不支持Dune的计算机来说就很难使用这样的浏览器，对吗？就像很难直接让Chrome使用Dune，因为不是所有的计算机都有这个内核模块。\nRobert教授：首先，这里提到的内容需要运行在支持VT-x的计算机上，也就是说底层的计算机必须支持VT-x，所以需要VT-x来运行Dune。\n其次Dune需要被加载来运行浏览器以利用前面说到的特性。\n所以是的，你需要将所有的东西都设置好。并且Dune是一个研究项目，它的目标是使得人们去思考可以部署在真实世界，并且有足够的价值的一些东西。\n就像Linux一样，Linux有成千上万个功能，如果某人决定将Dune添加到Linux中作为一个标准功能，那么我们就可以依赖这个功能，并且Chrome也可以直接用它了。\n学生提问：所以从整体来看，这里就像是创建了一个VM，但是实际上运行的又是一个进程？\nRobert教授：你可以这么描述。这里主要是对进程进行抽象，但是这里没有用Page Table硬件来时先进程间的隔离（注，其实也使用了，但是主要不依赖Page Table硬件），这里使用的是CPU上的硬件来支持进程，这里说的CPU上的硬件就是VT-x，它包含了一些额外的功能，例如设置好属于进程的Page Table。\n学生提问：论文里提到了，如果Dune管理的一个进程fork了，那就会变成一个不被Dune管理的进程，这不会是一个安全漏洞吗？\n比如说你通过Dune运行了一个进程，并且认为它现在是安全的。但是fork之后的进程因为不被管理所以可能会逃逸。\nRobert教授：Dune管理的进程的Guest Supervisor mode中，不存在安全的问题。这部分代码已经拥有了相应的权限，通过fork也不会获得更多的权限。\n但是另一方面，Dune的Guest User mode代码中，我们有未被信任的代码，如果让它在没有Dune管理的情况下运行会有一定的风险。\n所以这部分代码不能fork，如果它尝试执行fork系统调用，会通过trap走到进程的Guest Supervisor mode。\n假设进程的Guest Supervisor mode部分代码写的非常的小心，并且不会被欺骗，那么它不会执行fork，所以这时fork不能工作。\n如果Supervisor mode的代码允许fork，它会调用Linux的fork系统调用，并得到一个fork进程包含了与原进程有相同的内存镜像，所以我们在新进程中包含可能是恶意的插件代码。\n如果新进程没有意识到Dune已经被关闭了，那么原来的Supervisor mode中的privileged指令会是非法的。\n所以我们需要假设Dune管理的进程里面的Supervisor mode部分代码能够足够的小心且足够的聪明，来阻止User mode中的插件代码执行fork。\n学生：被Dune管理的进程拥有Supervisor mode并没有不安全，因为它实际上是non-root mode下的Supervisor mode，就像是Guest操作系统中的Supervisor mode一样，你可以让它做任何事情，因为VT-x的存在，进程就像是一个虚拟机一样，并不能伤害到真正的操作系统。\nRobert教授：是的，进程不能逃逸出来，因为存在EPT，而EPT会限制进程的地址空间。\n学生提问：在VT-x的方案中，当我们访问Page Table时，因为我们需要通过EPT进行第二层翻译，将Guest物理内存地址翻译到Host物理内存地址，这样从Page Table返回的延时是不是增加了？\nRobert教授：这里可能会花费更多的时间让硬件MMU来翻译内存地址。\n在最坏的情况下，比如在RISC-V中，会有多层Page Table，MMU需要一层一层的去查找PTE，x86下同样也有多层Page Table，所以在x86中首先会查找主Page Table，如果要访问更多的内存地址，每一次内存地址的访问都需要再次走到EPT，而EPT也是一个多层的Page Table。\n所以我并不知道最差情况下需要访问Page Table多少次才能完成翻译，但是很明显在VT-x下会比普通情况下差得多。不过实际中会有cache所以通常不会走到最坏的情况。\n学生提问：今天的虚拟机还是普遍会更慢吗？如果是的话，AWS是怎么工作的，因为看起来还挺快的，并且工作的也很好。\nRobert教授：我认为他们使用了硬件上的VT-x支持，并且使用了我们讨论过的一些功能，这样使得AWS虚拟机比较快，或者并不比真实的计算机慢多少。\n学生提问：我对于Trap and Emulate中的Shadow Page Table有个问题，每次都会创建Shadow Page Table吗？难道不能记住上次的Shadow Page Table吗？\nRobert教授：VMM需要创建新的Shadow Page Table以供真实的硬件使用。当然在很多时候都可以增加缓存，对于一个聪明的VMM，它可以注意到Guest更新了一个PTE，VMM可以做相应的有限的工作来更新Shadow Page Table。\n如果机器是在多个虚拟机上分时复用的，VMM会为还在运行的虚拟机保存Shadow Page Table，这样这些虚拟机可以在恢复时直接重用。\n学生提问：这难道不是意味着VMM为每个虚拟机中的每个进程都保存了Shadow Page Table的拷贝？\nRobert教授：是的，虚拟机里面有很多很多个Page Table，所以维护Shadow Page Table需要大量的工作。\n而类似于VT-x的硬件支持使得这部分工作更加的容易了，因为EPT表示你不用构建Shadow Page Table了。\n学生提问：我有个问题有关GC的，如果有dirty位的话需要重新扫描对象，那么有没有可能会无限扫描？\nRobert教授：是的，这有个问题，如果一直有对象在更新，扫描能正常结束吗？\n实际中，GC会先扫描一次，之后它会冻结除了GC线程以外的其他线程，所以这期间不可能再发生任何其他的变更。之后GC才会查看所有PTE的dirty位，但是因为其他所有线程都冻结了，所以不可能会有更多的dirty位了，所以GC查看了所有的dirty位，之后结束GC会结束扫描并创建需要释放对象的列表，最后再恢复所有之前冻结的线程的执行。GC是一个复杂的流程，Dune的论文中并没有足够的篇幅讨论它。\n","date":"2024-10-04T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08119virtual-machines/bg_hu2165606678065125661.jpg","permalink":"https://echudet.github.io/p/mit6s08119virtual-machines/","title":"MIT6S081：19Virtual Machines  "},{"content":"Lecture 18 OS organization 课前预习 论文速览 论文的架构大致分为以下几个部分，每个部分的内容总结如下：\n引言 (Introduction)\n本节介绍了微内核（µ-kernel）的背景，特别是第一代微内核系统因其性能低下和灵活性不足而受到批评。随后引入了L4，这是一种第二代精简的微内核，旨在通过减少内核中的功能来提高效率。作者的目标是通过实验表明基于微内核的系统可以在实际中使用，并且具有良好的性能。文章对比了L4与Linux以及MkLinux（基于Mach微内核的Linux版本），通过基准测试来衡量性能差异。\n相关工作 (Related Work)\n本节介绍了与微内核系统相关的研究历史，并对已有的实验和实现进行总结，包括对Mach、Chorus等微内核的描述。作者指出，过去的大量评估工作集中在操作系统部分功能的加速上，而对完整Unix系统实现的全面评估较少。文章的贡献在于探讨如何在第二代微内核上移植现有的单体操作系统，并评估其性能。\nL4基本概念 (L4 Essentials)\nL4微内核的设计围绕线程和地址空间展开。它支持跨地址空间的通信（IPC），这是L4的核心机制之一。本节详细介绍了L4中的递归地址空间构建、页管理、I/O端口的映射以及硬件中断的处理方式。L4使用用户级的调页器来处理页错误，实现了灵活的内存管理策略。此外，还讨论了L4在不同硬件架构（如Pentium、Alpha、MIPS）上的移植情况，强调了L4的跨平台特性。\nL4上的Linux (Linux on Top of L4)\n本节描述了将Linux移植到L4上的过程。L4Linux是作为一个用户级单服务器运行在L4微内核之上，且保持与Linux的二进制兼容。作者详细解释了系统调用的实现、设备驱动程序的处理、进程的调度等。此外，文章讨论了L4Linux设计中的一个错误（dual-space），并解释了它如何影响性能。最终，通过更简化的设计（single-space），实现了更高效的L4Linux实现。\n性能评估 (Performance Evaluation)\n本节通过微基准测试（如getpid系统调用）、宏基准测试（如Linux内核编译）和AIM基准测试来评估L4Linux的性能。结果表明，L4Linux的性能仅比原生Linux差5%-10%，而且明显优于基于Mach的MkLinux。性能评估表明，底层微内核的效率对整个系统性能有显著影响。\n扩展性评估 (Extensibility Evaluation)\n本节探讨了通过µ-kernel扩展系统功能的可能性。通过几个实验（如管道通信、内存管理操作）展示了在µ-kernel上进行系统扩展的潜力，尤其是在不影响已有Unix功能的前提下引入新的功能，如实时内存管理系统。实验表明，通过µ-kernel实现的扩展性不仅能提升性能，还能支持多种不同应用场景的高效共存。\n替代的基本概念 (Alternative Basic Concepts)\n本节讨论了是否可以通过引入更底层的机制（如保护性控制传输PCT）来进一步提高µ-kernel的性能。作者分析了PCT与IPC的性能差异，并认为虽然PCT理论上应该更快，但经过优化的IPC在很多实际场景中表现得更好。此外，文章还探讨了其他改进µ-kernel性能的方法，如引入\u0026quot;grafting\u0026quot;模型。\n结论 (Conclusion)\n文章总结了L4微内核在性能和扩展性方面的优势，表明µ-kernel可以在性能接近原生操作系统的情况下，支持更灵活和高效的系统扩展。同时，文章指出未来的工作应继续探索µ-kernel在不同平台和应用场景中的适用性。\n基于线程和地址空间的核心设计思想 L4微内核的核心设计思想是基于线程和地址空间，并通过跨地址空间通信（IPC）机制实现线程间的高效消息传递。同时，L4微内核允许用户级的服务器递归地构建地址空间，并通过这种方式实现灵活的内存管理。让我具体说明这个过程的实现。\n跨地址空间通信（IPC）\nIPC（Inter-Process Communication）是L4微内核中最基本、最核心的机制之一，用于支持线程间在不同地址空间中的通信。每个线程都在特定的地址空间中执行，而当需要进行线程间通信时，L4微内核提供了一种安全且高效的方式来进行同步消息传递。\nIPC的基本步骤\n发送方线程通过IPC将消息发送到接收方线程所在的地址空间中。这是同步的，即发送方会等待接收方的响应。 IPC消息可以携带多种信息，除了普通的消息数据外，还可以包含地址空间中的页面映射信息，允许在不同地址空间之间共享内存页。 通过这种方式，L4可以实现远程过程调用（RPC）以及受控的线程迁移。 递归地址空间构建\nL4微内核通过支持递归地址空间构建来实现内存的灵活管理。最初的地址空间（即σ0σ0）代表了整个物理内存，其他地址空间都是在此基础上由用户级服务器逐步构建的。\n地址空间构建的基本过程\n映射与解除映射：地址空间的构建是通过映射（mapping）、授予（granting）和解除映射（unmapping）三种操作来实现的。L4中使用了一种叫做“柔性页”（flexpages）的概念，这些页的大小可以从一个物理页（通常是4KB）扩展到整个地址空间（几GB）。 映射（Mapping）：用户态的内存管理程序可以将自己的某些页面映射到另一个地址空间，从而使该页面可以在多个地址空间中访问。 授予（Granting）：拥有某个地址空间的线程可以将页面的完全控制权授予另一个地址空间，从而允许接收方也能够修改或进一步映射该页面。 解除映射（Unmapping）：拥有页面的地址空间可以随时解除该页面的映射，取消其他地址空间对该页面的访问权限。 递归地址空间的安全性：这些操作是安全的，因为它们是基于虚拟页面的，而不是直接作用于物理内存。也就是说，线程只能映射或解除映射它已经拥有的页面。这样，L4确保了跨地址空间的操作不会破坏系统的安全性。 页面错误处理与用户级分页器\n当线程访问了尚未映射的页面时，会发生页面错误。在L4微内核中，内核会通过IPC将该页面错误传递给与该线程相关联的分页器（Pager）。分页器通常是一个用户态的内存管理程序，负责处理页面错误并决定如何分配内存。\n当页面错误发生时，L4微内核会向相应的分页器发送消息，通知其处理该页面错误。分页器可以通过映射新页面来解决错误，也可以选择拒绝该请求。\n由于分页器的实现是完全由用户定义的，L4支持灵活的内存管理策略。这意味着，系统可以根据需要实现不同的内存管理模型，如实时内存管理或分层缓存管理。\n输入/输出端口与硬件中断的处理\n除了内存页面，L4还将I/O端口视为地址空间的一部分，使得它们可以像内存页一样进行映射和解除映射。L4通过将硬件中断转换为消息的形式，实现了中断处理的用户级实现。当硬件中断发生时，微内核会将中断转化为一条消息，发送给相应的线程，这为用户级设备驱动程序的实现提供了基础。\nL4微内核通过跨地址空间通信（IPC）实现线程间的高效消息传递，而递归地址空间的构建则为灵活的内存管理提供了强大的支持。用户级分页器和灵活的映射/解除映射机制允许L4实现多种内存管理策略，同时确保系统的安全性和效率。\nL4的二级缓存 在计算机系统中，缓存（Cache）是一种快速存取的存储器，用于存储频繁使用的数据，以提高系统性能。通常，缓存分为多级，例如一级缓存（L1 Cache）和二级缓存（L2 Cache），它们的主要区别在于速度、容量以及与处理器的距离。\n一级缓存（L1 Cache） 二级缓存（L2 Cache） 位置 一级缓存通常内置于处理器的核心内，每个核心都有自己的一级缓存。 二级缓存位于处理器与内存之间，可能是每个核心独立拥有，也可能是多个核心共享的。 速度 一级缓存是处理器最快的缓存，因为它最接近处理器核心，访问延迟非常低。 二级缓存比一级缓存稍慢，但仍比内存快得多。 容量 一级缓存容量较小，一般为几十KB。它主要用于存储最频繁使用的指令和数据。 二级缓存的容量通常比一级缓存大得多，一般为几百KB到几MB。它用于缓存一级缓存未命中的数据或指令。 分区 一级缓存通常分为两个部分：一个用于存储指令（指令缓存，L1I），一个用于存储数据（数据缓存，L1D）。 当处理器在一级缓存中找不到所需数据时，会去二级缓存中查找。如果二级缓存也没有命中，才会访问内存。 二级缓存的作用\n二级缓存通过缓存较大数据块，减轻了处理器访问主存（RAM）的频率，因此可以显著提高处理器的执行效率。虽然二级缓存的访问速度不及一级缓存，但因为其容量较大，能够存储更多的数据，减少了缓存未命中的次数，从而进一步提高性能。\n一级缓存与二级缓存的区别\n访问速度：一级缓存比二级缓存快，但二级缓存仍然远远快于主存。 容量：一级缓存容量较小，而二级缓存容量更大，能够存储更多的数据。 位置：一级缓存紧靠处理器核心，二级缓存则稍远，通常在核心外或是多核心共享。 结合 xv6 缓存实现\n在 xv6 操作系统中，缓存的一个典型实现是缓冲区缓存（Buffer Cache），它通过在内存中缓存磁盘块的数据，来减少磁盘 I/O 操作的开销。xv6 中的缓存机制与 L1 和 L2 缓存有一些相似之处：\n缓存层级：虽然 xv6 中的缓冲区缓存不是严格意义上的 L1 或 L2 缓存，但它在存储体系中起到缓存的作用，用于临时存储从磁盘读取的数据。类似于 L1 和 L2 缓存，缓冲区缓存通过缓存经常访问的数据块，减少了对慢速磁盘的直接访问。 缓存命中与未命中：在缓冲区缓存中，当进程请求一个数据块时，系统首先检查该块是否已经在缓冲区缓存中。这类似于处理器首先在 L1 或 L2 缓存中查找数据。如果命中，则直接返回缓存中的数据，避免了磁盘 I/O；如果未命中，则需要从磁盘读取数据块并将其存入缓存，以备后续访问。 性能优化：缓冲区缓存的设计目标之一就是通过减少对磁盘的频繁访问来提高性能，这与硬件缓存设计中的目标相似。L1 和 L2 缓存的主要目的也是通过减少对主存的访问来提高处理器的执行效率。 在L4上的二级缓存管理\n论文中的二级缓存管理指的是控制二级缓存的分配，以提高实时应用的最坏情况执行时间。具体而言，通过在L4的用户级实现中，分配和隔离部分二级缓存的页面，减少了任务之间的缓存干扰。这个机制类似于xv6中的缓冲区缓存系统，只不过在L4中，这种缓存隔离更为灵活，能够动态分配缓存资源给实时任务，确保它们不被其他任务干扰，最终提升系统的可预测性。\n总的来说，一级缓存与二级缓存的主要区别在于速度、容量和作用范围，而这与xv6中的缓冲区缓存在系统性能优化中起到的作用有相似之处。通过缓存热数据并减少慢速存储访问次数，缓存系统能够显著提升系统整体性能。\nLower-level Communication Primitive 低级通信原语Lower-level Communication Primitive，顾名思义，是一种与操作系统或硬件紧密相关的基础通信机制。它比传统的高层抽象（如 L4 中的 IPC，进程间通信）更贴近硬件的运行方式。这些低级原语直接操作硬件或硬件相关的机制，理论上能够比高层抽象提供更高的性能和更少的延迟。一个例子是 Exokernel 中的 protected control transfer（PCT，保护性控制传输），它允许在不同地址空间之间进行快速的控制转移，类似于硬件级别的中断或函数调用。\nL4 中的 IPC 是一种高层抽象，用于在不同地址空间的线程之间进行通信。它是一种同步的消息传递机制，带有内核级别的安全保护，同时允许实现更复杂的通信模式，如远程过程调用（RPC）。虽然 IPC 是一种非常强大的通信方式，但由于它是一种高层抽象，可能在一些极端情况下性能不如更直接的低级通信原语。\n在论文中的实验中，研究人员想通过在 Alpha 处理器上实现一种类似于 Exokernel 的保护性控制传输（PCT）的低级通信原语，来测试这种更底层的机制是否能够在性能上超越 L4 的 IPC。虽然理论上低级通信原语应该更快，因为它省去了高层抽象的开销，但实验结果表明，通过优化，L4 的 IPC 仅比 PCT 慢了几个 CPU 周期。因此，在很多实际应用场景中，L4 IPC 的性能表现已经非常接近低级原语。\n结论：L4 的 IPC 和低级通信原语不是同一层级的机制。L4 的 IPC 属于操作系统的高层抽象，提供了更丰富的功能和安全性，而低级通信原语则直接面向硬件，属于更基础的系统层级。L4 IPC 的优势在于它能够通过提供同步消息传递和安全的跨地址空间通信来实现复杂的操作系统功能，而低级通信原语则更适合于对性能要求极高且不需要高层抽象的特殊场景。\nL4中的异常（Exceptions）与xv6中的陷阱（Trap）的区别 在L4中，异常和陷阱与线程的执行是同步的，也就是说，异常会在引发它的线程的执行过程中同步触发。在Pentium处理器上，L4将处理器的异常处理机制分配给每个线程。当某个线程发生异常时，处理器会将指令指针和标志寄存器的值压入该线程的用户态栈，然后调用该线程的用户态异常或陷阱处理程序。L4的内核会简单地将这些异常镜像到用户态，允许用户态处理这些异常。\n与此相对，xv6中的trap机制是操作系统内核捕捉处理器异常、系统调用和中断的机制。不同于L4的同步异常处理，xv6中的陷阱是由内核来处理的。每当发生一个trap，处理器会跳转到内核的trap处理程序（通常是由硬件触发的中断或系统调用），内核会根据陷阱类型采取不同的动作（例如，系统调用会进入对应的系统调用处理函数，硬件中断会跳转到设备的中断处理程序等）。xv6不会把这些异常直接反射到用户空间，而是由内核直接管理这些异常。\n主要区别在于，L4将异常处理推送给用户态线程，而xv6中，所有的trap（包括异常和系统调用）都由内核处理。L4的设计强调最小化内核功能，将更多的处理移交给用户态，这也是微内核设计的核心思想。\n在L4的Pentium实现中，“small address spaces”（小地址空间）是一种优化机制。它利用了Pentium处理器的段机制，通过共享所有页表物理上对齐的小地址空间（范围在4 MB到512 MB），模拟标签化的TLB，用于在小地址空间之间进行上下文切换时避免不必要的TLB刷新。这个机制能够在不影响系统功能的情况下提高性能，尤其是在频繁上下文切换时有效。\n当线程访问数据超出了当前小地址空间的范围时，内核会自动将其切换回正常的3 GB地址空间模型。每个任务中的线程可能会使用不同的地址空间模式，即一些线程使用正常的大地址空间，另一些则使用相应的小地址空间。这种机制仅用于优化，目标是减少TLB刷新和提升IPC性能。\n总结一下：\nL4中的exception处理直接交由用户态线程处理，而xv6则由内核捕捉并处理异常和系统调用。 small address spaces是L4中的一种性能优化，通过在小地址空间中模拟TLB标签化来减少TLB刷新，提高系统效率，尤其是在频繁上下文切换场景下有显著作用。 L4中的Small Address Space是如何通过模拟标签化TLB减少TLB刷新？ 在L4的Pentium实现中，small address spaces（小地址空间）是一种专门的优化机制，旨在减少TLB（Translation Lookaside Buffer，翻译后援缓冲区）的刷新频率，进而提高性能。TLB的作用是缓存虚拟地址到物理地址的映射，以加速虚拟内存的访问。当发生上下文切换时，如果不同的进程使用不同的虚拟地址空间，TLB必须被刷新，以防止一个进程的虚拟地址被误用于另一个进程。这种TLB刷新的过程会引入性能开销。\n标签化TLB的概念 标签化TLB（tagged TLB）是一种通过为不同的地址空间分配标签来避免频繁刷新TLB的机制。每个进程的地址空间都有一个唯一的标签，TLB可以根据这些标签区分不同的虚拟地址空间。当进程切换时，TLB不会被完全刷新，而是根据标签来区分哪些条目属于当前进程。因此，即使发生了上下文切换，只要进程的标签匹配，TLB中的条目依然有效，减少了重建TLB的开销。\n在Pentium架构上，L4使用段机制来模拟标签化TLB。当进程使用的地址空间较小时（比如4MB到512MB），L4能够通过共享所有页表并使用Pentium的段机制来保护这些地址空间，避免了频繁的TLB刷新。这一过程模拟了标签化TLB的效果：\n共享页表：当一个线程的地址空间很小时，所有这些小地址空间可以共享同一个物理页表，这意味着在不同进程或线程之间切换时，页表保持不变，从而减少了对TLB的刷新需求。 使用段机制保护：Pentium的段机制允许L4通过段寄存器来限制线程对其内存的访问范围。当地址空间较小时，L4可以通过将这些小地址空间映射到特定段中，并使用段机制保护这些地址空间。这样，当线程在这些小地址空间之间切换时，不需要完全刷新TLB，因为段机制提供了额外的保护。 上下文切换中的优化：当上下文切换发生在使用小地址空间的线程之间时，由于页表是共享的且通过段机制进行保护，内核无需刷新TLB。这种模拟的标签化TLB机制显著减少了上下文切换中的TLB刷新开销，提高了系统性能。 通过共享页表并使用段机制保护小地址空间，L4避免了每次上下文切换时的TLB刷新。通常情况下，上下文切换会导致TLB被清空，因为不同进程有不同的地址空间，而TLB中的虚拟地址到物理地址的映射可能会失效。通过使用段机制和共享页表，L4能够确保不同小地址空间的线程可以无缝切换而不刷新TLB，这就大大减少了上下文切换的开销，特别是在频繁发生上下文切换的环境中。\n总结：小地址空间的优化在L4中通过共享页表和段机制来模拟标签化TLB，减少了上下文切换时的TLB刷新，提升了系统性能。\nPALcode 在L4的Alpha实现中，PALcode（Privileged Architecture Library Code）是Digital公司设计的一段特殊固件，它在Alpha处理器上运行，用于执行特权级别的操作。这段代码帮助操作系统与底层硬件进行交互，通常被用来执行一些与硬件直接相关的操作，如寄存器的设置、中断处理、上下文切换等。\nL4微内核在Alpha处理器上的实现是完全重写了Digital原有的PALcode，目的是优化L4在Alpha平台上的性能。PALcode负责处理一些时间紧迫的操作（例如：上下文切换、TLB操作等），这些操作会通过手动调优来减少时间消耗，并且直接在PALcode中执行。而对于那些需要较长时间且可以被中断的操作，这些操作会先进入PALcode，然后切换到内核模式，剩余部分使用标准的机器指令执行。\nPALcode是一种低级别的固件代码，通常用来处理与硬件相关的关键操作，确保这些操作能够在不干扰内核复杂性的情况下快速执行。L4选择将一些紧急的操作在PALcode中完成，是为了最大限度地减少操作开销，而对于复杂的、可以被中断的操作，则会交给内核进一步处理，这样可以在不牺牲灵活性的同时实现性能优化。\n紧急操作（如上下文切换等）通常与处理器的硬件机制紧密相关，PALcode可以直接控制这些硬件机制，从而快速完成操作，而不需要经过内核的复杂调度系统。因此，紧急操作在PALcode中执行，确保快速响应。\n而对于长期操作（例如可以被中断的操作），则在PALcode中启动后，切换到内核来处理剩余部分。这样做的好处是利用PALcode的高效处理硬件操作的能力，同时借助内核的丰富功能来处理复杂的操作逻辑。这样分层处理可以在确保系统灵活性的同时优化关键路径上的性能。\nPALcode是处理硬件相关操作的一个固件层，在L4/Alpha的实现中，它主要负责高效地完成紧急操作，减少时间开销。而对于可以被中断的长期操作，则由PALcode进入内核模式，利用内核的丰富功能完成操作。\nL4Linux中的三类系统调用接口 在L4Linux中，用户进程无法直接访问Linux内核，而是通过L4微内核的IPC机制与Linux服务器（作为用户态任务运行的Linux内核）进行通信。这三类系统调用接口是为了让用户进程能够向Linux服务器发起系统调用请求的不同方式。\n下面详细说明这三种接口，并配合示例帮助理解。\n修改后的标准共享库，使用L4 IPC调用Linux服务器\n这种方式修改了Linux标准的共享库函数，例如libc库中的系统调用函数。每当用户进程调用这些库函数时，这些函数不再直接执行传统的Linux系统调用，而是通过L4的IPC机制与运行在用户态的Linux服务器通信。\n例子：假设用户进程调用write()系统调用，将数据写入文件。在L4Linux中：\nwrite()函数调用不再直接调用Linux内核的系统调用入口。 相反，经过修改的libc库中的write()函数会打包该请求，然后通过L4 IPC将请求发送给Linux服务器。 Linux服务器收到这个IPC请求后，模拟传统的Linux内核来执行相应的系统调用，处理写文件操作。 最后，Linux服务器通过IPC将执行结果返回给用户进程。 这一机制允许用户进程无缝地与Linux服务器通信，而不需要内核态的系统调用机制。\n修改后的静态库版本\n与共享库类似，静态库版本是为了那些不使用共享库的程序提供的。静态库直接链接到应用程序中，应用程序在编译时将这些修改过的系统调用代码包含在可执行文件中。\n这种方式特别适合那些在编译时就打包所有依赖库的程序，它的工作方式与共享库类似，但在性能上可能有稍微的优势，因为不需要运行时动态加载库。\n例子：假设用户进程调用open()系统调用打开一个文件：\n在编译时，用户进程链接的是一个修改过的静态库版本，该版本中的open()函数通过L4 IPC发送请求给Linux服务器。 Linux服务器处理这个请求，模拟原始的Linux系统调用，执行文件打开操作。 结果通过IPC返回给用户进程。 与共享库不同的是，静态库在编译时已经确定，运行时不需要再加载外部库文件，减少了运行时的开销。\n用户级异常处理程序模拟原生的系统调用陷阱指令\n这是一种更底层的机制，允许通过模拟传统的系统调用陷阱（Trap）指令来实现系统调用。通常在传统的Linux中，系统调用是通过陷阱指令（如int 0x80或syscall）触发内核模式的系统调用入口的。L4Linux中，通过用户态异常处理程序来捕获这些系统调用陷阱，并将它们转换为L4 IPC请求发送给Linux服务器。\n例子：假设用户进程调用fork()，在传统的Linux中，这通常是通过一条系统调用陷阱指令进入内核的：\n用户进程发出一个陷阱指令，例如int 0x80，原本这会导致处理器切换到内核模式并调用内核中的fork()系统调用处理程序。 在L4Linux中，L4的用户级异常处理程序捕获了这个陷阱指令，而不会直接进入内核模式。 异常处理程序通过L4 IPC将这个系统调用请求发送给Linux服务器。 Linux服务器接收该请求，执行fork()操作，并将结果通过IPC返回给用户进程。 这种方式模拟了传统系统调用的行为，使得不需要修改应用程序本身，只需用户态的异常处理程序来捕获并转换陷阱指令。\n总结与对比\n修改后的标准共享库：通过修改libc等系统调用库，使用L4 IPC直接与Linux服务器通信。这种方法简单且高效，适合大多数应用场景。 修改后的静态库版本：与共享库方式相似，但在编译时链接到可执行文件中。它适合那些不依赖共享库的应用程序，能够在运行时减少一些开销。 用户级异常处理程序：更底层的方式，通过捕获用户态的系统调用陷阱，模拟内核行为。这种方法使得系统调用与传统Linux兼容性最好，但可能在性能上略低于其他两种方式，因为它需要捕获和处理异常。 这三种方式各有其使用场景，结合使用能够满足不同的需求。\narchitecture 在这段文字中，\u0026ldquo;architecture-dependent\u0026rdquo; 和 \u0026ldquo;architecture-independent\u0026rdquo; 中的 \u0026ldquo;architecture\u0026rdquo; 主要是指 硬件体系结构，特别是不同的 CPU 架构（如 x86、Alpha、SPARC 等）。换句话说，\u0026ldquo;architecture\u0026rdquo; 指的是不同的硬件平台，它不仅包括 CPU 结构，还可能涉及内存管理、I/O 设备等与特定硬件直接相关的功能。\nArchitecture-dependent 部分：是指那些与特定硬件（如 CPU、内存、I/O 等）密切相关的内核功能。这些部分需要根据不同硬件平台进行移植和调整，比如中断处理、设备驱动、地址空间管理等低级操作。这些功能完全封装了底层的硬件架构，确保 Linux 能够在不同的硬件平台上运行。这是 Linux 需要修改的地方，尤其当要移植到新平台时。 Architecture-independent 部分：是指那些与硬件架构无关的通用功能，比如进程管理、资源管理、文件系统、网络子系统等。无论是 x86 还是其他架构，这些部分都可以在不同平台之间保持不变，不需要特别修改。它们占据了 Linux 内核代码的大部分（约 98%），因此对于内核移植来说，绝大部分代码是通用的。 所以，\u0026ldquo;architecture\u0026rdquo; 主要指的是 CPU 及相关硬件架构，而不是 Linux 内核本身。不同的硬件平台可能需要调整内核的 architecture-dependent 部分，但大多数通用功能都属于 architecture-independent 部分，不需要变动。\nLinux as server 虽然 µ-kernel 和 Linux 都是内核，但它们的作用和架构是不同的。\nµ-kernel（微内核）：是一种内核设计方法，旨在将操作系统的核心功能（如进程管理、内存管理、中断处理等）精简到最低限度，只保留一些基础服务，其他功能则放在用户态中实现。微内核的目标是模块化、灵活性和可移植性，它本身只负责一些基本任务（比如进程切换、消息传递等）。 Linux：是一个传统的宏内核（monolithic kernel），意味着它的内核包括了大量的功能（如文件系统、设备驱动、网络协议等），这些功能都是运行在内核态的。 尽管 µ-kernel 和 Linux 都是内核，但它们并不是互相排斥的。Linux 可以被移植到 µ-kernel 上的原因在于将 Linux 作为一个用户态进程运行在 µ-kernel 上，而不是直接替换或拆解 Linux 的内核部分。\n具体来说，移植过程如下：\nL4 µ-kernel 提供底层支持：L4 作为一个微内核，提供基本的内核功能（如进程调度、内存管理、消息传递等），这些功能足够支持上层应用的运行。 Linux 运行在用户态：Linux 本身被设计为一个用户态的服务，运行在 µ-kernel 提供的框架之上。它本身仍然提供文件系统、网络等高级功能，只是这些功能从传统的内核态移到了用户态。 单服务器模型：通过单一的 Linux 服务器进程，Linux 的所有用户进程在 µ-kernel 上得到服务。这种单服务器的模型意味着，所有 Linux 用户进程通过该服务器进程进行系统调用和操作。 因此，Linux 并没有被拆解成各个小部分融入 µ-kernel，而是整体上作为一个用户态的服务来运行。这种方法不仅保持了二进制兼容性（即任何 Linux 软件仍然可以运行），还检验了在这种微内核架构下，能否不通过针对 µ-kernel 的深度优化就实现良好的性能。\n这种架构的好处在于，通过 µ-kernel 提供底层支持，Linux 可以保持其高层功能模块的一致性，而不需要针对每种硬件做太多的修改。\n可以这么理解：在移植到 µ-kernel 上时，Linux 原来内核中与底层硬件直接交互的部分会被替代或重新封装，因为这些功能（如进程调度、内存管理、消息传递等）已经由 L4 µ-kernel 提供了。也就是说，Linux 内核中与硬件紧密耦合的底层部分会被 µ-kernel 接管，从而让 Linux 可以运行在 µ-kernel 之上，成为一个用户态的服务。更具体地讲：\n底层内核功能被 µ-kernel 接管：原本由 Linux 内核直接负责的功能，例如进程调度、内存管理、消息传递机制等，这些底层硬件交互的操作将由 µ-kernel 负责处理。所以在 µ-kernel 的架构下，Linux 内核中的这些部分被剥离掉或者不再直接发挥作用。 Linux 的上层功能保留：Linux 内核中仍然保留了很多操作系统需要的高层功能模块（如文件系统、网络协议栈、设备驱动接口等），这些功能继续提供服务，只是它们运行在用户态，而不是传统内核态。这个被移植后的 Linux 内核整体上看起来更像是一个“单服务器”，提供操作系统的一些高级功能。 ABI 和系统调用保持不变：虽然 Linux 在 µ-kernel 上运行，但是它的应用程序二进制接口（ABI）保持不变，用户态进程通过类似传统 Linux 的系统调用接口与该“单服务器”交互。这意味着，从用户的角度来看，程序依然像是在一个传统的 Linux 系统上运行。 所以，Linux 原来内核中的部分确实被替换为 µ-kernel 提供的功能，但并不是全部被剔除。Linux 的核心功能模块被保留下来，并在用户态作为一个整体服务运行，而 µ-kernel 负责最底层的系统功能。这样，Linux 就从一个完整的操作系统内核变成了一个运行在 µ-kernel 上的用户态服务。\n可以把它想象成 Linux 成为一个服务器，我们通过与 µ-kernel 交互来间接获得 Linux 这台“服务器”提供的服务。更具体来说：\nµ-kernel 是底层管理者：µ-kernel 负责最底层的硬件交互、进程调度、内存管理和消息传递等功能。这些功能是操作系统核心中的基础部分。 Linux 作为单服务器进程：在这种架构下，Linux 被看作是一个单一的服务器进程，它提供文件系统、网络、设备驱动等高层操作系统服务。所有的用户进程不再直接与硬件交互，而是通过 µ-kernel 进行中转。 交互方式：用户进程首先通过系统调用或消息传递与 µ-kernel 交互，µ-kernel 再将请求转发给运行在用户态的“Linux 服务器”进程。这台“服务器”处理高级功能的请求，如文件系统操作、网络通信等，并将结果返回给 µ-kernel，最终传递回用户进程。 这种设计不仅保持了 Linux 高层服务的功能性，还允许 µ-kernel 作为底层的调度器和资源管理器，提供了模块化和更灵活的系统架构。\n总的来说，µ-kernel 是操作系统的核心支柱，Linux 作为其中的一个重要服务提供操作系统的大部分功能，类似于用户通过 µ-kernel 访问“Linux 服务器”来获取服务。\ntop half 和 bottom half 在 Linux 中，中断处理分为 top half 和 bottom half，它们协同工作来处理硬件中断，保证系统高效运行。\nTop Half（中断上半部） Bottom Half（中断下半部） 快速处理高优先级任务：当硬件中断发生时，top half 立即响应。它的目的是快速处理与中断相关的最关键部分，确保硬件能够尽快恢复正常工作。 延迟处理非关键任务：由于 top half 只负责处理最关键的部分，剩下的一些非实时性或耗时较长的任务会被推迟到 bottom half 来执行。 典型任务：包括确认中断、读取硬件状态、清除中断标志，以及做一些非常简单且关键的工作。 典型任务：可能包括数据包处理、调度更多任务、进行较复杂的硬件状态处理等。总之，任何不需要立即执行、但仍然需要对中断进行完整处理的工作都会放在 bottom half。 优先执行，优先级最高：top half 是在中断发生时首先被调用的。它是以高优先级执行的，因为它直接由硬件中断触发，且需要快速完成以避免影响系统的实时性。 在 top half 完成后执行：bottom half 会在所有相关的 top half 完成后被调用，确保硬件中断的快速处理。 不可抢占：top half 不会被其他 top half 或 bottom half 中断。它以最高优先级执行，其他中断会被屏蔽（在 SMP 系统中，一般是局部屏蔽），直到该 top half 处理完成。 低优先级：bottom half 的优先级通常低于 top half，但高于正常的用户态进程。它可以被其他 top half 中断，但不会被其他 bottom half 中断。 执行流程与关系\n硬件中断触发：硬件中断（如网络包到达、键盘输入等）会触发 µ-kernel 产生一个消息，该消息被映射到一个中断事件。 top half 执行：与该中断源相关联的 top half 处理程序（thread）会被触发。此时，top half 会以最高优先级运行，快速处理最关键的中断逻辑。 记录状态，推迟非关键任务：top half 在完成了对中断的快速处理后，会将剩余的任务（非关键任务）记录下来，通常是放入一个待处理队列。 bottom half 执行：当所有 top half 都完成后，专门的 bottom half 处理线程 会被触发，开始处理那些被推迟的非关键任务。此时，系统不再受硬件中断的影响，因此 bottom half 可以更从容地执行较为复杂的逻辑。 优先级关系：为了确保系统响应及时，top half 具有最高优先级，其次是 bottom half，最后才是普通的用户进程和 Linux 服务器进程。 总结\ntop half 和 bottom half 的关系：top half 负责快速处理中断，确保最关键的工作能够立刻完成。而 bottom half 则延迟处理耗时或非关键的任务，确保系统高效运行。 执行顺序：硬件中断 → top half 执行 → bottom half 执行 → 继续用户进程或 Linux 服务器进程。 这样划分的目的在于，最小化系统停滞的时间，即确保中断处理不会长时间阻塞其他系统操作，同时又能及时完成对硬件中断的响应。\nL4 的 copyin 和 copyout 机制 L4 的 copyin 和 copyout 机制看起来与传统 Linux 内核的做法相似，但实际上有一些关键区别，导致了性能差异。为什么 L4 的方法在这种情况下更有效率：\n在传统的 Linux/x86 中，用户态地址空间总是映射到内核态地址空间中。这样，copyin 和 copyout 操作只需要通过简单的内存拷贝来完成，地址转换是由硬件自动处理的。这种方法的优点是拷贝操作本身非常简单，不需要在软件层面进行额外的地址转换，因为硬件已经将用户地址映射到内核空间了。\n在 L4Linux 中，copyin 和 copyout 采用的是物理地址拷贝。这意味着在每次拷贝操作时，L4Linux 需要通过服务器内部的逻辑页表手动解析用户态的虚拟地址，并将其转换为“物理地址”（实际上是指在 L4Linux 服务器的地址空间中对应的地址）。完成地址转换后，再进行基于物理地址的内存拷贝。\n表面上看，这两种方法的区别似乎不大，都是在用户态和内核态之间进行数据拷贝，但实际的性能影响来源于 虚拟地址空间的管理方式 和 硬件开销：\n硬件开销的不同：在传统的 Linux/x86 中，虽然地址映射是由硬件自动处理的，但每次进行内核态和用户态的切换时，硬件需要处理大量的上下文切换，这包括重新加载页表等操作。在 L4 这样高度模块化的系统中，这些上下文切换的开销更为显著，导致传统方法在 L4 环境下表现不佳。 L4 的页表管理方式更轻量：L4 采用了一种更加轻量级的虚拟内存管理机制，它避免了频繁的硬件上下文切换。通过在软件层面手动解析地址（即通过逻辑页表将虚拟地址映射为物理地址），L4Linux 可以减少硬件参与的地址转换操作。这种方式虽然看似增加了软件上的处理负担，但却避免了硬件在上下文切换上的高开销，从而在实际应用中提高了整体性能。 内核态和用户态的隔离：L4 微内核架构中的内核态和用户态有更加严格的隔离，这种隔离在提高安全性和模块化的同时，也使得传统的全局地址空间映射在性能上不再适用。L4Linux 的物理地址拷贝机制更加符合 µ-kernel 的架构特点，在模块化的设计下能够更有效地处理数据交换。 L4 的 copyin 和 copyout 机制通过在软件中手动解析虚拟地址，从而减少了硬件上下文切换的开销。虽然传统 Linux/x86 的方法在硬件上显得更简单，但它在 L4 的 µ-kernel 架构下表现不佳，因为上下文切换的硬件开销过大。而 L4 通过物理地址拷贝的方式能够更好地适应这种高度模块化的微内核架构，因此效率更高。\n用户态和内核态的转换\nL4 在 copyin 和 copyout 时：L4 的 copyin 和 copyout 操作不需要进行传统的用户态和内核态的转换，因为这些操作通常是在 L4Linux 服务器进程中完成的。L4 采用的是用户态服务（比如 L4Linux 作为一个单服务器进程运行在用户态），而 µ-kernel 本身并不直接参与大量的用户态和内核态转换。\n什么时候 L4 需要用户态和内核态转换：\n在 L4 中，用户态和内核态的转换主要发生在 L4 微内核提供的基本系统服务中，例如进程调度、消息传递等。L4 作为一个微内核，其职责是提供最低限度的核心功能，而 Linux 服务器和其他用户态服务都运行在用户态。 L4 仅在处理用户进程请求 µ-kernel 服务时，或者需要执行内核态任务时，才需要执行用户态和内核态的转换。相比传统 Linux 内核，L4 这种结构减少了内核态和用户态切换的次数，从而提升性能。 页表和 TLB（Translation Lookaside Buffer）\nL4 的页表：在 L4Linux 的环境下，L4 微内核依然依赖页表机制进行地址翻译，但它的设计更加轻量化，可能不会直接使用传统的硬件页表机制。对于 copyin 和 copyout 操作，L4Linux 使用了 逻辑页表 来手动管理虚拟地址到物理地址的映射，而不依赖硬件来进行地址转换。\nTLB 和硬件地址转换：L4 微内核和 L4Linux 并没有完全放弃 TLB 和硬件地址转换功能，特别是在处理用户态进程的时候。页表和 TLB 仍然是操作系统的一部分，但由于 L4Linux 作为一个用户态服务运行在 µ-kernel 上，L4Linux 服务器在与 µ-kernel 交互时可能不需要频繁依赖硬件的 TLB 机制。在某些情况下，L4Linux 通过软件的逻辑页表完成了大部分的地址转换，这减少了对硬件 TLB 的依赖。\nL4Linux 是否完全不需要硬件页表和 TLB？\n并不是完全不需要：L4Linux 仍然需要 TLB 和硬件页表转换来支持 用户态进程的地址空间管理，特别是涉及到进程的虚拟内存管理时。L4 的轻量级设计和 L4Linux 的逻辑页表只是减少了对硬件地址转换的依赖，并不是完全不使用这些功能。\n当 L4Linux 服务器需要处理大量的进程时，TLB 和硬件地址翻译依然是不可或缺的，特别是在提高多进程或多线程系统的性能方面。\n总结\nL4 不依赖用户态和内核态切换进行 copyin 和 copyout，这些操作通常发生在用户态的 L4Linux 服务器中。\nL4 仅在处理核心服务时需要用户态和内核态转换，而大部分操作在用户态下完成。\n逻辑页表用于 L4Linux 服务器中的地址映射操作，这减少了对硬件 TLB 和页表机制的依赖，但 L4 及其用户态进程仍然需要硬件支持的页表和 TLB 进行地址翻译。\nTLB（Translation Lookaside Buffer）是一个缓存，专门用来加速虚拟地址到物理地址的转换。它并不是直接存储整个页表，而是存储一部分最近使用的虚拟地址到物理地址的映射。现在来详细解释为什么更换页表的时候需要flush TLB。\nTLB 与页表的关系 TLB 并不包含完整的页表，它只是对一部分页表内容的缓存。每次 CPU 需要进行地址转换（将虚拟地址转换为物理地址）时，首先会在 TLB 中查找。如果找到了虚拟地址的映射，这就是“TLB 命中”，CPU 直接使用 TLB 中的映射来访问物理内存，速度很快。如果没有命中，则会发生“TLB 未命中”，这时 CPU 就需要去页表中查找相应的虚拟地址映射并加载到 TLB 中，这个过程相对较慢。\n为什么更换页表需要 flush TLB 当操作系统切换进程或者更改页表时，意味着虚拟地址到物理地址的映射发生了改变。此时，TLB 里面缓存的映射可能已经过时或不再有效。如果我们继续使用这些过期的映射，CPU 可能会把错误的物理地址当作有效的进行访问，导致数据访问错误甚至系统崩溃。\n因此，每当页表发生变化时，需要将 TLB 中旧的条目清除掉，这个操作称为“flush TLB”。这确保了后续的地址转换操作不会使用到已经无效的虚拟地址到物理地址的映射。\nflush TLB 的具体作用 flush TLB 的目的并不是把页表的内容“装载”进 TLB，相反，它是为了清除 TLB 中的无效条目。更换页表之后，TLB 重新开始缓存新的页表条目。在新的地址映射被频繁访问时，它们会逐渐被加载进 TLB，但 TLB 并不会一下子缓存整个页表。\n总结 TLB 是页表的一种缓存，它只保存一部分页表条目。 当页表发生变化时，TLB 中的缓存可能会失效，继续使用这些失效的映射会导致错误的地址转换。 flush TLB 是为了清除旧的映射，保证 CPU 在新的映射生效时不会使用错误的地址映射。 所以，更换页表的时候，flush TLB 不是把页表条目装载进 TLB，而是为了防止使用无效的映射，确保地址转换的正确性。\nEmulation Library 在 L4Linux 的上下文中，“emulation library” 通常指的是一个用于支持系统调用或信号处理的库。它模拟了传统 Linux 系统调用或行为，以便在 L4 这种不同内核架构下运行标准的 Linux 应用程序。它可以让 Linux 应用程序以为它们在一个完整的 Linux 环境中运行，虽然底层实际上是 L4 微内核。\n修改 Signal Thread 和 Emulation Library 只会影响自身线程 在 L4Linux 中，每个 Linux 用户进程都会拥有一个单独的 Signal Thread（信号处理线程）和相关的 Emulation Library。这些组件只与该用户进程共享同一个地址空间，因此它们彼此间的修改不会对其他进程造成影响。具体原因如下：\n地址空间隔离：在 L4 中，信号线程和 emulation library 仅在与主线程共享的地址空间中运行。任何对这些线程或库的修改都不会超出这个特定的地址空间。因此，修改它们可能会影响这个进程的正常运行，但不会影响其他进程或系统的整体稳定性。 进程的自我损害：由于这些组件都在用户进程自己的地址空间内，只有该进程本身能够访问并修改它们。这样，即使进程修改了这些组件，只会影响到它自己，不会对其他进程造成干扰。 主线程进入 Linux Server 线程 这里的“主线程进入 Linux Server 线程”的意思并不是指主线程被暂停，然后由 Linux Server 线程取代它的执行。实际上，它指的是 主线程进入了 Linux 的处理模式，即从用户态切换到内核态，准备响应信号并进入 L4Linux 系统服务的处理流程。具体过程如下：\n信号到达：Linux Server 发送信号给信号处理线程（Signal Thread）。 信号处理线程修改主线程的状态：信号线程通过调整主线程的堆栈指针和指令指针，使其进入 Linux 内核态。这一动作相当于让主线程进入内核的信号处理逻辑。 主线程执行信号处理：主线程从修改后的堆栈和指令指针开始，进入 Linux 的信号处理流程，这个过程被称为“进入 Linux”，是从用户态到内核态的一个过渡，而不是完全暂停或切换成其他线程。 完成信号处理后恢复执行：主线程在完成信号处理后恢复正常执行（即用户态执行）。 因此，“进入” 指的是主线程进入了一个内核态的上下文处理过程，而不是实际进入 Linux Server 线程。Linux Server 本身还是在独立的线程中运行的，主线程只是通过被信号通知进入了一个特定的信号处理状态。\n“进入 Linux”是否指的是单个线程对应的 Linux server 状态的改变，实际上 L4 内核并没有真正改变用户态和内核态的概念。你的理解基本是对的。以下是对这种机制的进一步解释：\nL4 微内核的用户态和内核态\n在传统的 Linux 系统中，用户态（user mode）和内核态（kernel mode）是操作系统的两个运行模式。在用户态，程序的权限有限，不能直接访问硬件或内核资源；而在内核态，操作系统具有完全的控制权，可以执行特权指令和访问所有系统资源。\n但在 L4 微内核 架构中，内核（µ-kernel）的角色非常小，它只处理非常底层的任务，如进程调度、消息传递和内存管理等。L4 的设计减少了用户态与内核态之间的频繁切换，因为大多数操作系统的功能（例如文件系统、驱动程序等）是在用户态的服务中完成的（例如 L4Linux 作为用户态服务运行）。因此，L4 系统中内核态和用户态的区分相对弱化了很多，只有在极少数情况下才需要用户态和内核态的切换。\n\u0026ldquo;进入 Linux\u0026rdquo; 的过程\n在 L4Linux 中，当一个用户进程需要进入内核态来执行特权操作（例如处理信号），它实际上是从 L4Linux 服务器接收处理指令。在这个过程中：\nL4 内核本身不会进行用户态和内核态的切换。L4 内核保持运行在自己的模式中，只负责处理消息传递和基本的调度等操作。 \u0026ldquo;进入 Linux\u0026rdquo; 意味着用户进程（比如主线程）开始进入 L4Linux 服务器进程内部的信号处理逻辑（或者其他内核态相关的操作），这个操作由 L4Linux 服务器来管理，而不是 L4 微内核本身。 简而言之：\n在传统 Linux 中，\u0026ldquo;进入内核态\u0026rdquo; 是一个涉及特权级别变化的硬件操作。 而在 L4Linux 上，这更多是一个用户态服务逻辑的转换，意味着主线程开始处理本应该在传统 Linux 内核态执行的功能，但 L4 内核本身并不会进行实际的用户态到内核态的切换。 结论：“进程进入 Linux”确实主要是指 L4Linux 服务器进程内部状态的改变，而 L4 微内核本身并不改变用户态和内核态的划分。L4 系统的设计本质上就是通过减少这种切换来提升性能和简化操作系统的架构。\n两个执行路径 在L4Linux环境中，主线程会有一张页表，这张页表主要用于管理用户进程的地址空间，并映射该进程可以访问的虚拟地址到实际的物理地址。这意味着：\n主线程可以在自己的地址空间中执行它有权限访问的用户态代码。这个用户态代码是直接通过页表进行映射的，主线程可以直接在它的上下文中运行这些代码。 但是，这不包括Linux内核级别的代码，因为在L4Linux架构下，Linux本身被封装为一个用户态进程运行在L4微内核之上。也就是说，L4Linux服务器（即Linux内核）不会直接通过页表暴露给每个用户进程。主线程仅可以访问其自身的用户态代码，而非Linux内核的代码。 因此，主线程的页表不会直接映射到Linux服务器的代码空间中，它主要用于运行用户态代码，而无法直接执行与内核态相关的代码。\n在L4Linux架构中，主线程要执行需要Linux内核权限的代码（例如系统调用、信号处理等）时，通常会通过 IPC（进程间通信） 与 L4Linux服务器 进行交互。这是L4系统中非常重要的机制：\n当主线程需要进行系统调用或处理信号时，它会将这些请求封装成 IPC消息，并发送给L4Linux服务器（Linux内核进程）。 L4Linux服务器 接收到这些消息后，会在它自己的上下文中处理这些请求（即执行Linux内核中的相应代码）。 处理完成后，L4Linux服务器会通过IPC回复主线程，从而让主线程继续执行。 这意味着，主线程并不会直接执行Linux服务器的代码，而是通过IPC来请求Linux服务器的服务。L4微内核通过这种方式实现了用户态与L4Linux服务器（内核态）之间的隔离，减少了用户态与内核态之间的频繁切换，提升了系统的性能和安全性。\n总结：\n主线程通过页表直接执行的代码 仅限于用户态代码，无法直接执行L4Linux服务器的内核态代码。 主线程与L4Linux服务器的交互主要依赖IPC，当主线程需要系统级操作时，它通过发送消息给L4Linux服务器，由服务器代为执行内核级的代码。 这种架构设计使得L4Linux能够高效地在L4微内核上运行，同时确保了系统的模块化和隔离性。\nCoroutine（协程） 协程（Coroutine）是一种更轻量的并发执行单元，它与线程类似，但有一些关键的不同。协程通常运行在用户空间，由程序手动调度，而线程则通常由操作系统调度。\n协程的特点：\n手动切换：协程是通过程序中的某些机制显式切换的，而不是像线程那样由操作系统隐式调度。当一个协程执行到一定位置时，它可以主动让出控制权，并且保存执行状态，以便稍后恢复执行。\n非抢占式调度：协程不像线程或进程那样抢占系统资源，它们可以在任务之间主动切换，而不会**作系统强制中断。这减少了上下文切换的开销。\n协程 vs 线程 vs 进程\n进程 线程 协程 进程是操作系统分配资源和独立运行的最基本单元。每个进程拥有独立的内存空间，并且进程之间不能直接共享内存。 线程是进程内部的执行单元，一个进程可以包含多个线程。线程共享进程的内存空间，但每个线程都有自己独立的执行栈。 协程比线程更轻量。它们在同一线程的上下文中运行，并且由程序来调度而不是操作系统。它们共享同一个线程的栈帧，切换时只需保存和恢复少量的状态。 进程的上下文切换通常比较昂贵，因为涉及到虚拟内存空间的切换。 线程的上下文切换比进程轻量一些，但仍然涉及到保存和恢复CPU寄存器、堆栈等信息。 协程的上下文切换开销非常低，因为它不涉及操作系统的调度机制。 协程的例子\n异步编程：现代编程语言（如 Python 中的 async/await）常常使用协程来实现异步编程。协程允许程序在等待 I/O 操作（如网络请求、文件读写）时不阻塞线程，而是切换到其他任务继续执行。例子:\n1 2 3 async def fetch_data(): await some_io_operation() # 模拟 I/O 操作，协程让出控制权，等待操作完成 print(\u0026#34;Data fetched\u0026#34;) 游戏开发中的协程：在游戏开发中，协程常常用于处理时间敏感的任务。例如，Unity 引擎中的协程可以用于控制动画的播放或延迟事件的执行。例子：\n1 2 3 4 IEnumerator WaitAndPrint() { yield return new WaitForSeconds(2); // 延迟2秒 print(\u0026#34;2 seconds later\u0026#34;); } 总结：协程是一种轻量级的并发机制，允许程序在不依赖操作系统的调度的情况下管理多任务执行，减少了上下文切换的开销。相比线程和进程，协程更加高效，尤其在I/O密集型和异步任务处理中表现良好。\nL4Linux 的调度策略 支持“将 schedule() 函数的调用延迟到进程的时间片耗尽”的机制，主要在于 L4Linux 的调度策略，以及 µ-kernel 的轻量级结构和协程机制。以下是原因和实现方式：\n延迟 schedule() 调用的机制\n减少协程切换：L4Linux 中每个系统调用完成后，如果没有需要立即处理的内核任务或其他协程的切换请求，系统并不会立即调用 schedule() 来切换到另一个任务，而是让当前用户线程继续执行。这种做法的好处是减少了不必要的调度器调用，从而最小化了协程的上下文切换，减少了开销。\n基于时间片调度：系统设置了一个固定的时间片，每个线程在时间片到期之前可以连续执行任务。L4Linux 延迟调用 schedule()，意味着只在当前用户线程的时间片耗尽后，才会强制进行调度。这种调度策略与传统 Linux 类似，减少了不必要的调度器调用，提升了性能。\n支持用户进程在时间片内进行多次系统调用\n系统调用的执行不导致阻塞：L4Linux 的设计允许用户进程在其时间片内可以进行多个系统调用，而不必等待内核中的其他任务或协程切换。这是因为在时间片内系统不会立即调用 schedule() 切换到其他任务，而是尽可能让当前进程继续运行。除非有更高优先级的任务请求处理，比如中断，否则会尽量避免打断当前进程的执行。\n避免频繁上下文切换：系统调用完成后，L4Linux 并不会立即切换到其他任务，而是恢复用户进程的执行状态。这种方式避免了传统系统中每次系统调用可能触发上下文切换的情况，用户进程可以连续执行多个系统调用，直到时间片用尽才进行调度切换。这个特性减少了切换带来的性能损耗，提高了系统调用密集型任务的执行效率。\n具体执行方式：\n在系统调用完成后，如果没有内核中断或其他需要立即处理的任务，L4Linux 服务器会检查 reschedule 标志。若该标志没有被设置，系统不会立即调用 schedule()，而是恢复用户线程的执行。这种行为让用户进程在当前时间片内尽可能长地持续执行，且允许多个系统调用连续完成。 这种方式允许系统在一个时间片内完成多个系统调用，而不会在每次调用后立即进入调度切换。这提高了系统的整体吞吐量，因为多个系统调用可以在单个时间片内无缝地执行而不被中断。 总结：通过延迟 schedule() 调用至时间片耗尽，L4Linux 可以减少协程切换的频率，从而提高系统调用密集型任务的执行效率。这种方法在用户进程时间片内允许多个系统调用连续执行而不被调度器打断，从而避免了频繁的上下文切换，达到了提升性能的效果。\n系统调用完成后延迟 schedule() 的这种方法在现代操作系统中看似是一种常见的优化，但实际上，它的出色之处在于减少不必要的调度器调用，特别是在处理多任务并发时可以有效提升性能。让我们仔细看一下这个方法的背景以及为什么它能够带来改进。\n传统系统中的调度行为\n在传统的操作系统（如早期版本的 Linux）中，系统调用完成后，调度器可能会立即进行上下文切换，这取决于当时的系统状态和调度策略。例如：\n每次系统调用完成后，内核会评估是否有其他任务需要更高优先级的处理。如果有，调度器会立即切换到另一个进程或线程。 这种方式可能会导致频繁的上下文切换，尤其是在系统调用密集的场景下。上下文切换虽然使系统可以多任务运行，但过于频繁的切换会带来较高的开销（如保存和恢复寄存器状态、页表等），降低整体性能。 现代操作系统的优化策略\n现代操作系统中，特别是在高性能系统设计中，通过延迟 schedule() 调用，只在时间片用尽时进行调度，这是为了避免每次系统调用后立即进行上下文切换的开销。这个优化之处包括：\n减少上下文切换：传统方法可能会在每个系统调用后立即触发调度，而现代系统通过延迟 schedule() 调用，允许用户进程在其时间片内连续执行多个系统调用，而不会被过早中断。这减少了每个系统调用之后的上下文切换次数，进而减少了开销。 提高系统吞吐量：在 I/O 密集型任务或需要频繁系统调用的场景下，延迟调度能够使用户进程在单个时间片内完成更多工作（如多个系统调用）。这提高了系统的整体吞吐量，因为 CPU 花费在上下文切换上的时间被最小化，更多时间被用于实际工作。 L4Linux 的创新\nL4Linux 的这种方式虽然与现代操作系统相似，但它的设计针对微内核环境进行了优化：\n协程和调度结合：L4Linux 将系统调用后的调度延迟到时间片耗尽，结合了协程的轻量级特性，减少了频繁的上下文切换。由于协程相比传统线程切换开销更小，整个系统的调度开销进一步降低。 微内核架构的需求：在 L4 微内核上，许多操作（如系统调用）通过 IPC 完成，L4Linux 通过延迟调度器调用可以减少跨用户空间和内核空间的频繁切换，适合 µ-kernel 的高性能需求。 早期操作系统的情况\n在一些早期的操作系统或某些特定调度策略下，系统调用后立即触发调度确实可能发生，这在多任务环境下会引发频繁的上下文切换，导致性能瓶颈。例如：\n如果有大量进程同时发出系统调用，每个系统调用完成后，操作系统可能会认为需要重新调度来确保公平性，导致频繁的切换。 尤其在早期的 Linux 版本中，调度器的调度策略没有进行足够的优化，会导致这种问题。 总结：L4Linux 中延迟 schedule() 调用的策略虽然看起来像是现代操作系统的一种标准优化，但它的出色之处在于适应了 L4 微内核的架构，特别是减少了协程切换和上下文切换的频率，提高了系统的整体效率。在某些早期的操作系统中，系统调用后立即调度的方式曾是常见的，但这种方式的高开销在现代系统中逐渐被优化设计所取代。\n在所有地址空间中代码和数据的虚拟分配相同会最大化独立应用程序之间的 TLB 冲突 TLB 是一种高速缓存，用来加速虚拟地址到物理地址的转换。每次内存访问时，处理器首先查找 TLB，如果找到匹配的虚拟地址条目，就可以快速获取对应的物理地址，避免慢速的页表查找。\n在操作系统中，不同应用程序通常有各自的虚拟地址空间。但是，如果所有应用程序的代码和数据在虚拟地址空间中的分配位置相同（也就是说，所有程序的代码和数据都映射到相同的虚拟地址区间），会导致这些独立应用程序在 TLB 中的条目彼此冲突。\n假设我们有两个独立的应用程序 A 和 B：\n应用 A 的代码和数据被映射到虚拟地址 0x1000-0x2000。 应用 B 的代码和数据也被映射到相同的虚拟地址 0x1000-0x2000。 这时，两个应用程序在运行时，虽然它们的物理地址是不同的，但因为它们的虚拟地址相同，TLB 在缓存这些地址映射时只会保留最后一次的映射。当应用 A 在运行时，TLB 缓存了 A 的虚拟地址到物理地址的映射。然而，当应用 B 开始运行时，它的虚拟地址 0x1000 也会被写入 TLB，覆盖了 A 的映射。随后，当 A 再次运行时，发现 TLB 中的映射已经被 B 覆盖，就会导致TLB 未命中，系统不得不重新查找页表来更新 TLB。\n这种频繁的 TLB 未命中会导致性能下降，因为每次发生这种情况时，处理器都要执行耗时的页表查找。\n当所有应用程序的代码和数据在虚拟地址空间中的分配完全相同时，每个应用程序在 TLB 中的映射都会覆盖前一个应用程序的映射。由于 TLB 条目有限，不同应用程序之间频繁切换时，会频繁刷新和重新加载 TLB，从而最大化冲突。这种现象在同时运行多个进程或频繁上下文切换的场景中尤为显著，特别是在时间片较短或上下文切换频繁的情况下。\n为了解决这个问题，可以通过为不同应用程序构建不同的虚拟地址布局，避免所有应用程序使用相同的虚拟地址。例如，L4Linux 允许仿真库和信号线程被映射到应用程序附近的低地址空间，而不是总是使用高地址空间。通过这样的方法，可以减少不同进程之间的 TLB 冲突，提升性能。\n执行时间比较大的过程的排名 在描述的过程中，涉及多个系统操作的开销。根据常见情况和上述分析，以下是这些操作按通常的时间花费从大到小的大致排名：\nTLB 未命中导致的页表查找\n花费原因：每次 TLB 未命中（TLB miss），处理器都必须进行页表查找，这涉及内存访问，并且需要多级页表的遍历。这一过程相对耗时，特别是在频繁上下文切换或不同应用程序共享虚拟地址布局时，频繁发生未命中会带来显著开销。\n相对开销：时间消耗较高。\n频繁的上下文切换\n花费原因：每次上下文切换，系统都需要保存当前进程的寄存器状态，并恢复目标进程的状态。这一过程涉及 CPU 寄存器状态和内存管理数据的操作。当上下文切换频繁且时间片较短时，累积的时间开销较大。\n相对开销：高，但略低于 TLB 未命中。\nTLB 刷新（TLB Flush）\n花费原因：TLB 刷新会清除当前 TLB 的所有条目，导致后续访问都可能需要重新建立 TLB 工作集，带来额外的页表查找和 TLB 加载。与 TLB 未命中相似，这种操作的开销取决于 TLB 的大小和具体的页表配置。\n相对开销：通常低于频繁的 TLB 未命中，但在切换回原地址空间时依然显著。\n协程切换\n花费原因：协程切换涉及的状态信息较少，因为它们通常是在用户空间操作，且不涉及到系统级寄存器和内存管理数据的切换。协程切换一般较轻量，但如果切换过于频繁，仍会带来一定开销。\n相对开销：中等偏低。比线程切换低，但如果协程频繁上下文切换，开销会增加。\n系统调用开销\n花费原因：系统调用从用户态切换到内核态，会有一定的开销。系统调用的开销通常与其具体操作内容相关。例如 I/O 操作比普通的内存操作要耗时。\n相对开销：一般较小，但系统调用密集时，累计的开销会显著。\n与 Linux 服务器的 IPC 消息传递\n花费原因：在 L4Linux 中，许多内核操作是通过 IPC 传递给 L4Linux 服务器。虽然 IPC 是轻量级的，尤其是在 µ-kernel 系统中被优化，但如果过于频繁，仍然会增加时间开销。\n相对开销：低，但高频调用情况下也可能累积显著时间。\n综上，TLB 未命中和频繁的上下文切换通常是系统开销较大的过程，特别是在多任务环境下。相对而言，协程切换和系统调用在 µ-kernel 系统下的开销较低，但仍需视具体情况和调用频率而定。\nL4用户空间和内核空间 用户空间（User Space）：\n包含内容： 用户空间主要用于用户态程序的执行和数据存储。每个进程都在用户空间中运行，并且它们相互隔离。 包含应用程序代码、用户程序的数据、堆（heap）、栈（stack）、共享库等。 用户空间的大小取决于操作系统设计和硬件架构。通常在 x86 系统上，用户空间分配 2GB 或 3GB。 访问权限： 用户空间的进程只能访问自己所属的内存地址，不能直接访问内核空间的数据或代码。这是为了安全性和稳定性，防止用户进程意外或恶意修改系统关键数据。 内核空间（Kernel Space）：\n包含内容： 内核空间是操作系统内核运行的区域，负责系统级的操作，比如硬件管理、进程调度、内存管理、网络通信等。 包含内核代码、内核数据结构（比如页表）、设备驱动、系统调用接口、内核栈等。 在 x86 架构下，通常为内核空间分配 1GB 或 2GB 的地址空间。 访问权限： 只有运行在内核态（例如内核或驱动程序）的代码才能访问内核空间。用户态的进程如果想要与内核交互，必须通过系统调用接口，而不能直接操作内核空间。 用户空间与内核空间的关系：\n用户空间和内核空间是通过虚拟内存管理体系联系在一起的。每个进程都有自己的虚拟地址空间，并且通过内核管理的页表将这些虚拟地址转换为物理地址。 页表是一个重要的数据结构，它为每个进程维护虚拟地址到物理地址的映射表。内核管理这些页表，通过它们实现用户态进程与内核态之间的内存隔离，同时允许在适当的时候（如系统调用时）进行安全的内存共享或数据交换。 页表的作用： 页表不仅帮助用户进程管理自己的虚拟内存，还帮助系统在需要时切换到内核空间。例如，用户进程发起系统调用时，系统通过页表和内存管理单元（MMU）在用户空间和内核空间之间进行地址转换，使得进程可以通过内核接口访问硬件资源或系统服务。 页表的存在确保了用户空间和内核空间在物理上是分离的，但在逻辑上它们通过页表联合起来，使得进程在需要时可以受控地与内核交互。 Linux 内核和 L4 微内核的空间区别 原生 Linux 内核的地址空间管理\n单一内核空间和用户空间：在原生 Linux 中，内核直接运行在系统的最高权限级别上（通常叫做 ring 0）。它能够直接管理整个 4GB 的虚拟地址空间（对于 32 位系统），并在地址空间中分配了 用户空间（通常为 3GB） 和 内核空间（通常为 1GB）。\n内核访问用户空间的便利性：Linux 的这种结构使得内核能够直接访问用户空间，因为这两个部分共享一个统一的地址空间。在同一个地址空间中，Linux 内核可以通过硬件支持的地址转换机制直接访问用户数据，像是直接在用户空间和内核空间之间拷贝数据。\nMMU 和 TLB 的作用：Linux 利用内存管理单元（MMU）和转换后备缓冲区（TLB）来进行地址翻译和管理，允许内核在访问用户空间时借助硬件来处理大部分的内存管理，减少了开发人员在内存管理方面的复杂性。\nL4 微内核的设计特点\nL4 微内核架构：L4 是一个微内核，设计理念是将操作系统中许多复杂的功能从内核中移除，转而将这些功能放在用户态进程中处理。因此，L4 微内核只处理一些非常基础的功能，比如进程调度、地址空间管理、消息传递等，而像 Linux 这样的操作系统内核（即 L4Linux）则运行在用户态之上。\nL4Linux 的限制：L4Linux 并不直接运行在最高权限级别上，而是作为 L4 微内核的一个用户态服务器。因此：\n地址空间隔离：在 L4 中，每个进程有自己的独立地址空间，L4Linux 作为一个用户态服务器，也有自己的地址空间。这使得它与 Linux 用户进程之间的内存并不共享，无法像原生 Linux 那样轻松地访问用户空间。 无法直接访问用户空间：由于 L4Linux 运行在用户态，它无法直接访问其他用户进程的内存。因此，L4Linux 试图模仿原生 Linux 的内存管理方式，即将用户空间映射到自己的地址空间中，这样的设计在 L4 上会因为地址空间限制和切换开销过大而变得不现实。\n技术限制和性能问题\n内存映射的限制：在原生 Linux 中，内核和用户空间共享一个地址空间，这使得内核能够轻松访问每个用户进程的内存。然而，在 L4 的设计中，由于每个用户进程和 L4Linux 服务器都有独立的地址空间，这样导致无法将多个用户进程的内存同时映射到 L4Linux 的地址空间中。L4Linux 服务器的地址空间只有 3GB，而多个用户进程的内存加起来显然会超出这个范围。\nTLB 刷新和性能开销：L4 中的每个地址空间都是独立的，L4Linux 服务器与用户进程的切换会涉及到 TLB（转换后备缓冲区）的刷新。这意味着每次上下文切换（如系统调用）时，整个 TLB 工作集都需要重新加载，导致性能损失严重。而原生 Linux 内核不需要频繁进行这种刷新，因此访问用户空间更加高效。\n为什么原生 Linux 能做到，而 L4Linux 做不到\n特权级和内存访问权限：原生 Linux 内核运行在 ring 0（最高权限级别），可以控制整个虚拟地址空间，因此可以访问用户进程的内存。而 L4Linux 是一个运行在用户态（较低权限级别）的服务器，不能直接操控系统的内存管理单元（MMU），所以无法像原生 Linux 那样自由访问用户进程的地址空间。\n架构设计目标不同\n原生 Linux 强调的是性能和集成，它将用户空间和内核空间放在一个统一的地址空间中，从而允许快速的数据交换。 L4 微内核 强调的是模块化和隔离性，设计目的是将复杂性降低到内核之外，实现更小的信任计算基础（TCB），这也导致了 L4Linux 在地址空间管理上受到了一定的限制。 举例来描述原生 Linux 和 L4 微内核在虚拟地址空间管理方面的区别，并解释为什么 L4Linux 的地址空间管理会遇到困难。\n原生 Linux 内核中的虚拟地址空间管理\n假设我们有一个应用程序（比如 myapp），它正在运行在 Linux 操作系统上。Linux 会给 myapp 分配一个 虚拟地址空间。在 32 位系统上，这个虚拟地址空间通常是 4GB。\n地址空间分布： 3GB 给用户空间（通常为地址 0x00000000 到 0xbfffffff）。 1GB 给内核空间（通常为地址 0xc0000000 到 0xffffffff）。 共享地址空间： 在原生 Linux 中，每个进程都有它自己的用户空间部分，但所有进程的内核空间部分是相同的。也就是说，当某个进程运行系统调用进入内核时，它可以访问内核部分的地址空间，而内核可以看到该进程的用户地址空间。 例如：如果 myapp 调用了一个系统调用（比如 read()），内核可以直接访问 myapp 的用户空间中数据，通过内存拷贝或者硬件支持的地址翻译，将数据从用户空间直接传输到内核空间，反之亦然。因为它们共享同一个统一的 4GB 地址空间，内核可以直接通过指针引用访问这些数据。 TLB 与地址空间切换： 在 Linux 中，虚拟地址到物理地址的转换由 TLB（Translation Lookaside Buffer） 缓存来加速。 TLB 缓存虚拟地址到物理地址的映射，因此每次访问相同的内存地址时不必重复查找页表。 内核和用户空间共享 TLB，因此当系统在用户空间和内核空间之间切换时，不需要大规模刷新 TLB，保证了访问速度。 关键点：为什么这样有效\n内核直接访问用户空间数据：由于用户空间和内核空间共享一个虚拟地址空间，Linux 内核在进行用户数据的操作时，可以直接使用硬件进行地址转换。 TLB 刷新减少：切换用户空间和内核空间时不需要频繁刷新 TLB，从而减少了性能开销。 L4 微内核中的虚拟地址空间管理\nL4 微内核采取了一种 模块化与隔离 的架构，目的是为了提高安全性和稳定性。\n假设我们有同样的应用程序 myapp，但这次它运行在 L4 微内核上的 L4Linux 中。\nL4 微内核的架构： L4 微内核本身只处理基础功能，如进程调度和通信。Linux 运行于 L4Linux 服务器中，而 L4Linux 服务器本身是一个 用户态进程，受到和普通应用程序类似的权限限制。 L4 微内核中的每个用户进程（包括 L4Linux 和应用程序）都有自己独立的地址空间。这意味着，myapp 和 L4Linux 服务器并不共享同一个地址空间。 地址空间的隔离： 在 L4 微内核上，每个进程，包括 L4Linux 服务器，都有自己的独立地址空间，例如： L4Linux 服务器：有自己的 3GB 用户态地址空间。 应用程序 myapp：也有自己的独立 3GB 用户态地址空间。 因为 L4Linux 运行在用户态中，它的权限不允许它直接访问其他应用程序的地址空间（例如 myapp 的空间）。 用户空间和内核空间的访问： L4Linux 的目标是模拟原生 Linux 的功能，但它没有内核态的权限。因此，myapp 和 L4Linux 必须通过 IPC（进程间通信） 来传递数据。这与原生 Linux 内核中内核直接访问用户空间的数据方式不同，增加了数据拷贝和通信的开销。 L4Linux 试图实现类似原生 Linux 的共享用户空间和内核空间的方法，但由于每个地址空间是独立的，它无法直接访问应用程序的数据，必须通过复杂的映射或者复制方式，这就引发了管理上的问题。 TLB 切换与性能问题：\nTLB 的问题：在 L4 微内核上，由于 L4Linux 和应用程序地址空间是独立的，每次 L4Linux 服务器调用（例如在处理系统调用时），系统都需要切换地址空间，这会导致TLB 必须完全刷新\n每次地址空间切换后，原先缓存的虚拟地址到物理地址的映射就无效了，因此必须重新建立，这显著降低了性能。 例如，当 myapp 发起一个系统调用进入 L4Linux 时，由于它们不共享地址空间，TLB 中的映射无效，需要重新加载，增加了大量的性能开销。\n关键点：为什么不如原生 Linux 高效\n独立地址空间的隔离性：L4 微内核的设计初衷是提高安全性和模块化，保持进程间的强隔离性，但这导致 L4Linux 服务器不能像原生 Linux 那样自由地直接访问用户进程的数据。 TLB 刷新频繁：L4Linux 和用户进程之间的频繁切换导致 TLB 被频繁刷新，系统不得不在每次系统调用或进程切换时重新加载 TLB，性能因此严重受损。 总结与对比\n原生 Linux： 共享地址空间：内核和用户空间共享同一个虚拟地址空间，内核直接访问用户空间的内存。 TLB 切换少：因为内核和用户空间共享，切换时无需刷新 TLB，访问非常高效。 L4 微内核： 地址空间隔离：L4Linux 和用户应用程序处于独立的地址空间中，为了安全和稳定，但导致内存管理复杂。 TLB 刷新频繁：每次 L4Linux 和用户进程切换时需要刷新 TLB，影响性能。 IPC 增加开销：因为 L4Linux 没有内核态权限，数据传递需要通过 IPC，进一步增加了数据拷贝的开销。 因此，在原生 Linux 中，由于内核和用户空间共享同一个地址空间，访问更快且管理简单。而在 L4 微内核架构下，L4Linux 运行于用户态、进程间隔离严格，导致无法像原生 Linux 那样自由高效地管理和共享地址空间，这就是为什么在 L4 上无法简单复用 Linux 的虚拟地址管理方式的原因。\nL4中的共享库 L4中的共享库主要通过动态链接的方式，将共享的代码库（如标准C库 libc.so）与应用程序进程关联在一起，促进跨进程调用的优化和性能提升。以下是L4环境下共享库的工作流程和每个环节所需的时间：\n库的初始化与加载 每个应用进程都会从Linux服务器请求将共享库加载到高地址区域。这样，应用进程能够共享这些库的实例，减少内存占用。 初始化通常通过一个页错误（page fault）请求向Linux服务器进行，服务器会映射适当的库页到应用程序的地址空间。 系统调用的重定向 为实现系统调用，L4使用了远程过程调用（Remote Procedure Calls，RPC）机制，即通过L4的进程间通信（IPC）来完成用户进程与Linux服务器之间的系统调用。 共有三种系统调用接口可用： 修改版的共享库（如 libc.so），直接调用L4的IPC原语与Linux服务器通信。这种方式速度较快。 修改后的静态库（如 libc.a），通常用于少量非动态链接的程序。 用户级异常处理机制（\u0026ldquo;trampoline\u0026rdquo;），模拟本地系统调用。这种方式提供了较好的二进制兼容性，但性能较低。 信号处理 信号的传递在L4中通过一个额外的信号线程实现。这个线程接收来自Linux服务器的消息，通知应用进程进入信号处理流程。 为保障系统的隔离性，L4限制了不同地址空间线程间的直接操作，因此信号处理只能在同一地址空间中实现。 数据交换与地址映射 服务器内部管理虚拟地址与物理地址的映射，每当有数据交换需求时（如 copyin/copyout 操作），服务器解析逻辑页表，将虚拟地址转换为实际的物理地址，并执行数据复制操作。 各部分所需时间\n系统调用延迟：与共享库 libc.so 相关的系统调用大约需要3.95微秒，使用trampoline机制的调用则需要5.66微秒的调用则需要5.66微秒。 信号传递延迟：通常依赖L4 IPC机制完成，虽然具体时间未明确指出，但其效率受限于系统的优先级调度策略和信号线程的调度频率。 数据复制延迟：使用物理地址解析的 copyin/copyout 操作在L4环境下因减少了TLB刷新，效率约为原本的两倍B刷新，效率约为原本的两倍。 共享库 libc.so与trampoline机制的差异 共享库 libc.so 与 trampoline 机制的调用存在时间差异，尽管它们都使用了 IPC 通信，这是因为两者在实现上的差别主要体现在调用路径的复杂性和处理系统调用的方式上。\n共享库 libc.so 的调用路径\n在 L4 系统中，使用 libc.so 的共享库可以更直接地通过修改后的系统调用接口与 Linux 服务器进行通信。这个方式的调用路径较短，因为共享库中的系统调用直接使用 L4 的 IPC 原语，与服务器进行高效的进程间通信。整个过程基本是直接发起 IPC 请求，然后由服务器处理请求并返回结果。因此，这种机制可以更好地利用 L4 IPC 的高效性，减少了不必要的上下文切换和额外的指令处理，导致更快的调用时间。\n共享库调用所需的时间（如 getpid 系统调用大约需要 3.95 微秒）得益于这种更简化的通信路径和直接调用 IPC 原语的机制。\nTrampoline 机制的调用路径\n相比之下，trampoline 机制更复杂，它主要是为了提供二进制兼容性。trampoline 的实现方式是在用户空间模拟系统调用陷阱（trap），通过异常处理机制（exception handler）捕获系统调用，并在捕获到之后再将系统调用转发给 Linux 服务器。这意味着系统调用的处理需要先触发一次用户空间的异常处理，再进行 IPC 通信，然后再返回结果。这增加了一个额外的步骤（捕获和处理异常），也使得路径更长。\n正是由于多了异常捕获和处理这一步，trampoline 机制的调用比直接使用 libc.so 调用要慢。getpid 使用 trampoline 机制时大约需要 5.66 微秒，这比共享库调用多出约 43% 的时间。\n结论\n尽管两者都基于 IPC 通信，trampoline 机制由于引入了额外的异常处理步骤，导致其调用时间较长。而共享库 libc.so 的直接 IPC 调用路径较短，因此性能更高。在需要二进制兼容性时，trampoline 是必要的，但在性能上则不如直接修改后的共享库调用高效。\ntrampoline机制的二进制兼容性 二进制兼容性（binary compatibility）指的是一个操作系统或软件能够运行为其他环境编译的程序或二进制文件，而不需要重新编译。在L4系统中的 trampoline 机制，旨在提供与传统Linux系统调用的二进制兼容性。这意味着，即使程序是为普通Linux编译的二进制文件，也可以在L4上执行，而无需对这些程序进行修改或重新编译。\n虽然共享库 libc.so 和 trampoline 机制都涉及系统调用，并且都依赖于L4的IPC（进程间通信）机制，但二者之间的实现方式不同，导致共享库调用时不需要进行与trampoline机制相同的上下文切换。\nlibc.so 调用：在使用 libc.so 的情况下，系统调用是通过经过修改的库来直接与Linux服务器通信。共享库调用的过程中，尽管也需要进行从用户空间到内核空间的切换（因为系统调用本质上都是如此），但是，L4的IPC机制使得这类上下文切换更加高效。此外，由于共享库调用使用了较为直接的IPC原语，减少了不必要的额外步骤，从而减少了切换和处理时间。 trampoline机制：trampoline机制为了提供二进制兼容性，使用了一种模拟的系统调用陷阱。首先，它通过用户空间的异常处理机制捕捉系统调用，再将该系统调用转发到Linux服务器。这种方式增加了上下文切换的复杂性，除了从用户空间到内核空间的切换，还包括捕获和处理异常的额外步骤，这些都使得性能不如直接使用 libc.so 调用。 在简化的调用路径下，libc.so 调用的性能相对更好，因为它可以避免一些复杂的上下文切换和额外的处理步骤【5†source】。\n因此，虽然两者都涉及IPC通信，但共享库的调用更加高效，不需要像trampoline机制那样频繁的上下文切换和额外的异常处理步骤。\nIPC和RPC IPC RPC 作用 IPC用于在同一计算机上不同进程之间共享数据或发送消息。其主要目的是协调进程之间的操作，确保数据一致性和完整性。 RPC允许程序调用位于不同机器上的函数，就像调用本地函数一样。其主要目的是简化分布式系统中的通信，使程序员无需关心底层网络通信细节。 例子 在操作系统中，一个音频播放器（进程A）需要和音频解码器（进程B）通信。进程A可以使用共享内存（IPC的一种）来将数据传送给进程B，进而完成音频解码。 一个客户端应用需要从远程服务器获取用户信息。客户端使用RPC调用服务器上的函数，从而获取数据，而不必直接处理网络传输。 范围 IPC通常用于同一台计算机上的进程之间。 RPC用于不同计算机上的进程之间。 实现复杂性 IPC相对简单，只需在一个系统内管理。 RPC需要处理网络协议、数据序列化和反序列化等问题。 性能 IPC因数据传输距离短，通常具有较低的延迟。 RPC需要通过网络，可能有较高的延迟。 故障处理 IPC主要处理进程间的同步和数据一致性。 RPC还需考虑网络故障、节点不可用等问题。 相同点 通信目的：两者都用于进程之间的信息交换。 抽象层次：提供了一种更高层次的抽象，使得程序员无需直接操作底层通信机制。 总结：IPC和RPC帮助程序在进程间或跨网络协同工作，分别适用于本地和分布式系统环境，都提供了显著提高开发效率的抽象能力。\nblock IPC和RPC 阻塞IPC 阻塞RPC 进程挂起：当一个进程进行阻塞IPC调用时，该进程会挂起，直到通信完成。这会导致该进程在等待响应期间无法执行其他任务。 网络通信延迟：阻塞RPC涉及网络通信，任何网络延迟或故障都会使调用挂起的时间更长，影响系统响应速度。 系统资源利用率下降：如果多个进程处于阻塞状态而等待IPC完成，系统资源（如CPU、内存）利用率会下降，因为这些进程无法继续处理其他任务。 进程挂起：发起RPC调用的进程会挂起直到收到远程服务器的响应，导致本地进程无法继续进行其他操作。 延迟增加：阻塞通信会导致系统整体延迟增加，尤其是在需要快速响应的实时系统中，这可能会影响系统性能。 分布式系统复杂度增加：在分布式系统中，网络问题、服务器负载问题等都可能导致RPC调用的阻塞，从而影响整个系统的性能和稳定性。 优先级逆转问题：当高优先级进程被阻塞等待低优先级进程的响应时，可能会导致优先级逆转，从而影响系统的响应时间。 资源等待：像阻塞IPC一样，阻塞RPC可能导致多个进程或线程在等待资源时挂起，从而降低系统的并行处理能力。 总结：在微内核L4上，阻塞IPC和阻塞RPC都会导致进程挂起，影响系统响应时间和资源利用率。虽然阻塞机制可以简化编程模型，但需要在高性能和实时系统中谨慎使用，可以考虑异步通信方式来减少阻塞的影响，例如：\n使用异步IPC/RPC，实现非阻塞调用，让进程继续执行其他任务。 利用多线程或多进程架构，将通信和计算分离，减轻阻塞对系统的影响。 通过超时机制避免长时间的阻塞等待，从而提高系统的鲁棒性。 了解这些影响后，设计和优化系统时，可以根据需求选择合适的同步或异步通信模型，以平衡性能和编程的复杂度。\nL4的缓存 实时系统的缓存分区： 缓存分区可以显著减少实时任务受其他任务干扰的影响，从而优化最坏情况下的执行时间。 实验中，通过分配专用的缓存页面，最坏情况下的执行时间从96.1毫秒减少到24.9毫秒，显示了缓存分区对提高实时性能的有效性。 尽管这种方法会影响分时系统的性能，但它使得系统资源在实时和分时任务之间进行动态分配成为可能。 μ-kernel的优势： μ-kernel通过RPC和用户级分页器提供了改进Unix兼容性功能的机制，还可以实现非Unix兼容功能，且性能优越。 实时内存管理案例表明，μ-kernel架构可以有效支持不同范式的系统共存，并且能够提供高性能。 总体而言，这部分强调了μ-kernel体系结构在内存管理和任务调度方面的优势，尤其是在需要严格时序保证的实时系统中。通过将资源进行精细化控制和分配，μ-kernel能够显著提升系统的实时性能，同时保证在分时任务中的合理表现。\n在一个实验中，一个64×64矩阵乘法被周期性地中断，以产生最大化缓存冲突的合成负载。在不受干扰的情况下，矩阵乘法需要10.9毫秒。每100微秒（μs）中断一次，最坏情况下的执行时间为96.1毫秒，减慢了8.85倍。\n在缓存分区的情况下，分页器从总共64个二级缓存页面中专门为矩阵乘法分配了3个页面。这既不能避免主缓存的干扰，也不能避免矩阵乘法的数据工作集（64 KB）的二级缓存未命中。然而，通过避免与其他任务的二级缓存干扰，最坏情况下的执行时间减少到24.9毫秒，减慢了仅2.29倍。从实时角度来看，分区后的矩阵乘法比未分区的快近4倍。\n为实时系统分配资源会降低分时性能。然而，描述的技术使系统资源在实时和分时系统之间进行定制的动态分区成为可能。\nco-location 在计算机体系结构和操作系统领域，\u0026ldquo;co-location\u0026rdquo;（共置或同址）通常指的是将相关的或彼此需要频繁交互的代码或资源放置在相同的物理位置（如相同的内存或存储单元）或运行在相同的上下文或地址空间内，以减少上下文切换和通信开销。其目标是提高系统性能和效率。\n在操作系统设计中，co-location 指的是将某些代码模块、服务或资源放在一起，以减少它们之间的通信延迟和开销。例如：\n内核与用户态代码： 在某些设计中，用户态的操作系统服务可能被“共置”到内核态中，以避免频繁的用户态和内核态之间的切换。例如，某些系统扩展（如驱动程序或高频操作）可以直接被放置在内核中执行，减少了内核态和用户态之间的上下文切换。 进程和线程： 在多核系统上，频繁交互的线程可以被分配到相同的处理器核心，以减少它们之间的通信延迟，这也是一种共置策略。 在论文中提到的 \u0026ldquo;co-location\u0026rdquo; 技术，指的是将扩展或服务代码直接共置在内核中，以减少用户态和内核态之间的切换，并减少地址空间切换。这些技术的目标是提高系统性能，但也带来了新的挑战，例如需要完全信任这些共置的代码或者需要对其进行沙箱隔离。\n以下是文中的具体描述：\n简单共置技术：将共置代码直接信任，避免了复杂的安全检查。 更复杂的技术：使用隔离技术，限制内核扩展可能造成的潜在损害。 举例系统包括：\nVino 使用了复杂但昂贵的事务机制。 Exokernel 提供了从可解释的中间代码到运行时检查的多种机制，支持应用程序特定的处理程序。 SPIN 是一种高效且复杂的共置技术，它通过尽可能多地使用编译时的沙箱技术，而避免了额外的运行时沙箱开销（除了下标检查）。 文中还提到，共置技术如果过于简单，可能无法显著提高性能，例如“共置”的 MkLinux 相较于用户态的 L4Linux 表现更差。\n总结：co-location 在操作系统设计中通过将相互依赖的代码或资源放置在相同位置或地址空间，减少了通信和切换开销，提高了系统性能。然而，这需要在安全和信任之间找到平衡，以避免潜在的安全风险。\n扩展代码嵌接到操作系统内核 嵌接的优势： 将嵌接代码下载到内核模式运行，可以减少用户态和内核态的切换及地址空间的切换，潜在提高性能。 安全性和信任： 内核模式下的嵌接代码要么需要完全信任，要么需要进行沙箱隔离，以防范潜在的安全风险。 现有技术： 简单的共址技术仅是信任共置代码，但效果并不一定好，如共置的MkLinux表现比用户模式的L4Linux差。 Vino和Exokernel引入了更加复杂的处理机制，从事务机制到应用程序特定的处理程序。 SPIN是一种利用编译时沙箱技术的高效嵌接方法，避免了额外的运行时沙箱开销。 性能比较： 虽然L4在用户模式下的基准测试时间比SPIN快了2到4.7倍，但由于硬件平台的不一致性，这种比较需要谨慎。 性能接近显示了两者的实现大致相当。 尚未解决的问题： 嵌接代码下载到内核中是否在性能上优于微内核方法，仍然是一个开放的问题。 总结而言，虽然将扩展代码嵌接到内核中可能提高性能，但需要解决信任和安全问题。不同方法各有优劣，尚不完全明确哪一种方法能在所有情况下表现最佳。\nProtected Control Transfers (PCT) 定义：PCT是一种类似于硬件中断的机制，在微内核系统中用于实现跨地址空间的过程调用。使用PCT，不带参数的过程可以通过被调用者定义的调用门实现。\n作用：\n实现跨地址空间的快速调用。 比IPC更基础，提供快捷的控制转移。 PCT与IPC的区别\nPCT IPC 抽象层次 低级，类似硬件的操作，主要用于跨地址空间的直接控制转移。 较高级的机制，用于进程间发送和接收消息。 性能 通常更快，因为它不涉及消息传递和同步，仅进行控制转移。 稍慢，因为会进行消息传递、同步、和堆栈分配等额外操作。 功能 不涉及数据传输，主要是过程调用和控制转移。 支持数据传输和线程间的复杂通信。 资源需求 没有额外的内核资源需求，每个调用只需要用户级堆栈。 需要分配目标线程的系统资源，支持更复杂的通信场景。 总结：PCT是一种简化的、快速的控制转移方式，适用于需要低开销、快速切换的场景。而IPC作为更高级的通信机制，提供了丰富的功能用于线程间的数据交换和同步。选择使用哪种机制，通常取决于具体应用场景的需求。\n下图是一个通信过程中消耗的操作时间\n操作 PCT IPC 备注 进入PAL模式 5 5 打开帧 7 7 设置堆栈帧以允许多个中断、TLB未命中和简化线程切换 发送/接收 - 0.5 确定操作 检查接收者有效性 2 2 检查无主要传输 - 0.5 接收者接受？ - 1 我们可以进行传输吗 设置我的接收超时 - 1 保存接收参数 - 2 执行接收 验证排队状态 - 1 设置唤醒排队无效，如果超时NEVER 上下文切换 10 10 切换地址空间编号 内核线程切换 - 6 设置调用者ID 2 - 保存调用者ID到PCT返回 查找被调用者条目 2 - 被调用者的PCT条目地址 关闭帧 7 7 退出PAL模式 2 2 总计 38 45 二者的例子说明↓\nPCT 示例\n场景：操作系统中断处理。在操作系统中，中断处理程序需要快速响应硬件信号，例如键盘输入或网络数据包。这种处理需要快速切换到一个特定的代码段执行，然后迅速返回。\nPCT作用：\n快速切换：通过PCT，可以直接跳转到中断处理程序，而无需携带参数或进行额外的数据传输。 高效执行：减少上下文切换开销，快速响应硬件事件。 区别：\nPCT在这里用于快速控制流切换，而无需数据传递。 IPC 示例\n场景：客户端与服务器通信。假设有一个图像处理服务器，客户端应用程序需要将图像数据发送给服务器进行处理，并接收结果。\nIPC作用：\n数据传输：客户端通过IPC将图像数据发送给服务器。 结果接收：服务器处理完后，通过IPC将处理结果返回给客户端。 区别：\nIPC在这里用于数据传输和消息交换，实现复杂的客户-服务器模型。 总结\nPCT适用于需要快速控制流切换的场景，无需参数传递。 IPC用于进程之间的数据和消息传递，支持更复杂的通信和同步需求。 正课部分 Monolithic kernel 今天主要会讨论微内核（Mircro kernel）。\n为什么人们会研究微内核？微内核是人们在思考操作系统内核应该做什么事情的过程中设计出来的。\n比如说XV6是一个Unix系统，它按照Unix风格提供了各种功能，并提供了Unix系统调用。\n实现一个Unix系统就是我们设计XV6的目标。\n但是一个完全值得思考的问题是，一个操作系统内核本身应该具备什么功能？或许XV6和Linux具备的功能并不是一个完美操作系统应该有的功能，又或许是呢。\n我们现在正在讨论一个变化莫测的问题，内核可以看做是一种程序员的开发平台，而我们知道不同的程序员对于他们喜欢的开发平台有着非常不同的主观喜好，所以我们不能期望这个问题有一个完美的答案。\n但是我们可以仍然可以期望从思考这个问题的过程中学到一些东西，并且尝试想一下答案可能是什么。\n传统实现方式\n首先，让我说明一下操作系统的传统实现方式以及应该具备的功能。我个人将Linux，Unix，XV6称为用传统方式实现的操作系统。\n另一个形容这些操作系统的词是monolithic。\nmonolithic的意思是指操作系统内核是一个完成了各种事情的大的程序。\n实际上，这也反应了人们觉得内核应该具备什么样的功能。类似于Linux的典型操作系统内核提供了功能强大的抽象。\n它们选择提供例如文件系统这样一个极其复杂的组件，并且将文件，目录，文件描述符作为文件系统的接口，而不是直接将磁盘硬件作为接口暴露给应用程序。\nmonolithic kernel通常拥有例如文件系统这样强大的抽象概念，这比提供一些简单的抽象有着巨大的优势。\n其中一个好处是，这些高度抽象的接口通常是可移植的，你可以在各种各样的存储上实现文件和目录，你可以使用文件和目录而不用担心它们是运行在什么牌子的磁盘，什么类型的存储之上，或许是SSD，或许是HDD，或许是NFS，但是因为文件系统接口是高度抽象的，所以它们都拥有相同的接口。\n所以这里的一个好处是可以获取可移植性。你可以在不修改应用程序的前提下，将其运行在各种各样的硬件之上。\n另一个例子是，Linux/Unix提供地址空间的抽象而不是直接访问MMU硬件的权限。\n这不仅可以提供可移植性，并且也可以向应用程序隐藏复杂性。所以操作系统具备强大抽象的另一个好处是，它们可以向应用程序隐藏复杂性。\n举个例子，XV6提供的文件描述符非常简单，你只需要对文件描述符调用read/write就可以，但是在XV6内核中是非常复杂的代码来实现读写磁盘上的文件系统。这对于程序员是极好的，但是内核却因此变得又大又复杂。\n这里的强大的抽象还可以帮助管理共享资源。\n例如我们将内存管理委托给了内核，内核会跟踪哪些内存是空闲的。\n类似的，内核还会跟踪磁盘的哪个部分是空闲的，磁盘的哪个部分正在被使用，这样应用程序就不用考虑这些问题，所以这可以帮助简化应用程序。\n同时也可以提供健壮性和安全性，因为如果允许应用程序决定磁盘的某个位置是否是空闲的，那么应用程序或许可以使用一个已经被其他应用程序使用的磁盘位置。\n所以，内核管理硬件资源可以提供资源共享能力和安全性。\n但是同样的，这也使得内核变得更大。内核提供的这些诱人的抽象能力，使得内核包含了很多的复杂性，进而导致内核很大且复杂。\n有关monolithic kernel的另一个好处是，因为所有这些功能都在一个程序里面，所有的内核子系统，例如文件系统，内存分配，调度器，虚拟内存系统都是集成在一个巨大的程序中的一个部分，这意味着它们可以访问彼此的数据结构，进而使得依赖多个子系统的工具更容易实现。\n举个例子，exec系统调用依赖文件系统，因为它要从磁盘中读取二进制文件并加载到内存中，同时它也依赖内存分配和虚拟内存系统，因为它需要设置好新的进程的地址空间，但是它的实现是相对简单的。\n在XV6或者Linux中做到这些完全没问题，因为这些操作系统已经在内核程序中包含了文件系统和虚拟内存系统。\n但是如果严格分隔了文件系统和虚拟内存系统，那么实现类似exec的系统调用将会难得多。\n在一个monolithic操作系统中，因为本身就是一个大的程序，实现起来会容易的多。\n另一个使得类似于XV6或者Linux的操作系统中可以更简单实现软件的原因是，内核的所有代码都以完整的硬件权限在运行。\n举个例子，整个XV6都运行在Supervisor mode，这意味着你可以读写任意内存地址，并且所有的内核代码都以最大的权限在运行。Linux操作系统也是这样。\n所以，monolithic kernel这种设计策略对于内核开发人员来说非常方便，并且也更容易构建更强大的抽象，进而对于应用程序开发人员来说也更容易。\n然而，对于传统的monolithic kernel，也有一些缺点。这也是之所以会出现其他内核架构，比如说微内核的原因。\n所以这里的问题是，为什么不在所有的场合使用monolithic kernel呢？\n第一个原因是它们大且复杂。\n取决于你怎么计算，Linux总是有数十万到数百万行代码。Linux的一部分可以查看Linux的另一个部分的数据，的确使得编程更加容易，但是同样也使得内部代码有大量的交互和依赖。\n有的时候查看并弄明白Linux代码会有点挑战。任何时候你有了一个大的程序，尤其它们还具有复杂的结构，你都会有Bug，操作系统内核也不例外。\n在这些年，内核有过各种各样的Bug，包括了安全性相关的Bug。所以这是一个令人烦恼的关系图，如果你使用了大的内核，你不可避免的会遇到Bug和安全漏洞。\n另一个人们不喜欢monolithic kernel的原因是，随着时间的推移，它们倾向于发展成拥有所有的功能。\nLinux应用在各种场合中，从移动电话到桌面工作站，从笔记本电脑到平板电脑，从服务器到路由器。\nLinux可以支持这么多设备是极好的，但是这也使得Linux非常的通用，所以Linux支持了很多很多不同的东西。\n而任何一个应用程序，例如我的web server程序不太会需要用到Linux中非常复杂的声卡支持。\n所以，Linux中包含了大量的内容使得它很通用，这很好，但是另一方面，通用就意味着慢。\n对于各种不同的场景都能支持，或许就不能对某些特定场景进行优化。\n当你尝试快速运行一些程序时，如果程序只做一两件事情是极好的，因为这样你就可以专注在优化一两个代码路径上。\n但是如果你的程序想要做上千件事情，优化会更加难。\nLinux并不一定慢，但是你或许会想，它真的在所有场景下都达到了最快速度吗？如果你去看Linux或者XV6中的任何模块，你或许会想它们真的有必要去做所有的事情吗？\n例如，你从一个进程向一个Pipe写一个字节传输到另一个进程，即使在XV6这样一个简单的内核中，都有大量的指令需要被执行。\n这里有buffering，locking，或许在Pipe的读写中有sleep/wakeup，或许有线程调度导致context switching，对于从一个进程移动一个字节到另一个进程来说，这里有大量的内容或许并不是必须的。\n对于monolithic kernel来说，另一个潜在的问题是，因为它们是如此之大，它们会削弱一些复杂的抽象能力。\n在内核中会有大量的设计考虑，应用程序需要遵守这些设计并与之共存。\n反之，在一个理想世界中，应用程序或许可以做更多的决定。\n举个例子，在Unix中，你可以wait子进程，比如说你fork出来的子进程，但是你不能wait其他进程（注，详见13.7），或许你会想要wait孙子进程或者一个不相关的进程，但是这是不可能的。\n或许你会想要更改其他进程的地址空间，比如说替其它受你控制的进程调用mmap，但是这也不可能。mmap只能修改你自己的地址空间，但是不能修改其他进程的地址空间。\n或许你是个数据库，你在磁盘上有B树索引，你或许知道很多快速展开B树的方法，但是当你读写文件系统中的文件时，文件系统并不知道你正在读写一个B树，以及如何更快的在磁盘上展开B树。\n所以如果你是个数据库的话，你或许很高兴文件系统可以任你摆布，但是文件系统并不会按照你想要的方式工作。以上就是内核中需要考虑的设计。\nmonolithic kernel的最后一个问题是，可扩展性（Extensibility）。\n应用程序或许想要实时更改内核，比如说向内核下载代码并更改内核的工作方式，这样数据库或许就可以更改数据在磁盘上的分布方式。\n至少在10年前，monolithic kernel没有任何功能可以支持这里的Extensibility，你只能使用内核提供的能力。\n以上就是人们认为的monolithic kernel的问题。\n这些问题使得人们去思考操作系统的其他架构。这里有很多不同的想法，其中一些想法非常的激进，我们今天只会讨论其中的一种，也是现在非常流行的一种，这就是微内核，Micro kernel。\nMicro kernel 有关微内核的很多思想可以回溯到计算机的发展历史中。\n微内核从1980年代中后期开始就是一个非常热门的研究课题，它是指一种通用的方法或者概念，它并不特指任何特定的产品。\n有很多人遵循微内核的设计思想并构建了操作系统，但是这些项目中的每一个具体的操作系统都与另一个非常不一样。\n微内核的核心就是实现了IPC（Inter-Process Communication）以及线程和任务的tiny kernel。\n所以微内核只提供了进程抽象和通过IPC进程间通信的方式，除此之外别无他物。\n任何你想要做的事情，例如文件系统，你都会通过一个用户空间进程来实现，完全不会在内核中实现。\n画个图来展示一下，整个计算机还是分为两层，下面是kernel，上面是用户空间。\n在用户空间或许还是会有各种各样常见的程序，例如VI，CC，桌面系统。\n除此之外，在用户空间还会有文件系统以及知道如何与磁盘交互的磁盘驱动，或许我们还会有一个知道如何进行TCP通信的网络协议栈，或许还有一个可以实现酷炫虚拟内存技巧的虚拟内存系统。\n当文本编辑器VI需要读取一个文件时，它需要与文件系统进行交互，所以它通过IPC会发送一条消息到文件系统进程。\n文件系统进程中包含了所有的文件系统代码，它知道文件，目录的信息。\n文件系统进程需要与磁盘交互，所以它会发送另一个IPC到磁盘驱动程序。\n磁盘驱动程序再与磁盘硬件进行交互，之后磁盘驱动会返回一个磁盘块给文件系统。之后文件系统再将VI请求的数据通过IPC返回给VI。\n这里需要注意的关键信息是，在内核中唯一需要做的是支持进程/任务/线程，以及支持IPC来作为消息的传递途径，除此之外，内核不用做任何事情。\n内核中没有任何文件系统，没有任何设备驱动，没有网络协议栈，所有这些东西以普通用户进程在运行。\n所以这提供给你一种非常小的内核，以及相对少的代码去优化，你可以优化IPC，除此之外也没有别的东西了。\n上图就是我们在这节课剩下的时间要讨论的内容。\n在今天仍然有使用微内核的场景。实际上，今天论文 要讨论的L4微内核就有很多很多的运行实例，它用在很多手机中用来控制手机的射频，在最近的iphone中的一个旁路处理器中，也使用了L4微内核来隐藏加密密钥。\n所以在一些微型嵌入式系统中，微内核能够胜出，比如说在一些专门为某种功能设计的计算机中，你需要使用某种操作系统，而你又不需要Linux带来的复杂性。\n微内核中的用户进程通过IPC通信，这在很多操作系统都存在。\n例如我现在运行的macOS，它就是一个普通的monolithic kernel，它也很好的支持用户进程通过IPC进行通信。\n所以用户进程通过内核内的IPC相互通信，这是一个成功的思想并且被广泛采用。\n以上就是微内核的基本架构，我接下来会讨论一下这个架构的优势。\nWhy micro kernel? 人们构建微内核的动机是什么？\n其中一个动机你可能不常看到，这就是审美（注，原话是sense of aesthetic）。\n我认为很多人都觉得像Linux内核这样大的复杂的程序并不十分优雅。我们肯定可以构建一些小得多且专注得多的设计，而不是这样一个巨大的拥有各种随机特性的集合体。\n所以从审美角度来说，我们必然可以做的比大的内核更好。\n其他还有一些更具体且可量化的动机。\n更小的内核或许会更加的安全。只有几行代码的话，Bug也不会太多，其他人利用Bug来破坏安全性的可能也就更小。\n在特殊场景下，你需要证明一个操作系统是正确的，没有Bug，并且只做了它应该做的事情。\n现实中至少有一种经过验证是安全的微内核系统：seL4。这是L4微内核的多个衍生项目之一。\n人们知道怎么去验证中小型程序，但是不知道怎么验证巨大的程序。微内核通常都很小，这是它能够被证明是安全的一个关键因素。\n小内核的另一个优势是，少量代码的程序比巨大的程序更容易被优化。\n小内核可能会运行的更快，你不用为很多用不上的功能付出代价。\n相比monolithic kernel，微内核几乎不会做任何事情，所以你不用为你不使用的很多功能付出代价。\n使用小内核的另一个原因是，小内核或许自带了少得多的设计限制，进而使得应用程序的设计限制也更少。\n这样给应用程序提供了更多的灵活性，使得应用程序可以自己做出设计决定。\n以上都不是微内核必须要支持的特性，只是说人们期望通过使用微内核可以得到的特性。\n另一个微内核吸引人的原因是，有很多我们习惯了位于内核的功能和函数，现在都运行在用户空间。\n这种将内核拆分，并在用户空间的不同部分运行，比如说在用户空间运行文件系统服务，可以使得代码更模块化。\n用户空间代码通常会比内核更容易被修改，调整和替换，所以它更容易被定制化。\n将操作系统放在用户空间，或许可以使得它更加的健壮。\n如果内核出错了，通常你需要panic并重启，因为如果内核有Bug，并且会随机更改数据，那就不能信任内核了。\n然而，如果你将内核运行成一些用户空间的服务，其中一个出现故障，比如说除以0，索引了一个野指针，或许只有这一个服务会崩溃，操作系统的剩余部分还是完好的，这样你可以只重启那一个服务。\n所以，将操作系统的功能移到用户进程可以使得系统更加健壮。\n这对于驱动来说尤其明显，内核中大部分Bug都在硬件驱动中，如果我们能将设备驱动从内核中移出的话，那么内核中可能会有少的多的Bug和Crash。\n最后一个优势是，你可以在微内核上模拟或者运行多个操作系统。\n所以尽管微内核几乎不做任何事情，你还是可以在它之上运行一个Unix系统之类的，或许还可以在同一个机器上运行超过一个操作系统。\n今天的论文主要就是描述如何在微内核之上将Linux作为服务来运行。\n以上就是人们在微内核这条道路上期望得到的一些好处。\n当然，这里也有一些挑战。\n如果你想要设计属于你自己的微内核，其中一个挑战是你会想要你的微内核的系统调用接口尽可能的简单，因为使用微内核的出发点就是内核很小。\n那么什么才是有用的系统调用的最小集？这一点并不十分明确。\n所以这里我们要确定最少的系统调用API，你需要这些系统调用API尽可能的少，但是你又需要基于这些API构建一些非常复杂的功能，因为即使内核没有做太多工作，你最终还是要运行程序。\n或许你想要在微内核之上运行Unix，你需要能执行类似fork，mmap的工作。\n所以底层的系统调用在简单的同时，需要能够足够强大以支持人们需要做的各种事情，比如说exec，fork，copy-on-write fork，mmap file。\n但是内核又完全不知道文件和文件系统。\n所以现在系统调用需要支持exec，而内核又不知道文件。\n微内核或许会非常简单，但是我们仍然需要开发一些用户空间服务来实现操作系统的其他部分。\n最后，微内核的设计需要进程间通过IPC有大量的通信。\n所以有很大的需求使得IPC能够足够的快。我们会好奇，IPC可以足够的快来使得微内核足够有竞争力吗？\n有关性能，不仅与IPC的速度相关。\n通常来说，monolithic kernel可以获得更好的性能，是因为它里面的文件系统代码和与虚拟内存代码可以直接交互，它们开心的位于一个巨大的程序中。\n但是如果你需要将这些模块都拆分开成为不同的服务，那么在集成的时候就有更少的机会可以优化，这或许会影响性能。\n以上就是许多微内核项目要面对的用户期望和潜在的挑战。\nL4 micro kernel 今天要讨论的论文 ，有许多有关L4微内核的内容。这是今天论文作者开发和使用的一种微内核。\nL4必然不是最早的微内核，但是从1980年代开始，它是最早一批可以工作的微内核之一，并且它非常能展现微内核是如何工作的。\n在许多年里面它一直都有活跃的开发和演进。如果你查看Wikipedia，L4有15-20个变种，有一些从1980年代开始开发的项目现在还存在。\n接下来我将从我的理解向你们解释L4在今天的论文发表的时候是如何工作的。\n首先，L4是微内核，它只有7个系统调用，虽然其中有一些稍微有点复杂，但是它还是只有7个系统调用。\n然而现在的Linux，我上次数了下有大概350个系统调用。甚至XV6这个极其简单的内核，也有21个系统调用。\n从这个指标来看，L4更加简单。\n其次，L4并不大，论文发表的时候，它只有13000行代码，这并不多。\nXV6的代码更少，我认为XV6内核只有6000-7000行代码，所以作为内核XV6非常的简单。\nL4也没有复杂太多，它只有Linux代码的几十分之一，所以它非常的小。\n第三，它只包含几个非常基础的抽象。\n它在内部有一个叫做Task或者地址空间的概念，这或多或少的对应了Uinx内的进程概念。\nTask包含了一些内存，地址从0开始，并且可以像进程一样执行指令。\n区别于XV6的是，每个Task可以有多个线程，L4会调度每个Task内的多个线程的执行。这样设计的原因是，可以非常方便地用线程来作为组织程序结构的工具。\n我不知道在论文发表的时候，L4是否支持了多处理器，或许它包含了在多个处理器上运行同一个程序的能力。\n所以L4内核知道Task，知道线程，也知道地址空间，这样你就可以告诉L4如何映射地址空间内的内存Page。\n另一个L4知道的事情是IPC。\n每一个线程都有一个标识符，其中一个线程可以说，我想要向拥有这个标识符的另一个线程发送几个字节。\n这里的Task，线程，地址空间，IPC是L4唯一有的抽象。\n我不确定是否能列出所有的系统调用，这里涉及到的系统调用有：\nThreadcreate系统调用，你提供一个地址空间ID并要求创建一个新的线程。\n如果地址空间或者Task不存在，系统调用会创建一个新的Task。所以这个系统调用即可以创建线程，又可以创建Task。\nSend/Recv IPC系统调用。\nMapping系统调动可以映射内存Page到当前Task或者其他Task的地址空间中。\n你可以要求L4来改变当前Task的地址空间和Page Table，如果你有足够的权限，你也可以要求L4改变其他Task的地址空间。这实际上是通过IPC完成的，你会发送一个特殊的IPC消息到目标线程，内核可以识别这个IPC消息，并会修改目标线程的地址空间。\n如果你创建一个先的线程，新线程最开始没有任何内存。所以如果你想创建一个线程，你先调用Threadcreate系统调用来创建新的线程，新的Task和地址空间。\n然后你创建一个特殊 IPC，将你自己内存中的一部分，其中包含了指令和数据，映射到新的Task的地址空间中。\n之后你再发送一个特殊的Start IPC消息到这个新的Task，其中包含了你期望新的Task开始执行程序的程序计数器和Stack Pointer。\n之后新的Task会在你设置好的内存中，从你要求的程序计数器位置开始执行。\n虽然我不知道具体是怎么实现的，但是Privileged Task可以将硬件控制寄存器映射到自己的地址空间中。\n所以L4并不知道例如磁盘或者网卡的设备信息，但是实现了设备驱动的用户空间软件可以直接访问设备硬件。\n你可以设置L4将任何一个设备的中断转换成IPC消息。\n这样，运行设备驱动的Task不仅可以读写了设备，并且也可以设置L4将特定设备的中断通过IPC消息发送给自己。\n最后，一个Task可以设置L4内核通知自己有关另一个Task的Page Fault。\n所以如果一个Task发生了Page Fault，L4会将Page Fault转换成一个IPC消息，并发送给另一个指定的Pager Task。\n每一个Task都有个与之关联的Pager Task用来处理自己相关的Page Fault。这就是关联到Page Fault的方法，通过它可以实现类似copy-on-write fork或者lazy allocation。\n以上就是内核的内容，L4里面不包含其他的功能，没有文件系统，没有fork/exec系统调用，除了这里非常简单的IPC之外，没有其他例如pipe的通信机制，没有设备驱动，没有网络的支持等等。\n任何其他你想要的功能，你需要以用户空间进程的方式提供。\nL4能提供的一件事情是完成线程间切换。\nL4会完成线程调度和context switch，来让多个线程共用一个CPU。\n它实现的方式你会觉得非常熟悉，L4会为每个Task保存寄存器，当它执行一个线程时，它会跳到用户空间，切换到那个线程对应Task的Page Table，之后那个线程会在用户空间执行一会。之后或许会有一个定时器中断，定时器是L4知道的一个设备，定时器中断会使代码执行返回到L4内核，L4会保存线程的用户寄存器，然后在一个类似于XV6的线程调度循环中，选择一个Task来运行。\n通过将这个Task之前保存的寄存器恢复出来，切换Page Table，就可以跳转到Task中再运行一会，直到再发生另一个定时中断，或者当前Task出让了CPU。\n所以我认为L4或许还有一个yield系统调用。\n在这种情况下Task可以等待接收一个IPC消息，这时代码会跳转回L4内核，L4内核会保存寄存器，并切换到一个新的Task。所以L4中有关线程切换的部分你们会非常熟悉。\n我之前提到过这个概念，Pager。\n如果一个进程触发了Page Fault，通过trap走到了内核，内核会将Page Fault转换成IPC消息并发送到指定的Pager Task，并告诉Pager Task是哪个线程的哪个地址触发了Page Fault。\n在Pager Task中，如果它实现了lazy allocation，那么它会负责从L4分配一些内存，向触发Page Fault的Task发送一个特殊的IPC，来恢复程序的运行。\n所以Pager Task实现了XV6或者Linux在Page Fault Handler中实现的所有功能。\n如果你想的话，你可以在Pager Task中实现copy-on-write fork或者memory mapped files，Pager Task可以实现基于Page Fault的各种技巧。\n这是类似L4的微内核相比传统的内核，对于用户程序要灵活的多的众多例子之一。\n如果Linux并没有copy-on-write fork，并且你想要有这个功能，你不可能在不修改内核的前提下完成这个功能。\nLinux中没有办法写一些可移植的用户空间代码来实现copy-on-write fork。这样描述可能并不完全正确，但是一定要这么做的话会很复杂。\n然而，在L4里面，这就相对简单了。\nL4就好像是完全设计成让你去写用户空间代码来获取Page Fault，并实现copy-on-write fork。所有这些都可以在用户空间完成，而不用弄乱内核。\n学生提问：能说明一下Task和线程之间的区别吗？\nRobert教授：可以。一个Task就像XV6的一个进程一样，它有一些内存，一个地址空间，你可以在其中运行用户代码。\n如果你在XV6中有一个进程，它只能包含一个线程。\n但是在现代的操作系统和L4中，在一个进程，一个地址空间中，可以有多个线程。\n如果你有多个CPU核，那么多个CPU核可以同时运行一个Task。\n每个线程在Task的地址空间中都有一个设置好的Stack，这意味着你可以写一个程序，并通过并行运行在多个CPU核上得到性能的提升，其中的每个线程都运行在不同的CPU核上。\n所以你们可以看到，这里的设计非常依赖IPC，因为如果你想与你的文件系统交互，文件系统想要与设备驱动交互，你都需要来回发送IPC消息。\n对于每个系统调用，每个Page Fault，每个设备中断，都会有反复的IPC消息。所以IPC系统需要非常快。\nImproving IPC by Kernel Design 接下来我们讨论微内核里面一个非常重要的问题：IPC的速度。\n首先让我展示一个非常简单，但是也非常慢的设计。这个设计基于Unix Pipe。\n我之所以介绍这种方法，是因为一些早期的微内核以一种类似的方式实现的IPC，而这种方式实际上很慢。\n假设我们有两个进程，P1和P2，P1想要给P2发送消息。\n这里该怎么工作呢？一种可能是使用send系统调用，传入你想将消息发送到的线程的ID，以及你想发送消息的指针。\n这个系统调用会跳到内核中，假设我们是基于XV6的pipe来实现，那么这里会有一个缓存。\n或许P2正在做一些其他的事情，并没有准备好处理P1的消息，所以消息会被先送到内核的缓存中。\n所以当你调用send系统调用，它会将你的消息追加到一个缓存中等待P2来接收它。\n在实际中，几乎很少情况你会只想要发送一个消息，你几乎总是想要能再得到一个回复。\n所以P1在调用完send系统调用之后，会立即调用recv来获取回复。\n但是现在让我们先假设我们发送的就是单向的IPC消息，send会将你的消息追加到位于内核的缓存中，我们需要从用户空间将消息逐字节地拷贝到内核的缓存中。\n之后再返回，这样P1可以做一些其他的事情，或许是做好准备去接受回复消息。\n过了一会，P2可以接收消息了，它会调用recv系统调用，这个系统调用会返回发送消息线程的ID，并将消息从内核拷贝到P2的内存中。\n所以这里会从内核缓存中取出最前的消息，并拷贝到P2的内存中，之后再返回。\n这种方式被称为异步传输，因为P1发完消息之后，只是向缓存队列中追加了一条消息，并没有做任何等待就返回了。\n同时这样的系统这也被称作是buffered system，因为在发送消息时，内核将每条消息都拷贝到了内部的缓存中，之后当接收消息时，又从buffer中将消息拷贝到了目标线程。\n所以这种方法是异步buffered。\n如果P1要完成一次完整的消息发送和接收，那么可以假设有两个buffer，一个用来发送消息，一个用来接收消息。\nP1会先调用send，send返回之后。之后P1会立即调用recv，recv会等待接收消息的buffer出现数据，所以P1会出让CPU。\n在一个单CPU的系统中，只有当P1出让了CPU，P2才可以运行。\n论文中的讨论是基于单CPU系统，所以P1先执行，之后P1不再执行，出让CPU并等待回复消息。\n这时，P2才会被调度，之后P2调用recv，拷贝消息。\n之后P2自己再调用send将回复消息追加到buffer，之后P2的send系统调用返回。\n假设在某个时间，或许因为定时器中断触发导致P2出让CPU，这时P1可以恢复运行，内核发现在接收消息buffer有了一条消息，会返回到用户空间的P1进程。\n这意味着在这个慢的设计中，为了让消息能够发送和回复，将要包含：\n4个系统调用，两个send，两个recv 对应8次用户空间内核空间之间的切换，而每一次切换明显都会很慢 在recv的时候，需要通过sleep来等待数据出现 并且需要至少一次线程调度和context switching来从P1切换到P2 每一次用户空间和内核空间之间的切换和context switching都很费时，因为每次切换，都需要切换Page Table，进而清空TLB，也就是虚拟内存的查找缓存，这些操作很费时。\n所以这是一种非常慢的实现方式，它包含了大量的用户空间和内核空间之间的切换、消息的拷贝、缓存的分配等等。\n在微内核架构中，进程间通信（IPC） 的实现通常依赖内核的支持，因此当一个线程执行发送操作时，它确实会进入内核态。这涉及多个操作，包括上下文切换、内存访问和消息的传输。\n线程进入内核会导致上下文切换吗？\n是的，进入内核通常会导致上下文切换。\n在微内核的 IPC 实现中，线程在发送消息时需要从用户态进入内核态，至少有以下两个层面的上下文切换：\n用户态到内核态的切换：当用户线程执行系统调用（例如 send）时，CPU 会切换到内核态。这种切换虽然涉及一些保存和恢复寄存器的操作，但不一定是完整的进程上下文切换。它主要是用户态到内核态的权限转换。 进程间上下文切换：如果 IPC 涉及多个线程或者进程间通信，当发送消息的线程完成操作后，CPU 可能会切换到接收消息的线程或进程，这种情况下是完整的上下文切换。这时，需要保存和恢复所有的寄存器、页表信息、堆栈指针等。这是一个相对昂贵的操作。 进入内核会改变页表吗？\n不一定需要切换页表，但可能会发生。\n同一进程的线程间切换：如果线程属于同一个进程，页表通常不会发生变化。内核只需要完成权限转换（从用户态切换到内核态）而不改变虚拟内存的布局。 跨进程切换：如果线程属于不同的进程，那么切换到内核态可能会导致页表的改变。每个进程拥有自己独立的虚拟地址空间，因此当内核从一个进程切换到另一个进程时，内核必须加载接收方进程的页表，以确保接收方进程能正确访问自己的内存。 放入 buffer 的消息从哪里来？\n消息的来源是发送方线程的用户空间。一般情况下，发送方进程的线程在调用 IPC 时，会将消息从自己的用户空间缓冲区复制到内核提供的 buffer 中。 例如：\n线程 A 通过 send 调用向线程 B 发送一条消息。 线程 A 的消息存储在用户态的内存区域（如用户的堆内存）。 当执行系统调用时，内核将这条消息从线程 A 的用户空间复制到内核的 buffer 中。 之后，内核会将消息传递给线程 B，可能通过共享内存、消息队列等方式。 举例说明微内核 IPC 的操作流程\n假设有两个线程 A 和 B，分别属于不同的进程。A 想要发送一条消息给 B，流程可能如下：\n线程 A 调用 send 函数： A 从用户态进入内核态，执行 send 系统调用。 此时发生了用户态到内核态的上下文切换。 内核处理消息传递： 内核从线程 A 的用户空间中读取消息数据，将其复制到内核 buffer 中。 如果线程 B 当前正在等待这条消息，内核可能会准备唤醒 B，发生线程调度。 上下文切换： 如果线程 B 属于另一个进程，则需要加载线程 B 所在进程的页表，保证线程 B 可以正确访问自己的虚拟内存空间。 进行完整的上下文切换，保存 A 的上下文（寄存器、堆栈指针、页表等）并恢复线程 B 的上下文。 线程 B 处理消息： 线程 B 被唤醒，从内核 buffer 中读取消息，并复制到自己的用户空间。 线程 B 处理完消息后，可能返回用户态继续执行。 总结\n线程进入内核时，通常会发生上下文切换，至少会有用户态到内核态的切换，可能还会有完整的进程上下文切换。\n页表的切换取决于是否发生进程切换。同一进程的线程切换不需要更换页表，但不同进程之间的切换则需要更换页表。\n消息通常是从发送方进程的用户空间复制到内核 buffer，再传递给接收方。\n这种机制带来了 IPC 中的性能开销，特别是上下文切换和数据拷贝，这是早期微内核性能瓶颈的原因之一。\n尤其在系统设计中，安全性和性能经常需要权衡。在只有一个 CPU 的情况下，线程从用户态进入内核态，确实意味着线程的权限发生了变化，从较低的用户权限提升到较高的内核权限。这种权限切换使得线程可以访问更多的系统资源，比如硬件设备、内核内存等。这当然带来了潜在的安全风险，但操作系统设计中有多种机制来应对这些安全问题。\n进入内核态：权限切换 当线程在用户态运行时，操作系统的硬件保护机制会限制它只能访问用户空间内存和执行特定的非特权指令。如果线程需要执行特权操作（比如与硬件交互或访问内核资源），它必须通过系统调用来请求内核完成这些操作。\n在系统调用中，CPU 会切换到内核态，线程的权限位（通常是 CPU 的控制寄存器或状态寄存器中的标志位）也会相应地改变。这时，线程能够执行特权指令，访问内核内存和设备。但这个切换过程不是随意的，操作系统会通过以下几种方式来防止线程滥用这种高权限：\n防止安全问题的机制 （1）受限的系统调用接口\n用户态线程不能直接访问内核态资源，它只能通过特定的系统调用接口来请求内核执行操作。这些系统调用是经过严格定义的，并受到内核的权限控制。例如：\n用户态线程不能直接读写内核内存。 系统调用会进行参数校验，确保用户态线程传递的参数合法、合理。比如，如果用户传递了无效的地址或者非法的文件描述符，内核会返回错误。 这意味着，即使用户线程进入了内核态，它只能在内核规定的范围内行动，无法随意执行其他有害操作。\n（2）进程隔离与页表保护\n每个用户进程都有自己独立的虚拟内存空间，页表的设置确保了用户进程无法直接访问其他进程的内存或内核的内存。在进入内核态时，虽然权限提升了，但内核会严格管理线程能够访问的内存区域。这一保护措施确保即使线程进入了内核态，它也不能随意访问或修改系统内核的数据。\n（3）中断处理与内核模式的控制流\n当线程从用户态进入内核态时，系统会有一整套严格的控制流来保证安全：\n系统调用入口：系统调用是通过中断（或陷阱指令）进入内核的，内核中有专门的系统调用处理程序，它会接管线程的执行，并确保线程只能在安全的范围内运行。 中断处理器：系统调用或者硬件中断会触发硬件控制流，跳转到内核指定的安全位置，并保存当前用户态的执行上下文，防止恶意线程篡改这些数据。 （4）内核空间与用户空间分离\n操作系统将内核空间和用户空间进行隔离。用户空间的代码和数据处于较低的内存区域，而内核空间则处于较高的内存区域。内核态执行的代码是经过严格审查和保护的，用户态代码不能直接修改内核空间的数据。\n通过硬件和软件的结合，线程即使在进入内核态后也无法访问用户态之外的资源，除非通过安全的系统调用接口，这样防止了线程“乘机运行有害代码”。\n特权切换的安全性保障 虽然线程进入内核意味着获得了更高的权限，但这并不意味着用户态的恶意线程能够自由破坏系统。这是因为：\nCPU硬件支持的分级权限：现代 CPU 使用分级权限环（privilege rings），通常操作系统使用Ring 0（内核态）**和**Ring 3（用户态）。进入内核态的切换是通过特定的硬件中断或者陷阱指令实现的，并且内核会在处理结束后立即返回用户态。 系统调用检查：内核会对每个系统调用进行严格的参数验证和安全检查，防止用户进程通过恶意传递参数来破坏系统。错误或非法的调用会被拒绝，通常内核会返回错误码或者终止进程。 内存管理单元（MMU）的保护：CPU 中的内存管理单元（MMU）和页表机制确保用户态无法访问内核态内存，防止恶意用户进程修改或读取内核数据。 安全性示例：系统调用如何保护内核 举个简单的例子，假设一个用户态线程要进行文件写操作，它会调用 write 系统调用：\n用户态线程调用 write(fd, buffer, size)，通过陷阱指令进入内核态。 内核会接管控制，首先检查文件描述符 fd 是否有效，是否可以写操作。 然后，内核检查 buffer 是否是合法的用户空间地址，并确保该地址处的内存区域可以被读取。 如果所有检查都通过，内核才会进行实际的写操作。 操作完成后，内核返回用户态，并将控制交还给用户线程。 在这个过程中，内核确保了线程无法通过不合法的 fd 或 buffer 来执行有害操作，整个操作在严格受控的范围内进行。\n总结 在单个 CPU 系统中，线程进入内核态确实意味着权限提升，但操作系统通过硬件支持和系统调用接口的设计来保证安全性。 权限切换的过程并不直接暴露内核，内核会对系统调用进行严格的校验和控制。 即使权限提升了，恶意线程仍然无法绕过操作系统的保护机制，内核态的执行是受控制的，不会随意执行用户态提供的有害代码。 实际中，对于这里的场景：发送一个消息并期待收到回复，你可以抛开这种方法并获得简单的多的设计，L4就是采用了后者。\n有关简单的设计在一篇著名的论文中有提到，论文是Improving IPC by Kernel Design ，这篇论文在今天要讨论的论文前几年发布。相比上面的慢设计，它有几点不同：\n其中一点是，它是同步的（Synchronized）。\n所以这里不会丢下消息并等待另一个进程去获取消息，这里的send会等待消息被接收，并且recv会等待回复消息被发送。\n如果我是进程P1，我想要发送消息，我会调用send。send并不会拷贝我的消息到内核的缓存中，P1的send会等待P2调用recv。\nP2要么已经在内核中等待接收消息，要么P1的send就要等P2下一次调用recv。\n当P1和P2都到达了内核中，也就是P1因为调用send进入内核，P2因为调用recv进入内核，这时才会发生一些事情。\n这种方式快的一个原因是，如果P2已经在recv中，P1在内核中执行send可以直接跳回到P2的用户空间，从P2的角度来看，就像是从recv中返回一样，这样就不需要context switching或者线程调度。\n相比保存寄存器，出让CPU，通过线程调度找到一个新的进程来运行，这是一种快得多的方式。\nP1的send知道有一个正在等待的recv，它会立即跳转到P2，就像P2从自己的recv系统调用返回一样。\n这种方式也被称为unbuffered。它不需要buffer一部分原因是因为它是同步的。\n当send和recv都在内核中时，内核可以直接将消息从用户空间P1拷贝到用户空间P2，而不用先拷贝到内核中，再从内核中拷出来。\n因为现在消息收发的两端都在等待另一端系统调用，这意味着它们消息收发两端的指针都是确定的。\nrecv会指定它想要消息被投递的位置，所以在这个时间点，我们知道两端的数据内存地址，内核可以直接拷贝消息，而不是需要先拷贝到内核。\n如果消息超级小，比如说只有几十个字节，它可以在寄存器中传递，而不需要拷贝，你可以称之为Zero Copy。\n前面说过，发送方只会在P2进入到recv时继续执行，之后发送方P1会直接跳转到P2进程中。\n从P1进入到内核的过程中保存P1的用户寄存器，这意味着，如果P1要发送的消息很短，它可以将消息存放到特定的寄存器中。\n当内核返回到P2进程的用户空间时，会恢复保存了的寄存器，这意味着当内核从recv系统调用返回时，特定寄存器的内容就是消息的内容，因此完全不需要从内存拷贝到内存，也不需要移动数据，消息就存放在寄存器中，可以非常快的访问到。当然，这只对短的消息生效。\n对于非常长的消息，L4可以在一个IPC消息中携带一个Page映射，所以对于巨大的消息，比如说从一个文件读取数据，你可以发送一个物理内存Page，这个Page会被再次映射到目标Task地址空间，这里也没有拷贝。\n这里提供的是共享Page的权限。所以短的消息很快，非常长的消息也非常快。对于长的消息，你需要调整目的Task的Page Table，但是这仍然比拷贝快的多。\n最后一个L4使用的技巧是，如果它发现这是个RPC，有request和response，并且有非常标准的系统调用包括了send和recv，你或许会结合这两个系统调用，以减少用户态和内核态的切换。\n所以对于RPC这种特别的场景，同时也是人们使用IPC的一个常见场景，有一个call系统调用，它基本上结合了send和recv，区别是这里不会像两个独立的系统调用一样，先返回到用户空间，再次进入到内核空间。\n在消息的接收端，会有一个sendrecv系统调用将回复发出，之后等待来自任何人的request消息。这里基本是发送一个回复再加上等待接收下一个request，这样可以减少一半的内核态和用户态切换。\n实际中，所有的这些优化，对于短的RPC请求这样一个典型的场景，可以导致20倍速度的提升。这是论文中给出的对比之前慢设计提升的性能倍数。这个数字很了不起。Improving IPC by Kernel Design这篇论文是由今天这篇论文的同一个作者在前几年发表的，因为现在IPC可以变得非常的快，它使得人们可以更加认同微内核。\n学生提问：当使用这些系统调用时，进程是什么时候发送和接收消息的？\nRobert教授：对于包含request和response的RPC，进程使用call和sendrecv这一对系统调用，而不是send和recv。\n对于call，你会传入两个参数，你想要发送的消息，以及你要存放回复消息的位置，这个系统调用在内核中会结合发送和接收两个功能。\n你可以认为这是一种hack，因为IPC使用的是如此频繁，它值得一些hack来使得它变得更快。\n学生提问：在上面的图中，P2会调用recv系统调用，P2怎么知道应该去调用这个系统调用？\nRobert教授：在RPC的世界中，我们有client会发送request到server，server会做一些事情并返回。\n因为P2是一个server，我们会假设P2会一直在一个while循环中，它随时准备从任何client接收消息，做一些数据处理工作，比如在数据库中查找数据，之后再发送回复，然后再回到循环的最开始再等待接收消息。\n所以我们期望P2将所有时间都花费在等待从任何一个客户端接收消息上。\n前面讨论的设计需要依赖P2进程在暂停运行时，一直位于内核的recv系统调用中，并等待下一个request。\n这样，下一个request才可以直接从这个系统调用返回，这种快速路径在这里的设计中超级有效率。\n学生提问：这里提到从P1返回到P2，为了能返回到P1，需要P2发送response吗？\nRobert教授：是的，我们期望P2发送一个response，发送response与发送request是同一个代码路径，只是方向相反（之前是P1到P2现在是P2到P1），所以当P2发送一个response，这会导致返回到P1。\nP1实际调用的是call系统调用，通过从call系统调用返回到P1，会将P2的response送到P1。\n这里与你们以为的通常的设置略有不同，通常情况下，你从P1通过系统调用进入到内核，在内核中执行系统调用然后再返回，所有的工作都在P1这边，这也是pipe的read/write的工作方式。\n在这里，P1进入到内核，但是却返回到了P2。所以这里有点奇怪，但是却非常的快。\n远程过程调用（RPC） 和 进程间通信（IPC） 是两个相关但不同的概念，它们之间的关系以及其他 IPC 机制可以通过如下方式理解：\nRPC 和 IPC 的关系 进程间通信（IPC，Inter-Process Communication） 是指两个或多个进程之间交换数据或信号的机制。由于现代操作系统中每个进程都有自己独立的内存空间，进程间通信是必要的，因为进程无法直接访问彼此的内存。\n远程过程调用（RPC，Remote Procedure Call） 是一种特定的 IPC 方法。它允许一个进程通过调用某个“远程”的函数或过程，来请求另一个进程执行某个任务，就像调用本地函数一样。RPC 的背后可能隐藏了复杂的网络或本地通信操作，但对于用户而言，调用是透明的。\nRPC 是 IPC 的一种实现方式\nRPC 是一种封装良好的 IPC，常用于进程间的远程通信（进程可能在同一台机器上，也可能跨网络）。它将进程间的消息传递抽象为“过程调用”的形式，从而让开发者不必关心底层的消息传递细节。\n常见的 IPC 机制 除了 RPC 之外，还有许多其他的 IPC 机制，具体包括：\n（1）管道（Pipe）\n无名管道：提供单向的字节流通信，通常用于父子进程间的数据传递。一个进程写入数据，另一个进程从管道的另一端读取。 命名管道（FIFO）：允许不相关的进程之间进行双向通信，类似于无名管道，但可以由多个进程使用。 （2）消息队列（Message Queue）：消息队列是一种**先进先出（FIFO）**的消息传递方式，允许进程以消息的形式发送和接收数据。它可以在内核中存储一系列的消息，并提供同步和异步消息传递的能力。\n（3）共享内存（Shared Memory）：共享内存允许多个进程直接访问同一段内存，这是一种非常高效的 IPC 方式，因为数据不需要在进程之间复制。但是，共享内存本身不提供同步机制，所以通常需要结合其他同步原语（如信号量）使用，以避免数据竞争。\n（4）信号量（Semaphore）：信号量是一种同步机制，用于控制进程对共享资源的访问。它并不用于传递数据，而是用于解决多个进程访问同一资源时的竞争问题。\n（5）信号（Signals）：信号是一种异步的消息传递机制，主要用于通知进程某个事件的发生。例如，操作系统可以通过信号通知某个进程时间片用完，或者出现了需要处理的事件（如终止进程）。\n（6）套接字（Sockets）：套接字是一种强大的 IPC 机制，允许进程通过网络协议（如 TCP/IP）进行通信。它可以用于本地进程间通信（Unix 域套接字），也可以用于跨网络的远程通信。\n（7）内存映射文件（Memory-Mapped Files）：内存映射文件将文件内容直接映射到进程的虚拟地址空间中，允许多个进程通过映射相同的文件进行通信和数据共享。\nRPC 与其他 IPC 的对比 抽象层次：RPC 将进程间的通信抽象为远程函数调用，隐藏了底层的通信细节。其他 IPC 机制（如共享内存、消息队列）则更接近底层的实现，开发者需要处理细节。\n透明性：RPC 的一个特点是它使得进程间通信看起来像是本地函数调用，隐藏了通信的复杂性。其他 IPC 机制如管道、消息队列等则不提供这样的抽象，开发者需要明确处理消息的发送、接收和同步。\n效率：共享内存通常是最快的 IPC 方式，因为数据无需复制。然而，RPC 的透明性和易用性往往带来一些额外的性能开销，尤其是在涉及跨网络通信时。\n其他 IPC 示例和场景 文件锁（File Locking）：多个进程可以通过文件锁机制协调对文件的访问，避免数据竞争。这不是直接传递数据的 IPC，但是一种协调多进程访问共享资源的方式。\nDBus：这是 Linux 系统中常用的一种消息总线机制，提供了一个高层次的消息传递平台，允许系统中的应用程序和服务进行通信。\n总结：\nRPC 是一种特殊的 IPC 实现方式，通过封装函数调用的形式，简化了进程间的通信。 IPC 包含多种方式，包括管道、消息队列、共享内存、信号等，每种方式都有自己的使用场景和性能特点。 L4 微内核中的优化，如 call 和 sendrecv，是为提高 RPC 性能而设计的，减少了频繁的用户态和内核态切换。 Run Linux on top of L4 micro kernel 前一节对于IPC的优化使得人们开始认真考虑使用微内核替代monolithic kernel。\n然而，这里仍然有个问题，即使IPC很快了，操作系统的剩余部分从哪里去获取？现在的微内核大概只有一个完整操作系统的百分之几，我们该怎么处理操作系统剩下的部分？\n这个问题通常会在一些有着相对较少资源的学校研究项目中被问到，我们需要从某个地方获取到所有这些用户空间服务。\n实际上在一些特殊的应用场合，以上的问题并不是问题，比如说我们运行的一些设备的控制器，例如车里的点火控制器，只运行了几千行代码，它并且不需要一个文件系统，这样我们就只需要很少的用户空间内容，微内核也特别适合这种应用程序。\n但是微内核项目发起时，人们非常有雄心壮志，人们想的是完全替换操作系统，人们希望可以构建一些运行在工作站，服务器等各种地方的微内核操作系统，并取代大的monolithic kernel。\n对于这种场景，你需要一个传统操作系统所需要的所有内容。\n一种可能是，重新以微内核的方式，以大量的进程实现所有的内容。实际上有项目在这么做，但是这涉及到大量的工作。\n具体的说，比如我想要使用笔记本电脑，我的电脑必须要有emacs和我最喜欢的C编译器，否则我肯定不会用你的操作系统。\n这意味着，微内核要想获得使用，它必须支持现有的应用程序，它必须兼容或者提供相同的系统调用或者更高层的服务接口，它必须能够完全兼容一些现有的操作系统，例如Unix，Linux，这样人们才愿意切换到微内核。\n所以这些微内核项目都面临一个具体的问题，它们怎么兼容一些为Linux，Windows写的应用程序？对于论文中提到的项目，也就是L4，对标的是Linux。\n与其写一些完全属于自己的新的用户空间服务，并模仿Linux，论文中决定采用一种容易的多的方法，其实许多项目也都采用了这种方法，也就是简单的将一个现有的monolithic kernel运行在微内核之上，而不是重新实现一些新的东西。\n这就是今天论文要介绍的内容。\n在今天论文 的讨论中，L4微内核位于底部，但是同时，一个完整的Linux作为一个巨大的服务运行在用户空间进程中。听起来有点奇怪，一般的kernel都是运行在硬件之上，而现在Linux kernel是一个用户空间进程。\n实际上，如你在QEMU上运行XV6时所见，内核也是运行在用户空间。Linux kernel不过就是一个程序，对其做一些修改它就可以运行在用户空间，所以现在Linux需要被修改。论文中提到需要对Linux的底层做一些修改，例如Linux中期望能直接修改Page Table的内容，读写CPU寄存器。Linux中一部分需要被修改以将它们改成调用L4微内核的系统调用，或者发送IPC，而不是直接访问硬件。\n但是Linux的大部分内容都可以不做修改而直接运行。所以按照这种方式，作为Linux的一部分，现在得到了文件系统，网络支持，各种设备驱动等等，而不需要自己实现这些。这里的实现方式是将Linux内核作为一个L4 Task运行，每一个Linux进程又作为一个独立的L4 Task运行。所以当你登录到Linux中时，你要它运行一个Shell或者terminal，它会在用户空间创建一个L4 Task来运行这个Linux程序。所以现在有一个Task运行Linux，以及N个Task来运行每一个你在Linux中启动的进程。Linux不会直接修改进程的Page Table，而是会向L4发送正确的IPC让L4来修改进程的Page Table。\n这里有很多小的改动，其中一个有意思的地方是，当VI想要执行一个系统调用时，VI并不知道它运行在L4之上，在上面的方案中，所有的程序都以为它们运行在Linux中。当VI要执行系统调用时，L4并不支持，因为VI要执行的是Linux系统调用而不是L4系统调用。所以对于Linux进程，会有一个小的库与之关联，这个库会将类似于fork，exec，pipe，read，write的系统调用，转换成发送到Linux kernel Task的IPC消息，并等待Linux kernel Task的返回，然后再返回到进程中。\n从VI的角度看起来好像就是从系统调用返回了。所以这些小的库会将系统调用转成发送到Linux kernel Task的IPC消息。这意味着，如果Linux kernel Task没有做其他事情的话，它会在一个recv系统调用中等待接收从任何一个进程发来的下一个系统调用请求IPC。\n这导致了这里的Linux和普通的Linux明显不同的工作方式。在普通的Linux中，就像XV6一样，会有一个内核线程对应每一个用户空间进程。当用户空间进程调用系统调用时，内核会为这个系统调用运行一个内核线程。并且，在普通的Linux中，如果内核在内核线程之间切换，这基本上意味着从一个用户进程切换到另一个用户进程。\n所以这里Linux kernel的内核线程以及当Linux完成工作之后要运行的用户进程之间有一对一的关系。在这里架构中，这种一对一的关系断了，这里的Linux kernel运行在一个L4线程中。\n然而，就像XV6一样，这个线程会使用与XV6中的context switching非常相似的技术，在与每个用户进程对应的内核线程之间切换。\n不过这些内核线程完全是在Linux中实现的，与L4线程毫无关系，唯一的L4线程就是运行了Linux kernel的控制线程。但是哪个用户进程可以运行，是由L4决定的。\n所以在这里的设置中，Linux kernel或许在内核线程中执行来自VI的系统调用，同时，L4又使得Shell在用户空间运行了。\n这在XV6或者Linux极不可能发生，在这两个系统中，活跃的内核线程和用户进程有直接的对应关系，而L4会运行它喜欢的任何Task。\n因为Linux kernel中的内核线程都是私有的实现，Linux可以同时执行不同阶段的多个系统调用，或许一个进程在它的内核线程中在等待磁盘，这时Linux可以运行另一个进程的内核线程来处理另一个进程的系统调用。\n你或许会想知道为什么不直接使用L4线程来实现Linux内的内核线程，或者说Linux为什么要实现自己内部的内核线程，而不是使用L4线程，答案是，\n在论文发表时，还没有用到多核CPU硬件，他们使用的是单核CPU硬件。所以在内核中同时运行多个内核线程并没有性能优势，因为只有一个CPU核，所以第二个线程不能执行，由于硬件的限制，一次只能执行一个线程。\n另一个或许是更强大的原因是，在论文发表时，他们使用的Linux版本并不支持将Linux kernel运行在多个CPU核上。\n所以他们使用的是旧版本的单核Linux，一次只能期望在内核中使用一个CPU，它并没有类似于XV6的spinlock，可以使得它能正确的在内核中使用多核。\n所以在Linux内核中使用多个L4线程并没有性能优势。如果一定要使用的话，在没有性能优势的前提下，又需要加入spinlock和其他的内容来支持并发。所以论文中没有在Linux内核使用L4线程。\n这种架构的一个缺点是，在普通原生的Linux中，存在大量复杂的线程调度机制，例如在不同进程上增加优先级，确保调度公平性等等。\nLinux可以在你的笔记本上运行这些机制，因为Linux控制了哪些进程可以运行在哪些CPU核上。\n但是在这里的架构中，Linux完全控制不了哪些进程可以运行，因为现在是L4而不是Linux在完成调度，这些进程都是被L4所调度。\n所以这里的架构失去了Linux的调度能力，这是这种架构的缺点，我相信L4的后续版本有一些方法能够让Linux通知L4调度器，来给某个进程更高优先级等等。\nL4 Linux性能分析 你应该问自己：通过论文 可以学到有关微内核的什么内容呢？\n对于我们来说，论文中有很多有趣的有关微内核是如何运行，有关Linux是如何运行的小的知识点，以及你该如何设计这么一个系统。\n但是论文并没有回答这个问题：微内核是不是一个好的设计？论文只是讨论了微内核是否有足够的性能以值得使用。\n论文之所以讨论这个内容的原因是，在论文发表的前5-10年，有一场著名的测试针对一种更早的叫做MACH的微内核。\n它也运行了与L4类似的结构，但是内部的设计完全不一样。通过测试发现，当按照前一节的架构运行时，MACH明显慢于普通的Unix。\n这里有很多原因，比如IPC系统并没有如你期望的一样被优化，这样会有更多的用户空间和内核空间的转换，cache-miss等等。\n有很多原因使得MACH很慢。但是很多人并不关心原因，只是看到了这个测试结果，发现MACH慢于原生的操作系统，并坚信微内核是无可救药的低效，几乎不可能足够快且足够有竞争力。很多人相信应该都使用monolithic kernel。\n今天的论文像是对于这种观点的一个反驳，论文中的观点是，你可以构建类似上一节的架构，如果你花费足够的精力去优化性能，你可以获取与原生操作系统相比差不多的性能。\n因此，你不能只是因为性能就忽视微内核。今天的论文要说明的是，你可以因为其他原因不喜欢微内核，但是你不能使用性能作为拒绝微内核的原因。\n达成这一点的一个重要部分是，IPC被优化的快得多了，相应的技术在18.5中提到过。\n论文的表二做了性能对比，运行在硬件上的原生Linux执行一个简单的系统调用getpid花费1.7us，对于上一节的实现，需要发送一个IPC request，并获取一个IPC response，以实现getpid系统调用，这需要花费接近4us，这是原生Linux的两倍多。\n主要是因为这里有两倍的工作，这里涉及到两次用户空间到内核空间的切换，而不是一个简单的系统调用。\n这也说明L4已经将这种基于IPC的系统调用的代价降到了最低，也就是2倍于一个原生Linux的系统调用。因此，它可以做的大概与你期望的一样好。\n当然这里的系统调用仍然只有原生Linux一半的速度。现在还不清楚这是否是一个灾难，还是说并没有问题。\n如果你执行大量的系统调用或许就是个问题；如果你执行了相对较少的系统调用，或者系统调用本身就有很多工作，或者你的系统调用比getpid要复杂的多，这又或许不是个问题。\n论文中通过使用AIM做的测试结果，给出了答案。测试结果在论文的图8。\nAIM会执行各种系统调用，例如read/write文件，创建进程等等。\n从图8可以看出，在AIM设置的一个更完整的应用中，基于L4的Linux之比原生Linux慢几个百分点。\n因此，理想情况下你可以期望你想要运行在计算机上行的应用，如果在L4+Linux上运行可以与运行在原生操作系统上一样快。\n因为可以以近似原生Linux的速度运行，所以你们现在应该认真对待微内核。图8是一个非常不错的结果，有点超出预期。\n让时间快进20年，如果之前所说，现在人们实际上在一些嵌入式系统中使用L4，尤其在智能手机里有很多L4实例在运行，它们与Unix并没有兼容性。\n在一些更通用的场景下，像是工作站和服务器， 微内核从来没有真正的流行过，并不是因为这里的设计有什么问题，只是为了能够吸引一些软件，微内核需要做的更好，这样人们才会有动力切换到微内核。\n对于人们来说很难决定微内核是否足够好，这样才值得让他们经历从现在正在运行的Linux或者其他系统迁移到微内核的所需要的各种麻烦事。\n所以，微内核从来没有真正流行过，因为它们并没有明显的更好。\n另一方面来看，微内核的很多思想都有持久的影响。\n人们实现了更加灵活和有趣的方法来在微内核上使用虚拟内存。这些复杂的多的接口导致了mmap这样的系统调用合并到了例如Linux的主流操作系统中。 论文中将一个操作系统作为一个用户程序运行另一个操作系统之上，今天以另一种方式非常流行的存在：在Virtual Machine Monitor上运行虚拟机。这种方式在各种场景，例如云主机上，都有使用。 为了让内核能够具有一个用户空间服务一样的可扩展性，在Linux中演进成了可加载的内核模块，这使得你可以在线修改Linux的工作方式。 当然，这里基于IPC的Client-Server支持，也在macOS有所体现，macOS中也有好用的IPC。 以上就是这节课的所有内容，有什么问题吗？\n学生提问：论文4.3 Dual-Space Mistake能介绍一下吗？\nRobert教授：这里稍微有点复杂。\n这里的一部分背景是，论文发表时的Linux，甚至直到最近，当你运行在x86上，且运行在用户空间时，使用的Page Table同时会有用户空间的内存Page，以及所有的内核内存Page。\n所以当你执行系统调用，并跳转到内核中，内核已经映射到了Page Table中，因此不需要切换Page Table。\n所以当你执行一个系统调用时，代价要小得多，因为这里没有切换Page Table。\n如果你回想我们之前介绍的内容，trampoline代码会切换Page Table（注，也就是更新SATP寄存器，详见6.5）。\n这是个代价很高的操 作，因为这会涉及到清除TLB。所以出于性能的考虑，Linux将内核和用户进程映射到同一个Page Table，进而导致更快的系统调用。\n论文中期望的是，当用户空间进程向Linux发送一个系统调用，并且Linux的内核线程在处理系统调用，Page Table也包含发送系统调用的进程的所有虚拟内存映射，这会使得作为系统调用参数传入的虚拟内存地址查找更加的简单。但是为什么这里不能很好工作？\n首先，L4并不知道这里的任何具体实现，在L4的眼里这就是两个进程。\n当你从一个进程发送IPC到另一个进程，L4只是会切换Page Table。\n由于现在Linux的系统调用是基于L4实现的，没有办法在系统调用的过程中保持Page Table，因为L4在两个进程间切换时总是会切换Page Table。\n所以这里不能得到系统调用时不切换Page Table带来的性能优势。\n我认为这里希望得到可以在内核中直接使用用户空间的虚拟内存地址的便利，但是这意味着在Linux内核中需要知道是在执行哪个进程的系统调用，并使用那个进程的Page Table。\n当然L4并不知道这里的细节，它只是给每个进程关联了一个Page Table。\n所以L4只会给Linux关联一个Page Table，Linux并没有办法在处理不同进程的系统调用时使用不同的Page Table。\n为了解决这个问题，论文中为每个进程都做了共享内存拷贝，每一个共享内存拷贝都有内核的所有内存，所以都有相同的数据结构。\n因为每个进程都有一个kernel task与之关联，因此可以使得L4切换到合适的Page Table同时包含了进程和内核的内存。\n我认为这里可以工作，但是忘记了这里是否会很慢之类的，因为这里有大量的任务。\n这是个复杂的故事，我不知道解释清楚了没有。\n学生提问：看起来一些任务更适合在内核中，但是内核的方案中，要么所有东西都在内核要么都不在。所以要么你有一个monolithic kernel可以完成所有的事情，要么有个micro kernel什么也不做。我认为虚拟内存、文件系统和一些其他的事情在内核中做会非常的有效。不能有些系统具备有一些功能，然后你又可以选择用不用这些功能吗？\nRobert教授：所有你说的都有非常有道理。\n实际上有很多微内核相关的项目都构建了各种hybrid内核，MACH有一些不同的版本，其中一些就是hybrid内核，这些内核的核心是包括了IPC的微内核，同时在内核中又是一个完整的Unix系统，比如MACH 2.5就是这样一个hybrid内核，其中一些东西是按照微内核的方式构建，一些东西又是按照Unix方式构建。\n现代的macOS也以与你描述类似的方式构建，macOS本身是个完整的操作系统，包含了文件系统，同时也很好的支持了IPC和其他用来构建微内核的东西。Google Fuchsia也实现了一些这里的想法。\n","date":"2024-10-03T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08118os-organization/bg_hu15216042923906799033.jpg","permalink":"https://echudet.github.io/p/mit6s08118os-organization/","title":"MIT6S081：18OS organization"},{"content":"Lecture 17 Virtual memory for applications 课前预习 论文速览 在简介部分提到，论文的核心讨论点是：虚拟内存（Virtual Memory）不仅仅可以用来扩展操作系统的地址空间，还能用于很多其他的目的，例如内存共享、写时复制（copy-on-write）以及处理保护违规。作者探讨了操作系统应提供给用户程序的虚拟内存基本原语，特别是这些原语如何支持不同的用户级应用。\n在虚拟内存原语部分提到，论文中提到的关键虚拟内存操作包括：\ntrap： 处理用户模式下的页故障。 prot1 \u0026amp; protN： 降低单页或多页的访问权限。 unprot： 增加页面的访问权限。 dirty： 返回自上次检查以来被修改的页面列表。 map2： 将同一个物理页面映射到不同的虚拟地址。 这些原语旨在支持中粒度的同步操作和多处理器系统上的高效内存管理。\n在虚拟内存应用部分，论文展示了多种利用虚拟内存原语的应用，包括：\n并发垃圾回收： 通过页面保护技术检测未扫描的对象，以实现垃圾收集器与程序的同步。 共享虚拟内存： 在多计算机网络上实现页面级别的内存一致性。 并发检查点保存： 通过设置页面为只读，允许程序在不间断的情况下进行内存保存操作。 世代垃圾回收： 使用页面保护机制减少对老年内存段的修改检查开销。 持久性存储： 将内存映射文件作为跨程序执行周期的数据结构，使用虚拟内存实现持久性和事务管理。 扩展寻址能力： 用虚拟内存技术处理超过32位地址空间的大量数据。 数据压缩分页： 使用压缩技术减少磁盘检索时间。 堆溢出检测： 使用页错误机制检测堆溢出，而无需手动检查每次内存分配。 在虚拟内存原语的性能部分，作者对多个操作系统（例如 Ultrix、SunOS 和 Mach）进行了基准测试，结果显示各系统在处理页面保护、陷阱处理和转换缓冲区（TLB）一致性上的性能存在显著差异。\n在系统设计考虑部分提到，操作系统在设计时应重视以下几个方面：\nTLB一致性： 在多处理器系统中，确保高效的TLB一致性对于减少过时TLB条目带来的非法访问至关重要。 页面大小： 较小的页面可以减少故障处理延迟，但具体选择应根据应用的需求进行调整。 访问受保护页面： 用户模式服务例程可能需要在保持页面保护的情况下访问页面，操作系统应支持这种访问方式。 在结论部分作者指出，现代硬件和操作系统应将虚拟内存视为用户级应用的关键组件。这些原语能够在系统规模不断扩大时保持高效。操作系统设计中必须重视虚拟内存机制的高效性，如TLB一致性、页面大小灵活性和快速的页面保护机制。\nGenerational Garbage Collection（分代垃圾回收） 虚拟内存保护的重要应用之一是分代垃圾回收（Generational Garbage Collection），这是一种高效的算法，依赖于动态分配记录的两个特性，通常用于 LISP 和其他编程语言中：\n年轻记录更有可能迅速死亡，而老记录则有更大的存活可能。如果一个记录已经存活了很长时间，那么它很可能继续存活；相反，一个新的记录更可能只是一个临时的中间值。 年轻记录通常指向老记录，因为在 LISP 和函数式编程语言中，分配一个新记录时，它通常会被初始化为指向已经存在的记录。 根据特性1，大部分垃圾回收器的工作应集中在较年轻的记录上；根据特性2，垃圾回收器可以利用这一特性来优化回收效率。分配的记录会被保存在内存中的不同区域 $G_i$，称为“代”（Generations）。每代中的记录具有相似的年龄，$G_i$ 代中的所有记录都比 $G_{i+1}$代中的记录更老。基于观察，较低代 $G_i$ 通常不会指向较高代 $G_j$，因此垃圾回收器通常只在最年轻的代中进行回收，因为那里含有最多的垃圾。\n为了回收某一代，回收器需要知道所有指向这一代的指针。这些指针可能存在于机器寄存器、全局变量或堆栈中。由于年轻记录几乎不会指向老记录，通常只有对现有记录的赋值操作才可能打破这一假设。因此，垃圾回收器需要检查每次修改，以确保不会破坏这一规则。\n要检测这种指针的赋值操作，通常有以下两种方法：\n使用专用硬件支持，进行修改检测。 通过编译器检查赋值，需要执行额外的指令（大约2条或更多）。幸运的是，LISP、Smalltalk 等语言中的非初始化赋值相对较少，因此额外的检测开销约占总执行时间的5-10%。 虚拟内存硬件可以通过“脏页”机制来检测对老对象的修改。如果操作系统提供了“脏页”（dirty）功能，垃圾回收器可以检查被修改的页面，找出从老代到新代的指针，并进行处理。如果没有这种服务，垃圾回收器可以使用页面保护机制：将老代的页面设置为只读，任何对它们的写操作都会引发异常。用户模式的异常处理器可以将触发异常的页面添加到列表中供垃圾回收器扫描，之后页面必须被解除保护以允许写入操作。\n这一算法的多个变体表现出良好的性能。随着堆和内存的增大，这种机制开始优于其他技术。\n该技术依赖于以下虚拟内存原语：\ntrap：捕获指针赋值异常。 protN：将多个页面设置为只读。 unprot：解除页面的只读保护。 此外，由于用户模式处理器处理页面的时间与页面大小无关，而垃圾回收器扫描页面的时间与页面大小成正比，因此使用较小的页面大小可能更为合适。\n这部分总结了分代垃圾回收的工作原理及其如何利用虚拟内存原语提高效率。\n在**Generational Garbage Collection（分代垃圾回收）**部分中，指针起到了连接不同代对象的作用。新生成的对象（新代）往往会指向已经存在的老对象（老代）。论文提到，这种指向老代的指针行为在分配时很常见，而从老代指向新代的指针是较为罕见的。为了确保在垃圾回收过程中能够有效处理这些异常情况，需要专门监控这些指针变化。\n分代垃圾回收机制中，垃圾收集器重点处理新生成的对象，因为这些对象更可能迅速变为垃圾。为了高效回收，垃圾收集器只需要知道指向新代（年轻代）的指针，这些指针可能存在于寄存器、全局变量、或堆栈中。然而，根据观察，老代几乎不会有指针指向新代。唯一的可能性是通过赋值操作使得老代指向新代。\n因此，为了检测这种从老代指向新代的异常赋值操作，系统会在修改堆对象时进行检查，以确保不违反“新代指向老代”的假设。如果检测到从老代指向新代的指针，垃圾收集器需要做进一步处理。操作系统或硬件可以通过脏页机制（dirty page）来辅助这个过程：垃圾收集器可以扫描那些被标记为“脏页”的内存，找出其中的老代指向新代的指针并处理这些指针。\n如果操作系统没有提供脏页机制，垃圾收集器可以通过页面保护机制来实现这一目的：通过将老代的页面设置为只读模式，任何对这些页面的写操作都会引发页错误（trap）。用户态的错误处理程序会捕获这些错误，将涉及的页面添加到待扫描列表中，并解除该页面的只读保护以继续允许写操作。最后，在垃圾回收时，收集器会扫描这些列表中的页面，以查找并处理可能指向新代的指针。\n“找出从老代到新代的指针，并进行处理”，具体来说，这个过程如下：\n检测指针变动：当老代对象尝试指向新代时，会通过页面保护或脏页机制检测到该指针变动。 处理指针：垃圾收集器扫描相关页面，识别出从老代指向新代的指针。 解除保护：识别和处理后，解除该页面的只读保护，恢复其正常的读写权限。 这一机制允许系统高效地处理指针赋值带来的潜在错误，同时优化了垃圾回收的效率，尤其是在内存增大的情况下。\nPersistent stores(持久存储) 持久存储（Persistent Stores）是用于创建超越单个程序调用的动态分配堆。程序可以遍历和修改数据结构、提交更改，甚至可以在不进行永久更改的情况下放弃这些更改。\n在持久存储中，指针遍历（即读取操作）的设计与在主存储器中的数据读取操作（fetch）一样快速。这是因为持久存储通常被映射到磁盘文件上，从而允许在访问存储的新部分时触发页面错误（类似于内存中的正常操作）。\n因此，就读取数据而言，持久存储确实可以像正常的内存获取操作一样高效。然而，写入操作则需要额外处理，例如跟踪“脏页”（dirty pages）和提交更改到磁盘，这可能会引入一定的延迟，具体取决于实现方式。\n总结一下，从持久存储中读取数据的效率可以与普通的内存获取操作一样迅速，但写入操作可能会因为需要提交更改而稍有延迟。\nExtending addressability 在“扩展寻址能力”部分，作者讨论了如何处理持久存储增长到超过处理器地址空间的情况。\n由于现代磁盘驱动器（尤其是光盘）可以存储大量数据，持久存储有可能增长到包含超过232232个对象，因此32位的地址空间无法足够支持。虽然单个程序运行期间可能访问的对象数量不会超过232232，但整个数据库的规模可能会超出这一限制。\n为了解决这个问题，可以修改持久存储的机制，使得在内存中的对象使用32位地址，而在磁盘上的对象使用64位地址。具体的做法是，将磁盘上的页面设定为内存页面的两倍大小。当从磁盘加载页面到内存时，64位的磁盘指针将通过翻译表转换为32位的内存指针。当这些32位指针首次被解引用时，可能会触发页面错误，页面错误处理程序会从磁盘中加载另一个页面，并将其转换为短指针。\nExtending addressability翻译表 翻译表只为单次执行过程中访问的对象创建条目，这就是为什么32位的指针在内存中仍然足够使用。内存中的指针可以指向尚未被访问的页面，而这些页面还未被分配到内存中，但翻译表中会有条目指出它们在磁盘上的位置。\n这个方案的概念源自Smalltalk-80中的LOOM系统，而通过页面错误机制实现这一点是较新的方法。\n关于“翻译表只为单次执行过程中访问的对象创建条目，这就是为什么32位的指针在内存中仍然足够使用”的理解，我来帮您详细解释一下：\n这里的核心观点是：32位指针在内存中仍然足够使用，因为在任何一次程序运行时，不会访问超过232232个对象。也就是说，虽然持久存储的大小可能超过232232个对象（这时需要64位指针来表示这些对象在磁盘上的位置），但在单次程序执行中，程序通常只会访问这些对象中的一小部分。因此，在内存中，使用32位指针就足够了。\n翻译表的作用是：当从磁盘加载某个页面时，64位的磁盘指针通过翻译表转换成32位内存指针。只有访问到特定的对象时，才需要为该对象分配内存地址，并在翻译表中创建相应的条目。这意味着翻译表只包含当前程序运行中实际访问的对象。由于每次程序运行时访问的对象总数远远小于232232，因此32位指针足以处理这些内存中的对象。\n因此，32位指针在内存中足够使用，是因为翻译表动态地只为当前执行过程中访问到的对象分配内存和创建条目，内存中不需要同时容纳所有的对象。\nData-compression paging **数据压缩分页（Data-compression paging）**如下：\n在典型的链式数据结构中，许多字会指向相邻的对象，许多字是空（nil）值。那些包含整数而不是指针的字通常包含小整数或零。简而言之，平均每个字的熵很小；此外，垃圾收集器可以将互相指向的对象放置在邻近位置，从而将每个字的熵减少到大约7个比特。\n因此，通过使用数据压缩算法，一个由32位字组成的页面可以被压缩到大约四分之一页面大小。与其将最近未使用的页面直接分页到磁盘，不如先将它们压缩后放回主存储器中【36】。这样，当这些虚拟页面再次需要时，解压缩可能比从磁盘中取回要快得多。当然，经过长时间未使用后，压缩后的页面仍然可以被发送到磁盘中。\n当然，数据压缩分页可以在操作系统中以透明方式实现，对用户进程完全不可见【28】。然而，由于垃圾收集器可以移动对象以最小化它们的熵，如果用户进程能够控制如何以及何时进行压缩，可能会获得更好的效果。\n这个算法需要以下支持：\ntrap：用于捕捉页面访问故障。 prot1（或protN）：用于保护或解除保护某个页面（或一组页面），这通常需要谨慎的缓冲操作。 unprot：用于解除对页面的保护。 此外，还需要判断哪些页面是最近没有被使用的。可以通过偶尔保护页面（测试是否有引用）来完成，也可以通过操作系统和硬件的帮助来实现这一点。\n通过一个例子来解释“数据压缩分页”的过程。\n假设我们有一个应用程序使用链式数据结构来管理对象，这些对象相互引用，并且很多字段都是空值（nil），或者是常见的小整数（如0）。这些对象存储在内存的多个页面中，每个页面大小为4KB。随着程序的运行，有些页面可能长时间不被访问，因此可以将这些页面从内存中移出。\n在传统的虚拟内存系统中，未使用的页面会被直接分页到磁盘，等到需要时再从磁盘取回。但如果我们使用“数据压缩分页”的方法，这个过程会有所不同。\n压缩未使用的页面：当系统检测到某个页面长时间未被访问时，而不是直接将其分页到磁盘，系统首先对页面进行数据压缩。由于页面中的很多数据是空值或者小整数，这些数据的熵很低，可以使用数据压缩算法将这些4KB的页面压缩到大约1KB大小。 保留在内存中：压缩后的页面不会立即被分页到磁盘，而是依然保存在主存储器中，只是占用的空间大大减少了。这样，当这些页面再次需要时，程序可以直接从内存中解压缩页面，而不需要从磁盘读取。解压缩的速度通常比从磁盘读取要快得多。 需要时解压缩：当程序再次访问到这些页面时，系统会检测到该页面已经被压缩，这时只需进行快速的解压缩操作，就可以将页面恢复到原来的状态，让程序继续运行。 最终分页到磁盘：如果页面长时间不被访问，系统仍然可以选择将压缩后的页面最终分页到磁盘，但通过这种方法，系统可以减少频繁的磁盘访问，从而提高效率。 这个过程的一个关键优势在于：对于内存中那些长时间不被访问的页面，通过压缩操作可以在不牺牲性能的前提下释放大量内存。而且，由于解压缩比从磁盘读取快得多，这样的机制比传统的分页系统更高效。\nHeap overflow detection 进程或线程的栈需要保护，以防止溢出访问。一种常见且实际的技术是将栈顶上方的页面标记为无效或不可访问。任何对这些页面的内存访问都会导致页面错误，操作系统可以捕捉该错误并通知用户程序栈溢出。在大多数Unix实现中，栈页面在首次使用之前不会被分配；操作系统对页面错误的响应是分配物理页面，标记为可访问，并继续执行而不通知用户进程（除非资源限制被超过）。\n这种技术需要trap（捕获异常）、protN（减少多个页面的访问权限）和unprot（增加页面的访问权限）。但由于页面错误的发生率很低（大多数进程不会使用太多的栈空间），效率并不是一个问题。\n同样的技术可以用于检测垃圾回收系统中的堆溢出【2】。通常，堆溢出通过在每次内存分配时执行比较和条件分支来检测。通过让用户进程在一块由保护页面（guard page）终止的内存区域中分配新记录，可以消除比较和条件分支。当可分配的内存区域用完时，页面错误陷阱会调用垃圾回收器。通常可以安排不需要重新调整内存保护，因为在回收后可以重新使用相同的分配区域。因此，这项技术只需要prot1（减少单个页面的访问权限）和trap。\n在这里，陷阱的效率非常重要。某些语言的实现可能每50条指令就分配一个新单元。在分代垃圾回收器中，分配区域的大小可能非常小，以便让最年轻的一代完全适合于数据缓存。例如，一个64KB的分配区域可以容纳16,000个8字节的链表单元。在一个非常频繁分配的系统中（如将激活记录存储在堆上的系统），这样一小部分数据将是存活的，因此垃圾回收所需的时间本身将非常短。\n因此，我们可以得出：\n堆溢出之前执行的指令数量：$64K/8×50=400K$\n使用比较和分支的开销指令数量：$64K/8×2=16K$\n如果处理一个陷阱需要1200个周期（这是典型的情况，见第4节），那么这种技术可以将开销从4%减少到0.3%，这是一个值得的节省。如果陷阱处理时间更长，那么这种技术将不那么高效。\n由于还有其他好的技术可以减少堆限制检查的开销，比如在展开的循环中结合连续分配的限制检查，因此虚拟内存的这一应用可能是本文讨论的所有应用中最不有趣的。\n文中提到，传统的堆溢出检测通常是在每次内存分配时，通过比较和条件分支（compare and branch）的方式来检测是否发生了堆溢出。这种方法需要在每次分配时额外执行两条指令来进行比较和条件分支，因此在系统中，这个过程会增加额外的指令开销。\n而通过使用trap机制，这种额外的指令开销可以被消除。系统可以通过设置一个保护页面（guard page），当内存分配超出可分配的范围时，程序会触发页面错误（page fault），然后调用垃圾收集器来处理这个溢出。这就不再需要每次分配时进行比较和条件分支的操作，减少了指令数量。\n指令数量的关系计算：\n在传统的比较和分支方法中，每次内存分配都需要两条指令来进行检查。 在使用trap的情况下，这些检查可以通过页面错误来触发，减少了频繁的比较和分支操作。 trap所需的周期数是指，当程序触发页面错误时，操作系统或硬件需要花费的时间来处理该错误。文中提到，处理一次页面错误的trap大约需要1200个周期。相比之下，程序执行普通指令（如加法）所需的时间要少得多。例如，在一个典型的处理器上，执行一条指令可能只需要1到几个周期。\n文中提到：\n如果没有trap机制，堆溢出检测每次都需要比较和分支，这会导致**4%**的性能开销。 使用trap后，虽然每次页面错误处理需要大约1200个周期，但它只会在堆溢出时触发，而不是在每次分配时进行比较和分支，因此可以将性能开销减少到0.3%。 trap的原理是利用页面保护机制。当程序尝试访问一个受保护的页面时，系统会捕获这个错误，并触发一个陷阱（trap）。操作系统或用户态的错误处理程序会处理这个错误，然后可能会解除页面的保护，允许程序继续访问该页面。这个过程的关键在于减少程序在正常情况下的指令开销，而只在必要时通过页面错误机制处理特殊情况。\n在文中的例子中，垃圾收集器会在分配区域末端设置一个保护页面。当程序试图超出分配区域时，系统触发页面错误，这时垃圾收集器被调用，清理堆内存并允许程序继续运行。\nVM primitive performance 几乎我们在本文中描述的所有算法都属于以下两类之一。第一类算法会将页面批量保护起来（使其无法访问），然后在每次页面错误（page-fault）陷阱发生时，逐个解除这些页面的保护状态。第二类算法则是对页面进行单独的保护和解除保护操作。由于 protN（批量保护）、trap（陷阱）和 unprot（解除保护）始终是一起使用的，如果其中一个操作效率很高，而其他操作非常慢，那么该操作系统的性能也不会很好。\n我们对整体的用户模式虚拟内存性能进行了两项测试。第一项测试是测量 prot1、trap 和 unprot 的总耗时，方法是对以下基准程序执行100次：\n访问一个随机被保护的页面，然后在错误处理程序中保护另一个页面，并解除对发生错误的页面的保护。 为了得到更准确的时间测量，这个过程重复了100次。\n第二项测试是测量 protN、trap 和 unprot 的总耗时，基准程序的步骤如下：\n保护100个页面； 以随机顺序访问每个页面； 在错误处理程序中解除对发生错误的页面的保护。 在计时开始之前，这两个程序都会对每个页面进行写操作，以消除填充缓存和TLB（转换后备缓冲区）所带来的暂时性影响。\n我们对几种操作系统的性能进行了比较，包括 Ultrix、SunOS 和 Mach，分别在不同平台上运行这些基准程序。为了校准测量结果，我们还给出了单条指令（add）的执行时间，该时间是通过一个包含20条指令的循环（包括18个加法操作、一个比较和一个分支）测量出来的。我们还给出了一个 trap（陷阱处理程序）的时间，它不改变任何内存保护；这个时间对于堆溢出检测是有用的。结果见表1。需要注意的是，这个基准测试不是一个“整体操作系统吞吐量”的测试【27】，不应该受到磁盘速度的影响；它测量的是用户级程序对虚拟内存服务的CPU处理性能。\n我们还尝试在同一进程中使用两个不同的虚拟地址映射同一个物理页面，分别使用 SunOS 和 Ultrix 的共享内存操作（shmop），以及 Mach 中的 vmamap。SunOS 和 Mach 支持此操作，但 Ultrix 不允许在同一进程中使用 shmat 连接同一个共享内存对象到不同地址。\n显然，不同操作系统在相同硬件上的性能差异很大。这表明某些或所有系统中都有很大的改进空间。此外，一些操作系统的某些版本在 mprotect 调用后没有正确刷新 TLB，这表明许多操作系统实现者并没有认真对待这个功能。\n重要的是，这些操作系统服务的效率要高。这里的论点比“高效是好的”这个空洞的说法更具体。对于磁盘分页而言，页面错误通常意味着要等待20毫秒才能让磁盘旋转到正确的扇区；因此，3至5毫秒的错误处理开销几乎不会被视为错误处理延迟的贡献者。但是在本文中描述的算法中，错误将完全由CPU处理。例如，我们已经实现了一个垃圾收集器，在20 MIPS的机器上，处理一个页错误需要执行大约10条指令来完成对 to-space 的每个字（word）的操作。在一个页大小为4096字节（1024字）的系统中，计算处理时间大约为500微秒。如果操作系统的错误处理和页面保护开销为1200微秒（如平均所示），那么操作系统显然成为瓶颈。\n如果程序表现出良好的访问局部性，那么垃圾收集产生的错误将很少，操作系统的开销就不那么重要。但对于实时程序，它们必须满足严格的延迟要求，即使偶尔出现的“慢错误”也会造成问题。例如，如果客户端程序不能被中断超过1毫秒，那么500微秒的错误处理计算时间几乎没有给操作系统提供额外1200微秒的空间！(这个问题在我们考虑多个连续错误时变得更加复杂；详见【11】)\n为了比较不同架构上的虚拟内存基元性能，我们用处理器速度对测量值进行了归一化。图4显示了各处理器在保护一个页面、发生错误以及解除保护一个页面所需时间内可以执行的 add 指令数。\n我们的基准测试表明，不同操作系统对虚拟内存基元的实现效率存在很大的差异。在 NX/2 操作系统上运行的基于 Intel 80386 的机器【29】（一种用于 iPSC/2 超立方体多计算机的简单操作系统）在我们的基准测试中表现最佳。其归一化基准性能约为表现最差的系统（在 Sparcstation 上运行的 Mach）的十倍。显然，这些基元没有固有的原因一定会慢。硬件和操作系统设计者应将内存保护性能视为设计过程中需要权衡的一个重要方面。\nTLB的回顾 在现代计算机系统中，**转换后备缓冲区（TLB, Translation Lookaside Buffer）**是一个缓存，用来存储虚拟地址到物理地址的映射。它的作用是加速虚拟内存系统中地址转换的过程，避免每次都需要通过页表（page table）来查找地址映射。\n当程序访问一个虚拟地址时，处理器首先会在TLB中查找对应的物理地址。如果找到（TLB命中），处理器可以快速访问数据；如果找不到（TLB未命中），系统会去页表中查找并更新TLB。\n在进行内存访问控制时，可能会改变某些页面的访问权限。比如，程序可能设置某个页面为可访问或不可访问。此时，TLB 中缓存的映射信息可能已经过时，而这会影响系统的行为。\n当页面被设为“更易访问”时，TLB 中的过时信息是无害的。\n假设之前一个页面是不可访问的（例如，页面被保护以防止读写），并且TLB中缓存了这个页面的不可访问状态。现在，如果我们将该页面设为可访问的，但TLB 中依然存在不可访问的过时条目，那么最坏的情况是系统会因为TLB未更新，错误地认为页面不可访问，从而触发一次TLB未命中或者页错误（page fault）。此时，系统会发现页面实际上是可访问的，于是会更新TLB并允许访问。虽然会多一次错误处理，但这不会影响程序的正确性，因为页面本身现在是可访问的。\n当页面被设为“不易访问”时，TLB 中的过时信息可能导致非法访问。\n如果页面原本是可访问的，而现在被设置为不可访问，但TLB 中依然缓存着可访问的过时映射，那么程序仍可能通过过时的 TLB 条目访问该页面。这时，系统没有触发访问保护机制，程序可能进行非法读写操作，导致数据不安全或者程序崩溃。因此，这种情况是有害的。\n举个例子：内存页面权限更改\n场景1：页面设为更易访问\n假设有一个页面 P，最初是不可写的（只读），程序试图写入该页面时触发了保护机制，系统将该页面设为可写。但由于TLB中还缓存着页面 P 的不可写权限（过时信息），程序接下来的写操作可能触发TLB未命中。系统会发现这是过时信息，重新查找并确认页面 P 现在是可写的，于是更新TLB条目，并允许程序写入。这只是一次额外的查找，最终程序能够正常运行，因此无害。\n场景2：页面设为不易访问\n现在设想页面 P 原来是可写的，但为了保护数据，系统将该页面设为只读。如果 TLB 中仍然缓存着原来的可写权限，那么程序接下来的写操作将绕过系统的保护机制，导致非法修改。由于TLB条目没有更新，操作系统不会立即发现这个问题，直到后来数据不一致或崩溃。因此，这种过时的 TLB 信息会导致非法访问，必须避免。\n所以，\n设为更易访问时：过时的 TLB 条目导致一次额外的处理（如TLB未命中或页错误），但不会影响数据安全或程序正确性。\n设为不易访问时：过时的 TLB 条目会导致非法访问，破坏系统保护机制，必须通过刷新 TLB 来保证页面访问权限的正确性。\n这也是为什么在内存访问权限变得更严格（如设为不可访问）时，必须确保所有处理器的 TLB 中的信息一致。否则，就可能导致安全漏洞或程序异常。\nTLB Consistency 许多在本文中介绍的算法，都会以批量方式减少内存的可访问性，并在每次访问时逐页增加内存的可访问性。这对于并发垃圾收集、分代垃圾收集、并发检查点、持久存储和扩展寻址能力来说尤其如此。\n这对多处理器来说是一件好事，因为它能够解决转换后备缓冲区（TLB）一致性问题。当一个页面被设为更易访问时，TLB 中的过时信息是无害的，最多只会导致一个虚假的、容易修复的 TLB 未命中或 TLB 错误。然而，当页面被设为不易访问时，TLB 中的过时信息可能导致非法访问此页面。为了防止这种情况，有必要从每个可能包含该页面的 TLB 中刷新此页面。这个**“清除（shootdown）”**过程可以通过软件完成，即通过中断每个其他处理器并要求它们刷新 TLB 中的该页面，或者通过各种总线系统在硬件上完成【7，32】。\n如果有许多处理器需要中断，软件清除过程可能会非常昂贵。我们解决清除问题的方案是批量清除；覆盖多个页面的清除成本并不比单个页面清除的成本高多少，当处理多个页面时，清除的成本可以被分摊，单个页面的成本变得可以忽略不计。本文中描述的那些以批量方式保护页面的算法“无意中”利用了批量清除的优势。\n批量操作的概念是根据我们在本文中描述的算法结构自然得出的，但它也可以用于解决传统的磁盘分页中的清除问题。在磁盘分页中，为了释放物理页面供其他虚拟页面使用，页面被设置为不易访问（即“分页出去”）。如果操作系统能够维持一个较大的未使用物理页面的预留空间，那么它可以以批量方式执行分页操作（以补充预留空间）；这样可以将清除成本分摊到整个批量操作中。因此，尽管有人声称软件解决方案表现良好，但可能需要硬件支持【7】，通过批量处理，硬件支持可能就不再必要。\n这段内容主要讨论了如何通过批量处理页面访问控制来优化 TLB 的一致性问题，特别是在多处理器系统中批量处理能够显著减少系统开销。\n它是如何应用在传统的磁盘分页问题中的。\n首先，文中提到的“批量操作”是指同时对多个页面执行某种操作（例如设置访问权限或执行清除），而不是一次只对单个页面进行操作。这种批量操作在处理大量页面时更有效，因为可以将多个页面的处理开销合并，降低每个页面操作的成本。\n在传统的虚拟内存系统中，当内存不足时，操作系统会通过磁盘分页将不经常使用的物理内存页面分页到磁盘（即“交换出去”）。当需要这些页面时，再从磁盘读取回内存。这个过程中涉及到将内存页面标记为“不易访问”或“不可访问”，以释放物理内存。\n问题： 当页面被设置为不可访问时，系统需要确保这些页面的旧访问权限不会留在TLB中。否则，处理器可能仍然使用过时的权限，导致非法访问这些已分页出去的页面。为了防止这种情况发生，操作系统需要将这些页面从各个处理器的 TLB 中清除（即清除操作或 shootdown）。\n在传统的分页过程中，当操作系统分页出去页面时，如果每次都对每一个页面单独执行清除操作（逐个刷新每个页面的 TLB 条目），这个开销会很大，尤其是在多处理器系统中。每个处理器都必须被中断，要求它们刷新 TLB，可能会造成性能瓶颈。\n文中提出的批量操作解决方案可以用于优化这个过程：\n批量清除： 而不是逐个处理每个页面的清除（刷新 TLB），可以将多个页面一起批量执行。清除多个页面的成本与清除单个页面的成本相差不大，因此通过批量处理可以将开销分摊到多个页面上，从而降低每个页面的平均处理成本。 预留空间： 通过保持一部分未使用的物理内存页面（预留空间），操作系统可以一次性批量将多个虚拟页面分页出去，释放物理内存。这种批量分页操作减少了单个页面分页出去时所需的清除次数和中断处理器的次数，提升了性能。 在传统的磁盘分页中，操作系统通常需要依赖硬件支持来加速这些分页和清除操作。然而，如果批量处理做得足够高效，那么这些分页和清除操作的开销会显著降低，可能就不再需要特殊的硬件支持。这意味着即便没有硬件优化，操作系统仍然可以通过批量操作来实现较高的性能。\n文中通过将批量处理应用到磁盘分页，解决了以下问题：\nTLB 清除的开销： 多个页面的清除成本可以被分摊，减少了逐个页面执行清除时的成本。 硬件依赖性降低： 批量处理可以使分页操作的效率提升，降低对硬件支持的需求。 最终，批量操作不仅适用于本文中讨论的高级算法（如垃圾收集），也可以用于传统的磁盘分页系统，以更高效地释放物理内存和管理页面权限。\nOptimal page size 选择页面大小是一项重要的设计决策，受多种因素的影响。\n首先，小页面的主要优点是：当从磁盘读取或写入内存时，传输时间是线性增加的，而查找时间是固定的。因此，较小的页面减少了未使用数据传输到磁盘的浪费。这对于具有糟糕的局部性的应用程序来说尤为重要，因为它们的每个页面中只有很少的字被访问。\n另一方面，大页面有助于减少页表的开销，尤其是在使用二维或多级页表时。此外，大页面能够显著降低TLB未命中的频率，因为TLB每个条目可以映射更大的内存区域。如果TLB条目的数量是固定的，页面越大，每个条目能覆盖的内存区域就越多，从而减少了TLB未命中的可能性。\n因此，页面大小的选择涉及在减少磁盘传输时间和减少TLB未命中之间的权衡。为此，有几条经验法则：\n对于随机内存访问，即当程序没有表现出良好的局部性时，小页面往往更有效，因为它们减少了未使用数据的传输。 对于表现出良好局部性的程序，大页面更有利，因为它们能够减少TLB未命中和页表开销。 现代磁盘和内存系统的性能特征表明，随着传输速率的提高，大页面的相对优势在增加。 有些系统允许在运行时调整页面大小，而有些系统则通过使用多个页面大小（称为“大页面支持”）来允许某些进程使用较大的页面。后一种方法通常在大型科学应用或数据库系统中使用，它们具有良好的局部性并且需要大量内存。\n总而言之，最佳页面大小的选择依赖于应用程序的特性以及系统的具体硬件特征。\nAccess to protected pages \u0026ldquo;Access to protected pages\u0026quot;部分讲述了在多处理器系统中，用户模式的服务程序需要访问那些对客户端线程不可访问的页面。这种需求出现在并发垃圾收集、扩展寻址能力、共享虚拟内存以及数据压缩分页等算法中。\n实现对受保护页面的访问有几种方式：\n在同一个地址空间中，给相同页面提供多个映射，并且每个映射具有不同的保护级别。例如，垃圾收集器可以通过一个“非标准”地址访问to-space的页面，而mutator看到的to-space页面则是受保护的。 提供一个系统调用来复制受保护区域的内存。垃圾收集器可以在处理每个页面时调用该系统调用三次：一次用于从from-space复制记录到to-space；一次用于扫描to-space页面之前；还有一次是在扫描结束后，在让mutator访问该页面之前。这种方法效率较低，因为需要频繁地复制数据。 在允许共享页面的操作系统中，垃圾收集器可以运行在不同的重量级进程中，这个进程有一个不同的页表。这种方法的问题在于每次垃圾收集页面故障时需要进行两次昂贵的重量级上下文切换。然而，在多处理器系统中，可能通过远程过程调用（RPC）通知另一个已经处于正确上下文中的处理器，这样该选项可能会更加有吸引力。 垃圾收集器可以在操作系统内核中运行。这种方式效率最高，但可能不适合因为垃圾收集器并不应该运行在内核中。此外，每种编程语言的运行时数据格式不同，内核可能无法处理所有语言的垃圾收集。 文中建议，对于物理寻址缓存的计算机架构来说，在同一地址空间中提供多重虚拟地址映射是一个干净且高效的解决方案。它不需要重量级上下文切换，也不需要复制数据结构，也不需要将垃圾收集器运行在内核中。缺点是每个物理页面可能需要在页表中占用两个不同的条目，增加了物理内存的需求，不过这个增量很小。\n在具有虚拟寻址缓存的系统中，多重虚拟地址映射可能会带来缓存一致性问题，但这在垃圾收集算法中很容易解决。在垃圾收集器扫描页面时，mutator无法访问该页面，因此mutator地址空间中不会有该页面的缓存行。扫描结束后，垃圾收集器应刷新该页面的缓存行，从而避免不一致性问题。\n这段内容强调了在并发计算环境中如何实现受保护页面的高效访问，以及不同方法的优缺点。\n垃圾收集器在处理每个页面时调用系统调用三次，是因为它们分别对应不同的步骤，以确保页面的有效同步和访问控制。具体作用如下：\n第一次调用：从 from-space 复制到 to-space。垃圾收集器需要将已经不再使用的对象从一个内存区域 (from-space) 复制到另一个内存区域 (to-space)。为了进行这次复制操作，垃圾收集器需要调用系统以访问并复制受保护的内存页面。 第二次调用：在扫描页面之前。在垃圾收集器准备扫描 to-space 中的新页面时，它需要确保该页面中不再包含任何指向旧空间 (from-space) 的指针。为了在扫描过程中进行这项操作，垃圾收集器需要再次通过系统调用获取对该页面的访问权限。 第三次调用：扫描结束后，将页面提供给 mutator 访问。当垃圾收集器完成了对页面的扫描并确保页面中所有的指针都已更新为指向 to-space 的新对象后，它会调用系统，让 mutator (主线程) 可以安全地访问该页面，从而使程序的其他部分能够继续正常运行。 每次调用系统调用的主要目的在于保证内存页面的访问权限和内容在不同阶段的正确性和一致性。这样设计的好处在于保证了并发垃圾收集过程中，collector 和 mutator 之间的同步，同时也确保了指针的正确更新和数据的安全性。\n多重虚拟地址映射可能会带来缓存一致性问题的原因与计算机体系结构中的缓存机制有关。缓存的主要目的是提高内存访问速度，将常用数据存储在较靠近处理器的高速存储器中，而不是每次都从较慢的主内存中读取。缓存通常是按照虚拟地址进行索引的，这意味着相同的物理内存页面如果被映射到多个虚拟地址上，那么系统在不同的虚拟地址下访问该页面时，可能会在缓存中存储多个副本。由于这些副本可以存在于缓存的不同位置，这就可能导致缓存中的数据不一致问题。\n当系统中存在多个虚拟地址指向同一物理页面时，以下情况可能会发生：\n缓存不一致性：一个虚拟地址指向的物理页面可能会被更新，但另一个映射到该物理页面的虚拟地址却没有反映这些更新，因为它的缓存副本没有被同步更新。这会导致两个虚拟地址对应的缓存中的数据不同步，出现数据不一致的情况。 数据同步问题：如果同一个物理页面有多个缓存副本，当系统从某个虚拟地址修改数据后，如果不及时更新或清除其他虚拟地址的缓存副本，处理器在访问这些虚拟地址时可能会读取过时的数据，从而导致缓存一致性问题。 缓存通常会根据虚拟地址进行分配，如果同一个物理页面通过多个虚拟地址映射，这些虚拟地址可能在缓存中占用不同的缓存行，而这正是造成不一致的根本原因。这种不一致性可能导致程序的错误运行，尤其是在垃圾回收等需要保证内存一致性的场景下。\n所以，每个虚拟地址不会抽出一个特定的缓存块，但同一个物理页面可能通过多个虚拟地址映射，从而在缓存中创建多个条目，增加了保持这些条目一致的复杂性。\nIs this too much to ask? 有些Unix的实现，在某些机器上，提供了非常干净且同步的信号处理机制。引发页面故障的指令会调用信号处理程序，而不会改变处理器的其他状态；后续的指令不会执行，等等。信号处理程序可以完全同步地访问机器寄存器，改变内存映射或机器寄存器，然后重新开始执行故障指令。然而，在高度流水化的机器上，可能会存在多个未处理的页面故障【26】，并且可能在注意到故障之前，多个故障后的指令已经将其结果写入寄存器；这些指令可以恢复执行，但不能重新启动。\n当用户程序依赖于同步行为时，它们在流水线机器上运行会很困难。现代UNIX系统让用户程序主动参与内存管理功能，允许它们显式地操作其内存映射。这在某种程度上就像是发出了“通往地狱的镌刻请柬”【26】。\n如果所描述的这些算法确实与快速流水线机器不兼容，那将是一个严重的问题。幸运的是，除了一个例子之外，我们描述的所有算法都是足够异步的。从机器的角度来看，它们的行为非常像传统的磁盘分页：发生故障，提供物理页面，使该页面在页表中可访问，然后继续执行。\n例外的是堆溢出检测：页面故障会启动垃圾回收，垃圾回收会修改寄存器（通过将其指向堆记录的新位置），然后恢复执行。存储下一分配单元地址的寄存器会被调整以指向分配空间的起始位置。之前发生故障的指令会重新执行，但这次它不会再发生故障，因为它正在存储到一个不同的位置。\n这种行为在高度流水化的机器上是不可接受的（除非像在VAX 8800上那样【10】有硬件用于“撤销”那些已经完成的后续指令或地址模式副作用）。实际上，即便在Motorola 68020上，使用页面故障来检测堆溢出也不可靠。\n因此，除了堆溢出检测之外，我们提出的所有算法对于硬件来说不会比普通磁盘分页带来更多问题，可以退还这张“通往地狱的请柬”；然而，操作系统必须确保为硬件提供足够的支持，以正确恢复半同步陷阱处理程序的故障操作。\nheap overflow detection and other algorithms 堆溢出检测算法与其他内存管理算法在虚拟内存管理中的相同点和不同点如下：\nSimilarities\n依赖页面保护机制： 无论是堆溢出检测还是其他内存管理算法，二者都依赖于页面保护机制来监控内存访问。这些算法利用设置页面为“不可访问”或“只读”等来触发页面错误（page fault），并在发生错误时进行相应处理。 通过页面错误进行内存管理： 堆溢出检测与诸如并发垃圾回收（concurrent garbage collection）等其他算法都依赖于操作系统提供的页面错误处理机制，以便在内存使用达到某个限度时进行回收或管理。 内存同步机制： 两种算法都涉及多线程或多处理器环境下的内存同步。在并发垃圾回收算法中，利用页面保护机制来同步不同线程对同一内存区域的访问。而堆溢出检测则使用页面错误触发垃圾回收，以防止超出可分配内存。 Differences\n触发条件不同： 堆溢出检测算法：该算法主要依赖于页面错误来检测堆内存溢出。当分配空间到达预设的保护页面时，触发页面错误，并调用垃圾回收器来整理内存。 其他算法（例如并发垃圾回收或世代垃圾回收）：这些算法的页面错误触发更多与内存中对象的移动或跨线程同步相关。并发垃圾回收利用页面保护来防止线程之间的不正确数据访问，而世代垃圾回收通过监测跨代指针来进行内存管理。 错误处理的复杂性： 堆溢出检测：在堆溢出检测中，页面错误的处理相对简单，主要目的是调用垃圾回收器以释放内存并更新指针。垃圾回收完成后，内存可以再次分配。错误处理不涉及复杂的线程同步或指针更新。 并发垃圾回收：并发垃圾回收的页面错误处理则更加复杂。除了要处理指针的移动外，还要确保线程之间的正确同步，防止一个线程在另一个线程完成垃圾回收之前访问被移动的对象。 性能影响： 堆溢出检测：堆溢出检测中，页面错误的发生频率与内存分配频率直接相关。如果内存分配速度快，页面错误将频繁发生，从而导致更多的垃圾回收调用。文档中提到，堆溢出检测中，每分配约50次内存就会引发一次页面错误。 其他算法：其他算法（如世代垃圾回收）依赖于内存分代的特点，仅在较长时间内的对象进行回收时才会触发页面错误。因此，这些算法在处理频率上通常较低，尤其在新对象的分配主要集中在年轻代时。 适应硬件的能力： 堆溢出检测：在高度流水线化的处理器上，堆溢出检测可能面临问题。文档中指出，由于处理器可能在注意到页面错误之前已经执行了多个指令，因此无法可靠地恢复执行错误指令，尤其是在如Motorola 68020等架构中，堆溢出检测机制并不稳定。 其他算法：并发垃圾回收和共享虚拟内存等其他算法则更具异步性，对现代硬件架构更具适应性。这些算法通常不依赖于特定的指令状态，而是处理页面错误后继续执行程序。 堆溢出检测算法和其他内存管理算法都依赖页面错误和页面保护机制来进行内存管理，但在处理复杂性、触发条件和性能影响上存在显著不同。堆溢出检测主要用于检测堆内存的溢出并触发垃圾回收，而其他算法则更多用于并发处理、同步和跨代内存管理。此外，堆溢出检测在高度流水线处理器上面临更大的挑战，而并发垃圾回收等算法则具有更好的硬件适应性。\n正课 应用程序使用虚拟内存所需要的特性 今天的话题是用户应用程序使用的虚拟内存，它主要是受这篇1991年的论文 的启发。\n首先，你们已经知道了，操作系统内核以非常灵活的方式使用了虚拟内存Page Table。\n你们已经通过Lazy Allocation Lab，Copy on Write Lab，以及XV6中的各种内存实现了解到了这一点。\n而今天论文中的核心观点是，用户应用程序也应该从灵活的虚拟内存中获得收益，也就是说用户应用程序也可以使用虚拟内存。\n用户应用程序本身就是运行在虚拟内存之上，我们这里说的虚拟内存是指：User Mode或者应用程序想要使用与内核相同的机制，来产生Page Fault并响应Page Fault（注，详见Lec08，内核中几乎所有的虚拟内存技巧都基于Page Fault）。\n也就是说User Mode需要能够修改PTE的Protection位（注，Protection位是PTE中表明对当前Page的保护，对应了4.3中的Writeable和Readable位）或者Privileged level。\n今天的论文，通过查看6-7种不同的应用程序，来说明用户应用程序使用虚拟内存的必要性。\n这些应用程序包括了：\nGarbage Collector Data Compression Application Shared Virtual Memory 你可以发现这都是一些非常不同的应用程序，并且它们都依赖虚拟内存的一些特性来正常工作。\n所以第一个问题是，上面的应用程序需要的特性是什么？所以我们先来讨论一下需要的特性是什么？\n首先，你需要trap来使得发生在内核中的Page Fault可以传播到用户空间，然后在用户空间的handler可以处理相应的Page Fault，之后再以正常的方式返回到内核并恢复指令的执行。\n这个特性是必须的，否则的话，你不能基于Page Fault做任何事情。\n第二个特性是Prot1，它会降低了一个内存Page的accessability。\naccessability的意思是指内存Page的读写权限。\n内存Page的accessability有不同的降低方式，例如，将一个可以读写的Page变成只读的，或者将一个只读的Page变成完全没有权限。\n除了对于每个内存Page的Prot1，还有管理多个Page的ProtN。\nProtN基本上等效于调用N次Prot1，那为什么还需要有ProtN？\n因为单次ProtN的损耗比Prot1大不了多少，使用ProtN可以将成本分摊到N个Page，使得操作单个Page的性能损耗更少。\n在使用Prot1时，你需要修改PTE的bit位，并且在Prot1的结束时，需要清除TLB（注，详见4.4 Translation Lookaside Buffer），而清除TLB比较费时。\n如果能对所有需要修改的内存Page集中清理一次TLB，就可以将成本分摊。\n所以ProtN等效于修改PTE的bit位N次，再加上清除一次TLB。\n如果执行了N次Prot1，那就是N次修改PTE的bit位，再加上清除N次TLB，所以ProtN可以减少清除TLB的次数，进而提升性能。\n下一个特性是Unprot，它增加了内存Page的accessability，例如将本来只读的Page变成可读可写的。\n除此之外，还需要能够查看内存Page是否是Dirty。\n以及map2。map2使得一个应用程序可以将一个特定的内存地址空间映射两次，并且这两次映射拥有不同的accessability。\n（注，也就是一段物理内存对应两份虚拟内存，并且两份虚拟内存有不同的accessability）。\nXV6在用户程序中支持以上任意的特性吗？\n除了有类似于trap及其相关的alarm hander之外，XV6不支持任何一个以上的特性。\nXV6只有一个最小化的Unix接口，并不支持以上任何虚拟内存特性。\n尽管在XV6的内核中包含了所有的可用的虚拟内存的机制，但是并没有以系统调用的形式将它们暴露给用户空间。\n论文的观点是，任何一个好的操作系统都应该以系统调用的形式提供以上特性，以供应用程序使用。\n所以自然的，这就引出了另一个问题，当今的Unix系统的功能范围是什么？以上特性属于Unix的范畴吗？\n如果你查看现在的Unix系统，例如Linux，你会发现，或许并不与论文中描述的完全一样，但是这些特性都存在。\n在论文那个年代（1991年），某些操作系统只包含了部分以上特性，但是如今这些特性都已经在现代的Unix系统中广泛支持了。接下来我们看一下如何实现这些特性。\n支持应用程序使用虚拟内存的系统调用 mmap的系统调用 第一个或许也是最重要的一个，是一个叫做mmap的系统调用。\n它接收某个对象，并将其映射到调用者的地址空间中。\n举个例子，如果你想映射一个文件，那么你需要将文件描述符传递给mmap系统调用。\nmmap系统调用有许多令人眼花缭乱的参数（注，mmap的具体说明可以参考man page ）：\n第一个参数是一个你想映射到的特定地址，如果传入null表示不指定特定地址，这样的话内核会选择一个地址来完成映射，并从系统调用返回。\n第二个参数是想要映射的地址段长度len。\n第三个参数是Protection bit，例如读写R|W。\n第四个参数我们会跳过不做讨论，它的值可以是MAP_PRIVATE。它指定了如果你更新了传入的对象，会发生什么。\n（注，第四个参数是flags，MAP_PRIVATE是其中一个值，在mmap文件的场景下，MAP_PRIVATE表明更新文件不会写入磁盘，只会更新在内存中的拷贝，详见man page ）。\n第五个参数是传入的对象，在上面的例子中就是文件描述符。\n第六个参数是offset。\n通过上面的系统调用，可以将文件描述符指向的文件内容，从起始位置加上offset的地方开始，映射到特定的内存地址（如果指定了的话），并且连续映射len长度。\n这使得你可以实现Memory Mapped File，你可以将文件的内容带到内存地址空间，进而只需要方便的通过普通的指针操作，而不用调用read/write系统调用，就可以从磁盘读写文件内容。\n这是一个方便的接口，可以用来操纵存储在文件中的数据结构。\n实际上，你们将会在下个lab实现基于文件的mmap，下个lab结合了XV6的文件系统和虚拟内存，进而实现mmap。\nmmap还可以用作他途。\n除了可以映射文件之外，还可以用来映射匿名的内存（Anonymous Memory）。\n这是sbrk（注，详见8.2）的替代方案，你可以向内核申请物理内存，然后映射到特定的虚拟内存地址。\nmmap是实现应用程序虚拟内存的核心系统调用之一，我们稍后会将它与之前提到的特性关联起来。\n除此之外，还需要有一些系统调用来支持论文中讨论到的特性。\nmprotect系统调用 （注，详见man page ）。\n当你将某个对象映射到了虚拟内存地址空间，你可以修改对应虚拟内存的权限，这样你就可以以特定的权限保护对象的一部分，或者整个对象。\n举个例子，通过上图对于mprotect的调用将权限设置成只读（R），这时，对于addr到addr+len这段地址，load指令还能执行，但是store指令将会变成Page Fault。\n类似的，如果你想要将一段地址空间变成完成不可访问的，那么可以在mprotect中的权限参数传入None，那么任何对于addr到addr+len这段地址的访问，都会生成Page Fault。\nmunmap 对应mmap还有一个系统调用munmap，它使得你可以移除一个地址或者一段内存地址的映射关系。\nsigaction 最后一个系统调用是sigaction，它本质上是用来处理signal。\n它使得应用程序可以设置好一旦特定的signal发生了，就调用特定的函数。\n可以给它传入函数f作为特定signal的handler。\n在Page Fault的场景下，生成的signal是segfault。\n你或许之前在用户代码中看过了segfault，通常来说当发生segfault时，应用程序会停止运行并crash。\n但是如果应用程序为segfault signal设置了handler，发生segfault时，应用程序不会停止，相应的handler会被内核调用，然后应用程序可以在handler中响应segfault。\n当内核发现Page Fault时，或许会通过修复Page Table来使得应用程序还能继续执行。\n与内核响应Page Fault的方式类似，在这里的handler中或许会调用mprotect来修改内存的权限来避免segfault，这样应用程序的指令就可以恢复运行。\n与sigaction类似的有sigalarm（译注：sigalarm 是traps lab 中实现的一个系统调用，不属于标准Unix 接口），在sigalarm中可以设置每隔一段时间就调用handler。\nsigaction也可以实现这个功能，只是它更加的通用，因为它可以响应不同类型的signal。\n学生提问：看起来mprotect暗示了你可以为单独的地址添加不同的权限，然而在XV6中，我们只能为整个Page设置相同的权限，这里是有区别吗？\nFrans教授：不，这里并没有区别，它们都在Page粒度工作。如果你好奇的话，有一个单独的系统调用可以查看Page的大小。\n虚拟内存特性 如果你回想前面提到过的虚拟内存特性，我们可以将它们对应到这一节描述的Unix接口中来。\ntrap对应的是sigaction系统调用 Prot1，ProtN和Unprot可以使用mprotect系统调用来实现。mprotect足够的灵活，你可以用它来修改一个Page的权限，也可以用它来修改多个Page的权限。当修改多个Page的权限时，可以获得只清除一次TLB的好处。 查看Page的Dirty位要稍微复杂点，并没有一个直接的系统调用实现这个特性，不过你可以使用一些技巧完成它，我稍后会介绍它。 map2也没有一个系统调用能直接对应它，通过多次调用mmap，你可以实现map2特性。 或许并不完全受这篇论文所驱动，但是内核开发人员已经在操作系统中为现在的应用程序提供了这些特性。\n接下来，我将在框架层面简单介绍一下这些特性是如何实现的，之后再看看应用程序是如何使用这些特性。\n虚拟内存系统如何支持用户应用程序 有关实现，有两个方面较为有趣。\n第一个是虚拟内存系统为了支持这里的特性，具体会发生什么？\n这里我们只会讨论最重要的部分，并且它也与即将开始的mmap lab有一点相关，因为在mmap lab中你们将要做类似的事情。\n在现代的Unix系统中，地址空间是由硬件Page Table来体现的，在Page Table中包含了地址翻译。\n在现代的 Unix 系统中，地址空间的管理确实依赖于硬件级别的页表（Page Table），但这并不意味着用户或操作系统完全无法控制它。我们可以理解为：\n硬件辅助而非硬件独立控制\n页表的硬件实现：页表的实际翻译过程由硬件（比如 CPU 的内存管理单元 MMU）来执行。CPU 会根据页表中的内容将虚拟地址翻译成物理地址，这种翻译过程在访问内存时是由硬件快速完成的。\n软件管理与硬件依赖：操作系统在软件层面负责构建、修改和管理页表的内容，决定虚拟地址到物理地址的映射关系。虽然硬件负责执行实际的地址翻译，但操作系统通过系统调用和内核机制对页表进行控制，从而间接控制了地址空间。\n为什么称为“硬件Page Table”\n性能的考量：地址翻译是频繁的基本操作，每次访问内存时都需要进行。这种高频率的操作如果完全由软件完成会非常耗时。因此，硬件（CPU）来负责翻译加速，以保证性能。页表硬件机制可以使这些翻译在数个 CPU 时钟周期内完成。\n页表的多级结构：现代 CPU 使用多级页表（例如 x86 架构中的四级页表）来高效地管理大规模地址空间。这些复杂的结构由硬件支持并高效处理。\n用户的控制能力\n间接控制：用户态程序无法直接操作页表，但可以通过操作系统提供的接口（比如 mmap 或 malloc）来请求内存分配，操作系统会根据这些请求更新页表。\n内核态控制：在内核态，操作系统完全可以控制页表，包括创建新的页表、修改页表项、设置权限等。这些操作最终都影响到用户态进程的虚拟内存布局。\n因此，“硬件 Page Table”指的是地址翻译依赖于硬件完成，而不是说用户或操作系统完全无法控制页表。操作系统仍然可以通过内核态管理页表并间接影响地址空间，只不过实际的地址翻译过程由硬件（CPU 和 MMU）来加速处理，以提高效率。这种设计是为了平衡性能与灵活性，使得系统能够高效地管理和访问内存。\n但是通常来说，地址空间还包含了一些操作系统的数据结构，这些数据结构与任何硬件设计都无关，它们被称为Virtual Memory Areas（VMAs）。\nVMA会记录一些有关连续虚拟内存地址段的信息。\n在一个地址空间中，可能包含了多个section，每一个section都由一个连续的地址段构成，对于每个section，都有一个VMA对象。\n连续地址段中的所有Page都有相同的权限，并且都对应同一个对象VMA\n例如一个进程的代码是一个section，数据是另一个section，它们对应不同的VMA，VMA还可以表示属于进程的映射关系，例如下面提到的Memory Mapped File。\n举个例子，如果进程有一个Memory Mapped File，那么对于这段地址，会有一个VMA与之对应，VMA中会包含文件的权限，以及文件本身的信息，例如文件描述符，文件的offset等。\n在接下来的mmap lab中，你们将会实现一个非常简单版本的VMA，并用它来实现针对文件的mmap系统调用。你可以在VMA中记录mmap系统调用参数中的文件描述符和offset。\n第二个部分我们了解的就不多了，它或许值得仔细看一下，也就是User level trap是如何实现的？\n我们假设一个PTE被标记成invalid或者只读，而你想要向它写入数据。\n这时，CPU会跳转到kernel中的固定程序地址，也就是XV6中的trampoline代码（注，详见6.2）。\nkernel会保存应用程序的状态，在XV6中是保存到trapframe。\n之后再向虚拟内存系统查询，现在该做什么呢？\n虚拟内存系统或许会做点什么，例如在lazy lab和copy-on-write lab中，trap handler会查看Page Table数据结构。\n而在我们的例子中会查看VMA，并查看需要做什么。\n举个例子，如果是segfault，并且应用程序设置了一个handler来处理它，那么\nsegfault事件会被传播到用户空间 并且通过一个到用户空间的upcall在用户空间运行handler 在handler中或许会调用mprotect来修改PTE的权限 之后handler返回到内核代码 最后，内核再恢复之前被中断的进程。 当内核恢复了中断的进程时，如果handler修复了用户程序的地址空间，那么程序指令可以继续正确的运行，如果哪里出错了，那么会通过trap再次回到内核，因为硬件还是不能翻译特定的虚拟内存地址。\n\u0026ldquo;segfault事件会被传播到用户空间\u0026rdquo; 是什么意思？是指segfault这个信号会从内核态传给用户态吗？怎么传的？\n是的，\u0026ldquo;segfault事件会被传播到用户空间\u0026rdquo; 意思是内核会检测到段错误（segfault）这种内存访问异常，然后通过一种信号机制将这个信息传递给用户态进程。\n具体来说，在现代类 Unix 系统中，当进程试图非法访问内存（如访问一个无效的地址或尝试向只读页面写入数据）时，内核会捕获到这个异常并触发一个信号，通常是 SIGSEGV 信号（表示“段错误”）。这个信号会通知用户进程，内存访问出错。\n传播机制如下：\n内核捕获异常：CPU检测到非法内存访问，触发陷阱（trap）进入内核，内核负责处理该事件。 生成信号（signal）：内核会向引发该异常的进程生成一个信号，比如 SIGSEGV，这个信号会被记录到进程的待处理信号队列中。 向用户态传播：当进程被调度运行时，内核会检查该进程的待处理信号队列。如果有信号，内核会中断该进程的正常执行流程，并将控制权交给事先注册的信号处理程序（handler）。 如果进程注册了信号处理程序（handler），那么信号处理程序会在用户态被执行，处理这个异常。 如果没有设置处理程序，系统会终止进程并生成一个core dump（进程崩溃时的内存转储）。 在这个过程中，进程是trap，然后进入trampoline，存好上下文，进入内核，然后又从内核回到用户空间，执行handler吗？这个handler怎么有权限更改PTE的？\n是的，你描述的过程基本正确。以下是详细的解释：\n处理过程：\ntrap发生：当用户进程尝试写入一个被标记为 invalid 或 只读 的页面时，CPU触发异常，进入内核态。 进入trampoline代码：内核首先通过 trampoline 代码进入内核，并保存当前进程的上下文信息（如寄存器、程序计数器等）。在 XV6 系统中，这些信息会被存储在 trapframe 结构中。 查询虚拟内存系统：内核的trap handler会查看页面表（PTE），以及与虚拟内存相关的其他数据结构（如VMA，虚拟内存区域）。根据这些信息，内核决定接下来如何处理。 如果是合法的页面缺失（Page Fault），可能会分配一个新的页面或执行 Copy-on-Write 机制。 如果是段错误（segfault），内核会发送 SIGSEGV 信号给用户进程。 信号处理机制：如果用户程序设置了 SIGSEGV 信号的处理程序，内核会通过一种机制称为 upcall（上行调用）返回到用户态，让用户进程执行该信号处理函数（handler）。这是通过修改 trapframe 中保存的上下文来完成的，修改的内容包括将程序计数器（PC）指向用户态的 handler 地址。 handler修改PTE的权限：\n信号处理函数（handler）可以调用系统提供的接口来修改 PTE（页表项）。具体来说，虽然用户态程序无法直接操作页表，但它可以通过系统调用来间接修改页表。比如：\n调用 mprotect：mprotect 是一个系统调用，允许用户程序请求内核修改特定页面的访问权限。通过调用 mprotect，用户可以请求将某些页面的权限从只读修改为可写，或从无效状态修改为有效状态。 当 handler 中调用 mprotect 时，控制权会再次转交给内核，由内核负责对页表进行修改。内核可以直接操作 PTE，将其修改为新的权限配置（例如将页面从只读修改为可写）。这并不是用户态程序直接更改 PTE，而是通过系统调用请求内核来完成修改。\n内核恢复进程的执行：\n恢复上下文：handler 执行完毕后，控制权再次回到内核。内核会检查 trapframe，恢复之前保存的进程上下文，确保进程能够从上次中断的地方继续执行。 继续执行原程序：如果 handler 成功修复了页面（例如通过调用 mprotect 修改了页面的权限），那么进程会从触发 trap 的指令处继续执行。如果地址空间依然有问题（例如 handler 没有正确处理），则进程会再次触发 trap。 所以，总的来说：\n“segfault事件传播到用户空间” 是指通过信号机制，内核将 segfault 这个异常信息传递给用户态，用户态的 handler 处理程序可以在用户空间执行。 进程从trap到trampoline再到用户空间 的确是一个循环的过程。handler 本身无法直接更改 PTE 的权限，但它可以通过系统调用（如 mprotect）向内核请求修改，最终由内核来执行 PTE 的更新操作。 这种机制在性能和安全性之间取得了平衡：用户程序通过信号机制间接地影响页表内容，而页表的实际修改仍然由内核完成。\n学生提问：当我们允许用户针对Page Fault来运行handler代码时，这不会引入安全漏洞吗？\nFrans教授：这是个很好的问题。会有安全问题吗？你们怎么想的？这会破坏User/kernel或者不同进程之间的隔离性吗？或者从另一个角度来说，你的问题是sigalarm会破坏隔离性吗？\n当我们执行upcall的时候，upcall会走到设置了handler的用户空间进程中，所以handler与设置了它的应用程序运行在相同的context，相同的Page Table中。\n所以handler唯一能做的事情就是影响那个应用程序，并不能影响其他的应用程序，因为它不能访问其他应用程序的Page Table，或者切换到其他应用程序的Page Table。所以这里还好。\n当然，如果handler没有返回，或者做了一些坏事，最终内核还是会杀掉进程。所以唯一可能出错的地方就是进程伤害了自己，但是它不能伤害任何其他进程。\n构建大的缓存表 接下来，我将通过介绍几个例子来看一下如何使用之前介绍的内容。\n我会从一个非常简单的例子开始，之后我们会看一下Garbage Collector，因为很多同学都问了Garbage Collector的问题，所以GC是一个可以深入探讨的好话题。\n首先我想讨论的是一个非常简单的应用，它甚至都没有在论文中提到，但它却是展示这节课的内容非常酷的一个方法。\n这个应用里是构建一个大的缓存表，什么是缓存表？它是用来记录一些运算结果的表单。\n举个例子，你可以这么想，下面是我们的表单，它从0开始到n。\n表单记录的是一些费时的函数运算的结果，函数的参数就是0到n之间的数字。\n如果这个表单在最开始的时候就预计算好了，那么当你想知道f(i)的结果是什么时，你需要做的就是查看表单的i槽位，并获取f(i)的值。\n这样你可以将一个费时的函数运算转变成快速的表单查找，所以这里一个酷的技巧就是预先将费时的运算结果保存下来。如果相同的计算需要运行很多很多次，那么预计算或许是一个聪明的方案。\n这里的挑战是，表单可能会很大，或许会大过物理内存，这里可以使用论文提到的虚拟内存特性来解决这个挑战。\n首先，你需要分配一个大的虚拟地址段，但是并不分配任何物理内存到这个虚拟地址段。\n这里只是从地址空间获取了很大一段地址，并说我将要使用地址空间的这部分来保存表单。\n但是现在表单中并没有内容，表单只是一段内存地址。\n如果你现在查找表单的i槽位，会导致Page Fault。\n所以这里的计划是，在发生Page Fault时，先针对对应的虚拟内存地址分配物理内存Page，之后计算f(i)，并将结果存储于tb[i]，也就是表单的第i个槽位，最后再恢复程序的运行。\n这种方式的优势是，如果你需要再次计算f(i)，你不需要在进行任何费时的计算，只需要进行表单查找。\n即使接下来你要查找表单的i+1槽位，因为一个内存Page可能可以包含多个表单项，这时也不用通过Page Fault来分配物理内存Page。\n不过如果你一直这么做的话，因为表单足够大，你最终还是会消耗掉所有的物理内存。\n所以Page Fault Handler需要在消耗完所有的内存时，回收一些已经使用过的物理内存Page。\n当然，你需要修改已经被回收了的物理内存对应的PTE的权限，这样在将来使用对应地址段时，就可以获得Page Fault。\n所以你需要使用Prot1或者ProtN来减少这些Page的accessbility。\n学生提问：在分配物理内存Page时，我们需要让操作系统映射到地址空间的特定地址，否则的话可能会映射到任意地址，是吧？\nFrans教授：操作系统会告知是哪个地址，并且这里可能是任意的地址。\n为了更具体的描述这里的应用，我这里有个小的实现，我们可以看一看这里是如何使用现有的Unix特性。\n1 2 3 4 5 6 7 8 9 int main(int argc,char *argv[]) { page_size = sysconf(_SC_PAGESIZE); printf(\u0026#34;page_size is %ld\\n\u0026#34;,page_size); setup_sqrt_region(); test_sqrt_region(); return 0; } 在main函数中，首先调用setup_sqrt_region函数，它会从地址空间分配地址段，但是又不实际分配物理Page。之后调用test_sqrt_region。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 static void test_sqrt_region(void) { int i,pos = rand()%(MAX_SQRTS 1); double correct_sqrt; printf(\u0026#34;Validating square root table contents...\\n\u0026#34;); srand (0xDEADBEEF); for(i = 0; i \u0026lt; 500000; i++){ if(i%2==0) pos = rand()%(MAX_SQRTS - 1); else pos += 1; calculate_sqrts(\u0026amp;correct_sqrt,pos,1); if (sqrts[pos] != correct_sqrt){ fprintf(stderr, \u0026#34;Square root is incorrect.Expected %f,got %f.\\n\u0026#34;, correct_sqrt, sqrts[pos]); exit(EXIT_FAILURE); } } printf(\u0026#34;All tests passed!\\n\u0026#34;); } 在test_sqrt_region中，会以随机数来遍历表单，并通过实际运算对应的平方根值，来检查表单中相应位置值是不是保存了正确的平方根值。\n在test_sqrt_region运行的过程中，会产生Page Fault，因为现在还并没有分配任何物理内存Page。\n应用程序该如何收到Page Fault呢？\n在setup_sqrt_region函数中有一段代码，通过将handle_sigsegv函数注册到了SIGSEGV事件。\n这样当segfault或者Page Fault发生时，内核会调用handle_sigsegv函数。\nhandle_sigsegv函数与你们之前看过很多很多次的trap代码非常相似。\n它首先会获取触发Page Fault的地址， 之后调用mmap对这个虚拟内存地址分配一个物理内存Page（注，这里是mmap映射匿名内存）。这里的虚拟内存地址就是我们想要在表单中用来保存数据的地址。 然后我们为这个Page中所有的表单项都计算对应的平方根值，之后就完事了。 这个应用程序有点极端，它在运行的时候只会使用一个物理内存Page，所以不论上一次使用的Page是什么，在handle_sigsegv的最后都会通过munmap释放它。\n所以我们有一个巨大的表单，但是它只对应一个物理内存Page。\n接下来我将运行一下这个应用程序。test_sqrt_region会随机查看表单的内容，所以可以假设这会触发很多Page Fault，但是可以看出表单中的所有内容都能通过检查。\n所以，尽管这里有一个巨大的表单用来保存平方根，但是实际在物理内存中只有一个内存Page。\n这是一个简单的例子，它展示了用户应用程序使用之前提到的虚拟内存特性之后可以做的一些酷的事情。\n学生提问：能再讲一下为什么一个物理内存Page就可以工作吗？我觉得这像是lazy allocation，但是区别又是什么呢？\nFrans教授：当我们刚刚完成设置时，我们一个内存Page都没有，setup_sqrt_region分配了一个地址段，但是又立即通过munmap将与这个地址段关联的内存释放了。所以在启动的最开始对于表单并没有一个物理内存Page与之关联。\n之后，当我们得到了一个Page Fault，这意味着整个表单对应的地址中至少有一个Page没有被映射，虽然实际上我们一个Page都没有映射。\n现在我们得到了一个Page Fault，我们只需要映射一个Page，在这个Page中，我们会存入i，i+1。。。的平方根（注，因为一个Page4096字节，一个double8个字节，所以一个Page可以保存512个表单项）。\n因为这是第一个Page Fault，之前并没有映射了内存Page，所以不需要做任何事情。\n之后，程序继续运行并且查找了表单中的更多项，如果查找一个没有位于已分配Page上的表单项时，会得到另一个Page Fault。\n这时，在handle_sigsegv会分配第二个内存Page，并为这个Page计算平方根的值。之后会munmap记录在last_page_base中的内存。\nBaker\u0026rsquo;s Real-Time Copying Garbage Collector 接下来我会讨论另一个例子，也就是Garbage Collector（注，后面将Garbage Collector和Garbage Collection都简称为GC），并且我也收到了很多有关Garbage Collector的问题。\nGC是指编程语言替程序员完成内存释放，这样程序员就不用像在C语言中一样调用free来释放内存。\n对于拥有GC的编程语言，程序员只需要调用类似malloc的函数来申请内存，但是又不需要担心释放内存的过程。\nGC会决定内存是否还在使用，如果内存并没有被使用，那么GC会释放内存。\nGC是一个很好的特性，有哪些编程语言带有GC呢？Java，Python，Golang，几乎除了C和Rust，其他所有的编程语言都带有GC。\n你可以想象，GC有很大的设计空间。\n这节课讨论的论文并没有说什么样的GC是最好的，它只是展示了GC可以利用用户空间虚拟内存特性。\n论文中讨论了一种特定的GC，这是一种copying GC。什么是copying GC？假设你有一段内存作为heap，应用程序从其中申请内存。\n你将这段内存分为两个空间，其中一个是from空间，另一个是to空间。\n当程序刚刚启动的时候，所有的内存都是空闲的，应用程序会从from空间申请内存。假设我们申请了一个类似树的数据结构。\n树的根节点中包含了一个指针指向另一个对象，这个对象和根节点又都包含了一个指针指向第三个对象，这里构成了一个循环。\n或许应用程序在内存中还有其他对象，但是没有别的指针指向这些对象，所以所有仍然在使用的对象都可以从根节点访问到。\n在某个时间，或许因为之前申请了大量的内存，已经没有内存空间给新对象了，也就是说整个from空间都被使用了。\nCopying GC的基本思想是将仍然在使用的对象拷贝到to空间去，具体的流程是从根节点开始拷贝。\n每一个应用程序都会在一系列的寄存器或者位于stack上的变量中保存所有对象的根节点指针，通常来说会存在多个根节点，但是为了说明的简单，我们假设只有一个根节点。\n拷贝的流程会从根节点开始向下跟踪，所以最开始将根节点拷贝到了to空间，但是现在根节点中的指针还是指向着之前的对象。\n之后，GC会扫描根节点对象。因为程序的运行时知道对象的类型是什么，当然也就知道对象中的指针。\n接下来GC会将根节点对象中指针指向的对象也拷贝到to空间，很明显这些也是还在使用中的对象。\n当一个对象被拷贝到to空间时，根节点中的指针会被更新到指向拷贝到了to空间的对象。\n在之后的过程中，我们需要记住这个对象已经被拷贝过了。\n所以，我们还会存储一些额外的信息来记住相应的对象已经保存在了to空间，这里会在from空间保留一个forwarding指针。\n这里将对象从from空间拷贝到to空间的过程称为forward。\n接下来还剩下一个对象，我们将这个对象从from空间拷贝到to空间，这个对象还包含一个指针指向第二个对象。\n但是通过查看指针可以看到这个对象已经被拷贝了，并且我们已经知道了这个对象被拷贝到的地址（注，也就是之前在from空间留下的forwarding指针）。\n所以我们可以直接更新第三个对象的指针到正确的地址。\n现在与根节点相关的对象都从from空间移到了to空间，并且所有的指针都被正确的更新了，所以现在我们就完成了GC，from空间的所有对象都可以被丢弃，并且from空间现在变成了空闲区域。\n论文中讨论的是一种更为复杂的GC算法，它被称为Baker算法，这是一种很老的算法。\n它的一个优势是它是实时的，这意味着它是一种incremental GC（注，incremental GC是指GC并不是一次做完，而是分批分步骤完成）。\n在Baker算法中，我们还是有from和to两个空间。假设其中还是包含了上面介绍的几个对象。\n这里的基本思想是，GC的过程没有必要停止程序的运行并将所有的对象都从from空间拷贝到to空间，然后再恢复程序的运行。\nGC开始之后，唯一必要的事情，就是将根节点拷贝到to空间。\n所以现在根节点被拷贝了，但是根节点内的指针还是指向位于from空间的对象。\n根节点只是被拷贝了并没有被扫描，其中的指针还没有被更新。\n如果应用程序调用了new来申请内存，那就再扫描几个对象，并将这些对象从from空间forward到to空间。\n这很好，因为现在我们将拷贝heap中还在使用的所有对象的过程，拆分成了渐进的步骤。\n每一次调用new都使得整个拷贝过程向前进一步。\n当然应用程序也会使用这里对象所指向的指针。\n举个例子，现在当根节点需要读出其指向的一个对象时，这个对象仍然在from空间。这是危险的，因为我们不应该跟踪from空间的指针（注，换言之GC时的指针跟踪都应该只在同一个空间中完成）。\n所以每次获取一个指针指向的对象时（dereference），你需要检查对象是否在在from空间，如果是的话，将其从from空间forward到to空间。\n所以应用程序允许使用指针，但是编译器需要对每个指针的访问都包上一层检查，这样我们就可以保证在to空间的任何指针指向的是位于to空间的对象。\n我们需要确保这一点，因为在最后当GC完成了所有对象的跟踪之后，我们会清空from部分并重用这部分内存。\n论文对于这里的方案提出了两个问题：\n第一个是每次dereference都需要有以上的额外步骤，每次dereference不再是对于内存地址的单个load或者store指令，而是多个load或者store指令，这增加了应用程序的开销。\n第二个问题是并不能容易并行运行GC。\n如果程序运行在多核CPU的机器上，并且你拥有大量的空闲CPU，我们本来可以将GC运行在后台来遍历对象的图关系，并渐进的拷贝对象。\n但是如果应用程序也在操作对象，那么这里可能会有抢占。应用程序或许在运行dereference检查并拷贝一个对象，而同时GC也在拷贝这个对象。\n如果我们不够小心的话，我们可能会将对象拷贝两遍，并且最后指针指向的不是正确的位置。所以这里存在GC和应用程序race condition的可能。\n使用虚拟内存特性的GC 论文中介绍，如果拥有了前面提到的虚拟内存特性，你可以使用虚拟内存来减少指针检查的损耗，并且以几乎零成本的代价来并行运行GC。\n这里的基本思想是将heap内存中from和to空间，再做一次划分，每一个部分包含scanned，unscanned两个区域。\n在程序启动，或者刚刚完成了from和to空间的切换时，整个空间都是unscanned，因为其中还没有任何对象。\n之后的过程与前面描述的相同，在开始GC时，我们将根节点对象拷贝到to空间，但是根节点中的指针还是指向了位于from空间的对象。\n现在unscanned区域包括了所有的对象（注，现在只有根节点），我们会将unscanned区域的权限设置为None。\n这意味着，当开始GC之后，应用程序第一次使用根节点，它会得到Page Fault，因为这部分内存的权限为None。\n在Page Fault Handler中，GC需要扫描位于内存Page中所有的对象，然后将这些对象所指向的其他对象从from空间forward到to空间。\n所以，在GC最开始的时候，我们将根节点拷贝过来了；之后在Page Fault Handler中通过扫描，将根节点指向的对象也都拷贝过来了。\n在我们的例子中根节点指向的只有两个对象，这两个对象会被拷贝到unscanned区域中，而根节点会被标记成scanned。\n在我们扫描完一个内存Page中的对象时，我们可以通过Unprot（注，详见17.1）恢复对应内存Page的权限。\n之后，应用程序就可以访问特定的对象，因为我们将对象中的指针转换成了可以安全暴露给应用程序的指针（注，因为这些指针现在指向了位于to空间的对象），所以应用程序可以访问这些指针。\n当然这些指针对应的对象中还没有被扫描。如果dereference这些指针，我们会再次得到Page Fault，之后我们会继续扫描。\n这种方案的好处是，它仍然是递增的GC，因为每次只需要做一小部分GC的工作。\n除此之外，它还有额外的优势：现在不需要对指针做额外的检查了（注，也就是不需要查看指针是不是指向from空间，如果是的话，将其forward到to空间）。\n或者说指针检查还在，只是现在通过虚拟内存相关的硬件来完成了。\n学生提问：刚刚说到在Handler里面会扫描一个Page中的所有对象，但是对象怎么跟内存Page对应起来呢？\nFrans教授：在最开始的时候，to空间是没有任何对象的。\n当需要forward的时候，我刚刚描述的是拷贝一个对象，但是实际上拷贝的是一个内存Page中的N个对象，这样它们可以填满整个Page。\n所以现在我们在to空间中，有N个对象位于一个Page中，并且它们都没有被扫描。\n之后某个时间，Page Fault Handler会被调用，GC会遍历这个内存Page上的N个对象，并检查它们的指针。\n对于这些指针，GC会将对应的对象拷贝到to空间的unscanned区域中。\n之后，当应用程序使用了这些未被扫描的对象，它会再次得到Page Fault，进而再扫描这些对象，以此类推。\n学生提问：在完成了GC之后，会切换from和to空间吗？\nFrans教授：最开始我们使用的是from空间，当用完了的时候，你会将对象拷贝到to空间，一旦完成了扫描，from空间也被完全清空了，你可以切换两个空间的名字。现在会使用to空间来完成内存分配。直到它也满了，你会再次切换。\n在理解 to 空间的 unscanned 区域时，首先要明确 to 空间的作用及其与 from 空间的关系。这通常涉及到一种叫做 半空间垃圾回收算法（或称 复制收集算法），其中内存被划分为两个区域：from 空间和 to 空间。\n在复制收集算法中：\nFrom 空间：存放当前所有正在使用的对象。 To 空间：在垃圾回收过程中，用于接收从 from 空间拷贝过来的活动对象（即被引用的对象）。 to 空间的 unscanned 区域 是指尚未被扫描的对象区域。当垃圾回收（GC）开始时，to 空间为空。GC 会将所有活动对象从 from 空间拷贝到 to 空间，并将这些对象放入 unscanned 区域。\n一旦对象被拷贝到 to 空间，它就会被标记为未扫描（unscanned）。 随后，GC 会依次扫描 to 空间中的对象。扫描的过程中，GC 会查看这些对象的指针，如果指针引用了其他对象，就将这些对象也拷贝到 to 空间。 每次扫描时，未被扫描的对象会被移出 unscanned 区域。这样，可以确保所有活动对象都被处理到，而不活动（垃圾）对象则被丢弃。 Frans 教授提到“拷贝的是一个内存 Page 中的多个对象”，这是为了提高效率。在分页（Paging）机制下：\n系统通常会按页（Page）为单位处理内存，而非单个对象。 这意味着，在需要时，GC 会一次性地将整个页中的多个对象拷贝到 to 空间。这是因为单次页内存的映射和传输速度要比逐个拷贝对象更快。 当 GC 将整个页面中的对象拷贝到 to 空间时，它们可以填满整个页，避免频繁的页内存操作和碎片化问题。 在复制过程中，只拷贝活动对象。垃圾回收器通过对象的引用关系来判断哪些对象是活动的，哪些对象是垃圾：\n从 from 空间拷贝对象到 to 空间时，GC 只处理那些仍然被程序引用的对象。这些对象是通过扫描根对象引用链找到的，而不活动的对象（没有被引用）不会被拷贝。 所以，即使是一个页面中的部分对象是垃圾，GC 也不会将这些垃圾对象拷贝到 to 空间。 在垃圾回收完成后：\nfrom 空间中原有的对象会被标记为可以回收。此时，to 空间中的对象成为新的活动对象集。 空间交换：from 和 to 空间交换角色。原来的 to 空间变为新的 from 空间，用于分配和存放新对象；原来的 from 空间则被完全清空，作为下一次 GC 的目标。 在垃圾回收过程中，to 空间的 unscanned 区域存放的是从 from 空间拷贝过来的、尚未被扫描的活动对象。通过扫描这些对象，GC 可以递归地找到并拷贝其他活动对象，避免将垃圾对象转移到 to 空间。这样可以保证 to 空间在完成后只包含当前的活动对象，垃圾被完全丢弃在 from 空间中等待清空。\n论文中提到使用虚拟内存的另一个好处是，它简化了GC的并发。\nGC现在可以遍历未被扫描的内存Page，并且一次扫描一个Page，同时可以确保应用程序不能访问这个内存Page，因为对于应用程序来说，未被扫描的内存Page权限为None。\n虚拟内存硬件引入了这种显式的同步机制，或者说对于抢占的保护。\n现在只有GC可以访问未被扫描的内存Page，而应用程序不能访问。\n所以这里提供了自动的并发，应用程序可以运行并完成它的工作，GC也可以完成自己的工作，它们不会互相得罪，因为一旦应用程序访问了一个未被扫描的Page，它就会得到一个Page Fault。\n而GC也永远不会访问扫描过的Page，所以也永远不会干扰到应用程序。所以这里以近乎零成本获取到了并发性。\n但是实际上有个麻烦的问题。回到我们之前那张图，我们在heap中有from空间，to空间。在to空间中又分为了unscanned和scanned区域，对于应用程序来说，unscanned区域中的Page权限为None。\n这就引出了另一个问题，GC怎么能访问这个区域的内存Page？因为对于应用程序来说，这些Page是inaccessible。\n这里的技巧是使用map2（注，详见17.1）。\n这里我们会将同一个物理内存映射两次，第一次是我们之前介绍的方式，也就是为应用程序进行映射，第二次专门为GC映射。\n在GC的视角中，我们仍然有from和to空间。在to空间的unscanned区域中，Page具有读写权限。\n所以GC可以遍历这些内存Page，读取内容并forward必要的对象。\n这里使用了map2将物理内存映射到应用程序地址空间中两次的能力，其中每次映射都有不同的权限，这样这里的场景才能工作。\n学生提问：GC和应用程序是不是有不同的Page Table？\nFrans教授：不，它们拥有相同的Page Table。它们只是将物理内存映射到了地址空间的两个位置，也就是Page Table的两个位置。在一个位置，PTE被标记成invalid，在另一个位置，PTE被标记成可读写的。\n使用虚拟内存特性的GC代码展示 为了更清晰的说明上一节的内容，我这里有个针对论文中方法的简单实现，我可以肯定它包含了一些bug，因为我并没有认真的测试它。\n首先，应用程序使用的API包括了new和readptr。\n1 2 3 4 //API to collector struct elem *readptr(struct elem **ptr); struct elem *new(); readptr会检查指针是否位于from空间，如果是的话，那么它指向的对象需要被拷贝。\n当然，当我们使用虚拟内存时，这里的readptr成本会比较低，它会直接返回参数。\n在这个简单的例子中，我有一个循环链表，并且有两个根节点，其中一个指向链表的头节点，另一个指向链表的尾节点。\n应用程序线程的工作是循环1000次，每次创建list，再检查list。\n1 2 3 4 5 6 7 8 void app_thread(void *x){ for (int i 0;i\u0026lt;1000;i++) { make_clist(); check_clist(LISTSZ); } } 所以它会产生大量的垃圾，因为每次make_clist完成之后，再次make_clist，上一个list就成为垃圾了。所以GC必然会有一些工作要做。\nmake_clist的代码有点丑，主要是因为每个指针都需要被readptr检查包围。\n通常这里的检查代码是由编译器生成的。\n但是我这里并没有一个针对带GC的编程语言的编译器，所以我只能模仿一个编译器可能生成的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void make_clist(void){ struct elem *e; root head = new(); readptr(\u0026amp;root_head)-\u0026gt;val = 0; readptr(\u0026amp;root_head)-\u0026gt;next = readptr(\u0026amp;root_head); root_last = readptr(\u0026amp;root_head); for (int i = 1;i \u0026lt; LISTSZ;i++){ e = new(); readptr(\u0026amp;e)-\u0026gt;next = readptr(\u0026amp;root_head); readptr(\u0026amp;e)-\u0026gt;val = i; root_head = readptr(\u0026amp;e); readptr(\u0026amp;root_last)-\u0026gt;next = readptr(\u0026amp;root_head); check_clist(i+1); } } make_clist会构建一个LISTSZ大小的链表，分配新的元素，并将新元素加到链表的起始位置，之后更新链表尾指针指向链表新的起始位置，这样就能构成一个循环链表。\n这里更有趣的部分是，GC部分怎么实现。\n首先让我们看看如果没有虚拟内存会怎样。我们只需要查看两个API：new和readptr。\n以上就是new的实现，先不考虑这里的mutex，因为这是为基于虚拟内存的实现提供的。\n先假设我们不需要扫描，也不需要collect。\n接下来会检查是否有足够的空间，如果有足够的空间，我们就将指针地址增加一些，以分配内存空间给新的对象，最后返回。\n如果没有足够的空间，我们需要调用flip，也就是运行GC。\nflip首先会切换from和to指针，之后将这个应用程序的两个根节点从from空间forward到to空间。接下来我们看一下forward函数。\n这个函数会forward指针o指向的对象，首先检查指针o是不是在from空间，如果是的话，并且之前没有被拷贝过，那么就将它拷贝到to空间。\n如果之前拷贝过，那么就可以用to空间的指针代替对象指针，并将其返回。\n对于readptr，如果我们没有使用虚拟内存。\n会对指针p做forward操作，forward操作的意思是如果对象在from空间，那么就将其拷贝到to空间，所以这里会有耗时的检查。\n接下来我们看一下这里如何使用虚拟内存。\n首先是设置内存，通过shm_open 创建一个Share-memory object，shm_open是一个Linux/Uinx系统调用。Share-memory object表现的像是一个文件，但是它并不是一个文件，它位于内存，并没有磁盘文件与之对应，如果你愿意的话，可以认为它是一个位于内存的文件系统。\n之后我们裁剪这个Shared-memory object到from和to空间的大小。\n之后我们通过mmap先将其映射一次，以供mutator也就是实际的应用程序使用。然后再映射一次，以供GC使用。这里shm_open，ftruncate，和两次mmap，等效于map2。\n回过去看之前的代码，\n使用了虚拟内存之后，readptr将不做任何事情，直接将参数返回。\n当然，如果我们使用这里的指针，并且指针对应的对象位于unscanned区域，我们会得到Page Fault。\n在Page Fault hanlder中，GC会运行scan函数。\n但是scan函数是以GC对应的PTE来运行的，所以它能工作。\n而同时，应用程序或者mutator不能访问这些Page，如果访问了的话，这会产生Page Fault。\n一旦scan执行完成，handler中会将Page设置成对应用程序可访问的（注，也就是调用mprotect）。\n在flip函数中，\n完成from和to空间的切换时，如果使用了虚拟内存，我们会通过mprotect将整个to空间对应用程序标记成不可访问的。\n之后GC将root_head和root_last移到to空间中，这样应用程序就不能访问这两个对象，任何时候应用程序需要访问这两个对象，都会导致一个Page Fault。\n在Page Fault handler中，GC可以将其他对象从from空间拷贝到to空间，然后再Unprot对应的Page。\n在Page Fault handler中，先scan内存Page，再将内存Page标记成对应用程序可访问的这个顺序是至关重要的。\n因为如果你先将内存Page标记成应用程序可访问的，然后再扫描它，如果有多个应用程序线程，那么应用程序可能会查看到unscanned区域的对象。\n当然我们要禁止这一点（注，因为为了避免抢占，unscanned区域只能GC访问），所以这里的代码是先扫描，再增加内存的访问权限，这样应用程序就可以安全的访问这些内存Page。\n接下来，我总结一下这节课的内容。有一个问题，你应该在这里使用虚拟内存吗？或者说这里的这些技巧值得吗？\n许多的GC并没有使用虚拟内存，而是通过编译器生成的代码来完成GC，并且还有各种其他的技巧来减少性能损耗。所以GC的大部分场景都可以通过一些额外的指令来完成。\n这对于一个编译器，程序运行时，或者编程语言来说，并不是一个太糟糕的选择，因为编译器就可以完成这些操作。\n但是如果没有程序运行时或者编译器，那么这个过程就会很痛苦。所以对于一些完全没有编译器参与的应用程序，例如checkpointing，shared-virtual memory，它们的确需要这里提到的虚拟内存特性。\n实际中，足够多的应用程序开发人员发现了这些特性的价值，所以今天的操作系统都支持了这些虚拟内存特性。\n很多人问了这个问题，从91年（论文发表的年份）至今，虚拟内存系统发生了什么改变？其中一个改变是，大部分的Unix系统都支持了这些虚拟内存特性了，并且从91年至今有许多变化。\n或许很难想象，但是在虚拟内存系统中有持续的开发，所以如果你查看Linux的git log，你可以发现在内核的各个方面都有持续的开发，其中包括了对虚拟内存系统的持续开发。在过去有一些重大的改变，比如说：\n现在的Page Table是5级的，这样可以处理非常大的地址 可以通过地址空间标识符来处理TLB flush 大概一年前，一种叫做KPTI（kernel page table isolation）的功能被引入，它是针对Meltdown attack的功能 虚拟内存系统绝对不是一个静态的系统，几乎Linux内核的所有方向都不是静态的。\n几乎每两个月在内核的不同方向都会有大量的更新。所以每个子系统时不时的就会被重写。\n学生提问：VMA中的连续地址是什么意思？\nFrans教授：这里是指连续的虚拟内存地址，比如说一个VMA表示1000-2000这段地址。如果你有另一段地址，2100-2200，那么它会有属于自己的VMA。所以每个VMA覆盖了一段连续的地址，中间不会有中断。你们将会在mmap lab中看到这样的设计是更加的合理的。你们可以认为对于每个mmap系统调用，如果地址没有重叠的话，都会有一个VMA。\n学生提问：GC什么时候会停止，什么时候又会再开始？我认为GC可以一直运行，如果它是并发的。\nFrans教授：是的，基于虚拟内存的解决方案一个酷的地方在于，GC可以一直运行。它可以在没有unscanned对象时停止。\n学生提问：但是你需要遍历所有在from空间的对象，你怎么知道已经遍历了所有的对象呢？\nFrans教授：你会从根节点开始扫描整个对象的图，然后拷贝到to空间。在某个时间点，你不再添加新的对象了，因为所有的对象已经被拷贝过了。当你不再添加新的对象，你的unscanned区域就不再增长，如果它不再增长，那么你就遍历了所有的对象（注，可以想象一个普通的DFS或者BFS过程）。\n","date":"2024-10-02T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08117virtual-memory-for-applications/bg_hu271711415966109763.png","permalink":"https://echudet.github.io/p/mit6s08117virtual-memory-for-applications/","title":"MIT6S081：17Virtual memory for applications"},{"content":"Lecture 16 File system performance and fast crash recovery 课前预习 什么是文件系统 对于任何文件系统，我们都需要什么功能？文件系统所服务的操作系统有明确的要求。文件系统对应用程序的表现方式是：一个操作系统通常需要遵守某些约定的文件名，并且文件具有某些以特定方式解释的属性。\n然而，文件系统的许多内部方面没有那么受约束，文件系统实现者可以在一定程度上自由地设计这些方面。磁盘上数据的布局（或者，如果文件系统不是本地的，它的网络协议）、内部缓存的细节以及用于调度磁盘IO的算法——在不违反文件系统应用程序接口规范的前提下，这些都是可以改变的。\n我们可能选择一种而不是另一种设计的原因有很多。与旧文件系统的兼容性可能是一个问题：例如，Linux提供了一个UMSDOS文件系统，它在标准MSDOS磁盘文件结构的基础上实现了POSIX文件系统的语义学。\n当试图解决Linux上文件系统恢复时间过长的问题时，我们牢记许多目标：\n使用新文件系统不会严重影响性能； 不得破坏与现有应用程序的兼容性 文件系统的可靠性不得以任何方式受到损害。 当我们谈论文件系统的可靠性时，有许多问题利害攸关。就本特定项目而言，我们主要关心的是恢复崩溃文件系统内容的可靠性，我们可以确定其中的几个方面：\n保持（Preservation）：崩溃前磁盘上稳定的数据永远不会被损坏。显然，崩溃时正在写入的文件不能保证完全完好无损，但是恢复系统不能碰磁盘上已经安全的任何文件。\n可预测性（Predictability）：我们必须恢复的故障模式应该是可预测的，以便我们可靠地恢复。\n原子性（Atomicity）：许多文件系统操作需要大量独立的IO来完成。一个很好的例子是将文件从一个目录重命名到另一个目录。如果这样的文件系统操作在磁盘上完全完成，或者在恢复完成后完全撤销，恢复就是原子性的。（对于重命名的例子，恢复应该在崩溃后保留提交给磁盘的旧文件名或新文件名，但不能两者都保留。）\n日志文件的工作很简单：它在我们提交事务的过程中记录文件系统元数据块的新内容。日志的唯一其他要求是我们必须能够原子地提交它包含的事务。\n我们向日志写入三种不同类型的数据块：元数据块、描述符块和头块（metadata, descriptor and header blocks）。\n日志元数据块包含由事务更新的单个文件系统元数据块的全部内容。这意味着，无论我们对文件系统元数据块做了多么小的更改，我们都必须写出整个日志块来记录更改。然而，由于两个原因，这一成本相对较低：\n无论如何，日志写入非常快，因为对日志的大多数写入都是顺序的，我们可以很容易地将日志IO批处理成大型集群，磁盘控制器可以有效地处理这些集群； 通过将更改后的元数据缓冲区的全部内容从文件系统缓存写入日志，我们可以避免在日志代码中执行大量CPU工作。 Linux内核已经为我们提供了一种非常有效的机制，可以将buffer cache中现有块的内容写到磁盘上的不同位置。buffer cache中的每个缓冲区都由一个名为buffer_head的结构体描述，该结构体包括缓冲区的数据要写到哪个磁盘块的信息。如果我们想将整个缓冲区块在不干扰buffer_head的情况下写入新位置，我们可以简单地创建一个新的临时buffer_head，将旧的描述复制到其中，然后编辑临时buffer_head中的设备块编号字段，以指向日志文件中的块。然后，我们可以将临时buffer_head直接提交给设备IO系统，并在IO完成后丢弃它。\n描述符块是描述其他日志元数据块的日志块，每当我们要将元数据块写出到日志时，我们需要记录下元数据通常安置在哪些磁盘块，这样恢复机制就可以将元数据复制回主文件系统中。在日志中的每一组元数据块之前都会写出一个描述符块，其中包含要写入的元数据块的数量加上它们的磁盘块号。\n描述符块和元数据块都按顺序写入日志，每当我们运行超过末尾时，都会从日志的开头重新开始。在任何时候，我们都维护当前的日志头（最后写入的块的块号）和尾部（日志中尚未取消固定的最老的块，如下所述）。每当我们用完日志空间时——日志的头部已经循环回来并赶上了尾部——我们会停止新的日志写入，直到日志的尾部被清理干净，以释放更多的空间。\n最后，日志文件包含一些位于固定位置的头块。这些头块记录了日志的当前头部和尾部，加上序列号。在恢复时，头块被扫描以找到序列号最高的块，当我们在恢复过程中扫描日志时，我们只是运行从尾部到头部的所有日志块，就像头块中记录的那样。\n在某个时候，要么是因为上次提交后我们已经等了足够长的时间，要么是因为日志中的空间不足，我们希望将未完成的文件系统更新作为一个新的复合事务提交到日志中。\n复合事务被完全提交后，我们仍然没有完成它。我们需要跟踪记录在事务中的元数据缓冲区，这样我们就可以注意到它们何时被写回磁盘上的主位置。\n回想一下，当我们提交事务时，新更新的文件系统块位于日志中，但尚未同步回磁盘上的永久家块（家块就是写入操作对应的磁盘中文件系统对应的块，我们需要保持旧块的这种不同步，以防在提交日志之前崩溃）。一旦提交了日志，磁盘上的旧版本就不再重要，我们可以在闲暇时将缓冲区写回它们的主位置。但是，在同步完这些缓冲区之前，我们不能删除日志中数据的副本。\n要完全提交并完成事务的检查点，我们将经历以下阶段：\n关闭事务。在此刻，我们会建立一个新的事务以记录未来开始的任何文件系统操作。任何现有的、不完整的操作仍然会使用现有的事务：我们不能在多个事务上拆分单个文件系统操作！ 开始将事务刷新到磁盘。在一个单独的log-writer内核线程的上下文中，我们开始向日志写入所有被事务修改过的元数据缓冲区。在这个阶段，我们还必须写入任何依赖数据（参见上面的部分：事务解剖）。 提交缓冲区后，将其标记以固定事务，直到它不再脏（它已通过通常的写回机制写回主存储）。 等待此事务中所有未完成的文件系统操作完成。我们可以在所有操作完成之前安全地开始写日志，允许这两个步骤在某种程度上重叠会更快。 等待所有未完成的事务更新完全记录在日志中。 更新日志头块以记录日志的新头部和尾部，将事务提交到磁盘。space released in the journal can now be reused by a later transaction. 当我们将事务的更新缓冲区写到日志中时，我们将它们标记以将事务固定在日志中。只有当这些缓冲区已同步到磁盘上的主缓冲区时，它们才会解除固定。只有当事务的最后一个缓冲区取消固定时，我们才能重用事务占用的日志块。当发生这种情况时，写入另一组日志头，记录日志尾部的新位置。日志中释放的空间现在可以由以后的事务重用。 正课 Why logging 这节课讲的是Linux中的广泛使用的ext3文件系统所使用的logging系统，同时我们也会讨论在高性能文件系统中添加log需要面对的一些问题。\n首先我会花几分钟来回顾一下，为什么我们要学习logging。\n之所以我们认为logging很重要，是因为这是一个巨大的成功和重要的思想，几乎可以认为logging是一种魔法，它可以用在任何一个已知的存储系统的故障恢复流程中，它在很多地方都与你想存储的任何数据相关。\n所以你们可以在大量的存储场景中看到log，比如说数据库，文件系统，甚至一些需要在crash之后恢复的定制化的系统中。\n你们也可以发现log作为从故障中恢复的机制，在分布式系统中也有大量的应用。\n因为log是一种可以用来表示所有在crash之前发生事情的数据结构，如果能够理解log，那么就可以更容易的从crash中恢复过来。\n除此之外，当你尝试构建高性能logging系统时，log本身也有大量有意思的地方。\n这里有一个术语，当我们谈到log时，与今天阅读的论文 所用到的journal是同一件事情，它们是同义词。\n除此之外，今天的论文是有关向ext2增加journal，并得到ext3文件系统（注，所以可以认为ext3文件系统就是ext2加上了logging系统）。\n接下来我将从这几个方面来讨论ext3文件系统：某种程度上将其与XV6进行对比；解释ext3是如何修复XV6的logging存在的性能问题；解释ext3在故障恢复时语义上的一些变化。\nXV6 File system logging回顾 首先来回顾一下XV6的logging系统。\n我们有一个磁盘用来存储XV6的文件系统，你可以认为磁盘分为了两个部分：\n首先是文件系统目录的树结构，以root目录为根节点，往下可能有其他的目录，我们可以认为目录结构就是一个树状的数据结构。\n假设root目录下有两个子目录D1和D2，D1目录下有两个文件F1和F2，每个文件又包含了一些block。\n除此之外，还有一些其他并非是树状结构的数据，比如bitmap表明了每一个data block是空闲的还是已经被分配了。\n注，Frans和Robert在这里可能有些概念不统一，对于Frans来说，目录内容应该也属于文件内容，目录是一种特殊的文件，详见14.3；而对于Robert来说，目录内容是metadata。\ninode，目录内容，bitmap block，我们将会称之为metadata block，另一类就是持有了文件内容的block，或者叫data block。\n除了文件系统之外，XV6在磁盘最开始的位置还有一段log。\nXV6的log相对来说比较简单，它有header block，之后是一些包含了有变更的文件系统block，这里可以是metadata block也可以是data block。\nheader block会记录之后的每一个log block应该属于文件系统中哪个block，假设第一个log block属于block 17，第二个属于block 29。\n在计算机上，我们会有一些用户程序调用write/create系统调用来修改文件系统。\n在内核中存在block cache，最初write请求会被发到block cache。\nblock cache就是磁盘中block在内存中的拷贝，所以最初对于文件block或者inode的更新走到了block cache。\n在write系统调用的最后，这些更新都被拷贝到了log中，之后我们会更新header block的计数来表明当前的transaction已经结束了。\n在文件系统的代码中，任何修改了文件系统的系统调用函数中，某个位置会有begin_op，表明马上就要进行一系列对于文件系统的更新了，不过在完成所有的更新之前，不要执行任何一个更新。\n在begin_op之后是一系列的read/write操作。最后是end_op，用来告诉文件系统现在已经完成了所有write操作。\n所以在begin_op和end_op之间，所有的write block操作只会走到block cache中。当系统调用走到了end_op函数，文件系统会将修改过的block cache拷贝到log中。\n在拷贝完成之后，文件系统会将修改过的block数量，通过一个磁盘写操作写入到log的header block，这次写入被称为commit point。\n在commit point之前，如果发生了crash，在重启时，整个transaction的所有写磁盘操作最后都不会应用。\n在commit point之后，即使立即发生了crash，重启时恢复软件会发现在log header中记录的修改过的block数量不为0，接下来就会将log header中记录的所有block，从log区域写入到文件系统区域。\n这里实际上使得系统调用中位于begin_op和end_op之间的所有写操作在面对crash时具备原子性。\n也就是说，要么文件系统在crash之前更新了log的header block，这样所有的写操作都能生效；要么crash发生在文件系统更新log的header block之前，这样没有一个写操作能生效。\n在crash并重启时，必须有一些恢复软件能读取log的header block，并判断里面是否记录了未被应用的block编号，如果有的话，需要写（也有可能是重写）log block到文件系统中对应的位置；如果没有的话，恢复软件什么也不用做。\n这里有几个超级重要的点，不仅针对XV6，对于大部分logging系统都适用：\n包括XV6在内的所有logging系统，都需要遵守write ahead rule。\n这里的意思是，任何时候如果一堆写操作需要具备原子性，系统需要先将所有的写操作记录在log中，之后才能将这些写操作应用到文件系统的实际位置。\n也就是说，我们需要预先在log中定义好所有需要具备原子性的更新，之后才能应用这些更新。\nwrite ahead rule是logging能实现故障恢复的基础。\nwrite ahead rule使得一系列的更新在面对crash时具备了原子性。\n另一点是，XV6对于不同的系统调用复用的是同一段log空间，但是直到log中所有的写操作被更新到文件系统之前，我们都不能释放或者重用log。\n我将这个规则称为freeing rule，它表明我们不能覆盖或者重用log空间，直到保存了transaction所有更新的这段log，都已经反应在了文件系统中。\n所以在XV6中，end_op做了大量的工作，首先是将所有的block记录在log中，之后是更新log header。\n在没有crash的正常情况，文件系统需要再次将所有的block写入到磁盘的文件系统中。\n磁盘中的文件系统更新完成之后，XV6文件系统还需要删除header block记录的变更了的block数量，以表明transaction已经完成了，之后就可以重用log空间。\n在向log写入任何新内容之前，删除header block中记录的block数量也很重要。\n因为你不会想要在header block中记录的还是前一个transaction的信息，而log中记录的又是一个新的transaction的数据。\n可以假设新的transaction对应的是与之前不同的block编号的数据，这样的话，在crash重启时，log中的数据会被写入到之前记录的旧的block编号位置。所以我们必须要先清除header block。\nfreeing rule的意思就是，在从log中删除一个transaction之前，我们必须将所有log中的所有block都写到文件系统中。\n这些规则使得，就算一个文件系统更新可能会复杂且包含多个写操作，但是每次更新都是原子的，在crash并重启之后，要么所有的写操作都生效，要么没有写操作能生效。\n要介绍Linux的logging方案，就需要了解XV6的logging有什么问题？为什么Linux不使用与XV6完全一样的logging方案？这里的回答简单来说就是XV6的logging太慢了。\nXV6中的任何一个例如create/write的系统调用，需要在整个transaction完成之后才能返回。所以在创建文件的系统调用返回到用户空间之前，它需要完成所有end_op包含的内容，这包括了：\n将所有更新了的block写入到log 更新header block 将log中的所有block写回到文件系统分区中 清除header block 之后才能从系统调用中返回。\n在任何一个文件系统调用的commit过程中，不仅是占据了大量的时间，而且其他系统调用也不能对文件系统有任何的更新。\n所以这里的系统调用实际上是一次一个的发生，而每个系统调用需要许多个写磁盘的操作。\n这里每个系统调用需要等待它包含的所有写磁盘结束，对应的技术术语被称为synchronize。\nXV6的系统调用对于写磁盘操作来说是同步的（synchronized），所以它非常非常的慢。\n在使用机械硬盘时，它出奇的慢，因为每个写磁盘都需要花费10毫秒，而每个系统调用又包含了多个写磁盘操作。\n所以XV6每秒只能完成几个更改文件系统的系统调用。如果我们在SSD上运行XV6会快一些，但是离真正的高效还差得远。\n另一件需要注意的更具体的事情是，在XV6的logging方案中，每个block都被写了两次。第一次写入到了log，第二次才写入到实际的位置。\n虽然这么做有它的原因，但是ext3可以一定程度上修复这个问题。\next3 file system log format ext3文件系统就是基于今天要阅读的论文 ，再加上几年的开发得到的，并且ext3也曾经广泛的应用过。\next3是针对之前一种的文件系统（ext2）logging方案的修改，所以ext3就是在几乎不改变之前的ext2文件系统的前提下，在其上增加一层logging系统。\n所以某种程度来说，logging是一个容易升级的模块。\next3的数据结构与XV6是类似的。\n在内存中，存在block cache，这是一种write-back cache（注，区别于write-through cache，指的是cache稍后才会同步到真正的后端）。\nblock cache中缓存了一些block，其中的一些是干净的数据，因为它们与磁盘上的数据是一致的；\n其他一些是脏数据，因为从磁盘读出来之后被修改过；\n有一些被固定在cache中，基于前面介绍的write-ahead rule和freeing rule，不被允许写回到磁盘中。\n除此之外，ext3还维护了一些transaction信息。它可以维护多个在不同阶段的transaction的信息。\n每个transaction的信息包含有：\n一个序列号 一系列该transaction修改的block编号。这些block编号指向的是在cache中的block，因为任何修改最初都是在cache中完成。 以及一系列的handle，handle对应了系统调用，并且这些系统调用是transaction的一部分，会读写cache中的block 在磁盘上，与XV6一样：\n会有一个文件系统树，包含了inode，目录，文件等等 会有bitmap block来表明每个data block是被分配的还是空闲的 在磁盘的一个指定区域，会保存log 目前为止，这与XV6非常相似。\n主要的区别在于ext3可以同时跟踪多个在不同执行阶段的transaction。\n接下来我们详细看一下ext3的log中有什么，这与XV6中的log有点不一样。\n在log的最开始，是super block。\n这是log的super block，而不是文件系统的super block。\nlog的super block包含了log中第一个有效的transaction的起始位置和序列号。\n起始位置就是磁盘上log分区的block编号，序列号就是前面提到的每个transaction都有的序列号。\nlog是磁盘上一段固定大小的连续的block。\nlog中，除了super block以外的block存储了transaction。\n每个transaction在log中包含了：\n一个descriptor block，其中包含了log数据对应的实际block编号，这与XV6中的header block很像。 之后是针对每一个block编号的更新数据。 最后当一个transaction完成并commit了，会有一个commit block 因为log中可能有多个transaction，commit block之后可能会跟着下一个transaction的descriptor block，data block和commit block。\n所以log可能会很长并包含多个transaction。我们可以认为super block中的起始位置和序列号属于最早的，排名最靠前的，并且是有效的transaction。\n这里有一些细节对于后面的内容很重要。\n在crash之后的恢复过程会扫描log，为了将descriptor block和commit block与data block区分开，descriptor block和commit block会以一个32bit的魔法数字作为起始。\n这个魔法数字不太可能出现在数据中，并且可以帮助恢复软件区分不同的block。\n学生提问：有没有可能使用一个descriptor block管理两个transaction？是不是只能一个transaction结束了才能开始下一个transaction？\nRobert教授：Log中会有多个transaction，但是的确一个时间只有一个正在进行的transaction。\n上面的图片没能很好的说明这一点，当前正在进行的transaction对应的是正在执行写操作的系统调用。\n所以当前正在进行的transaction只存在于内存中，对应的系统调用只会更新cache中的block，也就是内存中的文件系统block。\n当ext3决定结束当前正在进行的transaction，它会做两件事情：\n首先开始一个新的transaction，这将会是下一个transaction；其次将刚刚完成的transaction写入到磁盘中，这可能要花一点时间。\n所以完整的故事是，磁盘上的log分区有一系列旧的transaction，这些transaction已经commit了，除此之外，还有一个位于内存的正在进行的transaction。\n在磁盘上的transaction，只能以log记录的形式存在，并且还没有写到对应的文件系统block中。\nlogging系统在后台会从最早的transaction开始，将transaction中的data block写入到对应的文件系统中。\n当整个transaction的data block都写完了，之后logging系统才能释放并重用log中的空间。\n所以log其实是个循环的数据结构，如果用到了log的最后，logging系统会从log的最开始位置重新使用。\n记住这里的log的结构，它对于后面的内容也很重要。\next3如何提升性能 ext3通过3种方式提升了性能：\n首先，它提供了异步的（asynchronous）系统调用，也就是说系统调用在写入到磁盘之前就返回了，系统调用只会更新缓存在内存中的block，并不用等待写磁盘操作。不过它可能会等待读磁盘。 第二，它提供了批量执行（batching）的能力，可以将多个系统调用打包成一个transaction。 最后，它提供了并发（concurrency）。 这些基本上就是ext3有的，而XV6没有的特性。接下来我将一一介绍这里的特性。\n学生提问：有关batching，XV6不是也支持多个系统调用同时执行start_op和end_op，然后再一起commit吗？\nRobert教授：是的，XV6具备有限能力的batching。\n首先是异步的系统调用。\n这表示系统调用修改完位于缓存中的block之后就返回，并不会触发写磁盘。\n所以这里明显的优势就是系统调用能够快速的返回。\n同时它也使得I/O可以并行的运行，也就是说应用程序可以调用一些文件系统的系统调用，但是应用程序可以很快从系统调用中返回并继续运算，与此同时文件系统在后台会并行的完成之前的系统调用所要求的写磁盘操作。\n这被称为I/O concurrency，如果没有异步系统调用，很难获得I/O concurrency，或者说很难同时进行磁盘操作和应用程序运算，因为同步系统调用中，应用程序总是要等待磁盘操作结束才能从系统调用中返回。\n另一个异步系统调用带来的好处是，它使得大量的批量执行变得容易。\n异步系统调用的缺点是系统调用的返回并不能表示系统调用应该完成的工作实际完成了。\n举个例子，如果你创建了一个文件并写了一些数据然后关闭文件并在console向用户输出done，最后你把电脑的电给断了。\n尽管所有的系统调用都完成了，程序也输出了done，但是在你重启之后，你的数据并不一定存在。\n这意味着，在异步系统调用的世界里，如果应用程序关心可能发生的crash，那么应用程序代码应该更加的小心。\n这在XV6并不是什么大事，因为如果XV6中的write返回了，那么数据就在磁盘上，crash之后也还在。\n而ext3中，如果write返回了，你完全不能确定crash之后数据还在不在。\n所以一些应用程序的代码应该仔细编写，例如对于数据库，对于文本编辑器，我如果写了一个文件，我不想在我写文件过程断电然后再重启之后看到的是垃圾文件或者不完整的文件，我想看到的要么是旧的文件，要么是新的文件。\n所以文件系统对于这类应用程序也提供了一些工具以确保在crash之后可以有预期的结果。\n这里的工具是一个系统调用，叫做fsync，所有的UNIX都有这个系统调用。\n这个系统调用接收一个文件描述符作为参数，它会告诉文件系统去完成所有的与该文件相关的写磁盘操作，在所有的数据都确认写入到磁盘之后，fsync才会返回。\n所以如果你查看数据库，文本编辑器或者一些非常关心文件数据的应用程序的源代码，你将会看到精心放置的对于fsync的调用。\nfsync可以帮助解决异步系统调用的问题。\n对于大部分程序，例如编译器，如果crash了编译器的输出丢失了其实没什么，所以许多程序并不会调用fsync，并且乐于获得异步系统调用带来的高性能。\n学生提问：这是不是有时也被称为flush，因为我之前经常听到这个单词？\nRobert教授：是的，一个合理的解释fsync的工作的方式是，它flush了所有文件相关的写磁盘操作到了磁盘中，之后再返回，所以flush也是针对这个场景的一个合理的单词。\n以上就是异步系统调用，下一个ext3使用的技术是批量执行（batching）。\n在任何时候，ext3只会有一个open transaction。\next3中的一个transaction可以包含多个不同的系统调用。\n所以ext3是这么工作的：它首先会宣告要开始一个新的transaction，接下来的几秒所有的系统调用都是这个大的transaction的一部分。\n我认为默认情况下，ext3每5秒钟都会创建一个新的transaction，所以每个transaction都会包含5秒钟内的系统调用，这些系统调用都打包在一个transaction中。\n在5秒钟结束的时候，ext3会commit这个包含了可能有数百个更新的大transaction。\n为什么这是个好的方案呢？\n首先它在多个系统调用之间分摊了transaction带来的固有的损耗。\n固有的损耗包括写transaction的descriptor block和commit block；在一个机械硬盘中需要查找log的位置并等待磁碟旋转，\n这些都是成本很高的操作，现在只需要对一批系统调用执行一次，而不用对每个系统调用执行一次这些操作，所以batching可以降低这些损耗带来的影响。\n另外，它可以更容易触发write absorption。\n经常会有这样的情况，你有一堆系统调用最终在反复更新相同的一组磁盘block。\n举个例子，如果我创建了一些文件，我需要分配一些inode，inode或许都很小只有64个字节，一个block包含了很多个inode，所以同时创建一堆文件只会影响几个block的数据。\n类似的，如果我向一个文件写一堆数据，我需要申请大量的data block，我需要修改表示block空闲状态的bitmap block中的很多个bit位，如果我分配到的是相邻的data block，它们对应的bit会在同一个bitmap block中，所以我可能只是修改一个block的很多个bit位。\n所以一堆系统调用可能会反复更新一组相同的磁盘block。\n通过batching，多次更新同一组block会先快速的在内存的block cache中完成，之后在transaction结束时，一次性的写入磁盘的log中。\n这被称为write absorption，相比一个类似于XV6的同步文件系统，它可以极大的减少写磁盘的总时间。\n最后就是disk scheduling。\n假设我们要向磁盘写1000个block，不论是在机械硬盘还是SSD（机械硬盘效果会更好），一次性的向磁盘的连续位置写入1000个block，要比分1000次每次写一个不同位置的磁盘block快得多。\n我们写log就是向磁盘的连续位置写block。\n通过向磁盘提交大批量的写操作，可以更加的高效。\n这里我们不仅通过向log中连续位置写入大量block来获得更高的效率，甚至当我们向文件系统分区写入包含在一个大的transaction中的多个更新时，如果我们能将大量的写请求同时发送到驱动，即使它们位于磁盘的不同位置，我们也使得磁盘可以调度这些写请求，并以特定的顺序执行这些写请求，这也很有效。\n在一个机械硬盘上，如果一次发送大量需要更新block的写请求，驱动可以对这些写请求根据轨道号排序。\n甚至在一个固态硬盘中，通过一次发送给硬盘大量的更新操作也可以稍微提升性能。\n所以，只有发送给驱动大量的写操作，才有可能获得disk scheduling。这是batching带来的另一个好处。\next3使用的最后一个技术就是concurrency，相比XV6这里包含了两种concurrency。\n首先ext3允许多个系统调用同时执行，所以我们可以有并行执行的多个不同的系统调用。\n在ext3决定关闭并commit当前的transaction之前，系统调用不必等待其他的系统调用完成，它可以直接修改作为transaction一部分的block。\n许多个系统调用都可以并行的执行，并向当前transaction增加block，这在一个多核计算机上尤其重要，因为我们不会想要其他的CPU核在等待锁。\n在XV6中，如果当前的transaction还没有完成，新的系统调用不能继续执行。\n而在ext3中，大多数时候多个系统调用都可以更改当前正在进行的transaction。\n另一种ext3提供的并发是，可以有多个不同状态的transaction同时存在。\n所以尽管只有一个open transaction可以接收系统调用，但是其他之前的transaction可以并行的写磁盘。\n这里可以并行存在的不同transaction状态包括了：\n首先是一个open transaction 若干个正在commit到log的transaction，我们并不需要等待这些transaction结束。当之前的transaction还没有commit并还在写log的过程中，新的系统调用仍然可以在当前的open transaction中进行。 若干个正在从cache中向文件系统block写数据的transaction 若干个正在被释放的transaction，这个并不占用太多的工作 通常来说会有位于不同阶段的多个transaction，新的系统调用不必等待旧的transaction提交到log或者写入到文件系统。\n对比之下，XV6中新的系统调用就需要等待前一个transaction完全完成。\n学生提问：如果一个block cache正在被更新，而这个block又正在被写入到磁盘的过程中，会怎样呢？\nRobert教授：这的确会是一个问题，这里有个潜在的困难点，因为transaction写入到log中的内容只能包含由该transaction中的系统调用所做的更新，而不能包含在该transaction之后的系统调用的更新。\n因为如果这么做了的话，那么可能log中会只包含系统调用的部分更新，而我们需要确保transaction包含系统调用的所有更新。所以我们不能承担transaction包含任何在该transaction之后的更新的风险。\next3是这样解决这个问题的，当它决定结束当前的open transaction时，它会在内存中拷贝所有相关的block，之后transaction的commit是基于这些block的拷贝进行的。\n所以transaction会有属于自己的block的拷贝。为了保证这里的效率，操作系统会使用copy-on-write（注，详见8.4）来避免不必要的拷贝，这样只有当对应的block在后面的transaction中被更新了，它在内存中才会实际被拷贝。\nconcurrency之所以能帮助提升性能，是因为它可以帮助我们并行的运行系统调用，我们可以得到多核的并行能力。\n如果我们可以在运行应用程序和系统调用的同时，来写磁盘，我们可以得到I/O concurrency，也就是同时运行CPU和磁盘I/O。这些都能帮助我们更有效，更精细的使用硬件资源。\next3文件系统调用格式 接下来我们大概过一下Linux中的文件系统调用，并介绍抽象上每个系统调用的结构。\n在Linux的文件系统中，我们需要每个系统调用都声明一系列写操作的开始和结束。\n实际上在任何transaction系统中，都需要明确的表示开始和结束，这样之间的所有内容都是原子的。\n所以系统调用中会调用start函数。\next3需要知道当前正在进行的系统调用个数，所以每个系统调用在调用了start函数之后，会得到一个handle，它某种程度上唯一识别了当前系统调用。\n当前系统调用的所有写操作都是通过这个handle来识别跟踪的（注，handle是ext3 transaction中的一部分数据，详见16.3）。\n之后系统调用需要读写block，它可以通过get获取block在buffer中的缓存，同时告诉handle这个block需要被读或者被写。\n当这个系统调用结束时，它会调用stop函数，并将handle作为参数传入。\n除非transaction中所有已经开始的系统调用都完成了，transaction是不能commit的。\n因为可能有多个transaction，文件系统需要有种方式能够记住系统调用属于哪个transaction，这样当系统调用结束时，文件系统就知道这是哪个transaction正在等待的系统调用，所以handle需要作为参数传递给stop函数。\n因为每个transaction都有一堆block与之关联，修改这些block就是transaction的一部分内容，所以我们将handle作为参数传递给get函数是为了告诉logging系统，这个block是handle对应的transaction的一部分。\nstop函数并不会导致transaction的commit，它只是告诉logging系统，当前的transaction少了一个正在进行的系统调用。\ntransaction只能在所有已经开始了的系统调用都执行了stop之后才能commit。所以transaction需要记住所有已经开始了的handle，这样才能在系统调用结束的时候做好记录。\next3 transaction commit步骤 基于上面的系统调用的结构，接下来我将介绍commit transaction完整的步骤。\n每隔5秒，文件系统都会commit当前的open transaction，下面是commit transaction涉及到的步骤：\n首先需要阻止新的系统调用。\n当我们正在commit一个transaction时，我们不会想要有新增的系统调用，我们只会想要包含已经开始了的系统调用，所以我们需要阻止新的系统调用。\n这实际上会损害性能，因为在这段时间内系统调用需要等待并且不能执行。\n第二，需要等待包含在transaction中的已经开始了的系统调用们结束。\n所以我们需要等待transaction中未完成的系统调用完成，这样transaction能够反映所有的写操作。\n一旦transaction中的所有系统调用都完成了，也就是完成了更新cache中的数据，那么就可以开始一个新的transaction，并且让在第一步中等待的系统调用继续执行。\n所以现在需要为后续的系统调用开始一个新的transaction。\n还记得ext3中的log包含了descriptor，data和commit block吗？\n现在我们知道了transaction中包含的所有的系统调用所修改的block，因为系统调用在调用get函数时都将handle作为参数传入，表明了block对应哪个transaction。\n接下来我们可以更新descriptor block，其中包含了所有在transaction中被修改了的block编号。\n我们还需要将被修改了的block，从缓存中写入到磁盘的log中。\n之前有同学问过，新的transaction可能会修改相同的block，所以在这个阶段，我们写入到磁盘log中的是transaction结束时，对于相关block cache的拷贝。所以这一阶段是将实际的block写入到log中。\n接下来，我们需要等待前两步中的写log结束。\n之后我们可以写入commit block。\n接下来我们需要等待写commit block结束。\n结束之后，从技术上来说，当前transaction已经到达了commit point，也就是说transaction中的写操作可以保证在面对crash并重启时还是可见的。\n如果crash发生在写commit block之前，那么transaction中的写操作在crash并重启时会丢失。\n接下来我们可以将transaction包含的block写入到文件系统中的实际位置。\n在第9步中的所有写操作完成之后，我们才能重用transaction对应的那部分log空间。\n在一个非常繁忙的系统中，log的头指针一直追着尾指针在跑（注，也就是说一直没有新的log空间）。\n在当前最早的transaction的所有步骤都完成之前，或许不能开始commit一个新的transaction，因为我们需要重复利用最早的transaction对应的log空间。\n不过人们通常会将log设置的足够大，让这种情况就不太可能发生。\n学生提问：你刚刚说没有进程会等待这些步骤完成，那么这些步骤是在哪里完成的呢？\nRobert教授：这些是在后台的内核线程完成的\n学生提问：我有个有关重用log空间的问题，假设我们使用了一段特定的log空间，并且这段log空间占据了是刚刚释放出来的所有log空间，但是还不够，那么文件系统会等待另一部分的log空间释放出来吗，还是会做点别的？\nRobert教授：是的，会等待。让我画张图来确保我回答的是正确的问题。我们可以认为log是磁盘中的一段线性空间，假设现存的transaction中最早的是T7，之后是T8，T9，我们想要将T10放在T9之后的空闲区域。\n我们或许要等待T7将所有的block写入到文件系统对应的位置，这样我们才能释放T7对应的空间。这意味着T10中的步骤需要暂停以等待T7释放出来。这是你的问题吗？\n同一个学生：是的，所以可能是这样，我先写入T10的block到现有的log空闲区域，但是如果最后log足够大并且我们用光了空闲区域，我们就需要等待T7的空间被释放出来，是吗？\nRobert教授：是的，如果需要写入的数据足够多，并且log迅速的用光了。我们甚至都不能在释放log空间之前开始新的系统调用。如果你们关注细节的话，这里会有一些潜在的死锁。首先系统调用需要预声明需要多少个block，这样logging系统才知道对于该transaction需要多少log空间，因为我们不会在没有足够空间来commit transaction时，开始一个新的transaction（注，难道不能将不能写入到磁盘log中的transaction先缓存在内存中吗？虽然这样可能会导致堆积）。\n学生提问：如果新的transaction需要的空间走到了T8，那么现在就需要等待T7，T8结束，这是怎么工作的呢？\nRobert教授：图中的T7，T8，T9其中的系统调用都完成了，并且都已经在commit到log中了。在上面的图中，我们会直接开始T10，新的系统调用会写入到transaction T10，最终当T10需要commit到log中，并且它大到需用用到T8的空间时，它需要等待T7，T8结束。\n文件系统会记录每个transaction的大小，这样文件系统就知道要等待多少个之前的transaction结束。所以这里还有不少的记录工作，这样文件系统才能理解所有旧的transaction的状态。\n有关如何重用log空间，这里有个小细节。\n在log的最开始有一个super block，所以在任何时候log都是由一个super block和一些transaction组成。\n假设T4是最新的transaction，之前是T1，T2，T3。\n我们是否能重用一段log空间，取决于相应的transaction，例如T2，是否已经commit并且写入到文件系统的实际位置中，这样在crash并重启时就不需要重新执行这段transaction了。\n同时也取决于T2之前的的所有transaction是否已经被释放了。所有的这些条件都满足时，我们就可以释放并重用T2对应的log空间。\next3 file system恢复过程 为了简化重启时恢复软件的工作，当决定释放某段log空间时，文件系统会更新super block中的指针将其指向当前最早的transaction的起始位置。\n之后如果crash并重启，恢复软件会读取super block，并找到log的起始位置。\n所以如果crash了，内存中的所有数据都会消失，例如文件系统中记录的哪些block被写入到了磁盘中这些信息都会丢失，所以可以假设这时内存中没有可用的数据，唯一可用的数据存在于磁盘中。\n当然我们这里的讨论都是基于磁盘还是完好的，所以你可以认为只是一次电力故障，系统突然停止了运行过程，在电力恢复时，断电那一瞬间磁盘中的数据还存在。我们并没有考虑磁盘被损坏或者被摧毁的情况。\ncrash或许会打断任何在进行中的transaction，或许transaction正在commit，或许transaction正在向文件系统写block。\n让我重新画一个例子，我们在log中有一个super block，之后是transaction T6，T7，T8，在T8之后是一个已近被释放了log空间的T5，假设T8已经用了T5的一部分空间。\n并且现在super block指向的是T6的起始位置，因为T6是最早的transaction。\n现在crash并重启，恢复软件读取super block就可以知道log的起始位置，之后恢复软件会在log中一直扫描并尝试找到log的结束位置，现在我们需要有一种方式确定log的结束位置。\n我们知道每个transaction包含了一个descriptor block，里面记录了该transaction中包含了多少个data block，假设descriptor block记录了17个block，那么恢复软件会扫描17个data block，最后是commit block。\n这样可以一直扫描到T8。\n在扫描T8时有两种可能，一种可能是T8完成了commit，并且包含了commit block。\n这时恢复软件并不知道T8就是最后一个transaction，所以它会接着看T8的commit block的下一个block，来看看这是不是一个有效的descriptor block。\n我们知道这不是一个descriptor block，而是一个包含在T5内的随机block。\n现在的问题是恢复软件如何可靠的区分出来呢？是的，每个descriptor和commit block都以某个魔法数字作为起始，这是一个32bit的数字。\n所以如果扫描完了T8，下一个block以魔法数字作为起始，那么恢复软件就会认为这是一个descriptor block。\n（注，也有可能T5正好完美的跟在T8后面，也就是说T8的commit block之后就是T5的descriptor block，同时T5的commit block也存在，所以这里必然还需要一些其他的机制，我猜是用到了transaction的序列号）\n但是，现在我们看到的block可能是包含了任意数据的data block，所以它可能是文件中的一个data block并且也是以魔法数字作为起始。\n所以这里的最后一个细节是，logging系统需要能区分一个以魔法数字作为起始的descriptor block和一个以魔法数字作为起始的data block。\n你可以想到各种方法来实现这种区分，ext3是这样做的，当它向log写一个block时，如果这个block既不是descriptor block也不是commit block，但是又以魔法数字作为起始，文件系统会以0替换前32bit，并在transaction的descriptor block中为该data block设置一个bit。\n这个bit表示，对应的data block本来是以魔法数字作为起始，但是现在我们将其替换成了0。而恢复软件会检查这个bit位，在将block写回到文件系统之前，会用魔法数字替换0。\n因此，在log中，除了descriptor和commit block，不会有其他的block以这32bit的魔法数字作为起始。\n所以我们不会有模棱两可的判断，如果一个commit block之后的block以魔法数字作为起始，那么它必然是一个descriptor block。\n所以恢复软件会从super block指向的位置开始一直扫描，直到：\n某个commit block之后的一个block并不是descriptor block 某个commit block之后是descriptor block，但是根据descriptor block找到的并不是一个commit block 这时，恢复软件会停止扫描，并认为最后一个有效的commit block是log的结束位置。\n或许在最后一个commit block之后会跟一个并没有commit完成的transaction（注，上面的第二种情况），但是恢复软件会忽略未完成的transaction，因为这个transaction并没有包含所有的写操作，所以它并不能原子性的恢复。\n之后恢复软件会回到log的最开始位置，并将每个log block写入到文件系统的实际位置，直到走到最后一个有效的commit block。\n之后才是启动剩下的操作系统，并且运行普通的程序。在恢复完成之前，是不能运行任何程序的，因为这个时候文件系统并不是有效的。\n学生提问：XV6相比这里的log机制，缺少了什么呢？\nRobert教授：XV6主要缺失的是在log中包含多个transaction的能力，在XV6的log中最多只会有一个transaction，所以在XV6中缺少了并发的能力。\n比如说当我在执行transaction T7的系统调用时，ext3可以同时向磁盘提交T6，而这在XV6中这是不可能的，因为log只保存了一个transaction。\n所以我们必须先完成一个transaction的所有工作，之后才能开始下一个transaction。所以XV6是简单且正确的，但是缺少了并发的能力。\n学生提问：但是在XV6我还是可以有多个transaction，只是说不能异步的执行它们，对吗？\nRobert教授：这里其实有点模糊，XV6实际上允许在一个transaction中包含多个系统调用（注，详见15.8），所以XV6有一些并发和batching的能力，但是当XV6决定要commit一个transaction时，在完全完成这个transaction之前，是不能执行任何新的系统调用的。\n因为直到前一个transaction完全完成，并没有log空间来存放新的系统调用。\n所以XV6要么是在运行一些系统调用，要么是在commit transaction，但是它不能同时干这两件事情，而ext3可以同时干这两件事情。\n为什么新transaction需要等前一个transaction中系统调用执行完成 以上就是ext3中相对来说直观的部分。实际上还有一些棘手的细节我想讨论一下。\n之前我提到过，ext3中存在一个open transaction，但是当ext3决定要关闭该transaction时，它需要等待该transaction中的所有系统调用都结束，之后才能开始新的transaction。\n假设我们现在有transaction T1，其中包含了多个系统调用。\n如果我们想要关闭T1，我们需要停止接收新的系统调用，因为我们想要等待现有的系统调用结束，这样才能commit transaction。\n所以直到这些系统调用都结束了，在ext3中不能允许开始任何新的系统调用。\n所以只有在T1中的系统调用完成之后，才能开始在接下来的transaction T2中接收系统调用。\n在这之间有一段时间，新的系统调用是被拦截的，这降低了性能，因为我们本来应该执行系统调用的但是又不被允许。\n这里的问题是，直到T1中所有的系统调用都结束之前，ext3为什么不让T2中的系统调用开始执行呢？\n让我们来看一下没有这个限制条件可能会带来的错误的场景。\n我们假设T1只包含了一个系统调用，这是一个create系统调用用来创建文件x。\n在create系统调用结束之前，文件系统决定开始一个新的transaction T2用来接收create之后的所有系统调用。\n我们假设T2在T1结束之前就开始了，T2对另一个文件y调用了unlink系统调用。unlink会释放与y关联的inode。\n假设在下面的时间点T2将inode标记为空闲的，create会为x分配inode，或许它在之后的一个时间点分配了inode。\n因为create在unlink释放inode之后分配的inode，它可能会重用同一个inode，所以x可能会获得y的inode，假设是inode 17。\n目前为止没有问题，因为unlink本来就是释放inode。\n当T1中的create结束之后，我们会关闭T1，在最后我们会将T1的所有更新都写入到磁盘的log中。\n之后unlink还要花点时间才能结束，但是在它结束之前计算机crash了。\n在重启并运行恢复软件时，可以发现T1已经commit了，而T2没有。\n所以恢复软件会完全忽略T2，这意味着T2中的unlink就跟没有发生过一样，恢复软件不会执行T2中的unlink，也就不会删除文件y。\n所以crash并重启之后y文件仍然存在，并还在使用inode 17。\n然而T1又完成了，x文件使用的也是inode 17，所以现在我们错误的有了两个文件都使用了相同的inode，这意味着它们共享了文件内容，向一个文件写数据会神奇的出现在另一个文件中。\n这完全是错误的，因为我们本来想的是删除y，并为x分配一个空闲的inode，而不是一个已经在使用中的inode。\n这里可以这么想，T2中的unlink修改了一个block，最终这个修改过的block被前一个transaction所使用。\nT2中修改的信息，被T1所使用了，这意味着我们丢失了T2的原子性。\n因为T2的目标是unlink的效果要么是全发生，要么是完全不发生。但是刚刚的例子中，因为T1使用了T2中释放的inode，这意味着T2中部分修改已经生效了，但是其他的修改随着crash又丢失了。\n或许你可以想到一些修复这里问题的方法，或许T1可以发现inode是由后一个transaction释放的而不去使用它。\n而ext3采用了一个非常简单的方法，在前一个transaction中所有系统调用都结束之前，它不允许任何新的系统调用执行。\n所以transaction T1也就不可能看到之后的transaction包含的更新。\n因为直到T1 commit了，整个unlink都不被允许执行。\n学生提问：当你关闭一个open transaction时，具体会发生什么呢？会对当前的缓存做一个快照吗？\nRobert教授：会的，当我们关闭一个transaction，文件系统会拷贝被transaction中的系统调用所修改的所有block，之后transaction才会commit这些block。后面的transaction会在真正的block cache上运行。当将block都commit到log之后，对于block cache的拷贝就可以丢弃了。\n总结 最后我希望同学们记住的有关logging和ext3的是：\nlog是为了保证多个步骤的写磁盘操作具备原子性。在发生crash时，要么这些写操作都发生，要么都不发生。这是logging的主要作用。\nlogging的正确性由write ahead rule来保证。\n你们将会在故障恢复相关的业务中经常看到write ahead rule或者write ahead log（WAL）。write ahead rule的意思是，你必须在做任何实际修改之前，将所有的更新commit到log中。在稍后的恢复过程中完全依赖write ahead rule。对于文件系统来说，logging的意义在于简单的快速恢复。log中可能包含了数百个block，你可以在一秒中之内重新执行这数百个block，不管你的文件系统有多大，之后又能正常使用了。\n最后有关ext3的一个细节点是，它使用了批量执行和并发来获得可观的性能提升，不过同时也带来了可观的复杂性的提升。\n学生提问：你刚刚说有一个文件系统线程会做这里所有的工作，那么只能有一个这样的线程，否则的话就会有不同步的问题了，对吗？\nRobert教授：或许真的只有一个线程，我其实不知道有多少个线程，但是1是个不错的数字，因为logging的正确性取决于旧的transaction要在新的transaction之前提交。\n但是逻辑上来说又没有必要只有一个线程，你可以想象不同的transaction使用不同的线程来提交（注，只要锁加的合适多个线程应该也是没问题的）。\n学生提问：当你在讨论crash的时候，你有一个图是T8正在使用之前释放的T5的空间，如果T8在crash的时候还没有commit，并且T5的commit block正好在T8的descriptor block所指定的位置，这样会不会不正确的表明T8已经被commit了（注，这时T8有一个假的commit block）？\nRobert教授：让我尝试画出这里的场景。\n首先我们有一个古老的transaction T5，因为log的循环特性，在顺序上T8位于T5之前。\n因为T5已经被释放了，T8正在蚕食T5的log空间。\n假设T8没有完成commit，但是如果完成commit的话，T8的commit block会写到T5的commit block位置。\nT8并没有能写入commit block，T8前面所有的block都写入了，但是最后跟的是T5的commit block。\n这里的答案是，descriptor block和commit block都有transaction的序列号，所以T8的descriptor block里面的序列号是8，但是T5的commit block里面的序列号是5，所以两者不能匹配。\n学生提问：我们可以在transaction T8开始的时候就知道它的大小吗？\nRobert教授：这是个复杂的问题。\n当T8作为活跃的transaction开始时，系统调用会写入数据，这时文件系统并不知道T8有多大。\n当文件系统开始commit T8时，是知道T8有多大的，因为文件系统只会在T8中所有的系统调用都结束之后才commit它，而在那个时间点，文件系统知道所有的写操作，所以就知道T8究竟有多大。\n除此之外，descriptor block里面包含了所有block的实际编号，所以当写入transaction的第一个block，也就是descriptor block时，logging系统知道T8会包含多少个block。\n学生提问：为什么不在descriptor block里面记录commit信息。虽然这样可能不太好，因为要回到之前的一个位置去更新之前的一个block。\nRobert教授：所以这里的提议是，与其要一个专门的commit block，可以让descriptor block来实现commit block的功能。\nXV6与这个提议非常像，我认为可以这么做，至少在ext3中这么做了不会牺牲性能。\n你需要像XV6一样来组织这里的结构，也就是需要在descriptor block包含某个数据表明这是一个已经提交过的transaction。\n这样做的话，可以节省一个commit block的空间，但是不能节省整个时间。\nLinux文件系统的后续版本实现了你的提议，ext4做了以下工作来更有效的写commit block。\next4会同时写入所有的data block和commit block，它并不是等待所有的data block写完了之后才写的commit block。\n但是这里有个问题，磁盘可以无序的执行写操作，所以磁盘可能会先写commit block之后再写data block。\n如果中间有了crash，那么我们有了commit block，但是却没有全部的data block。\next4通过在commit block中增加校验和来避免这种问题。\n所以commit block写入之后发生了crash，如果data block没有全写入那么校验和不能得出正确的结果，恢复软件可以据此判断出错了。\next4可以通过这种方式在机械硬盘上写入一批block而避免磁碟旋转，进而提升磁盘性能。\n学生提问：log中的data block是怎么写入到文件系统中的？\nRobert教授：这个问题有多个答案。\n对于data block，ext3有三种模式，但是我只记得两个，journaled data和ordered data（注，第三种是writeback）。\n当你在配置ext3文件系统时，你需要告诉Linux你想要哪种模式。\n如果你想要的是journaled data，文件内容就是写入到log中，如果你向一个文件写数据，这会导致inode更新，log中会包含文件数据和更新了的inode，也就是说任何更新了的block都会记录在log中。这种方法非常慢，因为数据需要先写在log中，再写到文件系统中。所以journaled data很直观，但是很慢。\nordered data是最流行的模式，它不会将文件数据写入到log中，只会将metadata block，例如inode，目录block，写入到log中，文件的内容会直接写到文件系统的实际位置中。\n所以这种模式要快得多，因为你不用将文件内容写两次。但是它也会导致更多的复杂性，因为你不能随时写入文件内容。假设你执行一个写操作导致一个新的block被分配给一个文件，并将包含了新分配block编号的inode写入到log中并commit，在实际写入文件内容至刚刚分配的data block之前发生crash。\n在稍后的恢复流程中，你将会看到包含了新分配的block编号的inode，但是对应data block里面的内容却属于之前使用了这个data block的旧的文件。\n如果你运行的是一个类似Athena的多用户系统，那么可能就是一个用户拥有一个文件，其中的内容又属于另一个用户已经删除的文件，如果我们不是非常小心的处理写入数据和inode的顺序就会有这样的问题。\next3的ordered data通过先写入文件内容到磁盘中，再commit修改了的inode来解决这里的问题。\n如果你是个应用程序，你写了一个文件并导致一个新的文件系统data block被分配出来，文件系统会将新的文件内容写到新分配的data block中，之后才会commit transaction，进而导致inode更新并包含新分配的data block编号。\n如果在写文件数据和更新inode之间发生了crash，你也看不到其他人的旧的数据，因为这时就算有了更新了的data block，但是也没关系，因为现在不仅inode没有更新，连bitmap block也没更新，相应的data block还处于空闲状态，并且可以分配给其他的程序，你并不会因此丢失block。\n这里的效果就是我们写了一个data block但是最终并没有被任何文件所使用。\nQ\u0026amp;A 拷贝具体过程 在 ext3 文件系统中，当关闭一个开放的事务（open transaction）时，涉及的拷贝过程是为了保证事务的一致性和数据的原子性。让我们逐步解析这个拷贝过程，了解它的各个环节及其对后续事务的影响。\n拷贝的目的主要是为了确保事务的一致性和隔离性。在事务提交之前，文件系统需要把修改过的块（block）从缓存中复制出来，确保这些修改在事务提交到日志（log）之前不会被其他事务覆盖。\n拷贝的对象：拷贝的对象是 被当前事务（T1）修改过的所有数据块，包括文件内容块（data blocks）、inode 元数据块、目录块等。具体来说，任何在这个事务期间被修改的缓存块（block cache），都需要被拷贝。 为什么要拷贝？：因为在事务提交之前，这些数据块仍然是可修改的。如果没有拷贝，后续的系统调用或者新的事务（T2）可能会修改这些相同的块，从而导致在 T1 还未提交时，出现不一致的状态。通过拷贝，可以确保 T1 的事务在独立的副本上执行，直到它的修改完全提交到日志。 拷贝的过程在事务关闭并准备提交时进行。具体过程如下：\n拷贝的时机：当 T1 事务关闭并进入提交状态时，文件系统会将该事务中所有被修改的块拷贝出来。这些块的拷贝存放在内存中，形成事务的一个快照，即将所有被修改的块保存到独立的区域，以保证这些块不会被新的事务修改。 拷贝的方式：拷贝并不是从磁盘到内存，而是 缓存块（block cache）到独立的事务区域，这可以理解为内存中的一个独立缓冲区。这个缓冲区专门保存当前事务修改的块。 什么时候清除拷贝的内容？\n清除的时机：当事务 T1 被成功提交到日志（即 descriptor block、data block 和 commit block 已全部写入日志），并且文件系统确认事务完成后，拷贝的这些块就不再需要了。这时，拷贝的内容会被丢弃，因为它们已经持久化到日志或者写回到实际的磁盘上。 清除的机制：系统会释放这些拷贝占用的内存资源，这样可以让新的事务和系统调用继续使用这些块。在 T1 事务提交并清除后，新的事务就可以直接使用这些被修改过的块，而不再需要访问拷贝的副本。 拷贝的内容是事务 T1 中修改的块。这些拷贝的存在对后续事务 T2 有着重要的影响，主要体现在以下几点：\n保证事务隔离：拷贝使得 T1 和后续事务 T2 在操作相同块时互相隔离。即使 T2 需要修改与 T1 相同的数据块，它也会在缓存中进行自己的修改，而不会影响 T1 的拷贝内容。这种隔离确保了不同事务之间的修改不会互相干扰。 确保事务原子性：拷贝的块在事务 T1 完全提交之前，是不会被其他事务修改的。这意味着即使 T2 修改了某个块，T1 的快照仍然保持一致。只有当 T1 提交并将修改持久化到磁盘后，T2 才能够基于 T1 的最终结果继续进行。 写时复制机制：这其实是一种**写时复制（Copy-on-Write, CoW）**机制。后续的事务在操作时，如果需要修改 T1 已经修改过的块，系统会先进行块的拷贝，然后让 T2 基于新副本进行修改，而 T1 的拷贝内容保持不变，直到 T1 提交。这种机制有效保障了数据一致性。 拷贝过程的完整总结\n事务关闭时的拷贝：当事务 T1 关闭，文件系统会将当前事务中所有被修改的块从缓存中拷贝到一个独立的区域，确保这些块的修改不会被后续事务覆盖或干扰。 拷贝内容的保存位置：拷贝的块在内存中独立保存，形成事务 T1 的“快照”。后续事务会在未被拷贝的缓存块上执行，而不会直接修改这些已经拷贝的块。 清除拷贝的时机：当 T1 成功提交到日志，并且文件系统确认修改持久化到磁盘后，这些拷贝的块会被丢弃，释放内存资源。 后续事务的影响：拷贝确保了事务的隔离性和原子性。T1 的修改对 T2 是透明的，直到 T1 完全提交后，T2 才会基于 T1 的结果进行修改。 总的来说：\n拷贝的对象 是当前事务修改的所有块。 拷贝的位置 是内存中的独立区域。 清除的时机 是当事务提交完成后，系统确认修改已经持久化。 拷贝对后续事务的影响 是确保事务之间的隔离性和一致性，防止数据被后续事务不正确地修改。 这个机制保障了每个事务的操作都是独立的，修改不会被其他事务破坏，同时能确保在崩溃恢复时维持文件系统的一致性。\n日志空间的操作执行的顺序 在 ext3 文件系统中，日志（log）空间的操作是按照 事务提交顺序 来执行的，并不是并发执行的。文件系统采用的是 顺序日志写入 模式，这样能够保证数据的一致性和恢复时的确定性。\n当一个事务被提交时，文件系统会按照以下顺序执行操作：\nDescriptor block：描述当前事务要修改的块编号。 Data block：写入缓存中修改过的数据。 Commit block：标志事务的完成。 所有这些 block 都按照顺序依次写入日志空间，并确保 在写入 commit block 之前，所有的 data block 都已经写入日志。换句话说，事务的所有操作按照顺序写入日志，这种顺序性保证了日志的完整性和恢复时的安全性。\n当一个事务完成日志写入，并且 commit block 写入成功后，这个事务的所有操作就被认为是提交完成的。然而，这时修改的数据块还仅仅存在于日志中，而不是文件系统的实际位置。\n接下来，文件系统会将这些已经提交的事务数据（data block）从日志写入到实际的文件系统（也称为“写回”过程）。这个过程也是按照 事务提交的顺序 来执行的，而不是并发执行。\n顺序写回数据：文件系统会按照事务提交的顺序依次将数据块从日志空间写回到文件系统的实际存储位置。虽然这个写回过程可能与系统中的其他操作并发进行，但每个事务的写入顺序是依赖事务提交顺序的。 避免并发冲突：为了避免文件系统的并发写入冲突，事务的写回过程是顺序化的。一个事务必须在之前的事务完全写入后，才能开始写回下一个事务。这确保了日志恢复的一致性和文件系统状态的可预测性。 如果系统崩溃或故障，在系统重启时，恢复软件会从日志中按照事务的提交顺序，逐一将数据块恢复到文件系统中。恢复过程严格按照日志中的顺序执行，确保事务按提交顺序被恢复到文件系统的实际位置。\n总的来说：\n提交顺序执行：虽然日志写入和写回的过程可能会与系统的其他操作并发进行，但每个事务的操作是按照它们提交的顺序依次执行的。文件系统确保事务的提交和数据写入严格遵循顺序，避免了并发冲突和数据不一致。 顺序性的重要性：保持顺序写入和写回能够保证日志恢复过程的可预测性和文件系统的一致性。 commit block ext3 文件系统使用 commit block 来判断一个事务是否已经成功提交，因此读取日志空间的最后一个 block 是否是 commit block 是判断该事务是否已完成的关键方式。具体流程解释：\n事务提交的流程：\n在一个事务的执行过程中，首先写入 descriptor block，该 block 描述了事务中哪些块会被修改。 紧接着，文件系统会写入 data block，这些数据块包含了文件系统实际修改的内容。 最后，文件系统会写入 commit block。这标志着这个事务已经完成。 判断事务是否完成：\n当系统在检查日志空间时，它会读取每个事务的最后一个 block。如果发现最后的 block 是 commit block，这就表明该事务已完成并成功提交。 如果系统崩溃或中断，重启时，文件系统会扫描日志空间来恢复事务。如果一个事务中没有 commit block，则表示这个事务没有完成，因此系统会回滚该事务的所有操作，确保文件系统的一致性。 具体来说，文件系统在重启时会：\n扫描日志空间中的所有 block。 找到事务的 descriptor block 和对应的 data block。 检查事务的最后一个 block 是否是 commit block。如果 commit block 存在，表示事务完成，数据可以被写入文件系统的永久位置；如果 commit block 缺失，则该事务未完成，文件系统会丢弃该事务的所有修改。 为什么 commit block 重要：\n一致性保障：commit block 是文件系统的一个检查点，确保在事务执行的过程中，即使发生崩溃或系统故障，文件系统仍然能够恢复到一致的状态。commit block 之前的事务被认为是完全提交的，而 commit block 之前未写入的事务将被回滚。 事务回滚：如果在提交事务时，系统未能写入 commit block（比如系统崩溃），重启后的文件系统会回滚这一事务，以避免数据的不一致。只有包含 commit block 的事务数据才能被认为是持久化的。 在系统崩溃或非正常关闭的情况下，ext3 文件系统的日志恢复机制会通过以下步骤恢复文件系统的状态：\n扫描日志空间：系统启动后，文件系统会扫描日志空间，寻找每个事务的 descriptor block 和 data block。 检查 commit block：系统检查事务的最后一个 block 是否为 commit block。 如果 commit block 存在：表示事务完成，文件系统会将日志中的 data block 数据写回到文件系统的实际存储位置。 如果 commit block 不存在：表示事务未完成，文件系统会回滚这个事务，丢弃该事务的所有操作，保持文件系统的原始状态。 总结：\ncommit block 是判断事务是否完成的核心标志。文件系统通过检查日志空间中的最后一个 block 是否为 commit block 来判断该事务是否已成功提交。 如果 commit block 存在，说明事务成功完成，系统会把日志中的数据写回到文件系统实际的位置。如果 commit block 不存在，说明事务中断，系统会回滚该事务。 这个设计确保了在崩溃恢复或系统重启时，ext3 文件系统可以有效地维持数据的一致性和完整性。\next3 文件系统的日志结构 在 ext3 文件系统中，日志 (log) 机制是基于事务日志（Journal）的，它用于跟踪对文件系统的修改，确保在系统崩溃或掉电等意外情况下可以恢复一致的文件系统状态。ext3 的日志是按事务提交的方式进行的，每个事务包含多个 block。具体来说，日志中的 block 结构分为几类，分别是descriptor block、data block 和 commit block。ext3 文件系统的日志结构如下。\nSuperblock： 这是文件系统的描述符，记录了日志的元信息。它保存了日志的位置、大小、状态等关键信息。Superblock 在整个文件系统中只有一个，并且包含关于日志区（Journal Area）的一些元数据。 日志空间 (Journal Area)： 日志区域由多个 block 组成，用于记录文件系统的事务操作。这些 block 依次连接组成一个循环日志空间。日志空间的大小是在创建文件系统时指定的。 日志中的 block 类型： Descriptor Block：这个 block 是每个事务的开头部分，用于描述这个事务中的修改。具体来说，它列出了哪些块在这个事务中被修改了，并且这些块的位置（即磁盘上的块编号）。Descriptor block 让文件系统在恢复时知道哪些块被修改。 Data Block：这些 block 存储具体的修改数据。它们紧跟在 descriptor block 后面。数据块包含了文件系统中的实际数据修改，比如某个文件的内容或元数据的修改。 Commit Block：这个 block 标记事务的结束，它告诉文件系统这个事务已经完成，所有的修改都可以视为已提交。Commit block 是事务提交的关键点。如果系统在 commit block 写入之前崩溃，整个事务都会被回滚。如果 commit block 已经写入，那么事务被视为成功提交。 一个完整的事务通常包含以下几个步骤：\nDescriptor Block： 事务开始时，首先会写入一个 descriptor block，这个 block 列出了哪些磁盘块在此次事务中将被修改。 Data Block： 接下来，紧跟在 descriptor block 之后，系统会将实际的数据（data block）写入日志。这些数据来自于缓存（buffer），并且每个数据块对应 descriptor block 中列出的某个块。 Commit Block： 在所有数据块写入日志之后，文件系统会写入 commit block。这个 block 标记着这个事务已经完成，所有在这个事务中的修改都可以被持久化到磁盘的真实位置。 Commit block 并不是在每个数据块后立即写入，而是在整个事务的所有数据块写入日志之后，才会写入日志区域的最后一个 block。它的作用是表明这个事务已经提交成功。如果系统在写入 commit block 之前崩溃，那么即使之前写入的数据块已经在日志中，事务也会在系统重启时被回滚。\n具体来说，commit block 记录了当前事务的结束，并且在崩溃恢复时，系统会根据 commit block 判断哪些事务已经提交、哪些事务需要回滚：\n事务提交成功：如果 commit block 已经写入日志，那么表示事务已经完整提交，系统可以安全地将数据块写入文件系统的实际位置。 事务未提交：如果系统崩溃时 commit block 尚未写入，则说明事务未完成。系统在恢复时会回滚这次事务，确保数据一致性。 日志块的生命周期：\n事务日志写入后，commit block 确认提交：当事务的 descriptor block 和 data block 写入日志后，文件系统会写入 commit block。一旦 commit block 写入成功，这个事务就被认为提交完成。此时，文件系统可以将这些数据块（data block）从日志区域持久化写入实际文件系统的数据结构中。 是否所有 block 都变成 commit block？：并不是说写入到磁盘的所有 data block 会变成 commit block。commit block 只是一个标识符，标记事务的提交成功。data block 依然是数据块，它们并不会变成 commit block。 日志空间重用：一旦事务成功提交并且数据已经写入文件系统的真实位置，日志空间中的那些被使用的 block（包括 descriptor block、data block 和 commit block）会被标记为可重用。接下来进行的事务可以继续使用这部分日志空间进行新的事务记录。这就是日志的循环利用机制。 日志提交后的流程：\n写入磁盘：提交事务成功之后，文件系统会将事务中涉及到的 data block 写入文件系统的实际位置（不是日志空间）。这些数据块会从缓存中持久化到文件系统对应的区域。 日志空间重用：当数据块写入实际文件系统后，相应的日志空间会被释放，以便记录新的事务。 总结：\n日志空间结构：日志空间中包含 descriptor block（描述修改的块）、data block（存储具体数据的块）和 commit block（标记事务完成的块）。 commit block：它是在事务的所有 data block 写入日志之后写入，用于标记事务的成功提交。 数据持久化与重用：事务提交后，数据写入文件系统实际位置，日志空间被释放，可以循环使用。 这种设计确保了文件系统的一致性，特别是在崩溃或故障恢复的场景下，通过日志系统，ext3 能够有效恢复文件系统的状态。\n为何使用魔法数字 在 descriptor block 中记录哪些是 data block，而不让 data block 自身存储魔法数字是合理的。这确实能避免 data block 中与魔法数字相关的冲突问题，但 ext3 文件系统中使用魔法数字的方式有其历史原因和技术权衡，具体来说：\n魔法数字在文件系统中扮演着一个识别块类型的重要角色。通过在每个 block 中使用魔法数字，文件系统可以快速识别 block 的类型并执行相应的操作。虽然你建议可以只在 descriptor block 中标识哪些是 data block，但魔法数字的存在带来了几项关键好处：\n快速识别与检查：魔法数字使得文件系统在处理 block 时能够快速识别这是一个 data block 还是 descriptor block，而不需要依赖其他结构（如 descriptor block）来查找并区分块的类型。这种设计能够提高处理和识别的效率，特别是在日志恢复和崩溃后的检查过程中。 自我描述性：每个 block 都是自我描述的，魔法数字提供了一种内嵌的机制，便于在调试和修复时直接通过读取 block 数据来了解它的类型。这种自我描述的设计在维护和诊断中具有很大的价值。 如果我们完全依赖 descriptor block 来标记哪些 block 是 data block，虽然能减少对魔法数字的需求，但会引入一些潜在的复杂性和风险：\n恢复过程的复杂化：在崩溃恢复时，文件系统需要扫描事务日志来重建状态。如果所有的 block 类型信息仅存储在 descriptor block 中，那么在恢复过程中，文件系统需要额外查找和关联每个 block 和其对应的 descriptor block。这个过程会增加恢复的复杂度和时间。相反，魔法数字让每个 block 自带标识，恢复过程更加直接和高效。 处理时间增加：如果每次访问某个 block 时，系统都必须回溯到对应的 descriptor block 来确定其类型，文件系统的处理效率会降低。通过在 block 本身使用魔法数字，系统可以立即判断 block 的类型，而不必在事务日志中查找。 一致性检查困难：在某些场景下，比如调试、日志审计或者磁盘一致性检查，如果 block 本身没有类型标识符（即魔法数字），系统或运维人员必须依赖 descriptor block 来进行检查。如果 descriptor block 丢失或损坏，可能会导致无法正确识别和恢复 data block。 ext3 的设计是平衡了性能和数据一致性。在大多数情况下，魔法数字提供了更直接的块类型识别方式，而不是让文件系统依赖 descriptor block 逐一去匹配。在实践中：\n用魔法数字快速识别块类型 提供了高效的方式，在极端情况下（如崩溃恢复或诊断中），可以立即识别 block 的类型。 避免数据冲突：通过将某些具有冲突魔法数字的 data block 前 32 位替换为 0，并在 descriptor block 中设置标志位来记住这个修改，ext3 提供了一种既能避免冲突又保持性能的折衷方案。 另外，文件系统的设计往往受到历史版本的影响。魔法数字的使用是 UNIX 系统中常见的一种自我描述性机制，它有助于跨平台和跨版本的兼容。尽管你提出的依赖 descriptor block 的方法在某些情况下更为简洁，但从设计的演化角度来看，使用魔法数字已经成为一种通用的、广泛接受的机制，修改这一点可能会影响到已有的工具和工作流。\n虽然理论上可以只依赖 descriptor block 来标识哪些 block 是 data block，省去魔法数字，但 ext3 通过使用魔法数字和补充的 bit 标记系统，确保了性能、可靠性和恢复过程的高效性。魔法数字提供了一种快速的、块级别的识别方式，使文件系统在正常运行和崩溃恢复过程中更加高效。这种设计是在可靠性、性能和历史兼容性之间的平衡折衷。\n为什么在事务提交过程中，系统会阻止新的系统调用？ 注：其实正课末尾已解释\n在事务提交过程中，阻止新的系统调用是为了确保文件系统在处理当前事务时，保持一致性和数据的完整性。这种做法虽然会暂时影响性能（因为新的系统调用需要等待），但从文件系统的一致性角度来看，它是必要的。为什么阻止新的系统调用：\n事务的原子性和一致性要求： 事务（transaction）在文件系统中是一个原子操作的集合，所有的操作要么全部成功提交，要么全部失败回滚。在提交事务时，文件系统需要确保这个事务中的所有系统调用都已经完成，且所有修改的数据块都已经记录在日志中。 如果允许在事务提交过程中开始新的系统调用，可能会导致数据不一致。新的系统调用可能会修改与当前事务相同的数据块。如果这些修改在事务提交的过程中发生，文件系统可能无法区分哪些修改属于当前正在提交的事务，哪些属于新的事务，从而导致事务日志记录不完整或错误。 防止数据块的竞争修改： 假设在提交事务时，有一个新的系统调用尝试修改与当前事务相同的磁盘块（block）。这就可能导致两个系统调用对同一个 block 进行操作，产生写入冲突。为了避免这种竞争条件的出现，文件系统在提交事务时会阻止新的系统调用。 如果不阻止新的系统调用，新的系统调用可能会修改缓存中的相同 block，文件系统将无法保证提交的事务反映了缓存的正确状态。这会使得事务提交后的状态与预期的事务日志内容不符，损害文件系统的一致性。 事务日志的完整性： 在提交事务的过程中，文件系统会将修改过的 block 写入事务日志的 data block，并记录在 descriptor block 中。当写入完成并提交 commit block 后，事务才能认为已经安全地写入磁盘，数据的持久性才得以保证。 如果此时允许新的系统调用开始，新的写入可能会干扰正在进行的日志写入过程。这样，文件系统可能无法正确识别哪些 block 是由新系统调用修改的，哪些属于当前事务，从而破坏日志的完整性。 事务提交的最终确定性： 当事务提交进入最后阶段（如写入 commit block 时），文件系统的状态必须是确定的。所有涉及到的系统调用都必须结束，并且系统需要保证事务中的所有修改已经记录在日志中。如果此时允许新的系统调用开始，可能会打破这种确定性，使得日志中的数据变得不可靠。 为了确保事务在提交之后可以正确恢复和回滚，阻止新的系统调用是必要的，这样可以保证提交点（commit point）前后文件系统的状态是一致的。 阻止新的系统调用不会影响文件系统的有序性，反而有助于保持事务之间的严格顺序和隔离性。具体来说：\n系统调用的顺序性：在阻止新的系统调用的时间段内，文件系统仅仅是延迟了新系统调用的开始，但并没有打乱系统调用的顺序。事务提交之后，新的系统调用将按顺序继续执行。因此，文件系统的操作顺序仍然是有序的。 隔离性：文件系统中的事务需要提供一定程度的隔离性，确保在事务 A 提交前，事务 B 的修改不会影响事务 A。如果不阻止新的系统调用，这种隔离性将无法得到保证，可能导致事务 A 和事务 B 的修改交错进行，进而导致数据不一致。 阻止新的系统调用在事务提交过程中是为了保证事务的一致性、原子性和日志的完整性。虽然这种做法会暂时影响系统的性能，但它确保了文件系统的正确性，避免了数据冲突和不一致的发生。阻止新的系统调用并不会影响系统调用的有序性，相反，它帮助维护了事务的隔离性和数据的可靠性。\nhandle 的作用 handle 在 ext3 文件系统中的角色，确实是用于标识系统调用并追踪与该系统调用相关的所有写操作。让我们从你的描述出发，澄清 handle 的作用以及它在 ext3 文件系统中的实际用途。\n在 ext3 文件系统中，每个系统调用在开始时会调用 start 函数，随后会得到一个 handle。这个 handle 是用来唯一标识当前系统调用的。\n唯一标识系统调用：每个系统调用在其生命周期内，handle 充当它的唯一识别符，帮助文件系统将其跟踪。文件系统中的所有写操作都通过这个 handle 关联到相应的系统调用和事务上。 跟踪写操作：在系统调用的过程中，如果它需要访问或修改某个 block（例如通过 get_block 函数），handle 会作为参数传递，表明该 block 的修改是由哪个系统调用发起的。 因此，handle 不仅用于标识系统调用，同时也用于跟踪该系统调用中的所有写操作。每个系统调用与其操作的 block 通过 handle 建立联系。\n虽然 handle 是系统调用的标识符，但它也与整个事务的执行紧密关联。ext3 文件系统会周期性地将当前事务提交到磁盘，在事务提交过程中，handle 的作用包括以下几点：\n识别系统调用所属的事务：文件系统需要知道哪些系统调用属于当前事务。当某个系统调用读取或修改 block 时，handle 传递给 logging 系统，表明这些 block 是该事务的一部分。因此，handle 也关联了系统调用与事务的关系。 跟踪事务中的系统调用进度：在事务提交时，所有与该事务相关的系统调用必须完成（即所有 handle 所代表的系统调用必须调用 stop 函数）才能进行提交。stop 函数通过 handle 通知文件系统，该系统调用已经完成，事务中的操作减少一个。 阻止新系统调用：在事务提交过程中，系统会阻止新的系统调用，以确保所有尚未完成的系统调用能够有序完成，并且所有的修改在提交时是一致的。 文件系统的日志（log）包含 descriptor block、data block 和 commit block。在系统调用过程中，通过 handle 确定哪些 block 被修改：\n当一个系统调用修改某个 block 时，文件系统会记录这些修改，并在事务提交时写入 descriptor block，标明哪些块被修改。 这些 block 编号会被记录到 descriptor block 中，随后在数据被写入到日志后，再写入 commit block，表明事务的所有修改都已完成并提交。 在 ext3 文件系统中，handle 主要用于以下几个方面：\n唯一标识系统调用：每个系统调用通过 handle 进行唯一标识，帮助文件系统跟踪哪些 block 被修改。 关联事务和系统调用：通过 handle，文件系统可以将系统调用与对应的事务关联起来，并确保所有系统调用完成后事务才能提交。 管理日志：handle 还用于将系统调用与其修改的 block 关联，确保这些修改能够正确写入日志和最终提交到文件系统中。 在整个过程中，handle 起到的核心作用是帮助文件系统管理和跟踪每个系统调用以及它对文件系统的修改操作。\n传递 block 编号的具体时机是在事务执行的过程中，尤其是在系统调用读取或写入某个数据块时。以下是几个关键点：\n读取时：当系统调用需要读取一个 block 时，文件系统会根据当前事务的 handle 记录哪些 block 被读取，并在事务日志中记录。 写入时：当系统调用修改某个 block 时，这个 block 的编号会被记录为修改的 block，并最终写入 descriptor block，确保在提交事务时，这些 block 能够正确写入到磁盘的 log 中。 ","date":"2024-10-01T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08116file-system-performance-and-fast-crash-recovery/bg_hu14028822929857313790.jpg","permalink":"https://echudet.github.io/p/mit6s08116file-system-performance-and-fast-crash-recovery/","title":"MIT6S081：16File system performance and fast crash recovery"},{"content":"Lec15 Crash recovery 课前预习 日志层 文件系统设计中最有趣的问题之一是崩溃恢复。\n出现此问题的原因是，许多文件系统操作都涉及到对磁盘的多次写入，并且在完成写操作的部分子集后崩溃可能会使磁盘上的文件系统处于不一致的状态。\n例如，假设在文件截断（将文件长度设置为零并释放其内容块）期间发生崩溃。根据磁盘写入的顺序，崩溃可能会留下对标记为空闲的内容块的引用的inode，也可能留下已分配但未引用的内容块。\n后者相对来说是良性的，但引用已释放块的inode在重新启动后可能会导致严重问题。\n重新启动后，内核可能会将该块分配给另一个文件，现在我们有两个不同的文件无意中指向同一块。\n如果xv6支持多个用户，这种情况可能是一个安全问题，因为旧文件的所有者将能够读取和写入新文件中的块，而新文件的所有者是另一个用户。\nXv6通过简单的日志记录形式解决了文件系统操作期间的崩溃问题。\nxv6系统调用不会直接写入磁盘上的文件系统数据结构。\n相反，它会在磁盘上的log（日志）中放置它希望进行的所有磁盘写入的描述。\n一旦系统调用记录了它的所有写入操作，它就会向磁盘写入一条特殊的commit（提交）记录，表明日志包含一个完整的操作。\n此时，系统调用将写操作复制到磁盘上的文件系统数据结构。\n完成这些写入后，系统调用将擦除磁盘上的日志。\n如果系统崩溃并重新启动，则在运行任何进程之前，文件系统代码将按如下方式从崩溃中恢复。\n如果日志标记为包含完整操作，则恢复代码会将写操作复制到磁盘文件系统中它们所属的位置。\n如果日志没有标记为包含完整操作，则恢复代码将忽略该日志。恢复代码通过擦除日志完成。\n为什么xv6的日志解决了文件系统操作期间的崩溃问题？\n如果崩溃发生在操作提交之前，那么磁盘上的登录将不会被标记为已完成，恢复代码将忽略它，并且磁盘的状态将如同操作尚未启动一样。 如果崩溃发生在操作提交之后，则恢复将重播操作的所有写入操作，如果操作已开始将它们写入磁盘数据结构，则可能会重复这些操作。 在任何一种情况下，日志都会使操作在崩溃时成为原子操作：恢复后，要么操作的所有写入都显示在磁盘上，要么都不显示。\n日志设计 日志驻留在超级块中指定的已知固定位置。\n超级块 是 xv6 文件系统中的一个重要结构，它存储了文件系统的基本信息和元数据。超级块在文件系统中扮演着类似于“目录”的角色，记录了关于文件系统布局的全局信息。超级块是存储在文件系统的特定块中，并且它会在文件系统被挂载时读入内存\n超级块的主要作用是提供以下关键信息：\n文件系统的总体布局：超级块中包含文件系统中各个区域的大小和位置，例如： inode（索引节点）区域的起始位置和数量。 数据块区域的起始位置和数量。 位图的起始位置，用于管理块的分配。 块的总数和 inode 的总数：超级块记录了文件系统中总的块数和 inode 数。inode 是文件系统中的一种数据结构，每个 inode 对应一个文件或目录，存储了它们的元数据信息（如文件大小、权限等）。 文件系统的其他元数据：例如文件系统的版本号、磁盘块大小等信息。 它由一个头块（header block）和一系列更新块的副本（logged block）组成。\n头块包含一个扇区号数组（每个logged block对应一个扇区号）以及日志块的计数。\n磁盘上的头块中的计数或者为零，表示日志中没有事务；或者为非零，表示日志包含一个完整的已提交事务，并具有指定数量的logged block。\n在事务提交（commit）时Xv6才向头块写入数据，在此之前不会写入，并在将logged blocks复制到文件系统后将计数设置为零。\n因此，事务中途崩溃将导致日志头块中的计数为零；提交后的崩溃将导致非零计数。\n注：logged block表示已经记录了操作信息的日志块，而log block仅表示日志块\n每个系统调用的代码都指示写入序列的起止，考虑到崩溃，写入序列必须具有原子性。\n为了允许不同进程并发执行文件系统操作，日志系统可以将多个系统调用的写入累积到一个事务中。\n因此，单个提交可能涉及多个完整系统调用的写入。\n为了避免在事务之间拆分系统调用，日志系统仅在没有文件系统调用进行时提交。\n同时提交多个事务的想法称为组提交（group commit）。\n组提交减少了磁盘操作的数量，因为成本固定的一次提交分摊了多个操作。\n组提交还同时为磁盘系统提供更多并发写操作，可能允许磁盘在一个磁盘旋转时间内写入所有这些操作。\nXv6的virtio驱动程序不支持这种批处理，但是Xv6的文件系统设计允许这样做。\nXv6在磁盘上留出固定的空间来保存日志。\n事务中系统调用写入的块总数必须可容纳于该空间。\n这导致两个后果：任何单个系统调用都不允许写入超过日志空间的不同块。\n这对于大多数系统调用来说都不是问题，但其中两个可能会写入许多块：write和unlink。\n一个大文件的write可以写入多个数据块和多个位图块以及一个inode块；unlink大文件可能会写入许多位图块和inode。\nXv6的write系统调用将大的写入分解为适合日志的多个较小的写入，unlink不会导致此问题，因为实际上Xv6文件系统只使用一个位图块。\n日志空间有限的另一个后果是，除非确定系统调用的写入将可容纳于日志中剩余的空间，否则日志系统无法允许启动系统调用。\n代码：日志 在系统调用中一个典型的日志使用就像这样：\n1 2 3 4 5 6 7 begin_op(); ... bp = bread(...); bp-\u0026gt;data[...] = ...; log_write(bp); ... end_op(); begin_op（kernel/log.c:126）等待直到日志系统当前未处于提交中，并且直到有足够的未被占用的日志空间来保存此调用的写入。\nlog.outstanding统计预定了日志空间的系统调用数；为此保留的总空间为log.outstanding乘以MAXOPBLOCKS。\n递增log.outstanding会预定空间并防止在此系统调用期间发生提交。代码保守地假设每个系统调用最多可以写入MAXOPBLOCKS个不同的块。\nlog_write（kernel/log.c:214）充当bwrite的代理。它将块的扇区号记录在内存中，在磁盘上的日志中预定一个槽位，并调用bpin将缓存固定在block cache中，以防止block cache将其逐出。\n注：固定在block cache是指在缓存不足需要考虑替换时，不会将这个block换出，因为事务具有原子性：假设块45被写入，将其换出的话需要写入磁盘中文件系统对应的位置，而日志系统要求所有内存必须都存入日志，最后才能写入文件系统。\nbpin是通过增加引用计数防止块被换出的，之后需要再调用bunpin\n在提交之前，块必须留在缓存中：在提交之前，缓存的副本是修改的唯一记录；只有在提交后才能将其写入磁盘上的位置；同一事务中的其他读取必须看到修改。\nlog_write会注意到在单个事务中多次写入一个块的情况，并在日志中为该块分配相同的槽位。这种优化通常称为合并（absorption）。\n例如，包含多个文件inode的磁盘块在一个事务中被多次写入是很常见的。通过将多个磁盘写入合并到一个磁盘中，文件系统可以节省日志空间并实现更好的性能，因为只有一个磁盘块副本必须写入磁盘。\n注：日志需要写入磁盘，以便重启后读取，但日志头块和日志数据块也会在block cache中有一个副本\nend_op（kernel/log.c:146）首先减少未完成系统调用的计数。\n如果计数现在为零，则通过调用commit()提交当前事务。这一过程分为四个阶段。\nwrite_log()（kernel/log.c:178）将事务中修改的每个块从缓冲区缓存复制到磁盘上日志槽位中。\nwrite_head()（kernel/log.c:102）将头块写入磁盘：这是提交点，写入后的崩溃将导致从日志恢复重演事务的写入操作。\ninstall_trans（kernel/log.c:69）从日志中读取每个块，并将其写入文件系统中的适当位置。\n最后，end_op写入计数为零的日志头；这必须在下一个事务开始写入日志块之前发生，以便崩溃不会导致使用一个事务的头块和后续事务的日志块进行恢复。\nrecover_from_log（kernel/log.c:116）是由initlog（kernel/log.c:55）调用的，而它又是在第一个用户进程运行（kernel/proc.c:539）之前的引导期间由fsinit（kernel/fs.c:42）调用的。它读取日志头，如果头中指示日志包含已提交的事务，则模拟end_op的操作。\n日志的一个示例使用发生在filewrite（kernel/file.c:135）中。事务如下所示：\n1 2 3 4 5 begin_op(); ilock(f-\u0026gt;ip); r = writei(f-\u0026gt;ip, ...); iunlock(f-\u0026gt;ip); end_op(); 这段代码被包装在一个循环中，该循环一次将大的写操作分解为几个扇区的单个事务，以避免日志溢出。作为此事务的一部分，对writei的调用写入许多块：文件的inode、一个或多个位图块以及一些数据块。\n代码：块分配器 文件和目录内容存储在磁盘块中，磁盘块必须从空闲池中分配。xv6的块分配器在磁盘上维护一个空闲位图，每一位代表一个块。0表示对应的块是空闲的；1表示它正在使用中。程序mkfs设置对应于引导扇区、超级块、日志块、inode块和位图块的比特位。\n块分配器提供两个功能：balloc分配一个新的磁盘块，bfree释放一个块。\nBalloc中位于kernel/fs.c:71的循环从块0到sb.size（文件系统中的块数）遍历每个块。它查找位图中位为零的空闲块。\n如果balloc找到这样一个块，它将更新位图并返回该块。为了提高效率，循环被分成两部分。\n外部循环读取位图中的每个块。内部循环检查单个位图块中的所有BPB位。\n由于任何一个位图块在buffer cache中一次只允许一个进程使用，因此，如果两个进程同时尝试分配一个块，可能会发生争用。\nBfree（kernel/fs.c:90）找到正确的位图块并清除正确的位。同样，bread和brelse隐含的独占使用避免了显式锁定的需要。\n与本章其余部分描述的大部分代码一样，必须在事务内部调用balloc和bfree。\n正课 File system crash概述 今天的课程是有关文件系统中的Crash safety。\n这里的Crash safety并不是一个通用的解决方案，而是只关注一个特定的问题的解决方案，也就是crash或者电力故障可能会导致在磁盘上的文件系统处于不一致或者不正确状态的问题。\n当我说不正确的状态时，是指例如一个data block属于两个文件，或者一个inode被分配给了两个不同的文件。\n这个问题可能出现的场景可能是这样，当你在运行make指令时，make与文件系统会有频繁的交互，并读写文件，但是在make执行的过程中断电了，可能是你的笔记本电脑没电了，也可能就是停电了，之后电力恢复之后，你重启电脑并运行ls指令，你会期望你的文件系统仍然在一个好的可用的状态。\n这里我们关心的crash或者故障包括了：\n在文件系统操作过程中的电力故障\n在文件系统操作过程中的内核panic\n包括XV6在内的大部分内核都会panic，panic可能是由内核bug引起，它会突然导致你的系统故障，但是你肯定期望能够在重启之后还能使用文件系统。\n文件系统的操作都包含了多个步骤，在多个步骤的错误位置crash或者电力故障了，存储在磁盘上的文件系统可能会是一种不一致的状态，之后可能会发生一些坏的事情。\n这类特定问题的解决方法，也就是logging。\n这是一个最初来自于数据库世界的很流行的解决方案，现在很多文件系统都在使用logging。\n之所以它很流行，是因为它是一个很好用的方法。\nXV6的实现非常简单，几乎是最简单的实现logging的方法，因为我们只是为了演示关键的思想。\n由于XV6实现的较为简单，XV6中的logging存在一个缺点，它的性能并不咋样，尽管logging系统原则上来说可以获得好的性能。\n在下节课我们将通过学习Linux的ext3文件系统所使用的logging系统，来看一下如何实现一个高性能logging系统。\n这是我们最后一节有关XV6的课程。这节课之后，我们将切换到阅读论文。\n因为这节课讲完了之后，我们就覆盖了操作系统的基本概念，我们可以通过阅读论文看一些更高级的操作系统思想。\n这节课关注的场景。类似于创建文件，写文件这样的文件系统操作，都包含了多个步骤的写磁盘操作。\n如何创建一个文件，这里多个步骤的顺序是（注，实际步骤会更多，详见14.5）：\n分配inode，或者在磁盘上将inode标记为已分配 之后更新包含了新文件的目录的data block 如果在这两个步骤之间，操作系统crash了。这时可能会使得文件系统的属性被破坏。\n这里的属性是指，每一个磁盘block要么是空闲的，要么是只分配给了一个文件。即使故障出现在磁盘操作的过程中，我们期望这个属性仍然能够保持。\n如果这个属性被破坏了，那么重启系统之后程序可能会运行出错，比如：\n操作系统可能又立刻crash了，因为文件系统中的一些数据结构现在可能处于一种文件系统无法处理的状态。 或者，更可能的是操作系统没有crash，但是数据丢失了或者读写了错误的数据。 File system crash示例 为了更清晰的理解这里的风险，让我们看一些基于XV6的例子，并看一下哪里可能出错。\nXV6有一个非常简单的文件系统和磁盘数据的排布方式。\n在super block之后就是log block，我们今天主要介绍的就是log block。\nlog block之后是inode block，每个block可能包含了多个inode。\n之后是bitmap block，它记录了哪个data block是空闲的。\n最后是data block，这里包含了文件系统的实际数据。\n操作系统与磁盘block的交互过程，创建一个文件涉及到了多个操作：\n首先是分配inode，因为首先写的是block 33 之后inode被初始化，然后又写了一次block 33 之后是写block 46，是将文件x的inode编号写入到x所在目录的inode的data block中 之后是更新root inode，因为文件x创建在根目录，所以需要更新根目录的inode的size字段，以包含这里新创建的文件x 最后再次更新了文件x的inode 在出现电力故障之后，因为内存数据保存在RAM中，所有的内存数据都丢失了。\n所有的进程数据，所有的文件描述符，内存中所有的缓存都没有了，因为内存数据不是持久化的。\n我们唯一剩下的就是磁盘上的数据，因为磁盘的介质是持久化的，所以只有磁盘上的数据能够在电力故障之后存活。\n基于这些事实，如果我们在上面的位置出现故障，并且没有额外的机制，没有logging，会有多糟糕呢？我们这里会有什么风险？\n在这个位置，我们先写了block 33表明inode已被使用，之后出现了电力故障，然后计算机又重启了。\n这时，我们丢失了刚刚分配给文件x的inode。这个inode虽然被标记为已被分配，但是它并没有放到任何目录中，所以也就没有出现在任何目录中，因此我们也就没办法删除这个inode。\n所以在这个位置发生电力故障会导致我们丢失inode。\n你或许会认为，我们应该改一改代码，将写block的顺序调整一下，这样就不会丢失inode了。所以我们可以先写block 46来更新目录内容，之后再写block 32来更新目录的size字段，最后再将block 33中的inode标记为已被分配。\n这里的效果是一样的，只是顺序略有不同。并且这样我们应该可以避免丢失inode的问题。那么问题来了，这里可以工作吗？我们应该问问自己，如果在下面的位置发生了电力故障会怎样？\n在这个位置，目录被更新了，但是还没有在磁盘上分配inode。\n电力故障之后机器重启，文件系统会是一个什么状态？或者说，如果我们读取根目录下的文件x，会发生什么，因为现在在根目录的data block已经有了文件x的记录？\n是的，我们会读取一个未被分配的inode，因为inode在crash之前还未被标记成被分配。更糟糕的是，如果inode之后被分配给一个不同的文件，这样会导致有两个应该完全不同的文件共享了同一个inode。\n如果这两个文件分别属于用户1和用户2，那么用户1就可以读到用户2的文件了。所以上面的解决方案也不好。\n所以调整写磁盘的顺序并不能彻底解决我们的问题，我们只是从一个问题换到了一个新的问题。\n让我们再看一个例子，这个例子中会向文件x写入“hi”（注，也就是14.5介绍的第二个部分）\n一旦成功的创建了文件x，之后会调用write系统调用，我们在上节课看到了write系统调用也执行了多个写磁盘的操作。\n首先会从bitmap block，也就是block 45中，分配data block，通过从bitmap中分配一个bit，来表明一个data block已被分配。 上一步分配的data block是block 595，这里将字符“h”写入到block 595。 将字符“i”写入到block 595。 最后更新文件夹x的inode来更新size字段。 这里我们也可以问自己一个问题，我们在下面的位置crash了会怎样？\n这里我们从bitmap block中分配了一个data block，但是又还没有更新到文件x的inode中。\n当我们重启之后，磁盘处于一个特殊的状态，这里的风险是什么？是的，我们这里丢失了data block，因为这个data block被分配了，但是却没有出现在任何文件中，因为它还没有被记录在任何inode中。\n你或许会想，是因为这里的顺序不对才会导致丢失data block的问题。我们应该先写block 33来更新inode来包含data block 595（同样的问题，这个时候data block都还没有分配怎么知道是595），之后才通过写block 45将data block 595标记为已被分配。\n所以，为了避免丢失data block，我们将写block的顺序改成这样。现在我们考虑一下，如果故障发生在这两个操作中间会怎样？(略)\n这时inode会认为data block 595属于文件x，但是在磁盘上它还被标记为未被分配的。之后如果另一个文件被创建了，block 595可能会被另一个文件所使用。所以现在两个文件都会在自己的inode中记录block 595。如果两个文件属于两个用户，那么两个用户就可以读写彼此的数据了。很明显，我们不想这样，文件系统应该确保每一个data block要么属于且只属于一个文件，要么是空闲的。所以这里的修改会导致磁盘block在多个文件之间共享的安全问题，这明显是错误的。\n所以这里的问题并不在于操作的顺序，而在于我们这里有多个写磁盘的操作，这些操作必须作为一个原子操作出现在磁盘上。\nFile system logging 我们这节课要讨论的针对文件系统crash之后的问题的解决方案，其实就是logging。\n这是来自于数据库的一种解决方案。它有一些好的属性：\n解决方案。它有一些好的属性：\n首先，它可以确保文件系统的系统调用是原子性的。比如你调用create/write系统调用，这些系统调用的效果是要么完全出现，要么完全不出现，这样就避免了一个系统调用只有部分写磁盘操作出现在磁盘上。 其次，它支持快速恢复（Fast Recovery）。在重启之后，我们不需要做大量的工作来修复文件系统，只需要非常小的工作量。这里的快速是相比另一个解决方案来说，在另一个解决方案中，你可能需要读取文件系统的所有block，读取inode，bitmap block，并检查文件系统是否还在一个正确的状态，再来修复。而logging可以有快速恢复的属性。 最后，原则上来说，它可以非常的高效，尽管我们在XV6中看到的实现不是很高效。 如何构建一个logging系统，并同时具有原子性的系统调用，快速恢复和高性能，而今天，我们只会关注前两点。\nlogging的基本思想还是很直观的。\n首先，你将磁盘分割成两个部分，其中一个部分是log，另一个部分是文件系统，文件系统可能会比log大得多。\n1.（log write）当需要更新文件系统时，我们并不是更新文件系统本身。\n假设我们在内存中缓存了bitmap block，也就是block 45。\n当需要更新bitmap时，我们并不是直接写block 45，而是将数据写入到log中，并记录这个更新应该写入到block 45。\n对于所有的写 block都会有相同的操作，例如更新inode，也会记录一条写block 33的log。\n所以基本上，任何一次写操作都是先写入到log，我们并不是直接写入到block所在的位置，而总是先将写操作写入到log中。\n2.（commit op）之后在某个时间，当文件系统的操作结束了，比如说我们前一节看到的4-5个写block操作都结束，并且都存在于log中，我们会commit文件系统的操作。\n这意味着我们需要在log的某个位置记录属于同一个文件系统的操作的个数，例如5。\n3.（install log）当我们在log中存储了所有写block的内容时，如果我们要真正执行这些操作，只需要将block从log分区移到文件系统分区。\n我们知道第一个操作该写入到block 45，我们会直接将数据从log写到block45，第二个操作该写入到block 33，我们会将它写入到block 33，依次类推。\n4.（clean log）一旦完成了，就可以清除log。清除log实际上就是将属于同一个文件系统的操作的个数设置为0。\n以上就是log的基本工作方式。\n为什么这样的工作方式是好的呢？假设我们crash并重启了。在重启的时候，文件系统会查看log的commit记录值，如果是0的话，那么什么也不做。\n如果大于0的话，我们就知道log中存储的block需要被写入到文件系统中，很明显我们在crash的时候并不一定完成了install log，我们可能是在commit之后，clean log之前crash的。\n所以这个时候我们需要做的就是reinstall（注，也就是将log中的block再次写入到文件系统），再clean log。\n这里的方法之所以能起作用，是因为可以确保当发生crash（并重启之后），我们要么将写操作所有相关的block都在文件系统中更新了，要么没有更新任何一个block，我们永远也不会只写了一部分block。\n为什么可以确保呢？我们考虑crash的几种可能情况。\n在第1步和第2步之间crash会发生什么？\n在重启的时候什么也不会做，就像系统调用从没有发生过一样，也像crash是在文件系统调用之前发生的一样。这完全可以，并且也是可接受的。\n在第2步和第3步之间crash会发生什么？\n在这个时间点，所有的log block都落盘了，因为有commit记录，所以完整的文件系统操作必然已经完成了。我们可以将log block写入到文件系统中相应的位置，这样也不会破坏文件系统。所以这种情况就像系统调用正好在crash之前就完成了。\n在install（第3步）过程中和第4步之前这段时间crash会发生什么？\n在下次重启的时候，我们会redo log，我们或许会再次将log block中的数据再次拷贝到文件系统。这样也是没问题的，因为log中的数据是固定的，我们就算重复写了文件系统，每次写入的数据也是不变的。\n重复写入并没有任何坏处，因为我们写入的数据可能本来就在文件系统中，所以多次install log完全没问题。当然在这个时间点，我们不能执行任何文件系统的系统调用。我们应该在重启文件系统之前，在重启或者恢复的过程中完成这里的恢复操作。\n换句话说，install log是幂等操作（注，idempotence，表示执行多次和执行一次效果一样），你可以执行任意多次，最后的效果都是一样的。\n学生提问：因为这里的接口只有read/write，但是如果我们做append操作，就不再安全了，对吧？\nFrans教授：某种程度来说，append是文件系统层面的操作，在这个层面，我们可以使用上面介绍的logging机制确保其原子性（注，append也可以拆解成底层的read/write）。\n学生提问：当正在commit log的时候crash了会发生什么？比如说你想执行多个写操作，但是只commit了一半。\nFrans教授：在上面的第2步，执行commit操作时，你只会在记录了所有的write操作之后，才会执行commit操作。所以在执行commit时，所有的write操作必然都在log中。而commit操作本身也有个有趣的问题，它究竟会发生什么？如我在前面指出的，commit操作本身只会写一个block。文件系统通常可以这么假设，单个block或者单个sector的write是原子操作（注，有关block和sector的区别详见14.3）。这里的意思是，如果你执行写操作，要么整个sector都会被写入，要么sector完全不会被修改。所以sector本身永远也不会被部分写入，并且commit的目标sector总是包含了有效的数据。而commit操作本身只是写log的header，如果它成功了只是在commit header中写入log的长度，例如5，这样我们就知道log的长度为5。这时crash并重启，我们就知道需要重新install 5个block的log。如果commit header没能成功写入磁盘，那这里的数值会是0。我们会认为这一次事务并没有发生过。这里本质上是write ahead rule，它表示logging系统在所有的写操作都记录在log中之前，不能install log。\nLogging的实现方式有很多，我这里展示的指示一种非常简单的方案，这个方案中clean log和install log都被推迟了。\n接下来我会运行这种非常简单的实现方式，之后在下节课我们会看到更加复杂的logging协议。\n不过所有的这些协议都遵循了write ahead rule，也就是说在写入commit记录之前，你需要确保所有的写操作都在log中。\n在这个范围内，还有大量设计上的灵活性可以用来设计特定的logging协议。\n在XV6中，我们会看到数据有两种状态，是在磁盘上还是在内存中。内存中的数据会在crash或者电力故障之后丢失。\nXV6的log结构如往常一样也是极其的简单。\n我们在最开始有一个header block，也就是我们的commit record，里面包含了：\n数字n代表有效的log block的数量 每个log block的实际对应的block编号 之后就是log的数据，也就是每个block的数据，依次为bn0对应的block的数据，bn1对应的block的数据以此类推。这就是log中的内容，并且log也不包含其他内容。\n当文件系统在运行时，在内存中也有header block的一份拷贝，拷贝中也包含了n和block编号的数组。\n这里的block编号数组就是log数据对应的实际block编号，并且相应的block也会缓存在block cache中，这个在Lec14有介绍过。\n与前一节课对应，log中第一个block编号是45，那么在block cache的某个位置，也会有block 45的cache。\nlog_write函数 接下来让我们看一些代码来帮助我们理解这里是怎么工作的。\n前面我提过事务（transaction），也就是我们不应该在所有的写操作完成之前写入commit record。\n这意味着文件系统操作必须表明事务的开始和结束。\n在XV6中，以创建文件的sys_open为例（在sysfile.c文件中）每个文件系统操作，都有begin_op和end_op分别表示事物的开始和结束。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 uint64 sys_open(void) { char path[MAXPATH]; int fd, omode; struct file *f; struct inode *ip; int n; if((n = argstr(0, path, MAXPATH)) \u0026lt; 0 || argint(1, \u0026amp;omode) \u0026lt; 0) return -1; begin_op();//THIS if(omode \u0026amp; O_CREATE){ ip = create(path, T_FILE, 0, 0); if(ip == 0){ end_op(); return -1; } } else { if((ip = namei(path)) == 0){ end_op(); return -1; } ilock(ip); if(ip-\u0026gt;type == T_DIR \u0026amp;\u0026amp; omode != O_RDONLY){ iunlockput(ip); end_op(); return -1; } } if(ip-\u0026gt;type == T_DEVICE \u0026amp;\u0026amp; (ip-\u0026gt;major \u0026lt; 0 || ip-\u0026gt;major \u0026gt;= NDEV)){ iunlockput(ip); end_op(); return -1; } if((f = filealloc()) == 0 || (fd = fdalloc(f)) \u0026lt; 0){ if(f) fileclose(f); iunlockput(ip); end_op(); return -1; } if(ip-\u0026gt;type == T_DEVICE){ f-\u0026gt;type = FD_DEVICE; f-\u0026gt;major = ip-\u0026gt;major; } else { f-\u0026gt;type = FD_INODE; f-\u0026gt;off = 0; } f-\u0026gt;ip = ip; f-\u0026gt;readable = !(omode \u0026amp; O_WRONLY); f-\u0026gt;writable = (omode \u0026amp; O_WRONLY) || (omode \u0026amp; O_RDWR); if((omode \u0026amp; O_TRUNC) \u0026amp;\u0026amp; ip-\u0026gt;type == T_FILE){ itrunc(ip); } iunlock(ip); end_op();//THIS return fd; } begin_op表明想要开始一个事务，在最后有end_op表示事务的结束。\n并且事务中的所有写block操作具备原子性，这意味着这些写block操作要么全写入，要么全不写入。\nXV6中的文件系统调用都有这样的结构，最开始是begin_op，之后是实现系统调用的代码，最后是end_op。\n在end_op中会实现commit操作。\n在begin_op和end_op之间，磁盘上或者内存中的数据结构会更新。\n但是在end_op之前，并不会有实际的改变（注，也就是不会写入到实际的block中）。\n在end_op时，我们会将数据写入到log中，之后再写入commit record或者log header。\n这里有趣的是，当文件系统调用执行写磁盘时会发生什么？\n让我们看一下fs.c中的ialloc，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Allocate an inode on device dev. // Mark it as allocated by giving it type type. // Returns an unlocked but allocated and referenced inode. struct inode* ialloc(uint dev, short type) { int inum; struct buf *bp; struct dinode *dip; for(inum = 1; inum \u0026lt; sb.ninodes; inum++){ bp = bread(dev, IBLOCK(inum, sb)); dip = (struct dinode*)bp-\u0026gt;data + inum%IPB; if(dip-\u0026gt;type == 0){ // a free inode memset(dip, 0, sizeof(*dip)); dip-\u0026gt;type = type; log_write(bp); // mark it allocated on the disk brelse(bp); return iget(dev, inum); } brelse(bp); } panic(\u0026#34;ialloc: no inodes\u0026#34;); } 在这个函数中，并没有直接调用bwrite，这里实际调用的是log_write函数。\nlog_write是由文件系统的logging实现的方法。\n任何一个文件系统调用的begin_op和end_op之间的写操作总是会走到log_write。\nlog_write函数位于log.c文件，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Caller has modified b-\u0026gt;data and is done with the buffer. // Record the block number and pin in the cache by increasing refcnt. // commit()/write_log() will do the disk write. // // log_write() replaces bwrite(); a typical use is: // bp = bread(...) // modify bp-\u0026gt;data[] // log_write(bp) // brelse(bp) void log_write(struct buf *b) { int i; if (log.lh.n \u0026gt;= LOGSIZE || log.lh.n \u0026gt;= log.size - 1) panic(\u0026#34;too big a transaction\u0026#34;); if (log.outstanding \u0026lt; 1) panic(\u0026#34;log_write outside of trans\u0026#34;); acquire(\u0026amp;log.lock); for (i = 0; i \u0026lt; log.lh.n; i++) { if (log.lh.block[i] == b-\u0026gt;blockno) // log absorbtion break; } log.lh.block[i] = b-\u0026gt;blockno; if (i == log.lh.n) { // Add new block to log? bpin(b); log.lh.n++; } release(\u0026amp;log.lock); } log_write还是很简单直观的，我们已经向block cache中的某个block写入了数据。\n比如写block 45，我们已经更新了block cache中的block 45。\n接下来我们需要在内存中记录，在稍后的commit中，要将block 45写入到磁盘的log中。\n这里的代码先获取log header的锁，之后再更新log header。\n首先代码会查看block 45是否已经被log记录了。\n如果是的话，其实不用做任何事情，因为block 45已经会被写入了。\n这种忽略的行为称为log absorbtion。\n如果block 45不在需要写入到磁盘中的block列表中，接下来会对n加1，并将block 45记录在列表的最后。\n之后，这里会通过调用bpin函数将block 45固定在block cache中，我们稍后会介绍为什么要这么做（注，详见15.8）。\n以上就是log_write的全部工作了。\n任何文件系统调用，如果需要更新block或者说更新block cache中的block，都会将block编号加在这个内存数据中（注，也就是log header在内存中的cache），除非编号已经存在。\n学生提问：这是不是意味着，bwrite不能直接使用？\nFrans教授：是的，可以这么认为，文件系统中的所有bwrite都需要被log_write替换。\nend_op函数 位于log.c中的end_op函数中会发生什么？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // called at the end of each FS system call. // commits if this was the last outstanding operation. void end_op(void) { int do_commit = 0; acquire(\u0026amp;log.lock); log.outstanding -= 1; if(log.committing) panic(\u0026#34;log.committing\u0026#34;); if(log.outstanding == 0){ do_commit = 1; log.committing = 1; } else { // begin_op() may be waiting for log space, // and decrementing log.outstanding has decreased // the amount of reserved space. wakeup(\u0026amp;log); } release(\u0026amp;log.lock); if(do_commit){ // call commit w/o holding locks, since not allowed // to sleep with locks. commit(); acquire(\u0026amp;log.lock); log.committing = 0; wakeup(\u0026amp;log); release(\u0026amp;log.lock); } } 可以看到，即使是这么简单的一个文件系统也有一些微秒的复杂之处，代码的最开始就是一些复杂情况的处理（注，15.8有这部分的解释）。\n我直接跳到正常且简单情况的代码。在简单情况下，没有其他的文件系统操作正在处理中。\n这部分代码非常简单直观，首先调用了commit函数。让我们看一下commit函数的实现，\n1 2 3 4 5 6 7 8 9 10 11 static void commit() { if (log.lh.n \u0026gt; 0) { write_log(); // Write modified blocks from cache to log write_head(); // Write header to disk -- the real commit install_trans(0); // Now install writes to home locations log.lh.n = 0; write_head(); // Erase the transaction from the log } } commit中有两个操作：\n首先是write_log。\n这基本上就是将所有存在于内存中的log header中的block编号对应的block，从block cache写入到磁盘上的log区域中（注，也就是将变化先从内存拷贝到log中）。\nwrite_head会将内存中的log header写入到磁盘中。\n看一下write_log的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Copy modified blocks from cache to log. static void write_log(void) { int tail; for (tail = 0; tail \u0026lt; log.lh.n; tail++) { struct buf *to = bread(log.dev, log.start+tail+1); // log block struct buf *from = bread(log.dev, log.lh.block[tail]); // cache block memmove(to-\u0026gt;data, from-\u0026gt;data, BSIZE); bwrite(to); // write the log brelse(from); brelse(to); } } 函数中依次遍历log中记录的block，并写入到log中。\n它首先读出log block，将cache中的block拷贝到log block，最后再将log block写回到磁盘中。\n这样可以确保需要写入的block都记录在log中。但是在这个位置，我们还没有commit，现在我们只是将block存放在了log中。\n如果我们在这个位置也就是在write_head之前crash了，那么最终的表现就像是transaction从来没有发生过。\n接下来看一下write_head函数，我之前将write_head称为commit point。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Write in-memory log header to disk. // This is the true point at which the // current transaction commits. static void write_head(void) { struct buf *buf = bread(log.dev, log.start); struct logheader *hb = (struct logheader *) (buf-\u0026gt;data); int i; hb-\u0026gt;n = log.lh.n; for (i = 0; i \u0026lt; log.lh.n; i++) { hb-\u0026gt;block[i] = log.lh.block[i]; } bwrite(buf); brelse(buf); } 函数也比较直观，首先读取log的header block。\n将n拷贝到block中，将所有的block编号拷贝到header的列表中。最后再将header block写回到磁盘。\n函数中的倒数第2行，bwrite是实际的commit point吗？如果crash发生在这个bwrite之前，会发生什么？\n这时虽然我们写了log的header block，但是数据并没有落盘。\n所以crash并重启恢复时，并不会发生任何事情。那crash发生在bwrite之后会发生什么呢？\n这时header会写入到磁盘中，当重启恢复相应的文件系统操作会被恢复。在恢复过程的某个时间点，恢复程序可以读到log header并发现比如说有5个log还没有install，恢复程序可以将这5个log拷贝到实际的位置。\n所以这里的bwrite就是实际的commit point。\n在commit point之前，transaction并没有发生，在commit point之后，只要恢复程序正确运行，transaction必然可以完成。\n回到commit函数，在commit point之后，就会实际应用transaction。\n这里很直观，就是读取log block再查看header这个block属于文件系统中的哪个block，最后再将log block写入到文件系统相应的位置。\n让我们看一下install_trans函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Copy committed blocks from log to their home location static void install_trans(int recovering) { int tail; for (tail = 0; tail \u0026lt; log.lh.n; tail++) { struct buf *lbuf = bread(log.dev, log.start+tail+1); // read log block struct buf *dbuf = bread(log.dev, log.lh.block[tail]); // read dst memmove(dbuf-\u0026gt;data, lbuf-\u0026gt;data, BSIZE); // copy block to dst bwrite(dbuf); // write dst to disk if(recovering == 0) bunpin(dbuf); brelse(lbuf); brelse(dbuf); } } 这里先读取log block，再读取文件系统对应的block。\n将数据从log拷贝到文件系统，最后将文件系统block缓存落盘。\n这里实际上就是将block数据从log中拷贝到了实际的文件系统block中。\n当然，可能在这里代码的某个位置会出现问题，但是这应该也没问题，因为在恢复的时候，我们会从最开始重新执行过。\n在commit函数中，install结束之后，会将log header中的n设置为0，再将log header写回到磁盘中。将n设置为0的效果就是清除log。\n学生提问：install_trans函数在写block的时候，先写的缓存。可不可以优化一下直接写磁盘而不写缓存让代码运行的更快一些？\nFrans教授：这里的接口是不太好。你可能会想问反正都要写入新数据，为什么要先读出目标block来。这里的代码肯定还有很多优化空间，但是为了看起来简单我们并没有这么做。\n以上就是commit内容。\nFile system recovering 接下来我们看一下发生在XV6的启动过程中的文件系统的恢复流程。\n当系统crash并重启了，在XV6启动过程中做的一件事情就是调用initlog函数。\n1 2 3 4 5 6 7 8 9 10 11 12 void initlog(int dev, struct superblock *sb) { if (sizeof(struct logheader) \u0026gt;= BSIZE) panic(\u0026#34;initlog: too big logheader\u0026#34;); initlock(\u0026amp;log.lock, \u0026#34;log\u0026#34;); log.start = sb-\u0026gt;logstart; log.size = sb-\u0026gt;nlog; log.dev = dev; recover_from_log(); } initlog基本上就是调用recover_from_log函数。\n1 2 3 4 5 6 7 8 static void recover_from_log(void) { read_head(); install_trans(1); // if committed, copy from log to disk log.lh.n = 0; write_head(); // clear the log } recover_from_log先调用read_head函数从磁盘中读取header，之后调用install_trans函数。\n这个函数之前在commit函数中也调用过，它就是读取log header中的n，然后根据n将所有的log block拷贝到文件系统的block中。recover_from_log在最后也会跟之前一样清除log。\n这就是恢复的全部流程。\n如果我们在install_trans函数中又crash了，也不会有问题，因为之后再重启时，XV6会再次调用initlog函数，再调用recover_from_log来重新install log。\n如果我们在commit之前crash了多次，在最终成功commit时，log可能会install多次。\n学生提问：如果一个进程向磁盘写了一些数据，但是在commit之前进程出现了故障，假设故障之后进程退出了，这样会有问题吗？\nFrans教授：简单回答是没问题，因此磁盘不会被更新，所以效果就像文件系统操作没有发生过一样。并且进程并不能在故障后恢复，唯一能在故障之后还能保持的是保存在磁盘中的状态。（注，应该是没有理解问题。进程通过write系统调用成功写入的数据，就算在成功落盘之前进程异常退出了，内核还是会写入到磁盘中，前提是内核还在运行。）\nLog写磁盘流程 我已经在bwrite函数中加了一个print语句。\nbwrite函数是block cache中实际写磁盘的函数，所以我们将会看到实际写磁盘的记录。\n在上节课（Lec 14）我将print语句放在了log_write中，log_write只能代表文件系统操作的记录，并不能代表实际写磁盘的记录。\n我们这里会像上节课一样执行echo \u0026ldquo;hi\u0026rdquo; \u0026gt; x，并看一下实际的写磁盘过程。\n很明显这里的记录要比只在log_write中记录要长的多。\n之前的log_write只有11条记录（注，详见14.5）但是可以看到实际上背后有很多个磁盘写操作，让我们来分别看一下这里的写磁盘操作：\n首先是前3行的bwrite 3，4，5。\n因为block 3是第一个log data block，所以前3行是在log中记录了3个写操作。这3个写操作都保存在log中，并且会写入到磁盘中的log部分。\n第4行的bwrite 2。\n因为block 2是log的起始位置，也就是log header，所以这条是commit记录。\n第5，6，7行的bwrite 33，46，32。\n这里实际就是将前3行的log data写入到实际的文件系统的block位置，这里实际是install log。\n第8行的bwrite 2，是清除log（注，也就是将log header中的n设置为0）。\n到此为止，完成了实际上的写block 33，46，32这一系列的操作。第一部分是log write，第二部分是install log，每一部分后面还跟着一个更新commit记录（注，也就是commit log和clean log）。\n学生提问：可以从这里的记录找到一次文件操作的begin_op和end_op位置吗？\nFrans教授：大概可以知道。我们实际上不知道begin_op的位置，但是所有的文件系统操作都从begin_op开始。更新commit记录必然在end_op中，所以我们可以找到文件系统操作的end_op位置，之后就是begin_op（注，其实这里所有的操作都在end_op中，只需要区分每一次end_op的调用就可以找到begin_op）。\n所以以上就是XV6中文件系统的logging介绍，即使是这么一个简单的logging系统也有一定的复杂度。\n这里立刻可以想到的一个问题是，通过观察这些记录，这是一个很有效的实现吗？很明显不是的，因为数据被写了两次。\n如果我写一个大文件，我需要在磁盘中将这个大文件写两次。所以这必然不是一个高性能的实现，为了实现Crash safety我们将原本的性能降低了一倍。\n当你们去读ext3论文时，你们应该时刻思考如何避免这里的性能降低一倍的问题。\nFile system challenges 前面说到XV6的文件系统有一定的复杂性，接下来我将介绍一下三个复杂的地方或者也可以认为是三个挑战。\n第一个是cache eviction。 假设transaction还在进行中，我们刚刚更新了block 45，正要更新下一个block，而整个buffer cache都满了并且决定撤回block 45。\n在buffer cache中撤回block 45意味着我们需要将其写入到磁盘的block 45位置，这里会不会有问题？如果我们这么做了的话，会破坏什么规则吗？\n是的，如果将block 45写入到磁盘之后发生了crash，就会破坏transaction的原子性。\n这里也破坏了前面说过的write ahead rule，write ahead rule的含义是，你需要先将所有的block写入到log中，之后才能实际的更新文件系统block。\n所以buffer cache不能撤回任何还位于log的block。\n前面在介绍log_write函数时，其中调用了一个叫做bpin的函数，这个函数的作用就如它的名字一样，将block固定在buffer cache中。\n它是通过给block cache增加引用计数来避免cache撤回对应的block。\n在之前（注，详见14.6）我们看过，如果引用计数不为0，那么buffer cache是不会撤回block cache的。\n相应的在将来的某个时间，所有的数据都写入到了log中，我们可以在cache中unpin block（注，在15.5中的install_trans函数中会有unpin，因为这时block已经写入到了log中）。\n所以这是第一个复杂的地方，我们需要pin/unpin buffer cache中的block。\n第二个挑战是，文件系统操作必须适配log的大小。 在XV6中，总共有30个log block（注，详见14.3）。\n当然我们可以提升log的尺寸，在真实的文件系统中会有大得多的log空间。\n但是无所谓啦，不管log多大，文件系统操作必须能放在log空间中。\n如果一个文件系统操作尝试写入超过30个block，那么意味着部分内容需要直接写到文件系统区域，而这是不被允许的，因为这违背了write ahead rule。\n所以所有的文件系统操作都必须适配log的大小。\n为什么XV6的log大小是30？因为30比任何一个文件系统操作涉及的写操作数都大，Robert和我看了一下所有的文件系统操作，发现都远小于30，所以就将XV6的log大小设为30。\n我们目前看过的一些文件系统操作，例如创建一个文件只包含了写5个block。\n实际上大部分文件系统操作只会写几个block。\n你们可以想到什么样的文件系统操作会写很多很多个block吗？是的，写一个大文件。如果我们调用write系统调用并传入1M字节的数据，这对应了写1000个block，这看起来会有很严重的问题，因为这破坏了我们刚刚说的“文件系统操作必须适配log的大小”这条规则。\n让我们看一下file.c文件中的file_write函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Write to file f. // addr is a user virtual address. int filewrite(struct file *f, uint64 addr, int n) { int r, ret = 0; if(f-\u0026gt;writable == 0) return -1; if(f-\u0026gt;type == FD_PIPE){ ret = pipewrite(f-\u0026gt;pipe, addr, n); } else if(f-\u0026gt;type == FD_DEVICE){ if(f-\u0026gt;major \u0026lt; 0 || f-\u0026gt;major \u0026gt;= NDEV || !devsw[f-\u0026gt;major].write) return -1; ret = devsw[f-\u0026gt;major].write(1, addr, n); } else if(f-\u0026gt;type == FD_INODE){ // write a few blocks at a time to avoid exceeding // the maximum log transaction size, including // i-node, indirect block, allocation blocks, // and 2 blocks of slop for non-aligned writes. // this really belongs lower down, since writei() // might be writing a device like the console. int max = ((MAXOPBLOCKS-1-1-2) / 2) * BSIZE; int i = 0; while(i \u0026lt; n){ int n1 = n - i; if(n1 \u0026gt; max) n1 = max; begin_op(); ilock(f-\u0026gt;ip); if ((r = writei(f-\u0026gt;ip, 1, addr + i, f-\u0026gt;off, n1)) \u0026gt; 0) f-\u0026gt;off += r; iunlock(f-\u0026gt;ip); end_op(); if(r != n1){ // error from writei break; } i += r; } ret = (i == n ? n : -1); } else { panic(\u0026#34;filewrite\u0026#34;); } return ret; } 从这段代码可以看出，如果写入的block数超过了30，那么一个写操作会被分割成多个小一些的写操作。\n这里整个写操作不是原子的，但是这还好，因为write系统调用的语义并不要求所有1000个block都是原子的写入，它只要求我们不要损坏文件系统。\n所以XV6会将一个大的写操作分割成多个小的写操作，每一个小的写操作通过独立的transaction写入。这样文件系统本身不会陷入不正确的状态中。\n这里还需要注意，因为block在落盘之前需要在cache中pin住，所以buffer cache的尺寸也要大于log的尺寸。\n最后一个挑战是并发文件系统调用。 让我先来解释一下这里会有什么问题，再看对应的解决方案。\n假设我们有一段log，和两个并发的执行的transaction，其中transaction t0在log的前半段记录，transaction t1在log的后半段记录。可能我们用完了log空间，但是任何一个transaction都还没完成。\n现在我们能提交任何一个transaction吗？我们不能，因为这样的话我们就提交了一个部分完成的transaction，这违背了write ahead rule，log本身也没有起到应该的作用。\n所以必须要保证多个并发transaction加在一起也适配log的大小。\n所以当我们还没有完成一个文件系统操作时，我们必须在确保可能写入的总的log数小于log区域的大小的前提下，才允许另一个文件系统操作开始。\nXV6通过限制并发文件系统操作的个数来实现这一点。\n在begin_op中，我们会检查当前有多少个文件系统操作正在进行。\n如果有太多正在进行的文件系统操作，我们会通过sleep停止当前文件系统操作的运行，并等待所有其他所有的文件系统操作都执行完并commit之后再唤醒。\n这里的其他所有文件系统操作都会一起commit。\n有的时候这被称为group commit，因为这里将多个操作像一个大的transaction一样提交了，这里的多个操作要么全部发生了，要么全部没有发生。\n学生提问：group commit有必要吗？不能当一个文件系统操作结束的时候就commit掉，然后再commit其他的操作吗？\nFrans教授：如果这样的话你需要非常非常小心。因为有一点我没有说得很清楚，我们需要保证write系统调用的顺序。如果一个read看到了一个write，再执行了一次write，那么第二个write必须要发生在第一个write之后。在log中的顺序，本身就反应了write系统调用的顺序，你不能改变log中write系统调用的执行顺序，因为这可能会导致对用户程序可见的奇怪的行为。所以必须以transaction发生的顺序commit它们，而一次性提交所有的操作总是比较安全的，这可以保证文件系统处于一个好的状态。\n最后我们再回到最开始，看一下begin_op，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // called at the start of each FS system call. void begin_op(void) { acquire(\u0026amp;log.lock); while(1){ if(log.committing){ sleep(\u0026amp;log, \u0026amp;log.lock); } else if(log.lh.n + (log.outstanding+1)*MAXOPBLOCKS \u0026gt; LOGSIZE){ // this op might exhaust log space; wait for commit. sleep(\u0026amp;log, \u0026amp;log.lock); } else { log.outstanding += 1; release(\u0026amp;log.lock); break; } } } 首先，如果log正在commit过程中，那么就等到log提交完成，因为我们不能在install log的过程中写log；\n其次，如果当前操作是允许并发的操作个数的后一个，那么当前操作可能会超过log区域的大小，我们也需要sleep并等待所有之前的操作结束；\n最后，如果当前操作可以继续执行，需要将log的outstanding字段加1，最后再退出函数并执行文件系统操作。\n再次看一下end_op函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // called at the end of each FS system call. // commits if this was the last outstanding operation. void end_op(void) { int do_commit = 0; acquire(\u0026amp;log.lock); log.outstanding -= 1; if(log.committing) panic(\u0026#34;log.committing\u0026#34;); if(log.outstanding == 0){ do_commit = 1; log.committing = 1; } else { // begin_op() may be waiting for log space, // and decrementing log.outstanding has decreased // the amount of reserved space. wakeup(\u0026amp;log); } release(\u0026amp;log.lock); if(do_commit){ // call commit w/o holding locks, since not allowed // to sleep with locks. commit(); acquire(\u0026amp;log.lock); log.committing = 0; wakeup(\u0026amp;log); release(\u0026amp;log.lock); } } 在最开始首先会对log的outstanding字段减1，因为一个transaction正在结束；\n其次检查committing状态，当前不可能在committing状态，所以如果是的话会触发panic；\n如果当前操作是整个并发操作的最后一个的话（log.outstanding == 0），接下来立刻就会执行commit；\n如果当前操作不是整个并发操作的最后一个的话，我们需要唤醒在begin_op中sleep的操作，让它们检查是不是能运行。\n（注，这里的outstanding有点迷，它表示的是当前正在并发执行的文件系统操作的个数，MAXOPBLOCKS定义了一个操作最大可能涉及的block数量。在begin_op中，只要log空间还足够，就可以一直增加并发执行的文件系统操作。所以XV6是通过设定了MAXOPBLOCKS，再间接的限定支持的并发文件系统操作的个数）\n所以，即使是XV6中这样一个简单的文件系统，也有一些复杂性和挑战。\n最后让我总结一下：\n这节课讨论的是使用logging来解决crash safety或者说多个步骤的文件系统操作的安全性。这种方式对于安全性来说没有问题，但是性能不咋地。\n学生提问：前面说到cache size至少要跟log size一样大，如果它们一样大的话，并且log pin了30个block，其他操作就不能再进行了，因为buffer中没有额外的空间了。\nFrans教授：如果buffer cache中没有空间了，XV6会直接panic。这并不理想，实际上有点恐怖。所以我们在挑选buffer cache size的时候希望用一个不太可能导致这里问题的数字。这里为什么不能直接返回错误，而是要panic？因为很多文件系统操作都是多个步骤的操作，假设我们执行了两个write操作，但是第三个write操作找不到可用的cache空间，那么第三个操作无法完成，我们不能就直接返回错误，因为我们可能已经更新了一个目录的某个部分，为了保证文件系统的正确性，我们需要撤回之前的更新。所以如果log pin了30个block，并且buffer cache没有额外的空间了，会直接panic。当然这种情况不太会发生，只有一些极端情况才会发生。\n","date":"2024-09-29T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08115crash-recovery/bg_hu9213768696141182459.jpg","permalink":"https://echudet.github.io/p/mit6s08115crash-recovery/","title":"MIT6S081：15Crash recovery"},{"content":"Lecture 14 File Systems 预习 概述 xv6文件系统实现分为七层，如图8.1所示。\n文件描述符（File descriptor） 路径名（Pathname） 目录（Directory） 索引结点（Inode） 日志（Logging） 缓冲区高速缓存（Buffer cache） 磁盘（Disk） 图8.1 XV6文件系统的层级\n磁盘层读取和写入virtio硬盘上的块。\n缓冲区高速缓存层缓存磁盘块并同步对它们的访问，确保每次只有一个内核进程可以修改存储在任何特定块中的数据。\n日志记录层允许更高层在一次事务（transaction）中将更新包装到多个块，并确保在遇到崩溃时自动更新这些块（即，所有块都已更新或无更新）。\n索引结点层提供单独的文件，每个文件表示为一个索引结点，其中包含唯一的索引号（i-number）和一些保存文件数据的块。\n目录层将每个目录实现为一种特殊的索引结点，其内容是一系列目录项，每个目录项包含一个文件名和索引号。\n路径名层提供了分层路径名，如***/usr/rtm/xv6/fs.c***，并通过递归查找来解析它们。\n文件描述符层使用文件系统接口抽象了许多Unix资源（例如，管道、设备、文件等），简化了应用程序员的工作。\n文件系统必须有将索引节点和内容块存储在磁盘上哪些位置的方案。\n为此，xv6将磁盘划分为几个部分，如图8.2所示。\n文件系统不使用块0（它保存引导扇区）。\n块1称为超级块：它包含有关文件系统的元数据（文件系统大小（以块为单位）、数据块数、索引节点数和日志中的块数）。\n从2开始的块保存日志。\n日志之后是索引节点，每个块有多个索引节点。\n然后是位图块，跟踪正在使用的数据块。\n其余的块是数据块：每个都要么在位图块中标记为空闲，要么保存文件或目录的内容。\n超级块由一个名为mkfs的单独的程序填充，该程序构建初始文件系统。\n本章的其余部分将从缓冲区高速缓存层开始讨论每一层。\n注意那些在较低层次上精心选择的抽象可以简化较高层次的设计的情况。\nBuffer cache层 Buffer cache有两个任务：\n同步对磁盘块的访问，以确保磁盘块在内存中只有一个副本，并且一次只有一个内核线程使用该副本 缓存常用块，以便不需要从慢速磁盘重新读取它们。代码在bio.c中。 Buffer cache层导出的主接口主要是bread和bwrite；\n前者获取一个buf，其中包含一个可以在内存中读取或修改的块的副本，后者将修改后的缓冲区写入磁盘上的相应块。\n内核线程必须通过调用brelse释放缓冲区。\nBuffer cache每个缓冲区使用一个睡眠锁，以确保每个缓冲区（因此也是每个磁盘块）每次只被一个线程使用；bread返回一个上锁的缓冲区，brelse释放该锁。\n让我们回到Buffer cache。\nBuffer cache中保存磁盘块的缓冲区数量固定，这意味着如果文件系统请求还未存放在缓存中的块，Buffer cache必须回收当前保存其他块内容的缓冲区。\nBuffer cache为新块回收最近使用最少的缓冲区。这样做的原因是认为最近使用最少的缓冲区是最不可能近期再次使用的缓冲区。\n代码：Buffer cache Buffer cache是以双链表表示的缓冲区。\nmain（kernel/main.c:27）调用的函数binit使用静态数组buf（kernel/bio.c:43-52）中的NBUF个缓冲区初始化列表。\n对Buffer cache的所有其他访问都通过bcache.head引用链表，而不是buf数组。\n缓冲区有两个与之关联的状态字段。\n字段valid表示缓冲区是否包含块的副本。\n字段disk表示缓冲区内容是否已交给磁盘，这可能会更改缓冲区（例如，将数据从磁盘写入data）。\nBread（kernel/bio.c:93）调用bget为给定扇区（kernel/bio.c:97）获取缓冲区。\n如果缓冲区需要从磁盘进行读取，bread会在返回缓冲区之前调用virtio_disk_rw来执行此操作。\nBget（kernel/bio.c:59）扫描缓冲区列表，查找具有给定设备和扇区号（kernel/bio.c:65-73）的缓冲区。\n如果存在这样的缓冲区，bget将获取缓冲区的睡眠锁。然后Bget返回锁定的缓冲区。\n如果对于给定的扇区没有缓冲区，bget必须创建一个，这可能会重用包含其他扇区的缓冲区。\n它再次扫描缓冲区列表，查找未在使用中的缓冲区（b-\u0026gt;refcnt = 0）：任何这样的缓冲区都可以使用。\nBget编辑缓冲区元数据以记录新设备和扇区号，并获取其睡眠锁。\n注意，b-\u0026gt;valid = 0的布置确保了bread将从磁盘读取块数据，而不是错误地使用缓冲区以前的内容。\n每个磁盘扇区最多有一个缓存缓冲区是非常重要的，并且因为文件系统使用缓冲区上的锁进行同步，可以确保读者看到写操作。\nBget的从第一个检查块是否缓存的循环到第二个声明块现在已缓存（通过设置dev、blockno和refcnt）的循环，一直持有bcache.lock来确保此不变量。\n这会导致检查块是否存在以及（如果不存在）指定一个缓冲区来存储块具有原子性。\nbget在bcache.lock临界区域之外获取缓冲区的睡眠锁是安全的，因为非零b-\u0026gt;refcnt防止缓冲区被重新用于不同的磁盘块。\n睡眠锁保护块缓冲内容的读写，而bcache.lock保护有关缓存哪些块的信息。\n如果所有缓冲区都处于忙碌，那么太多进程同时执行文件系统调用；bget将会panic。\n一个更优雅的响应可能是在缓冲区空闲之前休眠，尽管这样可能会出现死锁。\n一旦bread读取了磁盘（如果需要）并将缓冲区返回给其调用者，调用者就可以独占使用缓冲区，并可以读取或写入数据字节。\n如果调用者确实修改了缓冲区，则必须在释放缓冲区之前调用bwrite将更改的数据写入磁盘。\nBwrite（kernel/bio.c:107）调用virtio_disk_rw与磁盘硬件对话。\n当调用方使用完缓冲区后，它必须调用brelse来释放缓冲区(brelse是b-release的缩写，这个名字很隐晦，但值得学习：它起源于Unix，也用于BSD、Linux和Solaris）。\nbrelse（kernel/bio.c:117）释放睡眠锁并将缓冲区移动到链表的前面（kernel/bio.c:128-133）。\n移动缓冲区会使列表按缓冲区的使用频率排序（意思是释放）：列表中的第一个缓冲区是最近使用的，最后一个是最近使用最少的。\nbget中的两个循环利用了这一点：在最坏的情况下，对现有缓冲区的扫描必须处理整个列表，但首先检查最新使用的缓冲区（从bcache.head开始，然后是下一个指针），在引用局部性良好的情况下将减少扫描时间。\n选择要重用的缓冲区时，通过自后向前扫描（跟随prev指针）选择最近使用最少的缓冲区。\n这个流程描述了 Buffer cache 在 XV6 操作系统中的管理方式。\nBuffer cache 是通过双向链表来表示缓冲区。使用 binit 函数初始化缓存，创建了一个包含 NBUF 个缓冲区的静态数组 buf，并通过 bcache.head 引用缓冲区链表进行所有访问。每个缓冲区有两个状态字段：\nvalid：表示缓冲区是否包含有效的磁盘块副本。 disk：表示缓冲区内容是否已提交到磁盘。 bread 函数调用 bget 来获取给定扇区的缓冲区。它根据设备号和扇区号查找缓冲区。如果找到匹配的缓冲区，锁定它并返回。如果没有找到，则会尝试重用未使用的缓冲区。\nbget 进行两轮扫描：\n第一轮：扫描查找是否已经有匹配的缓冲区缓存。如果找到，锁定并返回。 第二轮：如果没有找到匹配的缓存，则扫描未使用的缓冲区，修改元数据记录新设备和扇区号，并返回。 确保缓存的一致性和唯一性：\n为确保同一个磁盘扇区在任何时刻只有一个缓存缓冲区，bget 在两个扫描中都持有 bcache.lock，使得查找和缓存块的过程是原子操作。缓冲区的引用计数 refcnt 确保缓冲区不会在锁定后被其他进程重用。\n读取与写入：当调用者获取到缓冲区后，可以读取或者写入数据。如果缓冲区需要从磁盘读取数据，bread 会通过 virtio_disk_rw 执行读取操作。如果调用者修改了缓冲区内容，必须通过调用 bwrite 函数将更改写入磁盘。\n释放缓冲区：调用者使用完缓冲区后，必须通过 brelse 来释放缓冲区。brelse 释放睡眠锁，并将缓冲区移动到链表前面，这样链表会按最近使用顺序排序（LRU策略）。\n缓存管理与优化\n通过链表的双向扫描，bget减少了缓冲区查找和重用的开销： 向前扫描是从最近使用的缓冲区开始查找，这种局部性提高了效率。 向后扫描时选择最近最少使用的缓冲区来重用，保证了缓存的合理利用。 如果所有缓冲区都处于忙碌状态，bget 会触发 panic，虽然更优雅的处理方式是等待空闲缓冲区，但可能导致死锁。 Buffer cache 管理流程的核心在于利用双向链表来组织缓冲区，并通过局部性优化缓冲区的查找和重用。同时，通过锁机制确保多个进程之间对缓冲区的访问和操作是安全的。\n索引结点层 术语inode（即索引结点）可以具有两种相关含义之一。\n它可能是指包含文件大小和数据块编号列表的磁盘上的数据结构。\n或者“inode”可能指内存中的inode，它包含磁盘上inode的副本以及内核中所需的额外信息。\n磁盘上的inode都被打包到一个称为inode块的连续磁盘区域中。\n每个inode的大小都相同，因此在给定数字n的情况下，很容易在磁盘上找到第n个inode。\n事实上，这个编号n，称为inode number或i-number，是在具体实现中标识inode的方式。\n磁盘上的inode由struct dinode（kernel/fs.h:32）定义。\n1 2 3 4 5 6 7 8 9 // On-disk inode structure struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+1]; // Data block addresses }; 字段type区分文件、目录和特殊文件（设备）。type为零表示磁盘inode是空闲的。\n字段nlink统计引用此inode的目录条目数，以便识别何时应释放磁盘上的inode及其数据块。\n字段size记录文件中内容的字节数。\n字段addrs数组记录保存文件内容的磁盘块的块号。\n内核将活动的inode集合保存在内存中；struct inode（kernel/file.h:17）是磁盘上struct dinode的内存副本。\n只有当有C指针引用某个inode时，内核才会在内存中存储该inode。\nref字段统计引用内存中inode的C指针的数量，如果引用计数降至零，内核将从内存中丢弃该inode。\niget和iput函数分别获取和释放指向inode的指针，修改引用计数。\n指向inode的指针可以来自文件描述符、当前工作目录和如exec的瞬态内核代码。\nxv6的inode代码中有四种锁或类似锁的机制。\nicache.lock保护以下两个不变量：inode最多在缓存中出现一次；缓存inode的ref字段记录指向缓存inode的内存指针数量。\n每个内存中的inode都有一个包含睡眠锁的lock字段，它确保以独占方式访问inode的字段（如文件长度）以及inode的文件或目录内容块。\n如果inode的ref大于零，则会导致系统在cache中维护inode，而不会对其他inode重用此缓存项。\n最后，每个inode都包含一个nlink字段（在磁盘上，如果已缓存则复制到内存中），该字段统计引用文件的目录项的数量；如果inode的链接计数大于零，xv6将不会释放inode。\niget()返回的struct inode指针在相应的iput()调用之前保证有效：inode不会被删除，指针引用的内存也不会被其他inode重用。\niget()提供对inode的非独占访问，因此可以有许多指向同一inode的指针。\n文件系统代码的许多部分都依赖于iget()的这种行为，既可以保存对inode的长期引用（如打开的文件和当前目录），也可以防止争用，同时避免操纵多个inode（如路径名查找）的代码产生死锁。\niget返回的struct inode可能没有任何有用的内容。\n为了确保它保存磁盘inode的副本，代码必须调用ilock。\n这将锁定inode（以便没有其他进程可以对其进行ilock），并从磁盘读取尚未读取的inode。\niunlock释放inode上的锁。\n将inode指针的获取与锁定分离有助于在某些情况下避免死锁，例如在目录查找期间。\n多个进程可以持有指向iget返回的inode的C指针，但一次只能有一个进程锁定inode。\ninode缓存只缓存内核代码或数据结构持有C指针的inode。\n它的主要工作实际上是同步多个进程的访问；缓存是次要的。\n如果经常使用inode，在inode缓存不保留它的情况下buffer cache可能会将其保留在内存中。\ninode缓存是直写的，这意味着修改已缓存inode的代码必须立即使用iupdate将其写入磁盘。\ninode究竟是什么？\ninode是一种用于描述文件的结构，可以表示为磁盘上的inode或内存中的inode：\n磁盘上的inode：是存储在磁盘上的数据结构，用于保存文件的元数据（例如文件大小、文件类型、数据块编号等）。 内存中的inode：是磁盘inode的副本，保存在内存中，并且包含内核需要的额外信息。内存中的inode用于跟踪活动文件和目录。 inode和dinode有什么关系？\ndinode 是 磁盘上的inode 的结构定义，用于表示存储在磁盘上的文件的元数据。 inode 是 内存中的dinode 的副本，它通过引用 dinode 的数据来进行文件操作。此外，内存中的 inode 还包含额外的信息，如引用计数和锁等。 总的来说，dinode 是磁盘上的结构，而 inode 是其在内存中的表现形式，并增加了内核所需的额外字段。\n和inode/dinode相关的字段及其作用\ndinode（磁盘上的inode）字段（kernel/fs.h:32）：\ntype：表示文件的类型，如普通文件、目录、设备文件等。type = 0 表示该inode为空闲状态。\nnlink：引用该inode的目录项的数量。用于跟踪文件的链接数量，如果 nlink 降为零，意味着没有文件指向该inode，系统可以释放inode及其关联的磁盘块。\nsize：记录文件的字节数，即文件的大小。\naddrs：保存文件内容的磁盘块编号列表，指向文件内容在磁盘上的存储位置。\ninode（内存中的inode）字段（kernel/file.h:17）\nref：引用此inode的内存指针的数量。当 ref \u0026gt; 0 时，该inode保存在内存中。 lock：inode的睡眠锁，确保独占访问inode的字段及文件内容块，防止多个进程同时操作相同的inode，造成竞争。 dev：inode所属的设备号，指向文件所在的设备。 inum：inode number（i-number），用于标识inode，是磁盘上inode的编号。 valid：用于标记此内存inode是否包含有效的磁盘inode数据。 nlink：引用计数字段，与磁盘上的 nlink 相同，用于同步两个层次的inode数据。 和inode/dinode相关的函数及其作用\niget（kernel/fs.c）： 作用：从磁盘加载inode到内存，返回指向该inode的指针。该函数不提供inode的独占访问，但确保返回的inode指针在 iput 调用之前是有效的。 功能：它管理内存中的inode，增减引用计数，确保内存中的inode不会在多个进程间被错误地重用。 iput（kernel/fs.c）： 作用：减少inode的引用计数。当引用计数降为零时，释放inode，并可能从内存中丢弃该inode。 功能：负责清理不再使用的inode，避免内存浪费。 ilock（kernel/fs.c）： 作用：锁定inode，确保inode的独占访问，通常在修改inode或访问文件数据时使用。 功能：从磁盘中读取未加载的inode，并保护多个进程对inode的并发访问。 iunlock（kernel/fs.c）： 作用：解锁inode，允许其他进程继续对inode进行操作。 功能：确保操作完成后，释放锁，使其他进程可以安全访问inode。 iupdate（kernel/fs.c）： 作用：将内存中的inode更新到磁盘，确保内存inode的更改反映到磁盘上的inode。 功能：保持磁盘上inode和内存中inode的一致性，避免数据丢失。 brelse： 作用：释放缓冲区并将其返回到缓存中。 功能：当对inode或文件操作结束时，缓解资源争用，将缓冲区释放回来。 总结：\ninode 是文件的核心元数据结构，磁盘上通过 dinode 定义，内存中通过 inode 管理。 两者通过字段如 type、nlink 和 size 共同维护文件的状态和信息。 关键函数如 iget、iput、ilock、iunlock 负责对 inode 的引用、锁定和同步，确保文件系统在并发访问下的正确性。 在文件系统中，link 指的是一个文件或目录在不同的目录项（directory entries）中有多个指向同一个 inode 的引用。这些指向同一个 inode 的引用称为 硬链接（hard links）。\nnlink 的含义\nnlink 是 inode 结构中的一个字段，记录了 指向该 inode 的目录项数量。简单来说，nlink 统计了文件有多少个硬链接。 当 nlink 变为 0 时，意味着没有任何目录项再指向该 inode，此时文件的实际内容（包括占用的磁盘块）就可以被删除。 假设我们有一个文件系统，文件 file1 存在于目录 /home/user 中。\n创建文件 file1：\n当创建文件 file1 时，文件系统会为它分配一个新的 inode。\n该 inode 会包含 file1 的元数据，比如文件大小、存储位置等。\n同时，inode 的 nlink 字段被设置为 1，表示有一个目录项（/home/user/file1）指向这个 inode。\n1 2 /home/user/file1 -\u0026gt; inode 100 nlink = 1 创建硬链接 file2 到 file1：\n现在，我们执行命令 ln /home/user/file1 /home/user/file2，这是在同一个目录下创建一个新的文件名 file2，但 file2 和 file1 共享相同的 inode。\n也就是说，file1 和 file2 指向同一个 inode，它们共享文件的元数据和内容。\n此时，inode 的 nlink 字段增加到 2，因为有两个目录项（/home/user/file1 和 /home/user/file2）指向同一个 inode。\n1 2 3 /home/user/file1 -\u0026gt; inode 100 /home/user/file2 -\u0026gt; inode 100 nlink = 2 删除文件 file1：\n当我们删除 file1 时（通过 rm /home/user/file1），文件系统会移除目录项 /home/user/file1，但由于 nlink 仍为 1（file2 还存在），文件的 inode 和其数据依然保留。\n只有当 nlink 降为 0 时，文件的实际数据才会被删除。\n1 2 /home/user/file2 -\u0026gt; inode 100 nlink = 1 删除 file2： 如果之后也删除了 file2，nlink 变为 0，此时该 inode 不再有任何目录项指向它，文件系统会将它的元数据和占用的磁盘块释放。 总结：\nnlink 记录了指向同一个文件（inode）的硬链接数。 当创建多个硬链接时，所有的链接指向同一个 inode，因此它们共享文件的内容和元数据。 只有当 nlink 变为 0 时，文件的内容和 inode 才会被真正删除。 代码：Inodes 为了分配新的inode（例如，在创建文件时），xv6调用ialloc（kernel/fs.c:196）。\nialloc类似于balloc：它一次一个块地遍历磁盘上的索引节点结构体，查找标记为空闲的一个。\n当它找到一个时，它通过将新type写入磁盘来声明它，然后末尾通过调用iget（kernel/fs.c:210）从inode缓存返回一个条目。\nialloc的正确操作取决于这样一个事实：一次只有一个进程可以保存对bp的引用：ialloc可以确保其他进程不会同时看到inode可用并尝试声明它。\nIget（kernel/fs.c:243）在inode缓存中查找具有所需设备和inode编号的活动条目（ip-\u0026gt;ref \u0026gt; 0）。\n如果找到一个，它将返回对该inode的新引用（kernel/fs.c:252-256）。\n在iget扫描时，它会记录第一个空槽（kernel/fs.c:257-258）的位置，如果需要分配缓存项，它会使用这个槽。\n在读取或写入inode的元数据或内容之前，代码必须使用ilock锁定inode。\nIlock（kernel/fs.c:289）为此使用睡眠锁。一旦ilock以独占方式访问inode，它将根据需要从磁盘（更可能是buffer cache）读取inode。\n函数iunlock（kernel/fs.c:317）释放睡眠锁，这可能会导致任何睡眠进程被唤醒。\nIput（kernel/fs.c:333）通过减少引用计数（kernel/fs.c:356）释放指向inode的C指针。\n如果这是最后一次引用，inode缓存中该inode的槽现在将是空闲的，可以重用于其他inode。\n如果iput发现没有指向inode的C指针引用，并且inode没有指向它的链接（发生于无目录），则必须释放inode及其数据块。\nIput调用itrunc将文件截断为零字节，释放数据块；将索引节点类型设置为0（未分配）；并将inode写入磁盘（kernel/fs.c:338）。\niput中释放inode的锁定协议值得仔细研究。\n一个危险是并发线程可能正在ilock中等待使用该inode（例如，读取文件或列出目录），并且不会做好该inode已不再被分配的准备。\n这不可能发生，因为如果缓存的inode没有链接，并且ip-\u0026gt;ref为1，那么系统调用就无法获取指向该inode的指针。\n那一个引用是调用iput的线程所拥有的引用。\n的确，iput在icache.lock的临界区域之外检查引用计数是否为1，但此时已知链接计数为零，因此没有线程会尝试获取新引用。\n另一个主要危险是，对ialloc的并发调用可能会选择iput正在释放的同一个inode。\n这只能在iupdate写入磁盘以使inode的type为零后发生。\n这个争用是良性的：分配线程将客气地等待获取inode的睡眠锁，然后再读取或写入inode，此时iput已完成。\niput()可以写入磁盘。\n这意味着任何使用文件系统的系统调用都可能写入磁盘，因为系统调用可能是最后一个引用该文件的系统调用。\n即使像read()这样看起来是只读的调用，也可能最终调用iput()。\n这反过来意味着，即使是只读系统调用，如果它们使用文件系统，也必须在事务中进行包装。\niput()和崩溃之间存在一种具有挑战性的交互。\niput()不会在文件的链接计数降至零时立即截断文件，因为某些进程可能仍在内存中保留对inode的引用：进程可能仍在读取和写入该文件，因为它已成功打开该文件。\n但是，如果在最后一个进程关闭该文件的文件描述符之前发生崩溃，则该文件将被标记为已在磁盘上分配，但没有目录项指向它。\n文件系统以两种方式之一处理这种情况。\n简单的解决方案用于恢复时：重新启动后，文件系统会扫描整个文件系统，以查找标记为已分配但没有指向它们的目录项的文件。\n如果存在任何此类文件，接下来可以将其释放。\n第二种解决方案不需要扫描文件系统。\n在此解决方案中，文件系统在磁盘（例如在超级块中）上记录链接计数降至零但引用计数不为零的文件的i-number。\n如果文件系统在其引用计数达到0时删除该文件，则会通过从列表中删除该inode来更新磁盘列表。\n恢复时，文件系统将释放列表中的任何文件。\nXv6没有实现这两种解决方案，这意味着inode可能被标记为已在磁盘上分配，即使它们不再使用。这意味着随着时间的推移，xv6可能会面临磁盘空间不足的风险。\n多个进程或线程可能同时尝试访问同一个 inode，而这些进程通过锁机制（如 ilock）来确保独占访问。在某些情况下，某些线程可能正在等待获取这个 inode 的锁，比如在读取文件内容或者列出目录时（因为列出目录也是在访问目录 inode），这些操作需要独占锁。然而，一个潜在的危险是，如果 iput() 释放了该 inode，而其他线程正在等待这个 inode 的锁，这些线程可能会面对该 inode 已被删除或释放的情况。\n具体流程可以分解为以下步骤：\n进程 A 获得 inode 锁： 进程 A 通过 ilock() 获得对某个 inode 的独占锁，执行文件读取或其他操作。此时 ip-\u0026gt;ref 表示有指针引用该 inode。 进程 B 等待 inode 锁： 进程 B 也尝试访问同一个 inode，但由于锁已被进程 A 占用，进程 B 进入等待状态，等待 ilock() 返回。 进程 A 调用 iput()： 进程 A 完成了对 inode 的操作，并调用 iput() 来减少引用计数，准备释放该 inode。此时，iput() 检查 inode 的引用计数 ip-\u0026gt;ref 和链接计数 nlink。 如果 inode 的链接计数 nlink = 0 并且引用计数 ip-\u0026gt;ref = 1，意味着没有其他进程引用该 inode，这时 iput() 可以将其释放。 潜在的问题：进程 B 继续等待： 如果没有锁定协议，那么当 iput() 释放 inode 后，进程 B 可能获得了一个已经被释放的 inode。为了避免这种情况，锁定机制确保了 inode 的引用计数和锁的使用没有冲突，也就是说，iput() 不会释放一个仍有其他进程等待的 inode，避免了资源错误释放。 \u0026ldquo;截断\u0026quot;文件的意思\n截断（truncate） 是指 减少文件的内容或大小。当我们说 \u0026ldquo;iput() 不会立即截断文件\u0026rdquo; 时，意思是即便某个文件的链接计数 nlink 已经为零（即没有目录项再指向这个文件），并且不再有任何进程打开这个文件，iput() 也不会立刻释放与文件相关的磁盘空间。 这是因为进程可能仍然持有该文件的文件描述符，仍在访问文件。因此，在 iput() 中，仅当最后一个进程关闭该文件描述符，且文件的链接计数为零时，系统才会真正释放文件占用的磁盘空间。 目录项与 inode 的关系\n目录项（directory entry） 是 文件系统中存储文件名与 inode 号的映射。目录项记录了文件名，并指向相应的 inode。换句话说，每个文件或目录都有一个或多个目录项，它们通过 inode number（i-number） 引用磁盘上的 inode 结构。 当创建文件时，文件系统会为该文件分配一个 inode，并在目录中创建一个目录项，目录项保存文件名以及 inode 的编号。这就是文件名和 inode 之间的联系。 \u0026ldquo;没有目录项指向它\u0026rdquo; 的意思是，当一个文件的链接计数 nlink 降为零时，说明文件系统中没有任何目录项再指向这个 inode。换句话说，所有指向该文件的硬链接都已被删除，意味着该文件的文件名已经从文件系统中消失了。 文件描述符和目录项的关系： 打开文件时，进程会通过文件描述符访问该文件，文件描述符实际上是对 inode 的引用，而不是对目录项的直接引用。 当所有进程关闭文件描述符，并且文件的 nlink 为零时，说明没有任何文件名指向该 inode，文件的磁盘空间才会真正释放。 具体流程和时间点\n进程 A 调用 ilock()： 时间点：当进程 A 需要访问某个文件的 inode（比如读取文件或列出目录）。 结果：进程 A 获得了 inode 的独占锁。 进程 B 进入等待： 时间点：进程 B 也需要访问同一个 inode，但锁被进程 A 占用，因此 B 进入等待状态。 结果：进程 B 等待，无法立即获得 inode 锁。 进程 A 调用 iput()： 时间点：进程 A 完成操作后，调用 iput() 释放 inode 引用。 结果： 如果 nlink = 0 且 ip-\u0026gt;ref = 1，iput() 开始准备释放该 inode。 如果 iput() 发现有其他线程（进程 B）正在等待访问 inode，它会确保不会在有引用时释放 inode。 iput() 写入磁盘： 时间点：当 iput() 发现需要释放的 inode 在磁盘上有更新时，它会调用 iupdate() 写入磁盘，将 inode 的 type 设为 0。 结果：此时 inode 被标记为无效（type 为 0），并且没有目录项指向它。 系统崩溃时： 时间点：如果系统在进程 B 正在等待时崩溃，并且 inode 的 nlink = 0 但 ref \u0026gt; 0，文件可能被保留在磁盘上（没有目录项指向它，但文件还在）。 结果：此类文件会在系统重新启动时面临被回收或者手动删除的情况。 总结\n并发线程在访问 inode 时，锁定机制确保了不会出现 inode 被错误释放的情况。 截断文件是指释放文件的磁盘空间，这仅在文件的 nlink 和 ref 都为零时发生。 目录项是文件名和 inode 之间的映射关系，当所有目录项删除且文件描述符关闭时，inode 才会真正释放。 具体流程中，iput() 检查 inode 的状态，决定是否立即释放它，同时处理可能的竞争条件。 ip-\u0026gt;ref（引用计数） 作用：ip-\u0026gt;ref 是 内存中的引用计数，用于跟踪有多少 内核指针（C 指针） 指向该 inode。每当某个系统调用获取对 inode 的引用时（例如，打开文件、使用文件描述符），引用计数就会增加。每当一个引用不再需要时（例如，关闭文件），引用计数就会减少。\n示例：\n当进程打开一个文件时，ip-\u0026gt;ref 增加。 当进程关闭文件时，ip-\u0026gt;ref 减少。 作用场景：当 ip-\u0026gt;ref = 1 时，通常表示内核中只有一个指针还引用该 inode（通常是当前 iput() 调用所持有的引用）。当引用计数降为 0 时，表示内存中没有任何系统调用或进程再引用该 inode，它可以被释放。\nnlink（链接计数） 作用：nlink 是 磁盘上目录项的计数，用于跟踪有多少个 目录项（directory entry） 指向该 inode。每当文件被硬链接时，链接计数增加；每当删除一个硬链接时，链接计数减少。\n示例：\n创建文件时，nlink = 1，表示一个目录项（即文件名）指向该 inode。 创建一个硬链接（ln 命令）时，nlink 增加。删除文件或硬链接时，nlink 减少。 作用场景：当 nlink = 0 时，表示没有目录项指向该 inode，文件的名字已经从文件系统中删除。即使文件还在内存中被引用（例如，进程还在使用文件描述符），文件系统也不再通过目录访问到它。\n两者的区别与关系 ip-\u0026gt;ref 计数内存中的引用：它反映的是内核当前有多少指针指向这个 inode，通常与文件描述符、当前工作目录等相关。当 ip-\u0026gt;ref = 0 时，意味着没有进程再引用这个 inode，内核可以从内存中释放它。 nlink 计数磁盘上的链接：它反映的是磁盘上有多少文件名（目录项）指向这个 inode。nlink = 0 时，表示文件在文件系统中不再可见，即已经被从目录中移除。 为什么两者都需要统计？ 独立作用：这两者统计不同层次的引用。nlink 反映文件在文件系统中的可见性，决定何时删除文件的目录项；而 ip-\u0026gt;ref 则是内存中的引用，决定 inode 何时可以从内存中释放。 结合决定 inode 何时释放：即便 nlink = 0，也不能立刻释放 inode，因为文件可能仍然在被进程使用（ip-\u0026gt;ref \u0026gt; 1）。只有当 nlink = 0 且 ip-\u0026gt;ref = 1 时，iput() 可以安全地释放 inode——此时，文件已经从文件系统中删除，没有目录项指向它，并且没有进程再引用它。 流程示例 当删除一个文件时，nlink 会降到 0，文件从目录中消失。但如果一个进程已经打开了该文件并正在使用它，ip-\u0026gt;ref 会大于 1。 直到最后一个进程关闭文件描述符，ip-\u0026gt;ref 也降为 1，iput() 最终会释放 inode。这时，文件的所有数据会被清除，inode 和磁盘块被回收。 代码：Inode包含内容 磁盘上的inode结构体struct dinode包含一个size和一个块号数组（见图8.3）。\ninode数据可以在dinode的addrs数组列出的块中找到。\n前面的NDIRECT个数据块被列在数组中的前NDIRECT个元素中；这些块称为直接块（direct blocks）。\n接下来的NINDIRECT个数据块不在inode中列出，而是在称为间接块（indirect block）的数据块中列出。\naddrs数组中的最后一个元素给出了间接块的地址。\n因此，可以从inode中列出的块加载文件的前12 kB（NDIRECT x BSIZE）字节，而只有在查阅间接块后才能加载下一个256 kB（NINDIRECT x BSIZE）字节。\n这是一个很好的磁盘表示，但对于客户端来说较复杂。\n函数bmap管理这种表示，以便实现我们将很快看到的如readi和writei这样的更高级例程。\nbmap(struct inode *ip, uint bn)返回索引结点ip的第bn个数据块的磁盘块号。\n如果ip还没有这样的块，bmap会分配一个。\n函数bmap（kernel/fs.c:378）从简单的情况开始：前面的NDIRECT个块在inode本身中列出（kernel/fs.c:383-387）中。\n下面NINDIRECT个块在ip-\u0026gt;addrs[NDIRECT]的间接块中列出。\nBmap读取间接块（kernel/fs.c:394），然后从块内的正确位置（kernel/fs.c:395）读取块号。\n如果块号超过NDIRECT+NINDIRECT，则bmap调用panic崩溃；writei包含防止这种情况发生的检查（kernel/fs.c:490）。\nBmap根据需要分配块。ip-\u0026gt;addrs[]或间接块中条目为零表示未分配块。当bmap遇到零时，它会用按需分配的新块（kernel/fs.c:384-385）（kernel/fs.c:392-393）替换它们。\nitrunc释放文件的块，将inode的size重置为零。Itrunc（kernel/fs.c:410）首先释放直接块（kernel/fs.c:416-421），然后释放间接块中列出的块（kernel/fs.c:426-429），最后释放间接块本身（kernel/fs.c:431-432）。\nBmap使readi和writei很容易获取inode的数据。\nReadi（kernel/fs.c:456）首先确保偏移量和计数不超过文件的末尾。\n开始于超过文件末尾的地方读取将返回错误（kernel/fs.c:461-462），而从文件末尾开始或穿过文件末尾的读取返回的字节数少于请求的字节数（kernel/fs.c:463-464）。\n主循环处理文件的每个块，将数据从缓冲区复制到dst（kernel/fs.c:466-474）。\nwritei（kernel/fs.c:483）与readi相同，但有三个例外：从文件末尾开始或穿过文件末尾的写操作会使文件增长到最大文件大小（kernel/fs.c:490-491）；循环将数据复制到缓冲区而不是输出（kernel/fs.c:36）；如果写入扩展了文件，writei必须更新其大小（kernel/fs.c:504-511）。\nreadi和writei都是从检查ip-\u0026gt;type == T_DEV开始的。这种情况处理的是数据不在文件系统中的特殊设备；我们将在文件描述符层返回到这种情况。\n函数stati（kernel/fs.c:442）将inode元数据复制到stat结构体中，该结构通过stat系统调用向用户程序公开。\n代码：目录层 目录的内部实现很像文件。\n其inode的type为T_DIR，其数据是一系列目录条目（directory entries）。\n每个条目（entry）都是一个struct dirent（kernel/fs.h:56），其中包含一个名称name和一个inode编号inum。\n名称最多为DIRSIZ（14）个字符；如果较短，则以NUL（0）字节终止。\ninode编号为零的条目是空的。\n函数dirlookup（kernel/fs.c:527）在目录中搜索具有给定名称的条目。\n如果找到一个，它将返回一个指向相应inode的指针，解开锁定，并将*poff设置为目录中条目的字节偏移量，以满足调用方希望对其进行编辑的情形。\n如果dirlookup找到具有正确名称的条目，它将更新*poff并返回通过iget获得的未锁定的inode。\nDirlookup是iget返回未锁定indoe的原因。\n调用者已锁定dp，因此，如果对.，当前目录的别名，进行查找，则在返回之前尝试锁定indoe将导致重新锁定dp并产生死锁(还有更复杂的死锁场景，涉及多个进程和..，父目录的别名。.不是唯一的问题。）调用者可以解锁dp，然后锁定ip，确保它一次只持有一个锁。\n函数dirlink（kernel/fs.c:554）将给定名称和inode编号的新目录条目写入目录dp。\n如果名称已经存在，dirlink将返回一个错误（kernel/fs.c:560-564）。\n主循环读取目录条目，查找未分配的条目。\n当找到一个时，它会提前停止循环（kernel/fs.c:538-539），并将off设置为可用条目的偏移量。\n否则，循环结束时会将off设置为dp-\u0026gt;size。\n无论哪种方式，dirlink都会通过在偏移off处写入（kernel/fs.c:574-577）来向目录添加一个新条目。\n目录的结构和作用 目录（Directory）是由一系列目录条目（directory entries）组成的，目录本身就是一个类型为T_DIR的 inode。目录条目是struct dirent类型的结构，每个条目包含：\nname：文件名或目录名，最多可有 14 个字符（DIRSIZ），如果文件名较短，则以空字符（NUL）结束。 inum：该目录条目对应的 inode 编号。如果 inum 为 0，表示这个条目是空的，未使用。 函数概览及其作用\ndirlookup（kernel/fs.c:527）：\n作用：在目录中查找给定名称的条目，返回指向对应 inode 的指针。\n位置：文件系统层次，用于搜索目录内是否有指定名称的文件或子目录。\n功能：\n1. **查找目录条目**：扫描目录中的条目，通过匹配文件名找到相应的 inode。 返回 inode：如果找到匹配的条目，返回未锁定的 inode，并设置该条目的字节偏移量（*poff）。这个偏移量表示条目在目录数据中的位置，方便后续修改。 避免死锁：由于 dirlookup 返回的是未锁定的 inode，调用者在处理 inode 时可以解锁目录，避免像对 \u0026ldquo;.\u0026quot;（当前目录）和 \u0026ldquo;..\u0026quot;（父目录）这样的条目产生的死锁问题。 dirlink（kernel/fs.c:554）：\n作用：向目录中写入一个新条目，将给定的名称和 inode 编号关联起来。\n位置：文件系统层次，用于将文件或子目录添加到目录中。\n功能：\n检查名称是否存在：在目录中查找是否已有同名条目。如果存在，返回错误。 寻找空条目：如果条目不存在，它扫描目录，寻找空闲的目录条目（inum 为 0），如果找到，记录下这个条目的偏移量。 写入新条目：将新的目录条目写入目录，将给定的名称和 inode 编号保存到偏移位置。 偏移量的含义\n偏移量 是目录条目在 目录文件数据 中的位置。目录本质上是一个特殊的文件，其中包含每个条目的 inode 编号和对应的文件名。偏移量是目录条目在这个文件中的字节位置，用于定位、修改或删除目录条目。 在 dirlookup 中，偏移量（*poff）被设置为找到的条目的位置，便于后续修改该条目。 在 dirlink 中，偏移量指向空闲条目的位置或者目录末尾，用于添加新条目。 目录和文件之间的定位过程 目录与文件的定位：文件系统通过 目录项（directory entry） 将文件名与 inode 关联。每个目录条目保存文件名和 inode 编号。当我们访问一个文件时，系统先从目录中找到与文件名对应的 inode 编号，然后通过这个编号定位到磁盘上对应的 inode 结构。 查找过程（dirlookup）：\n目录本身是一个 inode，它保存了该目录下所有文件的目录条目。\ndirlookup 函数扫描目录条目，查找与给定文件名匹配的条目，并返回对应的 inode。\n一旦找到对应的 inode 编号，文件系统就可以通过这个 inode 编号进一步读取文件的元数据（如文件大小、数据块位置等）。\n添加过程（dirlink）：\n在添加新文件或目录时，dirlink 首先查找目录中的空条目位置。\n然后，它将文件名和 inode 编号写入该条目，关联文件名与 inode。\n总结 目录是由一系列目录条目组成的文件结构，每个条目记录了文件名和 inode 编号的对应关系。 关键函数： dirlookup：用于在目录中查找文件名，返回对应 inode 并提供目录条目的偏移量。 dirlink：用于向目录中添加新条目，将文件名和 inode 编号关联。 偏移量用于定位目录条目在目录文件中的位置，便于读取或修改。 目录项与 inode 的关系是：目录项通过 inode 编号定位文件，inode 保存了文件的元数据及存储信息。 代码：路径名 路径名查找涉及一系列对dirlookup的调用，每个路径组件调用一个。\nNamei（kernel/fs.c:661）计算path并返回相应的inode。\n函数nameiparent是一个变体：它在最后一个元素之前停止，返回父目录的inode并将最后一个元素复制到name中。\n两者都调用通用函数namex来完成实际工作。\n路径名查找和基本流程\n路径名查找 涉及对 dirlookup 的多次调用，每个路径组件对应一次查找。例如，查找路径 /a/b/c 涉及查找 /a、b、c。 namei（kernel/fs.c:661）：负责计算路径名并返回对应的 inode。 nameiparent：是 namei 的变体，它在查找路径时返回最后一个路径元素的 父目录 的 inode，并将最后一个路径元素复制到 name。 两个函数都调用了通用函数 namex 来完成实际的路径查找工作。 Namex（kernel/fs.c:626）首先决定路径计算的开始位置。\n如果路径以斜线开始，则计算从根目录开始；否则，从当前目录开始（kernel/fs.c:630-633）。\n然后，它使用skipelem依次考察路径的每个元素（kernel/fs.c:635）。\n循环的每次迭代都必须在当前索引结点ip中查找name。\n迭代首先给ip上锁并检查它是否是一个目录。\n如果不是，则查找失败（kernel/fs.c:636-640）(锁定ip是必要的，不是因为ip-\u0026gt;type可以被更改，而是因为在ilock运行之前，ip-\u0026gt;type不能保证已从磁盘加载。）\n如果调用是nameiparent，并且这是最后一个路径元素，则根据nameiparent的定义，循环会提前停止；最后一个路径元素已经复制到name中，因此namex只需返回解锁的ip（kernel/fs.c:641-645）。\n最后，循环将使用dirlookup查找路径元素，并通过设置ip = next（kernel/fs.c:646-651）为下一次迭代做准备。当循环用完路径元素时，它返回ip。\nnamex 函数的查找逻辑\nnamex（kernel/fs.c:626）首先决定从哪里开始查找路径： 如果路径以斜杠 / 开头，从根目录开始。 如果路径不是以斜杠开头，从当前目录开始（kernel/fs.c:630-633）。 然后通过 skipelem 分解路径的每个组件（例如，a、b、c）。 每次迭代，namex 锁定当前的 inode（ip）并检查它是否是一个目录（kernel/fs.c:636-640）。如果不是目录，查找失败。 处理最后一个路径元素\n如果调用的是 nameiparent，且查找到了路径的最后一个元素，那么 namex 会提前停止，并将最后一个路径元素的名称复制到 name，返回 父目录 的 inode（kernel/fs.c:641-645）。 否则，namex 会使用 dirlookup 查找当前目录中的下一个路径元素，并为下一次迭代设置新的 inode（kernel/fs.c:646-651）。 namex过程可能需要很长时间才能完成：它可能涉及多个磁盘操作来读取路径名中所遍历目录的索引节点和目录块（如果它们不在buffer cache中）。\nXv6经过精心设计，如果一个内核线程对namex的调用在磁盘I/O上阻塞，另一个查找不同路径名的内核线程可以同时进行。\nNamex分别锁定路径中的每个目录，以便在不同目录中进行并行查找。\n路径名查找中的 I/O 操作和并发\nnamex 可能需要多个磁盘操作来读取遍历路径中的目录和 inode，特别是在相关目录和 inode 不在 buffer cache 中时。这可能导致磁盘 I/O 延迟。 xv6 设计允许多个内核线程并发查找不同的路径名。每个目录在查找时都被分别加锁，使得不同线程可以同时访问不同的目录，避免彼此阻塞。 这种并发性带来了一些挑战。\n例如，当一个内核线程正在查找路径名时，另一个内核线程可能正在通过取消目录链接来更改目录树。\n一个潜在的风险是，查找可能正在搜索已被另一个内核线程删除且其块已被重新用于另一个目录或文件的目录。\nXv6避免了这种竞争。\n例如，在namex中执行dirlookup时，lookup线程持有目录上的锁，dirlookup返回使用iget获得的inode。\nIget增加索引节点的引用计数。\n只有在从dirlookup接收inode之后，namex才会释放目录上的锁。\n现在，另一个线程可以从目录中取消inode的链接，但是xv6还不会删除inode，因为inode的引用计数仍然大于零。\n并发风险和解决方案\n并发风险：当一个线程在执行路径查找时，另一个线程可能在删除目录或更改目录树。查找可能正在访问已被删除的目录条目，其块可能被重新分配给其他文件或目录。 解决方案：在 namex 中，路径查找线程在执行 dirlookup 时会锁定目录。dirlookup 返回一个通过 iget 获取的 inode，并增加其引用计数。只有在获取到 inode 后，才会释放目录的锁。这样，即使另一个线程删除该 inode，它也不会立即被回收，因为引用计数仍然大于零。 另一个风险是死锁。\n例如，查找“.”时，next指向与ip相同的inode。在释放ip上的锁之前锁定next将导致死锁。\n在释放ip上的锁之前锁定next将导致死锁。\n为了避免这种死锁，namex在获得下一个目录的锁之前解锁该目录。\n这里我们再次看到为什么iget和ilock之间的分离很重要。\n死锁问题和解决方案\n死锁风险：如果在查找路径名如 \u0026quot;.\u0026quot; 时，next inode 和 ip 是同一个 inode，在释放 ip 的锁之前尝试锁定 next 会导致死锁。 解决方案：为了避免这种死锁，namex 在锁定下一个目录（next）之前，先解锁当前的 inode（ip）。这也是为什么 xv6 将 iget 和 ilock 分离的原因 关键函数和变量\nnamei：计算路径并返回对应的 inode。 nameiparent：返回父目录的 inode 并将最后的路径元素复制到 name。 namex：通用路径查找逻辑，处理路径分解、目录锁定与释放。 skipelem：解析路径名中的下一个元素。 dirlookup：查找目录中的指定条目，返回对应 inode。 iget：通过 inode 编号获取对应的 inode 并增加引用计数。 ilock：锁定 inode，确保对其独占访问。 文件描述符层 Unix界面的一个很酷的方面是，Unix中的大多数资源都表示为文件，包括控制台、管道等设备，当然还有真实文件。文件描述符层是实现这种一致性的层。\n正如我们在第1章中看到的，Xv6为每个进程提供了自己的打开文件表或文件描述符。\n每个打开的文件都由一个struct file（kernel/file.h:1）表示，它是inode或管道的封装，加上一个I/O偏移量。\n每次调用open都会创建一个新的打开文件（一个新的struct file）：如果多个进程独立地打开同一个文件，那么不同的实例将具有不同的I/O偏移量。\n另一方面，单个打开的文件（同一个struct file）可以多次出现在一个进程的文件表中，也可以出现在多个进程的文件表中。\n如果一个进程使用open打开文件，然后使用dup创建别名，或使用fork与子进程共享，就会发生这种情况。\n引用计数跟踪对特定打开文件的引用数。\n可以打开文件进行读取或写入，也可以同时进行读取和写入。\nreadable和writable字段可跟踪此操作。\n系统中所有打开的文件都保存在全局文件表ftable中。\n文件表具有分配文件（filealloc）、创建重复引用（filedup）、释放引用（fileclose）以及读取和写入数据（fileread和filewrite）的函数。\n前三个函数遵循现在熟悉的形式。\nFilealloc（kernel/file.c:30）扫描文件表以查找未引用的文件（f-\u0026gt;ref == 0），并返回一个新的引用；filedup（kernel/file.c:48）增加引用计数；fileclose（kernel/file.c:60）将其递减。\n当文件的引用计数达到零时，fileclose会根据type释放底层管道或inode。\n函数filestat、fileread和filewrite实现对文件的stat、read和write操作。\nFilestat（kernel/file.c:88）只允许在inode上操作并且调用了stati。\nFileread和filewrite检查打开模式是否允许该操作，然后将调用传递给管道或inode的实现。如果文件表示inode，fileread和filewrite使用I/O偏移量作为操作的偏移量，然后将文件指针前进该偏移量（kernel/file.c:122-123）（kernel/file.c:153-154）。\n管道没有偏移的概念。\n回想一下，inode的函数要求调用方处理锁（kernel/file.c:94-96）（kernel/file.c:121-124）（kernel/file.c:163-166）。inode锁定有一个方便的副作用，即读取和写入偏移量以原子方式更新，因此，对同一文件的同时多次写入不能覆盖彼此的数据，尽管他们的写入最终可能是交错的。\n在 Unix 系统中，有一个很有趣的特点：几乎所有的资源都可以表示为文件。不只是硬盘上的文件，像控制台、管道（即进程间通信的通道）这些设备也被视为“文件”。为了实现这种一致性，Unix 使用了一个叫做“文件描述符层”的机制。\n在 Unix 系统中，每个进程都有自己独立的“打开文件表”，也就是我们常说的“文件描述符”。这个表格包含了每个进程当前打开的所有文件。每个打开的文件由一个 struct file 结构体表示（这个结构体定义在 kernel/file.h:1），它封装了文件的元信息（如文件的 inode，或者管道）和文件的读写位置（即 I/O 偏移量）。\n当你调用 open 打开一个文件时，操作系统会为这个文件创建一个新的 struct file 实例。所以，如果两个进程独立打开同一个文件，它们会得到不同的 struct file 实例，这样每个进程可以有各自独立的 I/O 偏移量。另一方面，如果一个进程通过 dup 复制文件描述符，或者通过 fork 创建子进程与子进程共享文件，那么它们可能会指向同一个 struct file 实例。\n每个 struct file 实例都有一个引用计数器，用于追踪有多少个文件描述符指向它。文件可以以只读、只写或读写的方式打开，struct file 中的 readable 和 writable 字段分别记录文件的读写权限。\n系统中所有进程的所有打开文件都被存储在一个全局文件表 ftable 中。这个表有几个关键的操作函数：\nfilealloc：分配一个新的文件对象。 filedup：为一个已经打开的文件创建一个重复的引用。 fileclose：释放对文件的引用。 fileread 和 filewrite：读写文件数据。 前三个函数 (filealloc、filedup、fileclose) 实现起来相对简单：\nfilealloc（定义在 kernel/file.c:30）通过扫描全局文件表，找到一个未被引用的文件对象（f-\u0026gt;ref == 0）并返回一个新的引用。 filedup（定义在 kernel/file.c:48）增加该文件的引用计数。 fileclose（定义在 kernel/file.c:60）减少引用计数。如果计数变成零，表示没有进程再使用这个文件，fileclose 会释放文件的底层资源（比如管道或 inode）。 文件操作函数 filestat、fileread 和 filewrite：\nfilestat（定义在 kernel/file.c:88）用于获取文件的状态信息，但只能对 inode 类型的文件执行此操作，并会调用 stati 函数获取信息。 fileread 和 filewrite 函数用于读取和写入文件数据。它们首先会检查文件的打开模式，确保允许进行所需的操作，然后将具体的操作传递给管道或 inode 的实现层。 如果文件是一个 inode，fileread 和 filewrite 会根据文件的 I/O 偏移量进行读写操作，之后会将该偏移量向前推进（定义在 kernel/file.c:122-123 和 kernel/file.c:153-154）。 如果文件是一个管道，管道并不支持偏移量，所以操作逻辑会有所不同。 同时，所有 inode 操作都要求调用方自己负责管理锁定（在 kernel/file.c:94-96、kernel/file.c:121-124 和 kernel/file.c:163-166 中实现）。这种锁定有一个额外的好处：它确保了对同一个文件的并发写入不会相互覆盖，虽然数据写入的顺序可能会交错。\n代码：系统调用 通过使用底层提供的函数，大多数系统调用的实现都很简单（请参阅kernel/sysfile.c）。\n有几个调用值得仔细看看。\n函数sys_link和sys_unlink编辑目录，创建或删除索引节点的引用。\n它们是使用事务能力的另一个很好的例子。\nsys_link（kernel/sysfile.c:120）从获取其参数开始，两个字符串分别是old和new（kernel/sysfile.c:125）。\n假设old存在并且不是一个目录（kernel/sysfile.c:129-132），sys_link会增加其ip-\u0026gt;nlink计数。\n然后sys_link调用nameiparent来查找new（kernel/sysfile.c:145）的父目录和最终路径元素，并创建一个指向old的inode（kernel/sysfile.c:148）的新目录条目。\nnew的父目录必须存在并且与现有inode位于同一设备上：inode编号在一个磁盘上只有唯一的含义。\n如果出现这样的错误，sys_link必须返回并减少ip-\u0026gt;nlink。\n在 xv6 文件系统中，ip 和 dp 是常用的变量名，通常分别代表 inode 指针 和 目录 inode 指针。\n事务简化了实现，因为它需要更新多个磁盘块，但我们不必担心更新的顺序。\n他们要么全部成功，要么什么都不做。\n例如在没有事务的情况下，在创建一个链接之前更新ip-\u0026gt;nlink会使文件系统暂时处于不安全状态，而在这两者之间发生的崩溃可能会造成严重破坏。\n对于事务，我们不必担心这一点\nSys_link为现有inode创建一个新名称。\n函数create（kernel/sysfile.c:242）为新inode创建一个新名称。\n它是三个文件创建系统调用的泛化：带有O_CREATE标志的open生成一个新的普通文件，mkdir生成一个新目录，mkdev生成一个新的设备文件。\n与sys_link一样，create从调用nameiparent开始，以获取父目录的inode。\n然后调用dirlookup检查名称是否已经存在（kernel/sysfile.c:252）。\n如果名称确实存在，create的行为取决于它用于哪个系统调用：open的语义与mkdir和mkdev不同。\n如果create是代表open（type == T_FILE）使用的，并且存在的名称本身是一个常规文件，那么open会将其视为成功，create也会这样做（kernel/sysfile.c:256）。否则，这是一个错误（kernel/sysfile.c:257-258）。\n如果名称不存在，create现在将使用ialloc（kernel/sysfile.c:261）分配一个新的inode。如果新inode是目录，create将使用.和..条目对它进行初始化。\n最后，既然数据已正确初始化，create可以将其链接到父目录（kernel/sysfile.c:274）。\nCreate与sys_link一样，同时持有两个inode锁：ip和dp。不存在死锁的可能性，因为索引结点ip是新分配的：系统中没有其他进程会持有ip的锁，然后尝试锁定dp。\n使用create，很容易实现sys_open、sys_mkdir和sys_mknod。\nSys_open（kernel/sysfile.c:287）是最复杂的，因为创建一个新文件只是它能做的一小部分。如果open被传递了O_CREATE标志，它将调用create（kernel/sysfile.c:301）。\n否则，它将调用namei（kernel/sysfile.c:307）。Create返回一个锁定的inode，但namei不锁定，因此sys_open必须锁定inode本身。\n这提供了一个方便的地方来检查目录是否仅为读取打开，而不是写入。\n假设inode是以某种方式获得的，sys_open分配一个文件和一个文件描述符（kernel/sysfile.c:325），然后填充该文件（kernel/sysfile.c:337-342）。\n请注意，没有其他进程可以访问部分初始化的文件，因为它仅位于当前进程的表中。\n在我们还没有文件系统之前，第7章就研究了管道的实现。\n函数sys_pipe通过提供创建管道对的方法将该实现连接到文件系统。\n它的参数是一个指向两个整数的指针，它将在其中记录两个新的文件描述符。然后分配管道并安装文件描述符。\n在 Unix 操作系统中，系统调用是用户程序与内核交互的主要途径。虽然系统调用的实现依赖于底层的操作函数，但大多数系统调用的代码相对简单（可以在 kernel/sysfile.c 中找到）。接下来，让我们重点看看几个值得关注的调用。\nsys_link 和 sys_unlink这两个系统调用用于操作目录，分别创建或删除对文件的引用（即链接）。这两个函数展示了事务功能的另一个好处——它们可以确保多个磁盘操作要么全部成功，要么全部失败。\nsys_link：它的功能是为一个已有文件创建一个新的名字（也就是硬链接）。它首先获取两个参数，分别是旧路径old和新路径new\n。假设old文件存在且不是一个目录，sys_link会增加这个文件的链接计数（ip-\u0026gt;nlink），表示这个文件有了更多的引用。\n然后它会调用 nameiparent 函数，找到 new 的父目录，并在该目录下为 old 文件创建一个新条目。如果新条目的父目录不存在，或者新条目和旧条目不在同一个磁盘上，sys_link 就会返回错误并撤销刚刚增加的引用计数。 事务使得这种操作更为安全，因为所有的磁盘块更新要么一起成功，要么一起失败。如果没有事务，某个步骤（如增加链接计数）出错，可能会导致文件系统不一致。\ncreate 函数用于创建一个新文件，它是三个系统调用的泛化：open（带 O_CREATE 标志时创建一个普通文件）、mkdir（创建目录）和 mkdev（创建设备文件）。\n与sys_link类似，create从获取父目录开始。首先调用nameiparent获取父目录的 inode，并使用dirlookup检查要创建的名称是否已经存在。 如果名称存在，create 的行为取决于调用者是 open、mkdir 还是 mkdev。如果调用者是 open 且要创建的对象是一个普通文件，那么 create 会将其视为成功。否则，这是一个错误。 如果名称不存在，create 会分配一个新的 inode，并根据不同的文件类型（如目录）初始化它。之后，它会将新创建的文件链接到父目录。 create 和 sys_link 都会同时持有两个 inode 的锁：新分配的文件（ip）和父目录（dp）。由于 ip 是新分配的，其他进程不可能持有 ip 的锁，这就避免了死锁的可能性。\nsys_open、sys_mkdir 和 sys_mknod：通过 create 函数，很容易实现 sys_open（打开文件）、sys_mkdir（创建目录）和 sys_mknod（创建设备文件）。其中，sys_open 是最复杂的，因为它不仅仅负责创建新文件。\n如果调用时传递了O_CREATE标志，sys_open就会调用create来创建一个新文件。否则，它会调用namei获取文件的 inode（namei不会锁定 inode，因此sys_open需要自己加锁）。 在获得 inode 后，sys_open 会为该文件分配一个文件结构和文件描述符，并将其添加到进程的文件表中。 注意，此时文件只存在于当前进程的文件表中，其他进程无法访问这个“部分初始化”的文件，因此不存在竞争问题。\nsys_pipe：在第7章中我们已经探讨了管道的实现。sys_pipe 函数将管道系统连接到文件系统中。它的作用是创建一对管道：一个用于读取，一个用于写入。调用时，传递的参数是一个指向两个整数的指针，这两个整数用于记录管道的两个文件描述符。随后，sys_pipe 会分配管道并在文件描述符表中为其安装对应的读写端。\n真实世界 实际操作系统中的buffer cache比xv6复杂得多，但它有两个相同的用途：缓存和同步对磁盘的访问。\n与UNIX V6一样，Xv6的buffer cache使用简单的最近最少使用（LRU）替换策略；有许多更复杂的策略可以实现，每种策略都适用于某些工作场景，而不适用于其他工作场景。\n更高效的LRU缓存将消除链表，而改为使用哈希表进行查找，并使用堆进行LRU替换。\n现代buffer cache通常与虚拟内存系统集成，以支持内存映射文件。\nXv6的日志系统效率低下。\n提交不能与文件系统调用同时发生。\n系统记录整个块，即使一个块中只有几个字节被更改。\n它执行同步日志写入，每次写入一个块，每个块可能需要整个磁盘旋转时间。\n真正的日志系统解决了所有这些问题。\n日志记录不是提供崩溃恢复的唯一方法。\n早期的文件系统在重新启动期间使用了一个清道夫程序（例如，UNIX的fsck程序）来检查每个文件和目录以及块和索引节点空闲列表，查找并解决不一致的问题。\n清理大型文件系统可能需要数小时的时间，而且在某些情况下，无法以导致原始系统调用原子化的方式解决不一致问题。\n从日志中恢复要快得多，并且在崩溃时会导致系统调用原子化。\nXv6使用的索引节点和目录的基础磁盘布局与早期UNIX相同；这一方案多年来经久不衰。\nBSD的UFS/FFS和Linux的ext2/ext3使用基本相同的数据结构。\n文件系统布局中最低效的部分是目录，它要求在每次查找期间对所有磁盘块进行线性扫描。\n当目录只有几个磁盘块时，这是合理的，但对于包含许多文件的目录来说，开销巨大。\nMicrosoft Windows的NTFS、Mac OS X的HFS和Solaris的ZFS（仅举几例）将目录实现为磁盘上块的平衡树。\n这很复杂，但可以保证目录查找在对数时间内完成（即时间复杂度为O(logn)）。\nXv6对于磁盘故障的解决很初级：如果磁盘操作失败，Xv6就会调用panic。\n这是否合理取决于硬件：如果操作系统位于使用冗余屏蔽磁盘故障的特殊硬件之上，那么操作系统可能很少看到故障，因此panic是可以的。\n另一方面，使用普通磁盘的操作系统应该预料到会出现故障，并能更优雅地处理它们，这样一个文件中的块丢失不会影响文件系统其余部分的使用。\nXv6要求文件系统安装在单个磁盘设备上，且大小不变。\n随着大型数据库和多媒体文件对存储的要求越来越高，操作系统正在开发各种方法来消除“每个文件系统一个磁盘”的瓶颈。\n基本方法是将多个物理磁盘组合成一个逻辑磁盘。\nRAID等硬件解决方案仍然是最流行的，但当前的趋势是在软件中尽可能多地实现这种逻辑。\n这些软件实现通常允许通过动态添加或删除磁盘来扩展或缩小逻辑设备等丰富功能。\n当然，一个能够动态增长或收缩的存储层需要一个能够做到这一点的文件系统：xv6使用的固定大小的inode块阵列在这样的环境中无法正常工作。\n将磁盘管理与文件系统分离可能是最干净的设计，但两者之间复杂的接口导致了一些系统（如Sun的ZFS）将它们结合起来。\nXv6的文件系统缺少现代文件系统的许多其他功能；例如，它缺乏对快照和增量备份的支持。\n现代Unix系统允许使用与磁盘存储相同的系统调用访问多种资源：命名管道、网络连接、远程访问的网络文件系统以及监视和控制接口，如/proc（注：Linux 内核提供了一种通过/proc文件系统，在运行时访问内核内部数据结构、改变内核设置的机制。proc文件系统是一个伪文件系统，它只存在内存当中，而不占用外存空间。它以文件系统的方式为访问系统内核数据的操作提供接口。）。\n不同于xv6中fileread和filewrite的if语句，这些系统通常为每个打开的文件提供一个函数指针表，每个操作一个，并通过函数指针来援引inode的调用实现。\n网络文件系统和用户级文件系统提供了将这些调用转换为网络RPC并在返回之前等待响应的函数。\n在实际的操作系统中，缓冲区缓存要比 xv6 中复杂得多，但它们有相同的两个核心功能：缓存磁盘访问 和 管理对磁盘的同步访问。Xv6 使用了一种非常简单的缓存替换策略，即“最近最少使用（LRU）”，这意味着最久未使用的缓存项将被优先替换。当然，实际系统中有很多更复杂的替换策略，每种策略在不同的应用场景中表现更好。更高效的 LRU 实现通常会使用哈希表来快速查找数据，并通过堆来管理 LRU 替换顺序。此外，现代的缓冲区缓存通常与虚拟内存系统集成，这样就可以支持像内存映射文件这样的高级功能。\nXv6 的日志系统效率很低，主要因为在提交（写入日志）时无法同时进行文件系统操作，而且系统会记录整个磁盘块，即使只修改了几个字节。这些写入操作是同步的，即每次只能写一个块，导致操作非常慢。如果磁盘需要旋转一次才能完成一个块的写入，那么这次旋转可能就是瓶颈。实际的日志系统会解决这些问题，让写入更高效。\n日志记录并不是恢复文件系统的一种唯一方法。早期的文件系统使用了清理程序（如 Unix 中的 fsck 程序）在重启时检查文件和目录的完整性，并修复不一致的问题。这种方法很慢，尤其是在文件系统很大的情况下，可能需要花费数小时。相比之下，使用日志系统的恢复速度要快得多，而且它能确保系统调用是原子性的——这意味着操作要么完全成功，要么什么都不会改变。\nXv6 中使用的 inode（索引节点）和目录结构 是从早期 Unix 继承下来的，像 BSD 的 UFS/FFS 和 Linux 的 ext2/ext3 文件系统也使用了类似的结构。文件系统中的一个低效部分是目录查找，它在查找文件时需要对目录的所有磁盘块进行线性扫描，这对于包含大量文件的目录来说是一个巨大的性能开销。现代文件系统（如 Windows 的 NTFS、Mac OS X 的 HFS、Solaris 的 ZFS）通过在磁盘上使用平衡树结构来加速目录查找，使得查找时间复杂度变成 O(logn)，大大提升了性能。\nXv6 对于磁盘故障的处理非常基础：一旦磁盘操作失败，系统就会调用 panic 进行崩溃。这种处理方式是否合理取决于硬件情况：如果系统使用的硬件有冗余机制来屏蔽磁盘故障（如 RAID），那么 panic 处理是可以接受的；但如果操作系统运行在普通硬件上，系统应该能够优雅地处理这些故障，防止单个文件损坏影响整个文件系统。\nXv6 要求文件系统安装在单一的磁盘设备上，且文件系统大小是固定的。随着数据量的不断增长，这种设计已经无法满足现代应用的需求。如今，操作系统通过多种方法来打破“每个文件系统只能对应一个磁盘”的限制。RAID 是一种常见的硬件解决方案，它通过将多个物理磁盘组合成一个逻辑磁盘。但现代趋势是更多地依靠软件来实现这些功能，并支持动态添加或删除磁盘。\nXv6 文件系统缺乏许多现代文件系统的高级功能。例如，它没有 快照（snapshot）和增量备份 的功能。快照可以在某一时刻冻结文件系统状态，从而支持快速备份，而增量备份只备份变化的数据，大大节省存储空间。\n在现代的 Unix 系统中，很多不同类型的资源（如命名管道、网络连接、远程文件系统等）都可以通过与磁盘文件相同的系统调用（如 read 和 write）进行访问。这比 xv6 中使用简单的 if 语句来判断文件类型更灵活。在这些系统中，每个打开的文件都会有一个函数指针表，不同的操作会指向不同的实现函数。例如，网络文件系统 可能会将文件读写操作转化为网络请求并等待响应。\n正课 文件系统引入 今天介绍的是文件系统。实际上我们会花三节课的时间来学习文件系统。\n前两节课基于XV6来做介绍，第三节课基于Linux来做介绍。\n实际上，这将是有关XV6的最后一个话题，在这周之后我们就讲完了XV6。\n文件系统是操作系统中除了shell以外最常见的用户接口。\n我们希望通过这几节课来理解：文件系统的背后究竟是什么原理，文件系统是如何实现的。\nXV6的文件系统与你正在使用的文件系统有什么区别。\n学生回答：其中一点是，XV6支持的文件大小远小于其他文件系统。其次一些文件的文件名也较短。第三点，我不认为XV6的文件系统有copy-on-write。\nFrans教授：很好，那有什么相似的地方吗？\n学生回答：基本的结构是类似的，比如说都有文件名，都有inode，目录等等。\nFans教授：很好，我再问一个同学，XV6的文件系统与你正在使用的文件系统有什么异同？\n学生回答：文件目录结构都是层级的。\n接下来让我列出一些文件系统突出的特性：\n其中一点刚刚有同学提到了，就是对于用户友好的文件名，具体来说就是层级的路径名，这可以帮助用户组织目录中的文件。 通过将文件命名成方便易记的名字，可以在用户之间和进程之间更简单的共享文件。 相比我们已经看过的XV6其他子系统，这一点或许是最重要的，文件系统提供了持久化。这意味着，我可以关闭一个计算机，过几天再开机而文件仍然在那，我可以继续基于文件工作。这一点与进程和其他资源不一样，这些资源在计算机重启时就会消失，之后你需要重新启动它们，但是文件系统就可以提供持久化。 出于以下原因，文件系统背后的机制还比较有意思：\n文件系统对硬件的抽象较为有用，所以理解文件系统对于硬件的抽象是如何实现的还是有点意思的。 除此之外，还有个关键且有趣的地方就是crash safety。有可能在文件系统的操作过程中，计算机崩溃了，在重启之后你的文件系统仍然能保持完好，文件系统的数据仍然存在，并且你可以继续使用你的大部分文件。如果文件系统操作过程中计算机崩溃了，然后你重启之后文件系统不存在了或者磁盘上的数据变了，那么崩溃的将会是你。所以crash safety是一个非常重要且经常出现的话题，我们下节课会专门介绍它。 如何在磁盘上排布文件系统。例如目录和文件，它们都需要以某种形式在磁盘上存在，这样当你重启计算机时，所有的数据都能恢复。所以在磁盘上有一些数据结构表示了文件系统的结构和内容。在XV6中，使用的数据结构非常简单，因为XV6是专门为教学目的创建的。真实的文件系统通常会更加复杂。但是它们都是磁盘上保存的数据结构，我们在今天的课程会重点看这部分。 最后一个有趣的话题是性能。文件系统所在的硬件设备通常都较慢，比如说向一个SSD磁盘写数据将会是毫秒级别的操作，而在一个毫秒内，计算机可以做大量的工作，所以尽量避免写磁盘很重要，我们将在几个地方看到提升性能的代码。比如说，所有的文件系统都有buffer cache或者叫block cache。同时这里会有更多的并发，比如说你正在查找文件路径名，这是一个多次交互的操作，首先要找到文件结构，然后查找一个目录的文件名，之后再去查找下一个目录等等。你会期望当一个进程在做路径名查找时，另一个进程可以并行的运行。这样的并行运行在文件系统中将会是一个大的话题。 除此之外，你会对文件系统感兴趣是因为这是接下来两个lab的内容。下一个lab完全关注在文件系统，下下个lab结合了虚拟内存和文件系统。\n即使是这周的lab，也会尝试让buffer cache可以支持更多的并发。所以这就是为什么文件系统是有趣的。\nFile system实现概述 为了理解文件系统必须提供什么能力，让我们再看一下一些与文件系统相关的基础系统调用。\n首先让我们来看一个简单的场景，假设我们创建了文件“x/y”，或者说在目录x中创建了文件y，同时我们需要提供一些标志位，现在我们还不太关心标志位所以我会忽略它。\n上面的系统调用会创建文件，并返回文件描述符给调用者。\n调用者也就是用户应用程序可以对文件描述符调用write，有关write我们在之前已经看过很多次了，这里我们向文件写入“abc”三个字符。\n从这两个调用已经可以看出一些信息了：\n首先出现在接口中的路径名是可读的名字，而不是一串数字，它是由用户选择的字符串。 write系统调用并没有使用offset作为参数，所以写入到文件的哪个位置是隐式包含在文件系统中，文件系统在某个位置必然保存了文件的offset。因为如果你再调用write系统调用，新写入的数据会从第4个字节开始。 除此之外，还有一些我们之前没有看过的有趣的系统调用。\n例如XV6和所有的Unix文件系统都支持通过系统调用创建链接，给同一个文件指定多个名字。你可以通过调用link系统调用，为之前创建的文件“x/y”创建另一个名字“x/z”。\n所以文件系统内部需要以某种方式跟踪指向同一个文件的多个文件名。\n我们还可能会在文件打开时，删除或者更新文件的命名空间。\n例如，用户可以通过unlink系统调用来删除特定的文件名。如果此时相应的文件描述符还是打开的状态，那我们还可以向文件写数据，并且这也能正常工作。\n所以，在文件系统内部，文件描述符必然与某个对象关联，而这个对象不依赖文件名。\n这样，即使文件名变化了，文件描述符仍然能够指向或者引用相同的文件对象。\n所以，实际上操作系统内部需要对于文件有内部的表现形式，并且这种表现形式与文件名无关。\n除此之外，我还想提一点。\n文件系统的目的是实现上面描述的API，也即是典型的文件系统API。\n但是，这并不是唯一构建一个存储系统的方式。\n如果只是在磁盘上存储数据，你可以想出一个完全不同的API。\n举个例子，数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。\n所以记住这一点很重要：还存在其他的方式能组织存储系统。\n我们这节课关注在文件系统，文件系统通常由操作系统提供，而数据库如果没有直接访问磁盘的权限的话，通常是在文件系统之上实现的。\n（注，早期数据库通常直接基于磁盘构建自己的文件系统，因为早期操作系统自带的文件系统在性能上较差，且写入不是同步的，进而导致数据库的ACID不能保证。不过现代操作系统自带的文件系统已经足够好，所以现代的数据库大部分构建在操作系统自带的文件系统之上）\n学生提问：link增加了了对于文件的一个引用，unlink减少了一个引用？\nFrans教授：是的。我们稍后会介绍更多相关的内容。\n学生提问：能介绍一下soft link和hard link吗？\nFrans教授：我今天不会讨论这些内容。但是你们将会在下一个File system lab中实现soft link。所以XV6本身实现了hard link，需要你们来实现soft link。\n学生提问：link是对inode做操作，而不是对文件描述符做操作，对吧？\nFrans教授：是的，link是对inode做操作，我们接下来介绍这部分内容。\n接下来我们看一下文件系统的结构。文件系统究竟维护了什么样的结构来实现前面介绍的API呢？\n首先，最重要的可能就是inode，这是代表一个文件的对象，并且它不依赖于文件名。\n实际上，inode是通过自身的编号来进行区分的，这里的编号就是个整数。\n所以文件系统内部通过一个数字，而不是通过文件路径名引用inode。\n同时，基于之前的讨论，inode必须有一个link count来跟踪指向这个inode的文件名的数量。\n一个文件（inode）只能在link count为0的时候被删除。\n实际的过程可能会更加复杂，实际中还有一个openfd count，也就是当前打开了文件的文件描述符计数。\n一个文件只能在这两个计数器都为0的时候才能被删除。\n同时基于之前的讨论，我们也知道write和read都没有针对文件的offset参数，所以文件描述符必然自己悄悄维护了对于文件的offset。\n文件系统中核心的数据结构就是inode和file descriptor。后者主要与用户进程进行交互。\n尽管文件系统的API很相近并且内部实现可能非常不一样。\n但是很多文件系统都有类似的结构。\n因为文件系统还挺复杂的，所以最好按照分层的方式进行理解。\n可以这样看：\n在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。 在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘。这里我们将磁盘中的数据保存在了内存中。 为了保证持久性，再往上通常会有一个logging层。许多文件系统都有某种形式的logging，我们下节课会讨论这部分内容，所以今天我就跳过它的介绍。 在logging层之上，XV6有inode cache，这主要是为了同步（synchronization），我们稍后会介绍。inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache。 再往上就是inode本身了。它实现了read/write。 再往上，就是文件名，和文件描述符操作。 不同的文件系统组织方式和每一层可能都略有不同，有的时候分层也没有那么严格，即使在XV6中分层也不是很严格，但是从概念上来说这里的结构对于理解文件系统还是有帮助的。\n实际上所有的文件系统都有组件对应这里不同的分层，例如buffer cache，logging，inode和路径名。\n文件系统使用磁盘 接下来，我将简单的介绍最底层，也即是存储设备。\n实际中有非常非常多不同类型的存储设备，这些设备的区别在于性能，容量，数据保存的期限等。\n其中两种最常见，并且你们应该也挺熟悉的是SSD和HDD。\n这两类存储虽然有着不同的性能，但是都在合理的成本上提供了大量的存储空间。\nSSD通常是0.1到1毫秒的访问时间，而HDD通常是在10毫秒量级完成读写一个disk block。\n这里有些术语有点让人困惑，它们是sectors和blocks。\nsector通常是磁盘驱动可以读写的最小单元，它过去通常是512字节。 block通常是操作系统或者文件系统视角的数据。它由文件系统定义，在XV6中它是1024字节。所以XV6中一个block对应两个sector。通常来说一个block对应了一个或者多个sector。 有的时候，人们也将磁盘上的sector称为block。所以这里的术语也不是很精确。\n这些存储设备连接到了电脑总线之上，总线也连接了CPU和内存。\n一个文件系统运行在CPU上，将内部的数据存储在内存，同时也会以读写block的形式存储在SSD或者HDD。\n这里的接口还是挺简单的，包括了read/write，然后以block编号作为参数。\n虽然我们这里描述的过于简单了，但是实际的接口大概就是这样。\n在内部，SSD和HDD工作方式完全不一样，但是对于硬件的抽象屏蔽了这些差异。\n磁盘驱动通常会使用一些标准的协议，例如PCIE，与磁盘交互。\n从上向下看磁盘驱动的接口，大部分的磁盘看起来都一样，你可以提供block编号，在驱动中通过写设备的控制寄存器，然后设备就会完成相应的工作。\n这是从一个文件系统的角度的描述。\n尽管不同的存储设备有着非常不一样的属性，从驱动的角度来看，你可以以大致相同的方式对它们进行编程。\n有关存储设备我们就说这么多。\n学生提问：对于read/write的接口，是不是提供了同步/异步的选项？\nFrans教授：你可以认为一个磁盘的驱动与console的驱动是基本一样的。\n驱动向设备发送一个命令表明开始读或者写，过了一会当设备完成了操作，会产生一个中断表明完成了相应的命令。\n但是因为磁盘本身比console复杂的多，所以磁盘的驱动也会比我们之前看过的console的驱动复杂的多。\n不过驱动中的代码结构还是类似的，也有bottom部分和top部分，中断和读写控制寄存器（注，详见lec09）。\n从文件系统的角度来看磁盘还是很直观的。\n因为对于磁盘就是读写block或者sector，我们可以将磁盘看作是一个巨大的block的数组，数组从0开始，一直增长到磁盘的最后。\n而文件系统的工作就是将所有的数据结构以一种能够在重启之后重新构建文件系统的方式，存放在磁盘上。虽然有不同的方式，但是XV6使用了一种非常简单，但是还挺常见的布局结构。\n通常来说：\nblock0要么没有用，要么被用作boot sector来启动操作系统。 block1通常被称为super block，它描述了文件系统。它可能包含磁盘上有多少个block共同构成了文件系统这样的信息。我们之后会看到XV6在里面会存更多的信息，你可以通过block1构造出大部分的文件系统信息。 在XV6中，log从block2开始，到block32结束。实际上log的大小可能不同，这里在super block中会定义log就是30个block。 接下来在block32到block45之间，XV6存储了inode。我之前说过多个inode会打包存在一个block中，一个inode是64字节。 之后是bitmap block，这是我们构建文件系统的默认方法，它只占据一个block。它记录了数据block是否空闲。 之后就全是数据block了，数据block存储了文件的内容和目录的内容。 通常来说，bitmap block，inode blocks和log blocks被统称为metadata block。\n它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据。\n学生提问：boot block是不是包含了操作系统启动的代码？\nFrans教授：完全正确，它里面通常包含了足够启动操作系统的代码。之后再从文件系统中加载操作系统的更多内容。\n学生提问：所以XV6是存储在虚拟磁盘上？\nFrans教授：在QEMU中，我们实际上走了捷径。QEMU中有个标志位-kernel，它指向了内核的镜像文件，QEMU会将这个镜像的内容加载到了物理内存的0x80000000。所以当我们使用QEMU时，我们不需要考虑boot sector。\n学生提问：所以当你运行QEMU时，你就是将程序通过命令行传入，然后直接就运行传入的程序，然后就不需要从虚拟磁盘上读取数据了？\nFrans教授：完全正确。\n假设inode是64字节，如果你想要读取inode10，那么你应该按照下面的公式去对应的block读取inode。\n$n_{block} = 32 + inode * 64 / 1024$\n所以inode0在block32，inode17会在block33。只要有inode的编号，我们总是可以找到inode在磁盘上存储的位置。\ninode 接下来我们看一下磁盘上存储的inode究竟是什么？首先我们前面已经看过了，这是一个64字节的数据结构。\n通常来说它有一个type字段，表明inode是文件还是目录。 nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。 size字段，表明了文件数据有多少个字节。 不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1，等等。XV6的inode中总共有12个block编号。这些被称为direct block number。这12个block编号指向了构成文件的前12个block。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。 之后还有一个indirect block number，它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。 以上基本就是XV6中inode的组成部分。\n基于上面的内容，XV6中文件最大的长度是多少呢？\n学生回答：会是268*1024字节\n是的，最大文件尺寸对应的是下面的公式。\n$size_{max} = (256 + 12) * 1024$\n可以算出这里就是268KB，这么点大小能存个什么呢？所以这是个很小的文件长度，实际的文件系统，文件最大的长度会大的多得多。那可以做一些什么来让文件系统支持大得多的文件呢？\n学生回答：可以扩展inode中indirect部分吗？\n是的，可以用类似page table的方式，构建一个双重indirect block number指向一个block，这个block中再包含了256个indirect block number，每一个又指向了包含256个block number的block。\n这样的话，最大的文件长度会大得多（注，是2562561K）。\n这里修改了inode的数据结构，你可以使用类似page table的树状结构，也可以按照B树或者其他更复杂的树结构实现。\nXV6这里极其简单，基本是按照最早的Uinx实现方式来的，不过你可以实现更复杂的结构。\n实际上，在接下来的File system lab中，你将会实现双重indirect block number来支持更大的文件。\n学生提问：为什么每个block存储256个block编号？\nFrans教授：因为每个编号是4个字节。1024/4 = 256。这又带出了一个问题，如果block编号只是4个字节，磁盘最大能有多大？是的，2的32次方（注，4TB）。有些磁盘比这个数字要大，所以通常人们会使用比32bit更长的数字来表示block编号。\n在下一个File system lab，你们需要将inode中的一个block number变成双重indirect block number，这个双重indirect block number将会指向一个包含了256个indirect block number的block，其中的每一个indirect block number再指向一个包含了256个block number的block，这样文件就可以大得多。\n接下来，我们想要实现read系统调用。\n假设我们需要读取文件的第8000个字节，那么你该读取哪个block呢？从inode的数据结构中该如何计算呢？\n对于8000，我们首先除以1024，也就是block的大小，得到大概是7。\n这意味着第7个block就包含了第8000个字节。\n所以直接在inode的direct block number中，就包含了第8000个字节的block。\n为了找到这个字节在第7个block的哪个位置，我们需要用8000对1024求余数，我猜结果是是832。\n所以为了读取文件的第8000个字节，文件系统查看inode，先用8000除以1024得到block number，然后再用8000对1024求余读取block中对应的字节。\n总结一下，inode中的信息完全足够用来实现read/write系统调用，至少可以找到哪个disk block需要用来执行read/write系统调用。\n接下来我们讨论一下目录（directory）。\n文件系统的酷炫特性就是层次化的命名空间（hierarchical namespace），你可以在文件系统中保存对用户友好的文件名。\n大部分Unix文件系统有趣的点在于，一个目录本质上是一个文件加上一些文件系统能够理解的结构。\n在XV6中，这里的结构极其简单。每一个目录包含了directory entries，每一条entry都有固定的格式：\n前2个字节包含了目录中文件或者子目录的inode编号， 接下来的14个字节包含了文件或者子目录名。 所以每个entry总共是16个字节。\n对于实现路径名查找，这里的信息就足够了。假设我们要查找路径名“/y/x”，我们该怎么做呢？\n从路径名我们知道，应该从root inode开始查找。\n通常root inode会有固定的inode编号，在XV6中，这个编号是1。\n我们该如何根据编号找到root inode呢？从前一节我们可以知道，inode从block 32开始，如果是inode1，那么必然在block 32中的64到128字节的位置。\n所以文件系统可以直接读到root inode的内容。\n对于路径名查找程序，接下来就是扫描root inode包含的所有block，以找到“y”。\n该怎么找到root inode所有对应的block呢？根据前一节的内容就是读取所有的direct block number和indirect block number。\n结果可能是找到了，也可能是没有找到。如果找到了，那么目录y也会有一个inode编号，假设是251，我们可以继续从inode 251查找，先读取inode 251的内容，之后再扫描inode所有对应的block，找到“x”并得到文件x对应的inode编号，最后将其作为路径名查找的结果返回。\n学生提问：有没有一些元数据表明当前的inode是目录而不是一个文件？\nFrans教授：有的，实际上是在inode中。inode中的type字段表明这是一个目录还是一个文件。如果你对一个类型是文件的inode进行查找，文件系统会返回错误。\n这里的结构不是很有效。\n为了找到一个目录名，你需要线性扫描。实际的文件系统会使用更复杂的数据结构来使得查找更快，当然这又是设计数据结构的问题，而不是设计操作系统的问题。\n你可以使用你喜欢的数据结构并提升性能。出于简单和更容易解释的目的，XV6使用了这里这种非常简单的数据结构。\nFile system工作示例 接下来我们看一下实际中，XV6的文件系统是如何工作的，这部分内容对于下一个lab是有帮助的。\n首先我会启动XV6，这里有件事情我想指出。启动XV6的过程中，调用了makefs指令，来创建一个文件系统。\n所以makefs创建了一个全新的磁盘镜像，在这个磁盘镜像中包含了我们在指令中传入的一些文件。makefs为你创建了一个包含这些文件的新的文件系统。\nXV6总是会打印文件系统的一些信息，所以从指令的下方可以看出有46个meta block，其中包括了：\nboot block super block 30个log block 13个inode block 1个bitmap block 之后是954个data block。所以这是一个袖珍级的文件系统，总共就包含了1000个block。在File system lab中，你们会去支持更大的文件系统。\n我还稍微修改了一下XV6，使得任何时候写入block都会打印出block的编号。我们从console的输出可以看出，在XV6启动过程中，会有一些对于文件系统的调用，并写入了block 33，45，32。\n接下来我们运行一些命令，来看一下特定的命令对哪些block做了写操作，并理解为什么要对这些block写入数据。\n我们通过echo “hi” \u0026gt; x，来创建一个文件x，并写入字符“hi”。我会将输出拷贝出来，并做分隔以方便我们更好的理解。\n这里会有几个阶段\n第一阶段是创建文件 第二阶段将“hi”写入文件 第三阶段将“\\n”换行符写入到文件 如果你去看echo的代码实现，基本就是这3个阶段。\n上面就是echo的代码，它先检查参数，并将参数写入到文件描述符1，在最后写入一个换行符。\n让我们一个阶段一个阶段的看echo的执行过程，并理解对于文件系统发生了什么。\n相比看代码，这里直接看磁盘的分布图更方便：\nwrite 33代表了写inode。\n看起来给我们分配的inode位于block 33。之所以有两个write 33，第一个是为了标记inode将要被使用。\n在XV6中，我记得是使用inode中的type字段来标识inode是否空闲，这个字段同时也会用来表示inode是一个文件还是一个目录。\n所以这里将inode的type从空闲改成了文件，并写入磁盘表示这个inode已经被使用了。第二个write 33就是实际的写入inode的内容。inode的内容会包含linkcount为1以及其他内容。\nwrite 46是向第一个data block写数据，那么这个data block属于谁呢？\n学生回答：属于根目录。\n是的，block 46是根目录的第一个block。为什么它需要被写入数据呢？\n学生回答：因为我们正在向根目录创建一个新文件。\n是的，这里我们向根目录增加了一个新的entry，其中包含了文件名x，以及我们刚刚分配的inode编号。\n接下来的write 32又是什么意思呢？block 32保存的仍然是inode，那么inode中的什么发生了变化使得需要将更新后的inode写入磁盘？是的，根目录的大小变了，因为我们刚刚添加了16个字节的entry来代表文件x的信息。\n最后又有一次write 33，我在稍后会介绍这次写入的内容，这里我们再次更新了文件x的inode， 尽管我们又还没有写入任何数据。\n以上就是第一阶段创建文件的过程。第二阶段是向文件写入“hi”。\n首先是write 45，这是更新bitmap。文件系统首先会扫描bitmap来找到一个还没有使用的data block，未被使用的data block对应bit 0。\n找到之后，文件系统需要将该bit设置为1，表示对应的data block已经被使用了。所以更新block 45是为了更新bitmap。\n接下来的两次write 595表明，文件系统挑选了data block 595。\n所以在文件x的inode中，第一个direct block number是595。因为写入了两个字符，所以write 595被调用了两次。\n第二阶段最后的write 33是更新文件x对应的inode中的size字段，因为现在文件x中有了两个字符。\n学生提问：block 595看起来在磁盘中很靠后了，是因为前面的block已经被系统内核占用了吗？\nFrans教授：我们可以看前面makefs指令，makefs存了很多文件在磁盘镜像中，这些都发生在创建文件x之前，所以磁盘中很大一部分已经被这些文件填满了。\n学生提问：第二阶段最后的write 33是否会将block 595与文件x的inode关联起来？\nFrans教授：会的。这里的write 33会发生几件事情：首先inode的size字段会更新；第一个direct block number会更新。这两个信息都会通过write 33一次更新到磁盘上的inode中。\n以上就是磁盘中文件系统的组织结构的核心，希望你们都能理解背后的原理。\nXV6创建inode代码展示 接下来我们通过查看XV6中的代码，更进一步的了解文件系统。因为我们前面已经分配了inode，我们先来看一下这是如何发生的。\nsysfile.c中包含了所有与文件系统相关的函数，分配inode发生在sys_open函数中，这个函数会负责创建文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 uint64 sys_open(void) { char path[MAXPATH]; int fd, omode; struct file *f; struct inode *ip; int n; if((n = argstr(0, path, MAXPATH)) \u0026lt; 0 || argint(1, \u0026amp;omode) \u0026lt; 0) return -1; begin_op(); if(omode \u0026amp; O_CREATE){ ip = create(path, T_FILE, 0, 0); if(ip == 0){ end_op(); return -1; } } else { if((ip = namei(path)) == 0){ end_op(); return -1; } ilock(ip); if(ip-\u0026gt;type == T_DIR \u0026amp;\u0026amp; omode != O_RDONLY){ iunlockput(ip); end_op(); return -1; } } if(ip-\u0026gt;type == T_DEVICE \u0026amp;\u0026amp; (ip-\u0026gt;major \u0026lt; 0 || ip-\u0026gt;major \u0026gt;= NDEV)){ iunlockput(ip); end_op(); return -1; } if((f = filealloc()) == 0 || (fd = fdalloc(f)) \u0026lt; 0){ if(f) fileclose(f); iunlockput(ip); end_op(); return -1; } if(ip-\u0026gt;type == T_DEVICE){ f-\u0026gt;type = FD_DEVICE; f-\u0026gt;major = ip-\u0026gt;major; } else { f-\u0026gt;type = FD_INODE; f-\u0026gt;off = 0; } f-\u0026gt;ip = ip; f-\u0026gt;readable = !(omode \u0026amp; O_WRONLY); f-\u0026gt;writable = (omode \u0026amp; O_WRONLY) || (omode \u0026amp; O_RDWR); if((omode \u0026amp; O_TRUNC) \u0026amp;\u0026amp; ip-\u0026gt;type == T_FILE){ itrunc(ip); } iunlock(ip); end_op(); return fd; } 在sys_open函数中，会调用create函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 static struct inode* create(char *path, short type, short major, short minor) { struct inode *ip, *dp; char name[DIRSIZ]; if((dp = nameiparent(path, name)) == 0) return 0; ilock(dp); if((ip = dirlookup(dp, name, 0)) != 0){ iunlockput(dp); ilock(ip); if(type == T_FILE \u0026amp;\u0026amp; (ip-\u0026gt;type == T_FILE || ip-\u0026gt;type == T_DEVICE)) return ip; iunlockput(ip); return 0; } if((ip = ialloc(dp-\u0026gt;dev, type)) == 0) panic(\u0026#34;create: ialloc\u0026#34;); ilock(ip); ip-\u0026gt;major = major; ip-\u0026gt;minor = minor; ip-\u0026gt;nlink = 1; iupdate(ip); if(type == T_DIR){ // Create . and .. entries. dp-\u0026gt;nlink++; // for \u0026#34;..\u0026#34; iupdate(dp); // No ip-\u0026gt;nlink++ for \u0026#34;.\u0026#34;: avoid cyclic ref count. if(dirlink(ip, \u0026#34;.\u0026#34;, ip-\u0026gt;inum) \u0026lt; 0 || dirlink(ip, \u0026#34;..\u0026#34;, dp-\u0026gt;inum) \u0026lt; 0) panic(\u0026#34;create dots\u0026#34;); } if(dirlink(dp, name, ip-\u0026gt;inum) \u0026lt; 0) panic(\u0026#34;create: dirlink\u0026#34;); iunlockput(dp); return ip; } create函数中首先会解析路径名并找到最后一个目录，之后会查看文件是否存在，如果存在的话会返回错误。之后就会调用ialloc（inode allocate），这个函数会为文件x分配inode。ialloc函数位于fs.c文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Allocate an inode on device dev. // Mark it as allocated by giving it type type. // Returns an unlocked but allocated and referenced inode. struct inode* ialloc(uint dev, short type) { int inum; struct buf *bp; struct dinode *dip; for(inum = 1; inum \u0026lt; sb.ninodes; inum++){ bp = bread(dev, IBLOCK(inum, sb)); dip = (struct dinode*)bp-\u0026gt;data + inum%IPB; if(dip-\u0026gt;type == 0){ // a free inode memset(dip, 0, sizeof(*dip)); dip-\u0026gt;type = type; log_write(bp); // mark it allocated on the disk brelse(bp); return iget(dev, inum); } brelse(bp); } panic(\u0026#34;ialloc: no inodes\u0026#34;); } 以上就是ialloc函数，与XV6中的大部分函数一样，它很简单，但是又不是很高效。\n它会遍历所有可能的inode编号，找到inode所在的block，再看位于block中的inode数据的type字段。\n如果这是一个空闲的inode，那么将其type字段设置为文件，这会将inode标记为已被分配。\n函数中的log_write就是我们之前看到在console中有关写block的输出。这里的log_write是我们看到的整个输出的第一个。\n以上就是第一次写磁盘涉及到的函数调用。\n这里有个有趣的问题，如果有多个进程同时调用create函数会发生什么？对于一个多核的计算机，进程可能并行运行，两个进程可能同时会调用到ialloc函数，然后进而调用bread（block read）函数。\n所以必须要有一些机制确保这两个进程不会互相影响。\n让我们看一下位于bio.c的buffer cache代码。首先看一下bread函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Return a locked buf with the contents of the indicated block. struct buf* bread(uint dev, uint blockno) { struct buf *b; b = bget(dev, blockno); if(!b-\u0026gt;valid) { virtio_disk_rw(b, 0); b-\u0026gt;valid = 1; } return b; } bread函数首先会调用bget函数，bget会为我们从buffer cache中找到block的缓存。让我们看一下bget函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Look through buffer cache for block on device dev. // If not found, allocate a buffer. // In either case, return locked buffer. static struct buf* bget(uint dev, uint blockno) { struct buf *b; acquire(\u0026amp;bcache.lock); // Is the block already cached? for(b = bcache.head.next; b != \u0026amp;bcache.head; b = b-\u0026gt;next){ if(b-\u0026gt;dev == dev \u0026amp;\u0026amp; b-\u0026gt;blockno == blockno){ b-\u0026gt;refcnt++; release(\u0026amp;bcache.lock); acquiresleep(\u0026amp;b-\u0026gt;lock); return b; } } // Not cached. // Recycle the least recently used (LRU) unused buffer. for(b = bcache.head.prev; b != \u0026amp;bcache.head; b = b-\u0026gt;prev){ if(b-\u0026gt;refcnt == 0) { b-\u0026gt;dev = dev; b-\u0026gt;blockno = blockno; b-\u0026gt;valid = 0; b-\u0026gt;refcnt = 1; release(\u0026amp;bcache.lock); acquiresleep(\u0026amp;b-\u0026gt;lock); return b; } } panic(\u0026#34;bget: no buffers\u0026#34;); } 这里的代码还有点复杂。我猜你们之前已经看过这里的代码，那么这里的代码在干嘛？\n学生回答：这里遍历了linked-list，来看看现有的cache是否符合要找的block。\n是的，我们这里看一下block 33的cache是否存在，如果存在的话，将block对象的引用计数（refcnt）加1，之后再释放bcache锁，因为现在我们已经完成了对于cache的检查并找到了block cache。之后，代码会尝试获取block cache的锁。\n所以，如果有多个进程同时调用bget的话，其中一个可以获取bcache的锁并扫描buffer cache。\n此时，其他进程是没有办法修改buffer cache的（注，因为bacche的锁被占住了）。\n之后，进程会查找block number是否在cache中，如果在的话将block cache的引用计数加1，表明当前进程对block cache有引用，之后再释放bcache的锁。\n如果有第二个进程也想扫描buffer cache，那么这时它就可以获取bcache的锁。\n假设第二个进程也要获取block 33的cache，那么它也会对相应的block cache的引用计数加1。最后这两个进程都会尝试对block 33的block cache调用acquiresleep函数。\nacquiresleep是另一种锁，我们称之为sleep lock，本质上来说它获取block 33 cache的锁。\n其中一个进程获取锁之后函数返回。\n在ialloc函数中会扫描block 33中是否有一个空闲的inode。\n而另一个进程会在acquiresleep中等待第一个进程释放锁。\n学生提问：当一个block cache的refcnt不为0时，可以更新block cache吗？因为释放锁之后，可能会修改block cache。\nFrans教授：这里我想说几点；首先XV6中对bcache做任何修改的话，都必须持有bcache的锁；其次对block 33的cache做任何修改你需要持有block 33的sleep lock。所以在任何时候，release(\u0026amp;bcache.lock)之后，b-\u0026gt;refcnt都大于0。block的cache只会在refcnt为0的时候才会被驱逐，任何时候refcnt大于0都不会驱逐block cache。所以当b-\u0026gt;refcnt大于0的时候，block cache本身不会被buffer cache修改。这里的第二个锁，也就是block cache的sleep lock，是用来保护block cache的内容的。它确保了任何时候只有一个进程可以读写block cache。\n如果buffer cache中有两份block 33的cache将会出现问题。假设一个进程要更新inode19，另一个进程要更新inode20。\n如果它们都在处理block 33的cache，并且cache有两份，那么第一个进程可能持有一份cache并先将inode19写回到磁盘中，而另一个进程持有另一份cache会将inode20写回到磁盘中，并将inode19的更新覆盖掉。\n所以一个block只能在buffer cache中出现一次。你们在完成File system lab时，必须要维持buffer cache的这个属性。\n学生提问：如果多个进程都在使用同一个block的cache，然后有一个进程在修改block，并通过强制向磁盘写数据修改了block的cache，那么其他进程会看到什么结果？\nFrans教授：如果第一个进程结束了对block 33的读写操作，它会对block的cache调用brelse（block cache release）函数。\n这个函数会对refcnt减1，并释放sleep lock。这意味着，如果有任何一个其他进程正在等待使用这个block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。\n假设两个进程都需要分配一个新的inode，且新的inode都位于block 33。如果第一个进程分配到了inode18并完成了更新，那么它对于inode18的更新是可见的。另一个进程就只能分配到inode19，因为inode18已经被标记为已使用，任何之后的进程都可以看到第一个进程对它的更新。\n这正是我们想看到的结果，如果一个进程创建了一个inode或者创建了一个文件，之后的进程执行读就应该看到那个文件。\nSleep Lock block cache使用的是sleep lock。sleep lock区别于一个常规的spinlock。我们先看来一下sleep lock。\n1 2 3 4 5 6 7 8 9 10 11 void acquiresleep(struct sleeplock *lk) { acquire(\u0026amp;lk-\u0026gt;lk); while (lk-\u0026gt;locked) { sleep(lk, \u0026amp;lk-\u0026gt;lk); } lk-\u0026gt;locked = 1; lk-\u0026gt;pid = myproc()-\u0026gt;pid; release(\u0026amp;lk-\u0026gt;lk); } 首先是acquiresleep函数，它用来获取sleep lock。\n函数里首先获取了一个普通的spinlock，这是与sleep lock关联在一起的一个锁。\n之后，如果sleep lock被持有，那么就进入sleep状态，并将自己从当前CPU调度开。\n既然sleep lock是基于spinlock实现的，为什么对于block cache，我们使用的是sleep lock而不是spinlock？\n这里其实有多种原因。对于spinlock有很多限制，其中之一是加锁时中断必须要关闭。\n所以如果使用spinlock的话，当我们对block cache做操作的时候需要持有锁，那么我们就永远也不能从磁盘收到数据。\n或许另一个CPU核可以收到中断并读到磁盘数据，但是如果我们只有一个CPU核的话，我们就永远也读不到数据了。\n出于同样的原因，也不能在持有spinlock的时候进入sleep状态（注，详见13.1）。\n所以这里我们使用sleep lock。\nsleep lock的优势就是，我们可以在持有锁的时候不关闭中断。\n我们可以在磁盘操作的过程中持有锁，我们也可以长时间持有锁。\n当我们在等待sleep lock的时候，我们并没有让CPU一直空转，我们通过sleep将CPU出让出去了。\n接下来让我们看一下brelease函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Release a locked buffer. // Move to the head of the most-recently-used list. void brelse(struct buf *b) { if(!holdingsleep(\u0026amp;b-\u0026gt;lock)) panic(\u0026#34;brelse\u0026#34;); releasesleep(\u0026amp;b-\u0026gt;lock); acquire(\u0026amp;bcache.lock); b-\u0026gt;refcnt--; if (b-\u0026gt;refcnt == 0) { // no one is waiting for it. b-\u0026gt;next-\u0026gt;prev = b-\u0026gt;prev; b-\u0026gt;prev-\u0026gt;next = b-\u0026gt;next; b-\u0026gt;next = bcache.head.next; b-\u0026gt;prev = \u0026amp;bcache.head; bcache.head.next-\u0026gt;prev = b; bcache.head.next = b; } release(\u0026amp;bcache.lock); } brelease函数中首先释放了sleep lock；之后获取了bcache的锁；之后减少了block cache的引用计数，表明一个进程不再对block cache感兴趣；最后如果引用计数为0，那么它会修改buffer cache的linked-list，将block cache移到linked-list的头部，这样表示这个block cache是最近使用过的block cache。\n这一点很重要，当我们在bget函数中不能找到block cache时，我们需要在buffer cache中腾出空间来存放新的block cache，这时会使用LRU（Least Recent Used）算法找出最不常使用的block cache，并撤回它（注，而将刚刚使用过的block cache放在linked-list的头部就可以直接更新linked-list的tail来完成LRU操作）。\n为什么这是一个好的策略呢？因为通常系统都遵循temporal locality策略，也就是说如果一个block cache最近被使用过，那么很有可能它很快会再被使用，所以最好不要撤回这样的block cache。\n以上就是对于block cache代码的介绍。这里有几件事情需要注意：\n首先在内存中，对于一个block只能有一份缓存。这是block cache必须维护的特性。 其次，这里使用了与之前的spinlock略微不同的sleep lock。与spinlock不同的是，可以在I/O操作的过程中持有sleep lock。 第三，它采用了LRU作为cache替换策略。 第四，它有两层锁。第一层锁用来保护buffer cache的内部数据；第二层锁也就是sleep lock用来保护单个block的cache。 最后让我们来总结一下，并把剩下的内容留到下节课。\n首先，文件系统是一个位于磁盘的数据结构。我们今天的主要时间都用来介绍这个位于磁盘的数据结构的内容。XV6的这个数据结构实现的很简单，但是你可以实现一个更加复杂的数据结构。 其次，我们花了一些时间来看block cache的实现，这对于性能来说是至关重要的，因为读写磁盘是代价较高的操作，可能要消耗数百毫秒，而block cache确保了如果我们最近从磁盘读取了一个block，那么我们将不会再从磁盘读取相同的block。 下节课我将会介绍crash safety，这是文件系统设计中非常棒的一部分。我们将会在crash safety讲两节课。下节课我们会看到基于log实现的crash safety机制，下下节课我们会看到Linux的ext3是如何实现的logging，这种方式要快得多。\n学生提问：我有个关于brelease函数的问题，看起来它先释放了block cache的锁，然后再对引用计数refcnt减一，为什么可以这样呢？\nFrans教授：这是个好问题。如果我们释放了sleep lock，这时另一个进程正在等待锁，那么refcnt必然大于1，而b-\u0026gt;refcnt \u0026ndash;只是表明当前执行brelease的进程不再关心block cache。如果还有其他进程正在等待锁，那么refcnt必然不等于0，我们也必然不会执行if(b-\u0026gt;refcnt == 0)中的代码。\n","date":"2024-09-28T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08114file-systems/bg_hu670494933101605340.jpg","permalink":"https://echudet.github.io/p/mit6s08114file-systems/","title":"MIT6S081：14File Systems"},{"content":"Lecture 13 Sleep \u0026amp; Wake up 预习 sleep与wakeup 调度和锁有助于隐藏一个进程对另一个进程的存在，但到目前为止，我们还没有帮助进程进行有意交互的抽象。\n为解决这个问题已经发明了许多机制。\nXv6使用了一种称为sleep和wakeup的方法，它允许一个进程在等待事件时休眠，而另一个进程在事件发生后将其唤醒。\n睡眠和唤醒通常被称为序列协调（sequence coordination）或条件同步机制（conditional synchronization mechanisms）。\n为了说明，让我们考虑一个称为信号量（semaphore）的同步机制，它可以协调生产者和消费者。\n信号量维护一个计数并提供两个操作。\n“V”操作（对于生产者）增加计数。“P”操作（对于使用者）等待计数为非零，然后递减并返回。\n如果只有一个生产者线程和一个消费者线程，并且它们在不同的CPU上执行，并且编译器没有进行过积极的优化，那么此实现将是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 struct semaphore { struct spinlock lock; int count; }; void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { while (s-\u0026gt;count == 0) ; acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } 上面的实现代价昂贵。\n如果生产者很少采取行动，消费者将把大部分时间花在while循环中，希望得到非零计数。\n消费者的CPU可以找到比通过反复轮询s-\u0026gt;count繁忙等待更有成效的工作。\n避免繁忙等待，消费者需要一种方法来释放CPU，并且只有在V增加计数后才能恢复。\n这是朝着这个方向迈出的一步，尽管我们将看到这是不够的。\n让我们想象一对调用，sleep和wakeup，工作流程如下。\nSleep(chan)在任意值chan上睡眠，称为等待通道（wait channel）。\nSleep将调用进程置于睡眠状态，释放CPU用于其他工作。\nWakeup(chan)唤醒所有在chan上睡眠的进程（如果有），使其sleep调用返回。\n如果没有进程在chan上等待，则wakeup不执行任何操作。\n我们可以将信号量实现更改为使用sleep和wakeup（更改的行添加了注释）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); // !pay attention release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { while (s-\u0026gt;count == 0) sleep(s); // !pay attention acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } P现在放弃CPU而不是自旋，这很好。\n然而，事实证明，使用此接口设计sleep和wakeup而不遭受所谓的丢失唤醒（lost wake-up）问题并非易事。\n假设P在第9行发现s-\u0026gt;count==0。\n当P在第9行和第10行之间时，V在另一个CPU上运行：它将s-\u0026gt;count更改为非零，并调用wakeup，这样就不会发现进程处于休眠状态，因此不会执行任何操作。\n现在P继续在第10行执行：它调用sleep并进入睡眠。这会导致一个问题：P正在休眠，等待调用V，而V已经被调用。\n除非我们运气好，生产者再次呼叫V，否则消费者将永远等待，即使count为非零。\n这个问题的根源是V在错误的时刻运行，违反了P仅在s-\u0026gt;count==0时才休眠的不变量。\n保护不变量的一种不正确的方法是将锁的获取（下面以黄色突出显示）移动到P中，以便其检查count和调用sleep是原子的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); // !pay attention while (s-\u0026gt;count == 0) sleep(s); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } 人们可能希望这个版本的P能够避免丢失唤醒，因为锁阻止V在第10行和第11行之间执行。\n它确实这样做了，但它会导致死锁：P在睡眠时持有锁，因此V将永远阻塞等待锁。\n我们将通过更改sleep的接口来修复前面的方案：调用方必须将条件锁（condition lock）传递给sleep，以便在调用进程被标记为asleep并在睡眠通道上等待后sleep可以释放锁。\n如果有一个并发的V操作，锁将强制它在P将自己置于睡眠状态前一直等待，因此wakeup将找到睡眠的消费者并将其唤醒。\n一旦消费者再次醒来，sleep会在返回前重新获得锁。我们新的正确的sleep/wakeup方案可用如下（更改以黄色突出显示）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); while (s-\u0026gt;count == 0) sleep(s, \u0026amp;s-\u0026gt;lock); // !pay attention s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } P持有s-\u0026gt;lock的事实阻止V在P检查s-\u0026gt;count和调用sleep之间试图唤醒它。然而请注意，我们需要sleep释放s-\u0026gt;lock并使消费者进程进入睡眠状态的操作是原子的。\n代码：sleep与wakeup 让我们看看sleep（kernel/proc.c:548）和wakeup（kernel/proc.c:582）的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Atomically release lock and sleep on chan. // Reacquires lock when awakened. void sleep(void *chan, struct spinlock *lk) { struct proc *p = myproc(); // Must acquire p-\u0026gt;lock in order to // change p-\u0026gt;state and then call sched. // Once we hold p-\u0026gt;lock, we can be // guaranteed that we won\u0026#39;t miss any wakeup // (wakeup locks p-\u0026gt;lock), // so it\u0026#39;s okay to release lk. if(lk != \u0026amp;p-\u0026gt;lock){ //DOC: sleeplock0 acquire(\u0026amp;p-\u0026gt;lock); //DOC: sleeplock1 release(lk); } // Go to sleep. p-\u0026gt;chan = chan; p-\u0026gt;state = SLEEPING; sched(); // Tidy up. p-\u0026gt;chan = 0; // Reacquire original lock. if(lk != \u0026amp;p-\u0026gt;lock){ release(\u0026amp;p-\u0026gt;lock); acquire(lk); } } // Wake up all processes sleeping on chan. // Must be called without any p-\u0026gt;lock. void wakeup(void *chan) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++) { acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state == SLEEPING \u0026amp;\u0026amp; p-\u0026gt;chan == chan) { p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); } } 其基本思想是让sleep将当前进程标记为SLEEPING，然后调用sched释放CPU；\nwakeup查找在给定等待通道上休眠的进程，并将其标记为RUNNABLE。\nsleep和wakeup的调用者可以使用任何相互间方便的数字作为通道。\nXv6通常使用等待过程中涉及的内核数据结构的地址。\nsleep获得p-\u0026gt;lock（kernel/proc.c:559）。\n要进入睡眠的进程现在同时持有p-\u0026gt;lock和lk。\n在调用者（示例中为P）中持有lk是必要的：它确保没有其他进程（在示例中指一个运行的V）可以启动wakeup(chan)调用。\n既然sleep持有p-\u0026gt;lock，那么释放lk是安全的：其他进程可能会启动对wakeup(chan)的调用，但是wakeup将等待获取p-\u0026gt;lock，因此将等待sleep把进程置于睡眠状态的完成，以防止wakeup错过sleep。\n还有一个小问题：如果lk和p-\u0026gt;lock是同一个锁，那么如果sleep试图获取p-\u0026gt;lock就会自身死锁。\n但是，如果调用sleep的进程已经持有p-\u0026gt;lock，那么它不需要做更多的事情来避免错过并发的wakeup。\n当wait（kernel/proc.c:582）持有p-\u0026gt;lock调用sleep时，就会出现这种情况。\n由于sleep只持有p-\u0026gt;lock而无其他，它可以通过记录睡眠通道、将进程状态更改为SLEEPING并调用sched（kernel/proc.c:564-567）将进程置于睡眠状态。\n过一会儿，我们就会明白为什么在进程被标记为SLEEPING之前不将p-\u0026gt;lock释放（由scheduler）是至关重要的。\n在某个时刻，一个进程将获取条件锁，设置睡眠者正在等待的条件，并调用wakeup(chan)。\n在持有状态锁时调用wakeup非常重要。\n严格地说，wakeup只需跟在acquire之后就足够了（也就是说，可以在release之后调用wakeup）\nwakeup遍历进程表（kernel/proc.c:582）。\n它获取它所检查的每个进程的p-\u0026gt;lock，这既是因为它可能会操纵该进程的状态，也是因为p-\u0026gt;lock确保sleep和wakeup不会彼此错过。\n当wakeup发现一个SLEEPING的进程且chan相匹配时，它会将该进程的状态更改为RUNNABLE。调度器下次运行时，将看到进程已准备好运行。\n为什么sleep和wakeup的用锁规则能确保睡眠进程不会错过唤醒？\n休眠进程从检查条件之前的某处到标记为休眠之后的某处，要么持有条件锁，要么持有其自身的p-\u0026gt;lock或同时持有两者。\n调用wakeup的进程在wakeup的循环中同时持有这两个锁。\n因此，要么唤醒器（waker）在消费者线程检查条件之前使条件为真；要么唤醒器的wakeup在睡眠线程标记为SLEEPING后对其进行严格检查。\n然后wakeup将看到睡眠进程并将其唤醒（除非有其他东西首先将其唤醒）。\n有时，多个进程在同一个通道上睡眠；例如，多个进程读取同一个管道。\n一个单独的wakeup调用就能把他们全部唤醒。\n其中一个将首先运行并获取与sleep一同调用的锁，并且（在管道例子中）读取在管道中等待的任何数据。\n尽管被唤醒，其他进程将发现没有要读取的数据。从他们的角度来看，醒来是“虚假的”，他们必须再次睡眠。\n因此，在检查条件的循环中总是调用sleep。\n如果两次使用sleep/wakeup时意外选择了相同的通道，则不会造成任何伤害：它们将看到虚假的唤醒，但如上所述的循环将容忍此问题。\nsleep/wakeup的魅力在于它既轻量级（不需要创建特殊的数据结构来充当睡眠通道），又提供了一层抽象（调用者不需要知道他们正在与哪个特定进程进行交互）。\n代码：Pipes 使用睡眠和唤醒来同步生产者和消费者的一个更复杂的例子是xv6的管道实现。\n在第1章中看到了管道接口：写入管道一端的字节被复制到内核缓冲区，然后可以从管道的另一端读取。\n以后的章节将研究围绕管道的文件描述符支持，但现在让我们看看pipewrite和piperead的实现。\n每个管道都由一个struct pipe表示，其中包含一个锁lock和一个数据缓冲区data。\n1 2 3 4 5 6 7 8 struct pipe { struct spinlock lock; char data[PIPESIZE]; uint nread; // number of bytes read uint nwrite; // number of bytes written int readopen; // read fd is still open int writeopen; // write fd is still open }; 字段nread和nwrite统计从缓冲区读取和写入缓冲区的总字节数。\n缓冲区是环形的：在buf[PIPESIZE-1]之后写入的下一个字节是buf[0]。\n而计数不是环形。此约定允许实现区分完整缓冲区（nwrite==nread+PIPESIZE）和空缓冲区（nwrite==nread），但这意味着对缓冲区的索引必须使用buf[nread%PIPESIZE]，而不仅仅是buf[nread]（对于nwrite也是如此）。\n让我们假设对piperead和pipewrite的调用同时发生在两个不同的CPU上。\nPipewrite（kernel/pipe.c:77）从获取管道锁开始，它保护计数、数据及其相关不变量。\nPiperead（kernel/pipe.c:103）然后也尝试获取锁，但无法实现。它在acquire（kernel/spinlock.c:22）中旋转等待锁。\n当piperead等待时，pipewrite遍历被写入的字节（addr[0..n-1]），依次将每个字节添加到管道中（kernel/pipe.c:95）。\n在这个循环中缓冲区可能会被填满（kernel/pipe.c:85）。\n在这种情况下，pipewrite调用wakeup来提醒所有处于睡眠状态的读进程缓冲区中有数据等待，然后在\u0026amp;pi-\u0026gt;nwrite上睡眠，等待读进程从缓冲区中取出一些字节。\n作为使pipewrite进程进入睡眠状态的一部分，Sleep释放pi-\u0026gt;lock。\n现在pi-\u0026gt;lock可用，piperead设法获取它并进入其临界区域：它发现pi-\u0026gt;nread != pi-\u0026gt;nwrite（kernel/pipe.c:110）（pipewrite进入睡眠状态是因为pi-\u0026gt;nwrite == pi-\u0026gt;nread+PIPESIZE（kernel/pipe.c:85））。\n因此它进入for循环，从管道中复制数据（kernel/pipe.c:117），并根据复制的字节数增加nread。\n那些读出的字节就可供写入，因此piperead调用wakeup（kernel/pipe.c:124）返回之前唤醒所有休眠的写进程。\nWakeup寻找一个在\u0026amp;pi-\u0026gt;nwrite上休眠的进程，该进程正在运行pipewrite，但在缓冲区填满时停止。它将该进程标记为RUNNABLE。\n管道代码为读者和写者使用单独的睡眠通道（pi-\u0026gt;nread和pi-\u0026gt;nwrite）；这可能会使系统在有许多读者和写者等待同一管道这种不太可能的情况下更加高效。\n管道代码在检查休眠条件的循环中休眠；如果有多个读者或写者，那么除了第一个醒来的进程之外，所有进程都会看到条件仍然错误，并再次睡眠。\n代码：wait, exit和kill Sleep和wakeup可用于多种等待。\n第一章介绍的一个有趣的例子是子进程exit和父进程wait之间的交互。\n在子进程死亡时，父进程可能已经在wait中休眠，或者正在做其他事情；在后一种情况下，随后的wait调用必须观察到子进程的死亡，可能是在子进程调用exit后很久。\nxv6记录子进程终止直到wait观察到它的方式是让exit将调用方置于ZOMBIE状态，在那里它一直保持到父进程的wait注意到它，将子进程的状态更改为UNUSED，复制子进程的exit状态码，并将子进程ID返回给父进程。\n如果父进程在子进程之前退出，则父进程将子进程交给init进程，init进程将永久调用wait；因此，每个子进程退出后都有一个父进程进行清理。\n主要的实现挑战是父级和子级wait和exit，以及exit和exit之间可能存在竞争和死锁。\nWait使用调用进程的p-\u0026gt;lock作为条件锁，以避免丢失唤醒，并在开始时获取该锁（kernel/proc.c:398）。\n然后它扫描进程表。如果它发现一个子进程处于ZOMBIE状态，它将释放该子进程的资源及其proc结构体，将该子进程的退出状态码复制到提供给wait的地址（如果不是0），并返回该子进程的进程ID。\n如果wait找到子进程但没有子进程退出，它将调用sleep以等待其中一个退出（kernel/proc.c:445），然后再次扫描。\n这里，sleep中释放的条件锁是等待进程的p-\u0026gt;lock，这是上面提到的特例。\n注意，wait通常持有两个锁：它在试图获得任何子进程的锁之前先获得自己的锁；因此，整个xv6都必须遵守相同的锁定顺序（父级，然后是子级），以避免死锁。\nWait查看每个进程的np-\u0026gt;parent以查找其子进程。它使用np-\u0026gt;parent而不持有np-\u0026gt;lock，这违反了通常的规则，即共享变量必须受到锁的保护。\nnp可能是当前进程的祖先，在这种情况下，获取np-\u0026gt;lock可能会导致死锁，因为这将违反上述顺序。\n这种情况下无锁检查np-\u0026gt;parent似乎是安全的：进程的parent字段仅由其父进程更改，因此如果np-\u0026gt;parent==p为true，除非当前流程更改它，否则该值无法被更改，\nExit（*kernel/proc.c*:333）记录退出状态码，释放一些资源，将所有子进程提供给init进程，在父进程处于等待状态时唤醒父进程，将调用方标记为僵尸进程（zombie），并永久地让出CPU。\n最后的顺序有点棘手。\n退出进程必须在将其状态设置为ZOMBIE并唤醒父进程时持有其父进程的锁，因为父进程的锁是防止在wait中丢失唤醒的条件锁。\n子级还必须持有自己的p-\u0026gt;lock，否则父级可能会看到它处于ZOMBIE状态，并在它仍运行时释放它。\n锁获取顺序对于避免死锁很重要：因为wait先获取父锁再获取子锁，所以exit必须使用相同的顺序。\nExit调用一个专门的唤醒函数wakeup1，该函数仅唤醒父进程，且父进程必须正在wait中休眠（kernel/proc.c:598）。\n在将自身状态设置为ZOMBIE之前，子进程唤醒父进程可能看起来不正确，但这是安全的：\n虽然wakeup1可能会导致父进程运行，但wait中的循环在scheduler释放子进程的p-\u0026gt;lock之前无法检查子进程，所以wait在exit将其状态设置为ZOMBIE（kernel/proc.c:386）之前不能查看退出进程。\nexit允许进程自行终止，而kill（kernel/proc.c:611）允许一个进程请求另一个进程终止。\n对于kill来说，直接销毁受害者进程（即要杀死的进程）太复杂了，因为受害者可能在另一个CPU上执行，也许是在更新内核数据结构的敏感序列中间。\n因此，kill的工作量很小：它只是设置受害者的p-\u0026gt;killed，如果它正在睡眠，则唤醒它。\n受害者进程终将进入或离开内核，此时，如果设置了p-\u0026gt;killed，usertrap中的代码将调用exit。\n如果受害者在用户空间中运行，它将很快通过进行系统调用或由于计时器（或其他设备）中断而进入内核。\n如果受害者进程在sleep中，kill对wakeup的调用将导致受害者从sleep中返回。\n这存在潜在的危险，因为等待的条件可能不为真。\n但是，xv6对sleep的调用总是封装在while循环中，该循环在sleep返回后重新测试条件。\n一些对sleep的调用还在循环中测试p-\u0026gt;killed，如果它被设置，则放弃当前活动。\n只有在这种放弃是正确的情况下才能这样做。\n例如，如果设置了killed标志，则管道读写代码返回；最终代码将返回到陷阱，陷阱将再次检查标志并退出。\n一些XV6的sleep循环不检查p-\u0026gt;killed，因为代码在应该是原子操作的多步系统调用的中间。\nvirtio驱动程序（kernel/virtio_disk.c:242）就是一个例子：它不检查p-\u0026gt;killed，因为一个磁盘操作可能是文件系统保持正确状态所需的一组写入操作之一。\n等待磁盘I/O时被杀死的进程将不会退出，直到它完成当前系统调用并且usertrap看到killed标志\n真实世界 xv6调度器实现了一个简单的调度策略：它依次运行每个进程。这一策略被称为轮询调度（round robin）。\n真实的操作系统实施更复杂的策略，例如，允许进程具有优先级。其思想是调度器将优先选择可运行的高优先级进程，而不是可运行的低优先级进程。\n这些策略可能变得很复杂，因为常常存在相互竞争的目标：例如，操作系统可能希望保证公平性和高吞吐量。\n此外，复杂的策略可能会导致意外的交互，例如优先级反转（priority inversion）和航队（convoys）。\n当低优先级进程和高优先级进程共享一个锁时，可能会发生优先级反转，当低优先级进程持有该锁时，可能会阻止高优先级进程前进。\n当许多高优先级进程正在等待一个获得共享锁的低优先级进程时，可能会形成一个长的等待进程航队；一旦航队形成，它可以持续很长时间。\n为了避免此类问题，在复杂的调度器中需要额外的机制。\n睡眠和唤醒是一种简单有效的同步方法，但还有很多其他方法。\n所有这些问题中的第一个挑战是避免我们在本章开头看到的“丢失唤醒”问题。\n原始Unix内核的sleep只是禁用了中断，这就足够了，因为Unix运行在单CPU系统上。\n因为xv6在多处理器上运行，所以它为sleep添加了一个显式锁。\nFreeBSD的msleep采用了同样的方法。\nPlan 9的sleep使用一个回调函数，该函数在马上睡眠时获取调度锁，并在运行中持有；该函数用于在最后时刻检查睡眠条件，以避免丢失唤醒。\nLinux内核的sleep使用一个显式的进程队列，称为等待队列，而不是等待通道；队列有自己内部的锁。\n在wakeup中扫描整个进程列表以查找具有匹配chan的进程效率低下。\n一个更好的解决方案是用一个数据结构替换sleep和wakeup中的chan，该数据结构包含在该结构上休眠的进程列表，例如Linux的等待队列。\nPlan 9的sleep和wakeup将该结构称为集结点（rendezvous point）或Rendez。\n许多线程库引用与条件变量相同的结构；在这种情况下，sleep和wakeup操作称为wait和signal。\n所有这些机制都有一个共同的特点：睡眠条件受到某种在睡眠过程中原子级释放的锁的保护。\nwakeup的实现会唤醒在特定通道上等待的所有进程，可能有许多进程在等待该特定通道。\n操作系统将安排所有这些进程，它们将竞相检查睡眠条件。\n进程的这种行为有时被称为惊群效应（thundering herd），最好避免。\n大多数条件变量都有两个用于唤醒的原语：signal用于唤醒一个进程；broadcast用于唤醒所有等待进程。\n信号量（Semaphores）通常用于同步。\n计数count通常对应于管道缓冲区中可用的字节数或进程具有的僵尸子进程数。\n使用显式计数作为抽象的一部分可以避免“丢失唤醒”问题：使用显式计数记录已经发生wakeup的次数。计数还避免了虚假唤醒和惊群效应问题。\n终止进程并清理它们在xv6中引入了很多复杂性。\n在大多数操作系统中甚至更复杂，因为，例如，受害者进程可能在内核深处休眠，而展开其栈空间需要非常仔细的编程。\n许多操作系统使用显式异常处理机制（如longjmp）来展开栈。\n此外，还有其他事件可能导致睡眠进程被唤醒，即使它等待的事件尚未发生。\n例如，当一个Unix进程处于休眠状态时，另一个进程可能会向它发送一个signal。\n在这种情况下，进程将从中断的系统调用返回，返回值为-1，错误代码设置为EINTR。\n应用程序可以检查这些值并决定执行什么操作。\nXv6不支持信号，因此不会出现这种复杂性。\nXv6对kill的支持并不完全令人满意：有一些sleep循环可能应该检查p-\u0026gt;killed。\n一个相关的问题是，即使对于检查p-\u0026gt;killed的sleep循环，sleep和kill之间也存在竞争；后者可能会设置p-\u0026gt;killed，并试图在受害者的循环检查p-\u0026gt;killed之后但在调用sleep之前尝试唤醒受害者。\n如果出现此问题，受害者将不会注意到p-\u0026gt;killed，直到其等待的条件发生。\n这可能比正常情况要晚一点（例如，当virtio驱动程序返回受害者正在等待的磁盘块时）或永远不会发生（例如，如果受害者正在等待来自控制台的输入，但用户没有键入任何输入）。\n注：上节中说到kill的工作方式，kill设置p-\u0026gt;killed，如果遇到进程正在休眠，则会唤醒它，此后在usertrap中检测p-\u0026gt;killed，并使进程退出\n而如果像上面说的，在检查p-\u0026gt;killed之后调用sleep之前唤醒受害者进程，那么接下来执行sleep就会导致进程无法进入内核，无法在usertrap中退出，而必须等待所需事件的发生再次唤醒\n一个实际的操作系统将在固定时间内使用空闲列表找到自由的proc结构体，而不是allocproc中的线性时间搜索；xv6使用线性扫描是为了简单起见。\n正课 线程切换过程中锁的限制 在XV6中，任何时候调用switch函数都会从一个线程切换到另一个线程，通常是在用户进程的内核线程和调度器线程之间切换。\n在调用switch函数之前，总是会先获取线程对应的用户进程的锁。\n所以过程是这样，一个进程先获取自己的锁，然后调用switch函数切换到调度器线程，调度器线程再释放进程锁。\n实际上的代码顺序更像这样：\n一个进程出于某种原因想要进入休眠状态，比如说出让CPU或者等待数据，它会先获取自己的锁； 之后进程将自己的状态从RUNNING设置为RUNNABLE； 之后进程调用switch函数，其实是调用sched函数在sched函数中再调用的switch函数； switch函数将当前的线程切换到调度器线程； 调度器线程之前也调用了switch函数，现在恢复执行会从自己的switch函数返回； 返回之后，调度器线程会释放刚刚出让了CPU的进程的锁 在第1步中获取进程的锁的原因是，这样可以阻止其他CPU核的调度器线程在当前进程完成切换前，发现进程是RUNNABLE的状态并尝试运行它。\n为什么要阻止呢？因为其他每一个CPU核都有一个调度器线程在遍历进程表单，如果没有在进程切换的最开始就获取进程的锁的话，其他CPU核就有可能在当前进程还在运行时，认为该进程是RUNNABLE并运行它。\n而两个CPU核使用同一个栈运行同一个线程会使得系统立即崩溃。\n所以，在进程切换的最开始，进程先获取自己的锁，并且直到调用switch函数时也不释放锁。\n而另一个线程，也就是调度器线程会在进程的线程完全停止使用自己的栈之后，再释放进程的锁。\n释放锁之后，就可以由其他的CPU核再来运行进程的线程，因为这些线程现在已经不在运行了。\n以上是线程切换中非常重要的知识点。\n我们之后会用到它，因为这是Sleep\u0026amp;Wakeup设计中众多限制条件之一。\n学生提问：当我们有多个CPU核时，它们能看到同样的锁对象的唯一原因只可能是它们有一个共享的物理内存系统，对吧？\nRobert教授：是的。如果你有两个电脑，那么它们不会共享内存，并且我们就不会有这些问题。现在的处理器上，总是有多个CPU核，它们共享了相同的内存系统。\n在线程切换的过程中，还有一点我之前没有提过。\nXV6中，不允许进程在执行switch函数的过程中，持有任何其他的锁。\n所以，进程在调用switch函数的过程中，必须要持有p-\u0026gt;lock（注，也就是进程对应的proc结构体中的锁），但是同时又不能持有任何其他的锁。\n这也是包含了Sleep在内的很多设计的限制条件之一。\n接下来让我解释一下背后的原因，首先构建一个不满足这个限制条件的场景：\n我们有进程P1，P1的内核线程持有了p-\u0026gt;lock以外的其他锁，这些锁可能是在使用磁盘，UART，console过程中持有的。\n之后内核线程在持有锁的时候，通过调用switch/yield/sched函数出让CPU，这会导致进程P1持有了锁，但是进程P1又不在运行。\n假设我们在一个只有一个CPU核的机器上，进程P1调用了switch函数将CPU控制转给了调度器线程，调度器线程发现还有一个进程P2的内核线程正在等待被运行，所以调度器线程会切换到运行进程P2。\n假设P2也想使用磁盘，UART或者console，它会对P1持有的锁调用acquire，这是对于同一个锁的第二个acquire调用。\n当然这个锁现在已经被P1持有了，所以这里的acquire并不能获取锁。\n假设这里是spinlock，那么进程P2会在一个循环里不停的“旋转”并等待锁被释放。\n但是很明显进程P2的acquire不会返回，所以即使进程P2稍后愿意出让CPU，P2也没机会这么做。\n之所以没机会是因为P2对于锁的acquire调用在直到锁释放之前都不会返回，而唯一锁能被释放的方式就是进程P1恢复执行并在稍后release锁，但是这一步又还没有发生，因为进程P1通过调用switch函数切换到了P2，而P2又在不停的“旋转”并等待锁被释放。这是一种死锁，它会导致系统停止运行。\n虽然我刚刚的描述是基于机器上只有一个CPU核，但是你可以通过多个锁在多个CPU核的机器上构建类似的死锁场景。\n所以，我们在XV6中禁止在调用switch时持有除进程自身锁（注，也就是p-\u0026gt;lock）以外的其他锁。\n学生提问：难道定时器中断不会将CPU控制切换回进程P1从而解决死锁的问题吗？\nRobert教授：首先，所有的进程切换过程都发生在内核中，所有的acquire，switch，release都发生在内核代码而不是用户代码。实际上XV6允许在执行内核代码时触发中断，如果你查看trap.c中的代码你可以发现，如果XV6正在执行内核代码时发生了定时器中断，中断处理程序会调用yield函数并出让CPU。\n但是在之前的课程中我们讲过acquire函数在等待锁之前会关闭中断，否则的话可能会引起死锁（注，详见10.8），\n所以我们不能在等待锁的时候处理中断。所以如果你查看XV6中的acquire函数，你可以发现函数中第一件事情就是关闭中断，之后再“自旋”等待锁释放。你或许会想，为什么不能先“自旋”等待锁释放，再关闭中断？因为这样会有一个短暂的时间段锁被持有了但是中断没有关闭，在这个时间段内的设备的中断处理程序可能会引起死锁。\n所以不幸的是，当我们在自旋等待锁释放时会关闭中断，进而阻止了定时器中断并且阻止了进程P2将CPU出让回给进程P1。嗯，这是个好问题。\n学生提问：能重复一下死锁是如何避免的吗？\nRobert教授：哦，在XV6中，死锁是通过禁止在线程切换的时候加锁来避免的。\nXV6禁止在调用switch函数时，获取除了p-\u0026gt;lock以外的其他锁。如果你查看sched函数的代码（注，详见11.6），里面包含了一些检查代码来确保除了p-\u0026gt;lock以外线程不持有其他锁。所以上面会产生死锁的代码在XV6中是不合法的并被禁止的。\n将这里描述的对于锁的两个限制条件记住，因为我们后面讨论Sleep\u0026amp;Wakeup如何工作时会再次使用它们。\n在多核系统中，进程或者线程为了同步和共享资源，通常使用锁（如自旋锁）。但是在锁的操作中，如果处理不当，可能会引发死锁。死锁发生在多个进程/线程因为争抢资源（锁）而互相等待，导致系统无法前进。\n当一个进程/线程试图获取某个锁时（acquire），它可能会进入“自旋等待”状态，直到锁被释放。如果此时中断没有被关闭，可能会发生以下情况：\n假设某个进程（P1）已经持有锁，并正在执行一些关键代码，而另一个进程（P2）正在尝试获取同一个锁，但因为锁已经被 P1 持有，所以 P2 进入自旋等待状态。在这种情况下，如果不关闭中断，可能会发生以下情景：\n中断触发上下文切换：如果在 P2 自旋等待的过程中发生了定时器中断，操作系统可能会在 P2 等待锁时尝试进行上下文切换。这意味着 CPU 控制可能会切换到另一个进程（例如 P1），但这个切换可能会导致问题。 中断处理程序持有锁：假设中断处理程序需要持有同一个锁，但锁已经被 P1 持有，这可能会导致死锁。如果中断处理程序没有获取锁而被阻塞，并且 P1 继续等待另一个资源或中断处理结束，那么系统会陷入死锁状态。 设备中断与并发操作的冲突：如果在 P1 正在持有锁时，设备中断处理程序试图获取该锁，并在中断上下文中运行某些代码，那么设备中断的处理将被阻塞。这样，既无法完成设备的中断响应，又可能导致进程无法正常释放锁，进一步加剧系统的死锁风险。 为了避免这种情况，acquire 函数在等待锁之前会关闭中断。这样可以确保在进程/线程自旋等待锁的时候，不会被中断打断，从而防止中断处理程序引发的竞争条件或死锁。具体来说，关闭中断有以下好处：\n避免中断处理程序的竞争：当一个进程关闭中断并开始等待锁时，确保了 CPU 不会因为中断处理而切换到另一个进程。这样，持有锁的进程（如 P1）可以继续执行，直到释放锁，而不会因为中断导致死锁。 防止并发修改状态：锁保护的是共享数据的访问，关闭中断后，确保了其他硬件中断（比如定时器、设备中断）不会打断当前进程对锁和共享数据的操作。中断处理程序不能抢占当前进程，数据一致性得以保障。 如果我们在自旋等待锁释放时才关闭中断（即：先“自旋”再关闭中断），会存在一个竞态条件。在这个短暂的时间段内，虽然锁还没有被成功获取，但是中断已经没有关闭。这种情况下，设备中断可能会触发，并执行某些需要持有锁的操作，导致中断处理程序和进程竞争同一个锁，从而引发死锁。\nSleep\u0026amp;Wakeup接口 接下来看一下通过Sleep\u0026amp;Wakeup实现Coordination。\n锁可以使得线程本身不必关心其他线程的具体实现。\n我们为共享的数据增加锁，这样就不用担心其他线程也使用了相同的数据，因为锁可以确保对于数据的操作是依次发生的。\n当你在写一个线程的代码时，有些场景需要等待一些特定的事件，或者不同的线程之间需要交互。\n假设我们有一个Pipe，并且我正在从Pipe中读数据。但是Pipe当前又没有数据，所以我需要等待一个Pipe非空的事件。\n类似的，假设我在读取磁盘，我会告诉磁盘控制器请读取磁盘上的特定块。这或许要花费较长的时间，尤其当磁碟需要旋转时\n（通常是毫秒级别），磁盘才能完成读取。而执行读磁盘的进程需要等待读磁盘结束的事件。\n类似的，一个Unix进程可以调用wait函数。这个会使得调用进程等待任何一个子进程退出。所以这里父进程有意的在等待另一个进程产生的事件。\n以上就是进程需要等待特定事件的一些例子。\n特定事件可能来自于I/O，也可能来自于另一个进程，并且它描述了某件事情已经发生。\nCoordination是帮助我们解决这些问题并帮助我们实现这些需求的工具。Coordination是非常基础的工具，就像锁一样，在实现线程代码时它会一直出现。\n我们怎么能让进程或者线程等待一些特定的事件呢？一种非常直观的方法是通过循环实现busy-wait。\n假设我们想从一个Pipe读取数据，我们就写一个循环一直等待Pipe的buffer不为空。\n这个循环会一直运行直到其他的线程向Pipe的buffer写了数据。之后循环会结束，我们就可以从Pipe中读取数据并返回。\n实际中会有这样的代码。\n如果你知道你要等待的事件极有可能在0.1微秒内发生，通过循环等待或许是最好的实现方式。\n通常来说在操作设备硬件的代码中会采用这样的等待方式，如果你要求一个硬件完成一个任务，并且你知道硬件总是能非常快的完成任务，这时通过一个类似的循环等待或许是最正确的方式。\n另一方面，事件可能需要数个毫秒甚至你都不知道事件要多久才能发生，或许要10分钟其他的进程才能向Pipe写入数据，那么我们就不想在这一直循环并且浪费本可以用来完成其他任务的CPU时间。\n这时我们想要通过类似switch函数调用的方式出让CPU，并在我们关心的事件发生时重新获取CPU。\nCoordination就是有关出让CPU，直到等待的事件发生再恢复执行。\n人们发明了很多不同的Coordination的实现方式，但是与许多Unix风格操作系统一样，XV6使用的是Sleep\u0026amp;Wakeup这种方式。\n介绍完背景了，接下来我们看一下XV6的代码。\n为了准备这节课，我重写了UART的驱动代码，XV6通过这里的驱动代码从console中读写字符。\n首先是uartwrite函数。\n当shell需要输出时会调用write系统调用最终走到uartwrite函数中，这个函数会在循环中将buf中的字符一个一个的向UART硬件写入。\n这是一种经典的设备驱动实现风格，你可以在很多设备驱动中看到类似的代码。\nUART硬件一次只能接受一个字符的传输，而通常来说会有很多字符需要写到UART硬件。你可以向UART硬件写入一个字符，并等待UART硬件说：好的我完成了传输上一个字符并且准备好了传输下一个字符，之后驱动程序才可以写入下一个字符。\n因为这里的硬件可能会非常慢，或许每秒只能传输1000个字符，所以我们在两个字符之间的等待时间可能会很长。\n而1毫秒在现在计算机上是一个非常非常长的时间，它可能包含了数百万条指令时间，所以我们不想通过循环来等待UART完成字符传输，我们想通过一个更好的方式来等待。\n如大多数操作系统一样，XV6也的确存在更好的等待方式。\nUART硬件会在完成传输一个字符后，触发一个中断。\n所以UART驱动中除了uartwrite函数外，还有名为uartintr的中断处理程序。这个中断处理程序会在UART硬件触发中断时由trap.c代码调用。\n中断处理程序会在最开始读取UART对应的memory mapped register，并检查其中表明传输完成的相应的标志位，也就是LSR_TX_IDLE标志位。\n如果这个标志位为1，代码会将tx_done设置为1，并调用wakeup函数。\n这个函数会使得uartwrite中的sleep函数恢复执行，并尝试发送一个新的字符。\n所以这里的机制是，如果一个线程需要等待某些事件，比如说等待UART硬件愿意接收一个新的字符，线程调用sleep函数并等待一个特定的条件。\n当特定的条件满足时，代码会调用wakeup函数。\n这里的sleep函数和wakeup函数是成对出现的。\n我们之后会看sleep函数的具体实现，它会做很多事情最后再调用switch函数来出让CPU。\n这里有件事情需要注意，sleep和wakeup函数需要通过某种方式链接到一起。\n也就是说，如果我们调用wakeup函数，我们只想唤醒正在等待刚刚发生的特定事件的线程。\n所以，sleep函数和wakeup函数都带有一个叫做sleep channel的参数。\n我们在调用wakeup的时候，需要传入与调用sleep函数相同的sleep channel。\n不过sleep和wakeup函数只是接收表示了sleep channel的64bit数值，它们并不关心这个数值代表什么。\n当我们调用sleep函数时，我们通过一个sleep channel表明我们等待的特定事件，当调用wakeup时我们希望能传入相同的数值来表明想唤醒哪个线程。\n学生提问：进程会在写入每个字符时候都被唤醒一次吗？\nRobert教授：在这个我出于演示目的而特别改过的UART驱动中，传输每个字符都会有一个中断，所以你是对的，对于buffer中的每个字符，我们都会等待UART可以接收下一个字符，之后写入一个字符，将tx_done设置为0，回到循环的最开始并再次调用sleep函数进行睡眠状态，直到tx_done为1。当UART传输完了这个字符，uartintr函数会将tx_done设置为1，并唤醒uartwrite所在的线程。所以对于每个字符都有调用一次sleep和wakeup，并占用一次循环。\nUART实际上支持一次传输4或者16个字符，所以一个更有效的驱动会在每一次循环都传输16个字符给UART，并且中断也是每16个字符触发一次。更高速的设备，例如以太网卡通常会更多个字节触发一次中断。\n以上就是接口的演示。\nSleep\u0026amp;wakeup的一个优点是它们可以很灵活，它们不关心代码正在执行什么操作，你不用告诉sleep函数你在等待什么事件，你也不用告诉wakeup函数发生了什么事件，你只需要匹配好64bit的sleep channel就行。\n不过，对于sleep函数，有一个有趣的参数，我们需要将一个锁作为第二个参数传入，这背后是一个大的故事，我后面会介绍背后的原因。\n总的来说，不太可能设计一个sleep函数并完全忽略需要等待的事件。\n所以很难写一个通用的sleep函数，只是睡眠并等待一些特定的事件，并且这也很危险，因为可能会导致lost wakeup，而几乎所有的Coordination机制都需要处理lost wakeup的问题。\n在sleep接口中，我们需要传入一个锁是一种稍微丑陋的实现，我在稍后会再介绍。\nLost wakeup 在解释sleep函数为什么需要一个锁使用作为参数传入之前，我们先来看看假设我们有了一个更简单的不带锁作为参数的sleep函数，会有什么样的结果。这里的结果就是lost wakeup。\n假设sleep只是接收任意的sleep channel作为唯一的参数。\n它其实不能正常工作，我们称这个sleep实现为broken_sleep。\n你可以想象一个sleep函数内会将进程的状态设置为SLEEPING，表明当前进程不想再运行，而是正在等待一个特定的事件。\n如果你们看过了XV6的实现，你们可以发现sleep函数中还会做很多其他操作。\n我们需要记录特定的sleep channel值，这样之后的wakeup函数才能发现是当前进程正在等待wakeup对应的事件。\n最后再调用switch函数出让CPU。\n如果sleep函数只做了这些操作，那么很明显sleep函数会出问题，我们至少还应该在这里获取进程的锁。\n之后是wakeup函数。\n我们希望唤醒所有正在等待特定sleep channel的线程。\n所以wakeup函数中会查询进程表单中的所有进程，如果进程的状态是SLEEPING并且进程对应的channel是当前wakeup的参数，那么将进程的状态设置为RUNNABLE。\n在一些平行宇宙中，sleep\u0026amp;wakeup或许就是这么简单。\n在我回到XV6代码之前，让我演示一下如何在UART驱动中使用刚刚介绍的sleep和wakeup函数。\n首先是定义done标志位。之后是定义uartwrite函数。\n在函数中，对于buffer内的每一个字符，检查done标志位，如果标志位为0，就调用sleep函数并传入tx_channel。\n之后将字符传递给UART并将done设置为0。\n之后是中断处理函数uartintr。\n函数中首先将done标志位设置为1，并调用wakeup。\n以上就是使用broken_sleep的方式。这里缺失的是锁。\n这里uartwrite和uartintr两个函数需要使用锁来协调工作。\n第一个原因是done标志位，任何时候我们有了共享的数据，我们需要为这个数据加上锁。 另一个原因是两个函数都需要访问UART硬件，通常来说让两个线程并发的访问memory mapped register是错误的行为。 所以我们需要在两个函数中加锁来避免对于done标志位和硬件的竞争访问。\n现在的问题是，我们该在哪个位置加锁？在中断处理程序中较为简单，我们在最开始加锁，在最后解锁。\n难的是如何在uartwrite函数中加锁。一种可能是，每次发送一个字符的过程中持有锁，所以在每一次遍历buffer的起始和结束位置加锁和解锁。\n为什么这样肯定不能工作？一个原因是，我们能从while not done的循环退出的唯一可能是中断处理程序将done设置为1。\n但是如果我们为整个代码段都加锁的话，中断处理程序就不能获取锁了，中断程序会不停“自旋”并等待锁释放。\n而锁被uartwrite持有，在done设置为1之前不会释放。\n而done只有在中断处理程序获取锁之后才可能设置为1。\n所以我们不能在发送每个字符的整个处理流程都加锁。\n上面加锁方式的问题是，uartwrite在期望中断处理程序执行的同时又持有了锁。\n而我们唯一期望中断处理程序执行的位置就是sleep函数执行期间，其他的时候uartwrite持有锁是没有问题的。\n所以另一种实现可能是，在传输字符的最开始获取锁，因为我们需要保护共享变量done，但是在调用sleep函数之前释放锁。\n这样中断处理程序就有可能运行并且设置done标志位为1。\n之后在sleep函数返回时，再次获取锁。\n让我来修改代码，并看看相应的运行结果是什么。现有的代码中，uartwrite在最开始获取了锁，并在最后释放了锁。\n中断处理程序也在最开始获取锁，之后释放锁。\n接下来，我们会探索为什么只接收一个参数的broken_sleep在这不能工作。\n为了让锁能正常工作，我们需要在调用broken_sleep函数之前释放uart_tx_lock，并在broken_sleep返回时重新获取锁。\nbroken_sleep内的代码与之前在白板上演示的是一样的。也就是首先将进程状态设置为SLEEPING，并且保存tx_chan到进程结构体中，最后调用switch函数。\n接下来编译代码并看一下会发生什么。\n在XV6启动的时候会打印“init starting”，这里看来输出了一些字符之后就hang住了。如果我输入任意字符，剩下的字符就能输出。\n这里的问题必然与之前修改的代码相关。在前面的代码中，sleep之前释放了锁，但是在释放锁和broken_sleep之间可能会发生中断。\n一旦释放了锁，当前CPU的中断会被重新打开。\n因为这是一个多核机器，所以中断可能发生在任意一个CPU核。\n在上面代码标记的位置，其他CPU核上正在执行UART的中断处理程序，并且正在acquire函数中等待当前锁释放。\n所以一旦锁被释放了，另一个CPU核就会获取锁，并发现UART硬件完成了发送上一个字符，之后会设置tx_done为1，最后再调用wakeup函数，并传入tx_chan。\n目前为止一切都还好，除了一点：现在写线程还在执行并位于release和broken_sleep之间，也就是写线程还没有进入SLEEPING状态，所以中断处理程序中的wakeup并没有唤醒任何进程，因为还没有任何进程在tx_chan上睡眠。\n之后写线程会继续运行，调用broken_sleep，将进程状态设置为SLEEPING，保存sleep channel。\n但是中断已经发生了，wakeup也已经被调用了。所以这次的broken_sleep，没有人会唤醒它，因为wakeup已经发生过了。这就是lost wakeup问题。\n学生提问：是不是总是这样，一旦一个wakeup被丢失了，下一次wakeup时，之前缓存的数据会继续输出？\nRobert教授：这完全取决于实现细节。\n在我们的例子中，实际上出于偶然才会出现当我输入某些内容会导致之前的输出继续的现象。\n这里背后的原因是，我们的代码中，UART只有一个中断处理程序。\n不论是有输入，还是完成了一次输出，都会调用到同一个中断处理程序中。\n所以当我输入某些内容时，会触发输入中断，之后会调用uartintr函数。\n然后在中断处理程序中又会判断LSR_TX_IDLE标志位，并再次调用wakeup，所以刚刚的现象完全是偶然。\n如果出现了lost wakeup问题，并且你足够幸运的话，某些时候它们能自动修复。\n如果UART有不同的接收和发送中断处理程序的话，那么就没办法从lost wakeup恢复。\n学生提问：tx_done标志位的作用是什么？\nRobert教授：这是一种简单的在uartintr和uartwrite函数之间通信的方法。\ntx_done标志位为1表示已经完成了对于前一个字符的传输，并且uartwrite可以传输下一个字符，所以这是用来在中断处理程序和uartwrite之间通信的标志位。\n同一个学生提问：当从sleep函数中唤醒时，不是已经知道是来自UART的中断处理程序调用wakeup的结果吗？这样的话tx_done有些多余。\nRobert教授：我想你的问题也可以描述为：为什么需要通过一个循环while(tx_done == 0)来调用sleep函数？\n这个问题的答案适用于一个更通用的场景：实际中不太可能将sleep和wakeup精确匹配。\n并不是说sleep函数返回了，你等待的事件就一定会发生。\n举个例子，假设我们有两个进程同时想写UART，它们都在uartwrite函数中。\n可能发生这种场景，当一个进程写完一个字符之后，会进入SLEEPING状态并释放锁，而另一个进程可以在这时进入到循环并等待UART空闲下来。\n之后两个进程都进入到SLEEPING状态，当发生中断时UART可以再次接收一个字符，两个进程都会被唤醒，但是只有一个进程应该写入字符，所以我们才需要在sleep外面包一层while循环。\n实际上，你可以在XV6中的每一个sleep函数调用都被一个while循环包着。\n因为事实是，你或许被唤醒了，但是其他人将你等待的事件拿走了，所以你还得继续sleep。这种现象还挺普遍的。\n学生提问：我们只看到了一个lost wakeup，当我们随便输入一个字符，整个剩下的字符都能输出，为什么没有在输出剩下字符的时候再次发生lost wakeup？\nRobert教授：这会发生的。\n我来敲一下cat README，这会输出数千个字符。\n可以看到每过几个字符就会hang一次，需要我再次输入某个字符。\n这个过程我们可以看到很多lost wakeup。\n之前之所以没有出现，是因为lost wakeup需要中断已经在等待获取锁，并且uartwrite位于release和broken_sleep之间，这需要一定的巧合并不总是会发生。\n如何避免Lost wakeup 现在我们的目标是消灭掉lost wakeup。这可以通过消除下面的窗口时间来实现。\n首先我们必须要释放uart_tx_lock锁，因为中断需要获取这个锁，但是我们又不能在释放锁和进程将自己标记为SLEEPING之间留有窗口。\n这样中断处理程序中的wakeup才能看到SLEEPING状态的进程，并将其唤醒，进而我们才可以避免lost wakeup的问题。\n所以，我们应该消除这里的窗口。\n为了实现这个目的，我们需要将sleep函数设计的稍微复杂点。\n这里的解决方法是，即使sleep函数不需要知道你在等待什么事件，它还是需要你知道你在等待什么数据，并且传入一个用来保护你在等待数据的锁。\nsleep函数需要特定的条件才能执行，而sleep自己又不需要知道这个条件是什么。\n在我们的例子中，sleep函数执行的特定条件是tx_done等于1。\n虽然sleep不需要知道tx_done，但是它需要知道保护这个条件的锁，也就是这里的uart_tx_lock。\n在调用sleep的时候，锁还被当前线程持有，之后这个锁被传递给了sleep。\n在接口层面，sleep承诺可以原子性的将进程设置成SLEEPING状态，同时释放锁。\n这样wakeup就不可能看到这样的场景：锁被释放了但是进程还没有进入到SLEEPING状态。\n所以sleep这里将释放锁和设置进程为SLEEPING状态这两个行为合并为一个原子操作。\n所以我们需要有一个锁来保护sleep的条件，并且这个锁需要传递给sleep作为参数。\n更进一步的是，当调用wakeup时，锁必须被持有。\n如果程序员想要写出正确的代码，都必须遵守这些规则来使用sleep和wakeup。\n接下来我们看一下sleep和wakeup如何使用这一小块额外的信息（注，也就是传入给sleep函数的锁）和刚刚提到的规则，来避免lost wakeup。\n首先我们来看一下proc.c中的wakeup函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //Wake up all processes sleeping on chan. //Must be called without any p-\u0026gt;lock. void wakeup(void *chan) { struct proc *p; for(p = proc;p \u0026lt; \u0026amp;proc[NPROC];p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state =SLEEPING \u0026amp;\u0026amp;p-\u0026gt;chan =chan){ p-\u0026gt;state RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); } wakeup函数并不十分出人意料。\n它查看整个进程表单，对于每个进程首先加锁，这点很重要。\n之后查看进程的状态，如果进程当前是SLEEPING并且进程的channel与wakeup传入的channel相同，将进程的状态设置为RUNNABLE。最后再释放进程的锁。\n接下来我们忽略broken_sleep，直接查看带有锁作为参数的sleep函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Atomically release lock and sleep on chan. // Reacquires lock when awakened. void sleep(void *chan, struct spinlock *lk) { struct proc *p = myproc(); // Must acquire p-\u0026gt;lock in order to // change p-\u0026gt;state and then call sched. // Once we hold p-\u0026gt;lock, we can be // guaranteed that we won\u0026#39;t miss any wakeup // (wakeup locks p-\u0026gt;lock), // so it\u0026#39;s okay to release lk. if(lk != \u0026amp;p-\u0026gt;lock){ //DOC: sleeplock0 acquire(\u0026amp;p-\u0026gt;lock); //DOC: sleeplock1 release(lk); } // Go to sleep. p-\u0026gt;chan = chan; p-\u0026gt;state = SLEEPING; sched(); // Tidy up. p-\u0026gt;chan = 0; // Reacquire original lock. if(lk != \u0026amp;p-\u0026gt;lock){ release(\u0026amp;p-\u0026gt;lock); acquire(lk); } } 我们已经知道了sleep函数需要释放作为第二个参数传入的锁，这样中断处理程序才能获取锁。\n函数中第一件事情就是释放这个锁。\n当然在释放锁之后，我们会担心在这个时间点相应的wakeup会被调用并尝试唤醒当前进程，而当前进程还没有进入到SLEEPING状态。\n所以我们不能让wakeup在release锁之后执行。\n为了让它不在release锁之后执行，在release锁之前，sleep会获取即将进入SLEEPING状态的进程的锁。\n如果你还记得的话，wakeup在唤醒一个进程前，需要先获取进程的锁。\n所以在整个时间uartwrite检查条件之前到sleep函数中调用sched函数之间，这个线程一直持有了保护sleep条件的锁或者p-\u0026gt;lock。\n让我回到UART的代码并强调一下这一点。\nuartwrite在最开始获取了sleep的condition lock，并且一直持有condition lock直到调用sleep函数。\n所以它首先获取了condition lock，之后检查condition（注，也就是tx_done等于0），之后在持有condition lock的前提下调用了sleep函数。\n此时wakeup不能做任何事情，wakeup现在甚至都不能被调用直到调用者能持有condition lock。所以现在wakeup必然还没有执行。\nsleep函数在释放condition lock之前，先获取了进程的锁。\n在释放了condition lock之后，wakeup就可以被调用了，但是除非wakeup获取了进程的锁，否则wakeup不能查看进程的状态。\n所以，在sleep函数中释放了condition lock之后，wakeup也还没有执行。\n在持有进程锁的时候，将进程的状态设置为SLEEPING并记录sleep channel，之后再调用sched函数，这个函数中会再调用switch函数（注，详见11.6），此时sleep函数中仍然持有了进程的锁，wakeup仍然不能做任何事情。\n当我们从当前线程切换走时，调度器线程中会释放前一个进程的锁（注，详见11.8）。\n所以在调度器线程释放进程锁之后，wakeup才能终于获取进程的锁，发现它正在SLEEPING状态，并唤醒它。\n这里的效果是由之前定义的一些规则确保的，这些规则包括了：\n调用sleep时需要持有condition lock，这样sleep函数才能知道相应的锁。 sleep函数只有在获取到进程的锁p-\u0026gt;lock之后，才能释放condition lock。 wakeup需要同时持有两个锁才能查看进程。 这样的话，我们就不会再丢失任何一个wakeup，也就是说我们修复了lost wakeup的问题。\nPipe中的sleep和wakeup 前面我们介绍了在UART的驱动中，如何使用sleep和wakeup才能避免lost wakeup。\n前面这个特定的场景中，sleep等待的condition是发生了中断并且硬件准备好了传输下一个字符。\n在一些其他场景，内核代码会调用sleep函数并等待其他的线程完成某些事情。\n这些场景从概念上来说与我们介绍之前的场景没有什么区别，但是感觉上还是有些差异。\n例如，在读写pipe的代码中，如果你查看pipe.c中的piperead函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 int piperead(struct pipe *pi, uint64 addr, int n) { int i; struct proc *pr = myproc(); char ch; acquire(\u0026amp;pi-\u0026gt;lock); while(pi-\u0026gt;nread == pi-\u0026gt;nwrite \u0026amp;\u0026amp; pi-\u0026gt;writeopen){ //DOC: pipe-empty if(pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } sleep(\u0026amp;pi-\u0026gt;nread, \u0026amp;pi-\u0026gt;lock); //DOC: piperead-sleep } for(i = 0; i \u0026lt; n; i++){ //DOC: piperead-copy if(pi-\u0026gt;nread == pi-\u0026gt;nwrite) break; ch = pi-\u0026gt;data[pi-\u0026gt;nread++ % PIPESIZE]; if(copyout(pr-\u0026gt;pagetable, addr + i, \u0026amp;ch, 1) == -1) break; } wakeup(\u0026amp;pi-\u0026gt;nwrite); //DOC: piperead-wakeup release(\u0026amp;pi-\u0026gt;lock); return i; } 这里有很多无关的代码可以忽略。\n当read系统调用最终调用到piperead函数时，pi-\u0026gt;lock会用来保护pipe，这就是sleep函数对应的condition lock。\npiperead需要等待的condition是pipe中有数据，而这个condition就是pi-\u0026gt;nwrite大于pi-\u0026gt;nread，也就是写入pipe的字节数大于被读取的字节数。\n如果这个condition不满足，那么piperead会调用sleep函数，并等待condition发生。\n同时piperead会将condition lock也就是pi-\u0026gt;lock作为参数传递给sleep函数，以确保不会发生lost wakeup。\n之所以会出现lost wakeup，是因为在一个不同的CPU核上可能有另一个线程刚刚调用了pipewrite。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 int pipewrite(struct pipe *pi, uint64 addr, int n) { int i; char ch; struct proc *pr = myproc(); acquire(\u0026amp;pi-\u0026gt;lock); for(i = 0; i \u0026lt; n; i++){ while(pi-\u0026gt;nwrite == pi-\u0026gt;nread + PIPESIZE){ //DOC: pipewrite-full if(pi-\u0026gt;readopen == 0 || pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } wakeup(\u0026amp;pi-\u0026gt;nread); sleep(\u0026amp;pi-\u0026gt;nwrite, \u0026amp;pi-\u0026gt;lock); } if(copyin(pr-\u0026gt;pagetable, \u0026amp;ch, addr + i, 1) == -1) break; pi-\u0026gt;data[pi-\u0026gt;nwrite++ % PIPESIZE] = ch; } wakeup(\u0026amp;pi-\u0026gt;nread); release(\u0026amp;pi-\u0026gt;lock); return i; } pipewrite会向pipe的缓存写数据，并最后在piperead所等待的sleep channel上调用wakeup。\n而我们想要避免这样的风险：在piperead函数检查发现没有字节可以读取，到piperead函数调用sleep函数之间，另一个CPU调用了pipewrite函数。\n因为这样的话，另一个CPU会向pipe写入数据并在piperead进程进入SLEEPING之前调用wakeup，进而产生一次lost wakeup。\n在pipe的代码中，pipewrite和piperead都将sleep包装在一个while循环中。\npiperead中的循环等待pipe的缓存为非空（pipewrite中的循环等待的是pipe的缓存不为full）。\n之所以要将sleep包装在一个循环中，是因为可能有多个进程在读取同一个pipe。\n但是因为pipe中只有一个字节并且总是有一个进程能够先被唤醒。\nsleep函数中最后一件事情就是重新获取condition lock。所以调用sleep函数的时候，需要对condition lock上锁（注，在sleep函数内部会对condition lock解锁），在sleep函数返回时会重新对condition lock上锁。\n这样第一个被唤醒的线程会持有condition lock，而其他的线程在重新对condition lock上锁的时候会在锁的acquire函数中等待。\n那个幸运的进程（注，这里线程和进程描述的有些乱，但是基本意思是一样的，当说到线程时是指进程唯一的内核线程）会从sleep函数中返回，之后通过检查可以发现pi-\u0026gt;nwrite比pi-\u0026gt;nread大1，所以进程可以从piperead的循环中退出，并读取一个字节，之后pipe缓存中就没有数据了。\n之后piperead函数释放锁并返回。接下来，第二个被唤醒的线程，它的sleep函数可以获取condition lock并返回，但是通过检查发现pi-\u0026gt;nwrite等于pi-\u0026gt;nread（注，因为唯一的字节已经被前一个进程读走了），所以这个线程以及其他所有的等待线程都会重新进入sleep函数。\n所以这里也可以看出，几乎所有对于sleep的调用都需要包装在一个循环中，这样从sleep中返回的时候才能够重新检查condition是否还符合。\nsleep和wakeup的规则稍微有点复杂。\n因为你需要向sleep展示你正在等待什么数据，你需要传入锁并遵循一些规则，某些时候这些规则还挺烦人的。\n另一方面sleep和wakeup又足够灵活，因为它们并不需要理解对应的condition，只是需要有个condition和保护这个condition的锁。\n除了sleep\u0026amp;wakeup之外，还有一些其他的更高级的Coordination实现方式。\n例如今天课程的阅读材料中的semaphore，它的接口就没有那么复杂，你不用告诉semaphore有关锁的信息。而semaphore的调用者也不需要担心lost wakeup的问题，在semaphore的内部实现中考虑了lost wakeup问题。\n因为定制了up-down计数器，所以semaphore可以在不向接口泄露数据的同时（注，也就是不需要向接口传递condition lock），处理lost wakeup问题。\nsemaphore某种程度来说更简单，尽管它也没那么通用，如果你不是在等待一个计数器，semaphore也就没有那么有用了。这也就是为什么我说sleep和wakeup更通用的原因。\nexit系统调用 接下来，我想讨论一下XV6面临的一个与Sleep\u0026amp;Wakeup相关的挑战，也就是如何关闭一个进程。\n每个进程最终都需要退出，我们需要清除进程的状态，释放栈。\n在XV6中，一个进程如果退出的话，我们需要释放用户内存，释放page table，释放trapframe对象，将进程在进程表单中标为REUSABLE，这些都是典型的清理步骤。\n当进程退出或者被杀掉时，有许多东西都需要被释放。\n这里会产生的两大问题：\n首先我们不能直接单方面的摧毁另一个线程，因为：另一个线程可能正在另一个CPU核上运行，并使用着自己的栈；也可能另一个线程正在内核中持有了锁；也可能另一个线程正在更新一个复杂的内核数据，如果我们直接就把线程杀掉了，我们可能在线程完成更新复杂的内核数据过程中就把线程杀掉了。我们不能让这里的任何一件事情发生。 另一个问题是，即使一个线程调用了exit系统调用，并且是自己决定要退出。它仍然持有了运行代码所需要的一些资源，例如它的栈，以及它在进程表单中的位置。当它还在执行代码，它就不能释放正在使用的资源。所以我们需要一种方法让线程能释放最后几个对于运行代码来说关键的资源。 XV6有两个函数与关闭线程进程相关。\n第一个是exit，第二个是kill。\n让我们先来看位于proc.c中的exit函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 // Exit the current process. Does not return. // An exited process remains in the zombie state // until its parent calls wait(). void exit(int status) { struct proc *p = myproc(); if(p == initproc) panic(\u0026#34;init exiting\u0026#34;); // Close all open files. for(int fd = 0; fd \u0026lt; NOFILE; fd++){ if(p-\u0026gt;ofile[fd]){ struct file *f = p-\u0026gt;ofile[fd]; fileclose(f); p-\u0026gt;ofile[fd] = 0; } } begin_op(); iput(p-\u0026gt;cwd); end_op(); p-\u0026gt;cwd = 0; // we might re-parent a child to init. we can\u0026#39;t be precise about // waking up init, since we can\u0026#39;t acquire its lock once we\u0026#39;ve // acquired any other proc lock. so wake up init whether that\u0026#39;s // necessary or not. init may miss this wakeup, but that seems // harmless. acquire(\u0026amp;initproc-\u0026gt;lock); wakeup1(initproc); release(\u0026amp;initproc-\u0026gt;lock); // grab a copy of p-\u0026gt;parent, to ensure that we unlock the same // parent we locked. in case our parent gives us away to init while // we\u0026#39;re waiting for the parent lock. we may then race with an // exiting parent, but the result will be a harmless spurious wakeup // to a dead or wrong process; proc structs are never re-allocated // as anything else. acquire(\u0026amp;p-\u0026gt;lock); struct proc *original_parent = p-\u0026gt;parent; release(\u0026amp;p-\u0026gt;lock); // we need the parent\u0026#39;s lock in order to wake it up from wait(). // the parent-then-child rule says we have to lock it first. acquire(\u0026amp;original_parent-\u0026gt;lock); acquire(\u0026amp;p-\u0026gt;lock); // Give any children to init. reparent(p); // Parent might be sleeping in wait(). wakeup1(original_parent); p-\u0026gt;xstate = status; p-\u0026gt;state = ZOMBIE; release(\u0026amp;original_parent-\u0026gt;lock); // Jump into the scheduler, never to return. sched(); panic(\u0026#34;zombie exit\u0026#34;); } 这就是exit系统调用的内容。\n从exit接口的整体来看，在最后它会释放进程的内存和page table，关闭已经打开的文件，同时我们也知道父进程会从wait系统调用中唤醒，所以exit最终会导致父进程被唤醒。这些都是我们预期可以从exit代码中看到的内容。\n从上面的代码中，首先exit函数关闭了所有已打开的文件。\n这里可能会很复杂，因为关闭文件系统中的文件涉及到引用计数，虽然我们还没学到但是这里需要大量的工作。\n不管怎样，一个进程调用exit系统调用时，会关闭所有自己拥有的文件。\n接下来是类似的处理，进程有一个对于当前目录的记录，这个记录会随着你执行cd指令而改变。\n在exit过程中也需要将对这个目录的引用释放给文件系统。\n如果一个进程要退出，但是它又有自己的子进程，接下来需要设置这些子进程的父进程为init进程。\n我们接下来会看到，每一个正在exit的进程，都有一个父进程中的对应的wait系统调用。\n父进程中的wait系统调用会完成进程退出最后的几个步骤。\n所以如果父进程退出了，那么子进程就不再有父进程，当它们要退出时就没有对应的父进程的wait。\n所以在exit函数中，会为即将exit进程的子进程重新指定父进程为init进程，也就是PID为1的进程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // we need the parent\u0026#39;s lock in order to wake it up from wait(). // the parent-then-child rule says we have to lock it first. acquire(\u0026amp;original_parent-\u0026gt;lock); acquire(\u0026amp;p-\u0026gt;lock); // Give any children to init. reparent(p); // Parent might be sleeping in wait(). wakeup1(original_parent); p-\u0026gt;xstate = status; p-\u0026gt;state = ZOMBIE; release(\u0026amp;original_parent-\u0026gt;lock); // Jump into the scheduler, never to return. sched(); panic(\u0026#34;zombie exit\u0026#34;); } 之后，我们需要通过调用wakeup函数唤醒当前进程的父进程，当前进程的父进程或许正在等待当前进程退出。\n接下来，进程的状态被设置为ZOMBIE。\n现在进程还没有完全释放它的资源，所以它还不能被重用。\n所谓的进程重用是指，我们期望在最后，进程的所有状态都可以被一些其他无关的fork系统调用复用，但是目前我们还没有到那一步。\n现在我们还没有结束，因为我们还没有释放进程资源。我们在还没有完全释放所有资源的时候，通过调用sched函数进入到调度器线程。\n到目前位置，进程的状态是ZOMBIE，并且进程不会再运行，因为调度器只会运行RUNNABLE进程。\n同时进程资源也并没有完全释放，如果释放了进程的状态应该是UNUSED。\n但是可以肯定的是进程不会再运行了，因为它的状态是ZOMBIE。所以调度器线程会决定运行其他的进程。\nwait系统调用 通过Unix的exit和wait系统调用的说明，我们可以知道如果一个进程exit了，并且它的父进程调用了wait系统调用，父进程的wait会返回。\nwait函数的返回表明当前进程的一个子进程退出了。\n接下来看一下wait系统调用的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 // Wait for a child process to exit and return its pid. // Return -1 if this process has no children. int wait(uint64 addr) { struct proc *np; int havekids, pid; struct proc *p = myproc(); // hold p-\u0026gt;lock for the whole time to avoid lost // wakeups from a child\u0026#39;s exit(). acquire(\u0026amp;p-\u0026gt;lock); for(;;){ // Scan through table looking for exited children. havekids = 0; for(np = proc; np \u0026lt; \u0026amp;proc[NPROC]; np++){ // this code uses np-\u0026gt;parent without holding np-\u0026gt;lock. // acquiring the lock first would cause a deadlock, // since np might be an ancestor, and we already hold p-\u0026gt;lock. if(np-\u0026gt;parent == p){ // np-\u0026gt;parent can\u0026#39;t change between the check and the acquire() // because only the parent changes it, and we\u0026#39;re the parent. acquire(\u0026amp;np-\u0026gt;lock); havekids = 1; if(np-\u0026gt;state == ZOMBIE){ // Found one. pid = np-\u0026gt;pid; if(addr != 0 \u0026amp;\u0026amp; copyout(p-\u0026gt;pagetable, addr, (char *)\u0026amp;np-\u0026gt;xstate, sizeof(np-\u0026gt;xstate)) \u0026lt; 0) { release(\u0026amp;np-\u0026gt;lock); release(\u0026amp;p-\u0026gt;lock); return -1; } freeproc(np); release(\u0026amp;np-\u0026gt;lock); release(\u0026amp;p-\u0026gt;lock); return pid; } release(\u0026amp;np-\u0026gt;lock); } } // No point waiting if we don\u0026#39;t have any children. if(!havekids || p-\u0026gt;killed){ release(\u0026amp;p-\u0026gt;lock); return -1; } // Wait for a child to exit. sleep(p, \u0026amp;p-\u0026gt;lock); //DOC: wait-sleep } } 它里面包含了一个大的循环。\n当一个进程调用了wait系统调用，它会扫描进程表单，找到父进程是自己且状态是ZOMBIE的进程。\n从上一节可以知道，这些进程已经在exit函数中几乎要执行完了。\n之后由父进程调用的freeproc函数，来完成释放进程资源的最后几个步骤。\n看一下freeproc的实现，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // free a proc structure and the data hanging from it, // including user pages. // p-\u0026gt;lock must be held. static void freeproc(struct proc *p) { if(p-\u0026gt;trapframe) kfree((void*)p-\u0026gt;trapframe); p-\u0026gt;trapframe = 0; if(p-\u0026gt;pagetable) proc_freepagetable(p-\u0026gt;pagetable, p-\u0026gt;sz); p-\u0026gt;pagetable = 0; p-\u0026gt;sz = 0; p-\u0026gt;pid = 0; p-\u0026gt;parent = 0; p-\u0026gt;name[0] = 0; p-\u0026gt;chan = 0; p-\u0026gt;killed = 0; p-\u0026gt;xstate = 0; p-\u0026gt;state = UNUSED; } 这是关闭一个进程的最后一些步骤。\n如果由正在退出的进程自己在exit函数中执行这些步骤，将会非常奇怪。\n这里释放了trapframe，释放了page table。如果我们需要释放进程内核栈，那么也应该在这里释放。\n但是因为内核栈的guard page，我们没有必要再释放一次内核栈。\n不管怎样，当进程还在exit函数中运行时，任何这些资源在exit函数中释放都会很难受，所以这些资源都是由父进程释放的。\nwait不仅是为了父进程方便的知道子进程退出，wait实际上也是进程退出的一个重要组成部分。\n在Unix中，对于每一个退出的进程，都需要有一个对应的wait系统调用，这就是为什么当一个进程退出时，它的子进程需要变成init进程的子进程。\ninit进程的工作就是在一个循环中不停调用wait，因为每个进程都需要对应一个wait，这样它的父进程才能调用freeproc函数，并清理进程的资源。\n当父进程完成了清理进程的所有资源，子进程的状态会被设置成UNUSED。\n之后，fork系统调用才能重用进程在进程表单的位置。\n学生提问：在exit系统调用中，为什么需要在重新设置父进程之前，先获取当前进程的父进程？\nRobert教授：这里其实就是在防止一个进程和它的父进程同时退出。\n通常情况下，一个进程exit，它的父进程正在wait，一切都正常。\n但是也可能一个进程和它的父进程同时exit。\n所以当子进程尝试唤醒父进程，并告诉它自己退出了时，父进程也在退出。\n它应该是处理这种父进程和子进程同时退出的情况。\n如果不是这种情况的话，一切都会非常直观，子进程会在后面通过wakeup函数唤醒父进程。\n学生提问：为什么我们在唤醒父进程之后才将进程的状态设置为ZOMBIE？难道我们不应该在之前就设置吗？\nRobert教授：正在退出的进程会先获取自己进程的锁，同时，因为父进程的wait系统调用中也需要获取子进程的锁，所以父进程并不能查看正在执行exit函数的进程的状态。\n这意味着，正在退出的进程获取自己的锁到它调用sched进入到调度器线程之间（注，因为调度器线程会释放进程的锁），父进程并不能看到这之间代码引起的中间状态。\n所以这之间的代码顺序并不重要。\n大部分时候，如果没有持有锁，exit中任何代码顺序都不能工作。\n因为有了锁，代码的顺序就不再重要，因为父进程也看不到进程状态。\n所以，直到子进程exit的最后，它都没有释放所有的资源，因为它还在运行的过程中，所以不能释放这些资源。\n相应的其他的进程，也就是父进程，释放了运行子进程代码所需要的资源。这样的设计可以让我们极大的精简exit的实现。\nkill系统调用 Unix中的一个进程可以将另一个进程的ID传递给kill系统调用，并让另一个进程停止运行。\n如果我们不够小心的话，kill一个还在内核执行代码的进程，会有一些我几分钟前介绍过的风险，比如我们想要杀掉的进程的内核线程还在更新一些数据，比如说更新文件系统，创建一个文件。\n如果这样的话，我们不能就这样杀掉进程，因为这样会使得一些需要多步完成的操作只执行了一部分。\n所以kill系统调用不能就直接停止目标进程的运行。\n实际上，在XV6和其他的Unix系统中，kill系统调用基本上不做任何事情。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Kill the process with the given pid. // The victim won\u0026#39;t exit until it tries to return // to user space (see usertrap() in trap.c). int kill(int pid) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;pid == pid){ p-\u0026gt;killed = 1; if(p-\u0026gt;state == SLEEPING){ // Wake process from sleep(). p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); return 0; } release(\u0026amp;p-\u0026gt;lock); } return -1; } 它先扫描进程表单，找到目标进程。\n然后只是将进程的proc结构体中killed标志位设置为1。\n如果进程正在SLEEPING状态，将其设置为RUNNABLE。\n这里只是将killed标志位设置为1，并没有停止进程的运行。\n所以kill系统调用本身还是很温和的。\n而目标进程运行到内核代码中能安全停止运行的位置时，会检查自己的killed标志位，如果设置为1，目标进程会自愿的执行exit系统调用。\n你可以在trap.c中看到所有可以安全停止运行的位置，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // // handle an interrupt, exception, or system call from user space. // called from trampoline.S // void usertrap(void) { int which_dev = 0; if((r_sstatus() \u0026amp; SSTATUS_SPP) != 0) panic(\u0026#34;usertrap: not from user mode\u0026#34;); // send interrupts and exceptions to kerneltrap(), // since we\u0026#39;re now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-\u0026gt;trapframe-\u0026gt;epc = r_sepc(); if(r_scause() == 8){ // system call if(p-\u0026gt;killed) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-\u0026gt;trapframe-\u0026gt;epc += 4; // an interrupt will change sstatus \u0026amp;c registers, // so don\u0026#39;t enable until done with those registers. intr_on(); syscall(); } else if((which_dev = devintr()) != 0){ // ok } else { printf(\u0026#34;usertrap(): unexpected scause %p pid=%d\\n\u0026#34;, r_scause(), p-\u0026gt;pid); printf(\u0026#34; sepc=%p stval=%p\\n\u0026#34;, r_sepc(), r_stval()); p-\u0026gt;killed = 1; } if(p-\u0026gt;killed) exit(-1); 在usertrap函数中（注，详见6.6），在执行系统调用之前，如果进程已经被kill了，进程会自己调用exit。\n在这个内核代码位置，代码并没有持有任何锁，也不在执行任何操作的过程中，所以进程通过exit退出是完全安全的。\n类似的，在usertrap函数的最后，也有类似的代码。\n在执行完系统调用之后，进程会再次检查自己是否已经被kill了。\n即使进程是被中断打断，这里的检查也会被执行。\n例如当一个定时器中断打断了进程的运行，我们可以通过检查发现进程是killed状态，之后进程会调用exit退出。\n所以kill系统调用并不是真正的立即停止进程的运行，它更像是这样：如果进程在用户空间，那么下一次它执行系统调用它就会退出，又或者目标进程正在执行用户代码，当时下一次定时器中断或者其他中断触发了，进程才会退出。\n所以从一个进程调用kill，到另一个进程真正退出，中间可能有很明显的延时。\n这里有个很直观问题：如果进程不在用户空间执行，而是正在执行系统调用的过程中，然后它被kill了，我们需要做什么特别的操作吗？\n之所以会提出这个问题，是因为进程可能正在从console读取即将输入的字符，而你可能要明天才会输入一个字符，所以当你kill一个进程时，最好进程不是等到明天才退出。\n出于这个原因，在XV6的很多位置中，如果进程在SLEEPING状态时被kill了，进程会实际的退出。\n首先要看的是kill函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int kill(int pid) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;pid == pid){ p-\u0026gt;killed = 1; if(p-\u0026gt;state == SLEEPING){ // Wake process from sleep(). p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); return 0; } release(\u0026amp;p-\u0026gt;lock); } return -1; } 可以看到如果目标进程是SLEEPING状态，kill函数会将其状态设置为RUNNABLE，这意味着，即使进程之前调用了sleep并进入到SLEEPING状态，调度器现在会重新运行进程，并且进程会从sleep中返回。\n让我们来查看一下这在哪生效的。\n在pipe.c的piperead函数中，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 int piperead(struct pipe *pi, uint64 addr, int n) { int i; struct proc *pr = myproc(); char ch; acquire(\u0026amp;pi-\u0026gt;lock); while(pi-\u0026gt;nread == pi-\u0026gt;nwrite \u0026amp;\u0026amp; pi-\u0026gt;writeopen){ //DOC: pipe-empty if(pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } sleep(\u0026amp;pi-\u0026gt;nread, \u0026amp;pi-\u0026gt;lock); //DOC: piperead-sleep } for(i = 0; i \u0026lt; n; i++){ //DOC: piperead-copy if(pi-\u0026gt;nread == pi-\u0026gt;nwrite) break; ch = pi-\u0026gt;data[pi-\u0026gt;nread++ % PIPESIZE]; if(copyout(pr-\u0026gt;pagetable, addr + i, \u0026amp;ch, 1) == -1) break; } wakeup(\u0026amp;pi-\u0026gt;nwrite); //DOC: piperead-wakeup release(\u0026amp;pi-\u0026gt;lock); return i; } 如果一个进程正在sleep状态等待从pipe中读取数据，然后它被kill了。\nkill函数会将其设置为RUNNABLE，之后进程会从sleep中返回，返回到循环的最开始。\npipe中大概率还是没有数据，之后在piperead中，会判断进程是否被kill了（注，if(pr-\u0026gt;killed)）。\n如果进程被kill了，那么接下来piperead会返回-1，并且返回到usertrap函数的syscall位置，因为piperead就是一种系统调用的实现。\n之后在usertrap函数中会检查p-\u0026gt;killed，并调用exit。\n所以对于SLEEPING状态的进程，如果它被kill了，它会被直接唤醒，包装了sleep的循环会检查进程的killed标志位，最后再调用exit。\n同时还有一些情况，如果进程在SLEEPING状态中被kill了并不能直接退出。\n例如，一个进程正在更新一个文件系统并创建一个文件的过程中，进程不适宜在这个时间点退出，因为我们想要完成文件系统的操作，之后进程才能退出。\n我会向你展示一个磁盘驱动中的sleep循环，这个循环中就没有检查进程的killed标志位。\n下面就是virtio_disk.c文件中的一段代码：\n1 2 3 4 //Wait for virtio_disk_intr()to say request has finished. while(b-\u0026gt;disk == 1){ sleep(b, \u0026amp;disk.vdisk_lock); } 这里一个进程正在等待磁盘的读取结束，这里没有检查进程的killed标志位。\n因为现在可能正在创建文件的过程中，而这个过程涉及到多次读写磁盘。\n我们希望完成所有的文件系统操作，完成整个系统调用，之后再检查p-\u0026gt;killed并退出。\n学生提问：为什么一个进程允许kill另一个进程？这样一个进程不是能杀掉所有其他进程吗？\nRobert教授：如果你在MIT的分时复用计算机Athena上这么做的话，他们可能会开除你。\n在XV6中允许这么做是因为，XV6这是个教学用的操作系统，任何与权限相关的内容在XV6中都不存在。\n在Linux或者真正的操作系统中，每个进程都有一个user id或多或少的对应了执行进程的用户，一些系统调用使用进程的user id来检查进程允许做的操作。\n所以在Linux中会有额外的检查，调用kill的进程必须与被kill的进程有相同的user id，否则的话，kill操作不被允许。\n所以，在一个分时复用的计算机上，我们会有多个用户，我们不会想要用户kill其他人的进程，这样一套机制可以防止用户误删别人的进程。\n学生提问：init进程会退出吗？\nRobert教授：让我来看看。\n是的，如果fork失败了，init进程也会退出。不过，这个问题的真正的答案是，不会退出。init进程的目标就是不退出，它就是在一个循环中不停的调用wait。如果init进程退出了，我认为这是一个Fatal级别的错误，然后系统会崩溃。在exit函数的最开始就会有如下检查\n1 2 3 4 5 6 7 void exit(int status) { struct proc *p = myproc(); if(p == initproc) panic(\u0026#34;init exiting\u0026#34;); 如果调用exit的进程是init进程，那么会触发panic。\n因为如果没有init进程的话，系统最终还是会停止运行。\n如果没有init进程的话就没有人会为退出的进程调用wait系统调用，也就没有人完成进程资源的释放工作，我们最终会用光所有的进程，并引起一些其他的错误，所以我们必须要有init进程。所以这个问题的真正答案是init进程不允许退出。\n学生提问：这节课可能没有怎么讲到，但是如果关闭一个操作系统会发生什么？\nRobert教授：这个过程非常复杂，并且依赖于你运行的是什么系统。\n因为文件系统是持久化的，它能在多次重启之间保持数据，我们需要保持文件系统的良好状态，如果我们正在更新文件系统的过程中，例如创建文件，然后我们想关闭操作系统，断电之类的。\n我们需要一个策略来确保即使我们正在一个复杂的更新文件系统的过程中，我们并不会破坏磁盘上的文件系统数据。\n文件系统其实就是一个位于磁盘的数据结构。\n所以这里涉及到了很多的机制来确保如果你关闭操作系统或者因为断电之类，我们可以恢复磁盘上的文件系统。\n其他的，你是否需要做一些特殊的操作来关闭系统，取决于你正在运行什么进程。\n如果你正在运行一些重要的服务器，例如数据库服务器，并且许多其他计算机依赖这个数据库并通过网络使用它。\n那谁知道呢？答案或许是你不能就这么直接关闭操作系统，因为你正在提供一个对于其他计算机来说非常关键的服务。\n如果你的计算机并没有在做任何事情，那么你可以直接关闭它。\n或许对于你的问题来说，如果你想关闭一个计算机，确保文件系统是正确的，之后停止执行指令，之后就可以关闭计算机了。\n","date":"2024-09-22T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08113sleep-wake-up/bg_hu17549149395720678467.jpg","permalink":"https://echudet.github.io/p/mit6s08113sleep-wake-up/","title":"MIT6S081：13Sleep \u0026 Wake up"},{"content":"Lecture 11 Thread switching 预习 多路复用 Xv6通过在两种情况下将每个CPU从一个进程切换到另一个进程来实现多路复用（Multiplexing）。\n第一：当进程等待设备或管道I/O完成，或等待子进程退出，或在sleep系统调用中等待时，xv6使用睡眠（sleep）和唤醒（wakeup）机制切换。\n第二：xv6周期性地强制切换以处理长时间计算而不睡眠的进程。\n这种多路复用产生了每个进程都有自己的CPU的错觉，就像xv6使用内存分配器和硬件页表来产生每个进程都有自己内存的错觉一样。\n什么是多路复用 多路复用（Multiplexing） 是一种在通信或计算领域的技术，允许多个信号或数据流通过同一个资源传输或处理。\n它的主要目的是提高资源的利用效率，使得多个信号能够共享有限的硬件或通信通道，而不会相互干扰。\n在多路复用中，多个独立的数据流通过某种机制被组合在一起，在同一个信道或设备上传输或处理。\n多路复用通常在网络通信、操作系统资源管理等场景中广泛使用。\n多路复用的挑战 实现多路复用带来了一些挑战。\n首先，如何从一个进程切换到另一个进程？尽管上下文切换的思想很简单，但它的实现是xv6中最不透明的代码之一。\n第二，如何以对用户进程透明的方式强制切换？Xv6使用标准技术，通过定时器中断驱动上下文切换。\n第三，许多CPU可能同时在进程之间切换，使用一个用锁方案来避免争用是很有必要的。\n第四，进程退出时必须释放进程的内存以及其他资源，但它不能自己完成所有这一切，因为（例如）它不能在仍然使用自己内核栈的情况下释放它。\n第五，多核机器的每个核心必须记住它正在执行哪个进程，以便系统调用正确影响对应进程的内核状态。\n最后，sleep允许一个进程放弃CPU，wakeup允许另一个进程唤醒第一个进程。需要小心避免导致唤醒通知丢失的竞争。\nXv6试图尽可能简单地解决这些问题，但结果代码很复杂。\n多路复用进程与线程的区别 属性 进程 线程 地址空间 独立地址空间 共享同一进程的地址空间 资源 独立资源，如内存、文件描述符等 共享同一进程的资源 开销 创建、销毁、切换开销大 创建、销毁、切换开销小 通信 进程间通信（如管道、消息队列等）较复杂 线程之间的通信相对简单和高效 容错性 进程崩溃不会影响其他进程 线程崩溃可能影响整个进程 代码：上下文切换 图7.1概述了从一个用户进程（旧进程）切换到另一个用户进程（新进程）所涉及的步骤\n一个到旧进程内核线程的用户-内核转换（系统调用或中断）\n一个到当前CPU调度程序线程的上下文切换\n一个到新进程内核线程的上下文切换\n一个返回到用户级进程的陷阱\n调度程序在旧进程的内核栈上执行是不安全的：其他一些核心可能会唤醒进程并运行它，而在两个不同的核心上使用同一个栈将是一场灾难，因此xv6调度程序在每个CPU上都有一个专用线程（保存寄存器和栈）。\n这段话的核心问题在于多核系统中的进程调度，特别是在进程切换时如何安全地管理内核栈和寄存器状态。\n在多核系统中，操作系统需要在多个 CPU 核上管理多个进程，并根据调度策略在进程之间切换执行。调度程序负责管理哪些进程在何时运行，以及如何在不同进程之间切换。\n每个进程在运行时都有用户态和内核态两种状态。当进程从用户态进入内核态时（例如，处理系统调用或中断），进程的状态需要保存在内核栈中。操作系统使用内核栈来保存进程当前的状态（如寄存器的值、程序计数器等），以便在切换回来时可以继续执行。\n这段话提到的一个重要问题是：在多核系统中，如果多个 CPU 核同时使用同一个内核栈，会导致严重的冲突和错误。这是因为：\n内核栈是每个进程在内核态运行时保存状态的地方。当进程从用户态切换到内核态（例如处理系统调用或中断）时，操作系统会将进程的寄存器、程序计数器等信息保存在这个内核栈中。 如果某个进程被唤醒并在一个 CPU 核上运行，而调度程序正在另一个 CPU 核上切换该进程，这两个 CPU 核将会同时访问同一个内核栈，导致冲突。由于栈是一个线性结构，两个 CPU 同时操作会导致数据混乱和系统崩溃。 因此，如果调度程序在旧的进程的内核栈上执行，而此时该进程在另一个 CPU 核上被唤醒并运行，就会导致两个 CPU 共享同一个栈，进而导致系统崩溃。\n为了避免这个问题，xv6 操作系统的设计为每个 CPU 核提供了一个专用线程，并为这个线程分配了一个独立的栈。这个线程称为调度程序线程（scheduler thread）。\n调度程序线程负责在每个 CPU 上执行调度任务。当操作系统决定切换进程时，调度程序线程在它自己的栈上执行，不会干扰其他正在运行的进程。 这种设计确保调度程序不依赖于任何特定的进程栈，而是有自己独立的栈空间。这样即使一个进程在多个 CPU 核之间切换，也不会出现多个 CPU 同时访问同一个栈的情况。 在内核态，进程和调度程序之间需要频繁切换，特别是在多核系统中。这种切换通常会通过保存当前进程的状态（寄存器、程序计数器等）到内核栈上，然后调度程序在线程的栈上执行，决定下一个运行的进程。切换过程如下：\n进程进入内核态：当某个进程由于系统调用或中断进入内核态时，它的状态（如寄存器内容、程序计数器）会保存在它的内核栈上。 调度程序介入：调度程序线程运行在每个 CPU 上，并有自己的内核栈。它从当前进程的内核栈中恢复保存的状态，执行调度逻辑，选择下一个要运行的进程。 切换到下一个进程：调度程序将切换到下一个进程，恢复该进程的状态，并从这个进程的内核栈中恢复执行。 避免共享内核栈的并发问题\n问题：在多核系统中，多个 CPU 核可能会同时运行调度程序和进程。如果它们共享同一个内核栈，会导致栈冲突和系统崩溃。 解决方案：xv6 为每个 CPU 提供了一个独立的调度程序线程和栈，以确保调度程序和进程不会同时使用同一个栈。 内核线程和调度程序的切换机制：调度程序在独立的栈上执行，避免了在多核环境中不同核心共享同一个栈的风险。 在多核系统中，每个 CPU 都有自己的专用内核栈，这个栈用于调度程序的执行。调度程序负责在进程之间切换，也就是当某个进程被暂停时，调度程序会选择下一个要运行的进程。\n调度程序的内核栈主要有以下作用：\n独立于进程的栈：调度程序有自己独立的栈，不依赖任何进程的栈。这样，当调度程序在多个 CPU 核上运行时，它们可以独立地管理每个 CPU 的调度过程，不会与正在运行的进程发生冲突。 保存调度程序的状态：在执行进程切换时，调度程序需要保存当前的 CPU 状态、寄存器、调度算法的变量等信息。这些信息会保存在调度程序自己的内核栈中，而不会影响进程的内核栈。 执行进程调度逻辑：当调度程序决定暂停当前进程并切换到另一个进程时，它使用自己的内核栈来处理这个操作。调度程序在选择下一个进程时，所有与调度相关的局部变量和函数调用都存储在调度程序自己的栈中。 这种设计保证了在多核环境下安全、高效地切换进程，并且避免了内核栈冲突。\n在本节中，我们将研究在内核线程和调度程序线程之间切换的机制。\n从一个线程切换到另一个线程需要保存旧线程的CPU寄存器，并恢复新线程先前保存的寄存器；栈指针和程序计数器被保存和恢复的事实意味着CPU将切换栈和执行中的代码。\n函数swtch为内核线程切换执行保存和恢复操作。\nswtch对线程没有直接的了解；它只是保存和恢复寄存器集，称为上下文（contexts）。\n当某个进程要放弃CPU时，该进程的内核线程调用swtch来保存自己的上下文并返回到调度程序的上下文。\n每个上下文都包含在一个struct context（kernel/proc.h:2）中，这个结构体本身包含在一个进程的struct proc或一个CPU的struct cpu中。\nSwtch接受两个参数：struct context *old和struct context *new。\n它将当前寄存器保存在old中，从new中加载寄存器，然后返回。\n让我们跟随一个进程通过swtch进入调度程序。\n我们在第4章中看到，中断结束时的一种可能性是usertrap调用了yield。\n依次地：Yield调用sched，sched调用swtch将当前上下文保存在p-\u0026gt;context中，并切换到先前保存在cpu-\u0026gt;scheduler（kernel/proc.c:517）中的调度程序上下文。\n注：当前版本的XV6中调度程序上下文是cpu-\u0026gt;context\nSwtch（kernel/swtch.S:3）只保存被调用方保存的寄存器（callee-saved registers）；调用方保存的寄存器（caller-saved registers）通过调用C代码保存在栈上（如果需要）。\nSwtch知道struct context中每个寄存器字段的偏移量。它不保存程序计数器。\n但swtch保存ra寄存器，该寄存器保存调用swtch的返回地址。\n现在，swtch从新进程的上下文中恢复寄存器，该上下文保存前一个swtch保存的寄存器值。\n当swtch返回时，它返回到由ra寄存器指定的指令，即新线程以前调用swtch的指令。\n另外，它在新线程的栈上返回。\n以cc切换到ls为例，且ls此前运行过\nXV6将cc程序的内核线程的内核寄存器保存在一个context对象中\n因为要切换到ls程序的内核线程，那么ls 程序现在的状态必然是RUNABLE ，表明ls程序之前运行了一半。这同时也意味着：\na. ls程序的用户空间状态已经保存在了对应的trapframe中\nb. ls程序的内核线程对应的内核寄存器已经保存在对应的context对象中\n所以接下来，XV6会恢复ls程序的内核线程的context对象，也就是恢复内核线程的寄存器。\n之后ls会继续在它的内核线程栈上，完成它的中断处理程序\n恢复ls程序的trapframe中的用户进程状态，返回到用户空间的ls程序中\n最后恢复执行ls\n进程的上下文信息（内核态的寄存器、程序计数器等）保存在每个进程的 context 对象中，而不是在 CPU 专属的调度程序栈中。 CPU 专属的调度程序栈 仅用于调度程序在进程切换期间的执行，不保存具体进程的状态。 调度程序执行进程切换时，会切换到 CPU 专属的栈，而当新的进程开始运行时，它会切换回到该进程的内核栈，继续执行。 在我们的示例中，sched调用swtch切换到cpu-\u0026gt;scheduler，即每个CPU的调度程序上下文。\n调度程序上下文之前通过scheduler对swtch（kernel/proc.c:475）的调用进行了保存。\n当我们追踪swtch到返回时，他返回到scheduler而不是sched，并且它的栈指针指向当前CPU的调用程序栈（scheduler stack）。\n代码：调度 上一节介绍了swtch的底层细节；\n现在，让我们以swtch为给定对象，检查从一个进程的内核线程通过调度程序切换到另一个进程的情况。\n调度器（scheduler）以每个CPU上一个特殊线程的形式存在，每个线程都运行scheduler函数。\n此函数负责选择下一个要运行的进程。\n想要放弃CPU的进程必须先获得自己的进程锁p-\u0026gt;lock，并释放它持有的任何其他锁，更新自己的状态（p-\u0026gt;state），然后调用sched。\nYield（kernel/proc.c:515）遵循这个约定，sleep和exit也遵循这个约定，我们将在后面进行研究。\nSched对这些条件再次进行检查（kernel/proc.c:499-504），并检查这些条件的隐含条件：由于锁被持有，中断应该被禁用。\n最后，sched调用swtch将当前上下文保存在p-\u0026gt;context中，并切换到cpu-\u0026gt;scheduler中的调度程序上下文。\nSwtch在调度程序的栈上返回，就像是scheduler的swtch返回一样。\nscheduler继续for循环，找到要运行的进程，切换到该进程，重复循环。\nxv6在对swtch的调用中持有p-\u0026gt;lock：swtch的调用者必须已经持有了锁，并且锁的控制权传递给切换到的代码。\n这种约定在锁上是不寻常的；通常，获取锁的线程还负责释放锁，这使得对正确性进行推理更加容易。\n对于上下文切换，有必要打破这个惯例，因为p-\u0026gt;lock保护进程state和context字段上的不变量，而这些不变量在swtch中执行时不成立。\n如果在swtch期间没有保持p-\u0026gt;lock，可能会出现一个问题：在yield将其状态设置为RUNNABLE之后，但在swtch使其停止使用自己的内核栈之前，另一个CPU可能会决定运行该进程。结果将是两个CPU在同一栈上运行，这不可能是正确的。\n内核线程总是在sched中放弃其CPU，并总是切换到调度程序中的同一位置，而调度程序（几乎）总是切换到以前调用sched的某个内核线程。\n因此，如果要打印xv6切换线程处的行号，将观察到以下简单模式：（kernel/proc.c:475），（kernel/proc.c:509），（kernel/proc.c:475），（kernel/proc.c:509）等等。\n在两个线程之间进行这种样式化切换的过程有时被称为协程（coroutines）；在本例中，sched和scheduler是彼此的协同程序。\n“这种样式化切换”是指在 xv6 中，上下文切换时，程序总是以一种有规律的模式在两个地方来回切换：\n调度程序的 sched 函数负责放弃当前进程的 CPU，保存进程状态，并切换到调度程序。 调度程序再选择下一个要运行的进程，切换回它之前调用 sched 的进程。 这种切换的模式是循环往复的，比如代码总是从 sched 返回到 scheduler 的特定行号，再从 scheduler 切换到某个进程的特定行号。这个有规律的模式就是所谓的“样式化切换”。\n总结这段话的核心内容：\n进程锁和上下文切换：进程在调用 sched 时需要先持有自己的 p-\u0026gt;lock，这是为了防止在 swtch 期间其他 CPU 并发地操作同一个进程。 swtch 的工作原理：sched 调用 swtch 将当前进程的状态保存到 p-\u0026gt;context，然后切换到调度程序的上下文。调度程序随后选择下一个要运行的进程。 锁的特殊约定：xv6 打破了通常的“获取锁的线程负责释放锁”的惯例，sched 持有的锁传递给调度程序，以确保状态安全切换。 协程（coroutines）模式：调度程序和 sched 之间的这种循环切换，被称为协程，它们互相配合着切换工作，形成协作运行的模式。 传统的函数调用与返回：\n当你调用一个函数时，程序会暂停当前的执行流程，进入被调用的函数。 当被调用的函数执行 return 后，程序回到调用点继续执行。 但 函数调用是“单向的”：调用者调用被调用者，被调用者完成后返回调用者。调用者没有暂停或保存自己的状态等待被调用。 协程的关键区别：\n协程可以“暂停”和“恢复”：协程允许在执行过程中暂停，并且可以在某个时刻恢复执行。当协程暂停时，它的执行上下文（包括局部变量、程序计数器等）被保存下来，当它被重新调用时，会从暂停的地方继续执行，而不是从调用点继续像函数返回那样。 协同执行：协程允许多个函数互相让出控制权，并在彼此之间切换。这与传统的函数调用不同，传统的调用只是单向的，而协程可以在不同协程之间来回切换，形成一种“合作式多任务”的执行方式。 函数调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 void funcA() { printf(\u0026#34;In funcA\\n\u0026#34;); funcB(); printf(\u0026#34;Back to funcA\\n\u0026#34;); } void funcB() { printf(\u0026#34;In funcB\\n\u0026#34;); } int main() { funcA(); } 协程（伪代码示例）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ccodepreviewrunvoid coroutineA() { printf(\u0026#34;In coroutineA\\n\u0026#34;); yield(); // 暂停并交出控制权 printf(\u0026#34;Back to coroutineA\\n\u0026#34;); } void coroutineB() { printf(\u0026#34;In coroutineB\\n\u0026#34;); yield(); // 暂停并交出控制权 } int main() { startCoroutine(coroutineA); startCoroutine(coroutineB); } 整个过程的执行类似于进程 A -\u0026gt; sched -\u0026gt; scheduler（-\u0026gt;swtch） -\u0026gt; 进程 B -\u0026gt; sched -\u0026gt; scheduler -\u0026gt; 进程 C -\u0026gt; sched -\u0026gt; \u0026hellip;\n关键点：\nsched 负责保存当前进程的状态并交出控制权。 scheduler 负责选择下一个进程，并通过 swtch 恢复这个进程的状态。 这个过程不断循环，类似于协程的交替执行，每个进程在被切换出去后暂停，再被调度回来时继续执行。 存在一种情况使得调度程序对swtch的调用没有以sched结束。\n一个新进程第一次被调度时，它从forkret（kernel/proc.c:527）开始。\nForkret存在以释放p-\u0026gt;lock；否则，新进程可以从usertrapret开始。\nscheduler（kernel/proc.c:457）运行一个简单的循环：找到要运行的进程，运行它直到它让步，然后重复循环。\nscheduler在进程表上循环查找可运行的进程，该进程具有p-\u0026gt;state == RUNNABLE。\n一旦找到一个进程，它将设置CPU当前进程变量c-\u0026gt;proc，将该进程标记为RUNINING，然后调用swtch开始运行它（kernel/proc.c:470-475）。\n考虑调度代码结构的一种方法是，它为每个进程强制维持一个不变量的集合，并在这些不变量不成立时持有p-\u0026gt;lock。\n其中一个不变量是：如果进程是RUNNING状态，计时器中断的yield必须能够安全地从进程中切换出去；这意味着CPU寄存器必须保存进程的寄存器值（即swtch没有将它们移动到context中），并且c-\u0026gt;proc必须指向进程。\n另一个不变量是：如果进程是RUNNABLE状态，空闲CPU的调度程序必须安全地运行它；这意味着p-\u0026gt;context必须保存进程的寄存器（即，它们实际上不在实际寄存器中），没有CPU在进程的内核栈上执行，并且没有CPU的c-\u0026gt;proc引用进程。请注意，在保持p-\u0026gt;lock时，这些属性通常不成立。\n这段话提到的两个不变量是：\n第一个不变量（对于 RUNNING 状态的进程）：\n如果进程处于 RUNNING 状态，计时器中断发生时，操作系统的 yield 必须能够安全地切换到调度程序。这意味着，CPU 寄存器必须保持进程的寄存器值，并且 c-\u0026gt;proc 必须指向该进程。也就是说，寄存器状态还没有被保存到 p-\u0026gt;context 中。 第二个不变量（对于 RUNNABLE 状态的进程）：\n如果进程处于RUNNABLE状态，空闲的 CPU 调度程序必须能够\n安全地运行该进程。这意味着：p-\u0026gt;context 必须保存该进程的寄存器状态，因为它已经不再在 CPU 的实际寄存器中。\n没有 CPU 正在使用该进程的内核栈，确保没有其他 CPU 在执行该进程。\n没有 CPU 的 c-\u0026gt;proc（当前正在运行的进程指针）引用该进程，说明没有任何 CPU 正在运行它。\n为什么叫做不变量？\n不变量 是指程序在运行时始终需要保持成立的条件，不论程序执行的状态如何。在这里，这两个不变量必须始终保持成立，以确保在上下文切换时，进程状态能够正确保存和恢复，从而保证系统的稳定性和安全性。 “安全地从进程中切换出去”和“空闲 CPU 的调度程序必须安全地运行它”是什么意思？\n“安全地从进程中切换出去”： 当一个正在运行的进程（RUNNING 状态）被计时器中断触发时，操作系统可能决定切换到另一个进程。这时，系统必须能够安全地切换，意味着当前进程的寄存器状态必须妥善保存，否则在切换回来时进程可能会丢失当前状态。因此，CPU 寄存器必须保持进程的寄存器值，不能在 swtch 中过早地将其保存到 p-\u0026gt;context 中。 简单来说，这意味着在切换到调度程序前，系统必须确保当前进程的所有运行状态都已正确保存，以便下次切换回来时能够恢复。 “空闲 CPU 的调度程序必须安全地运行它”： 当某个 CPU 处于空闲状态，调度程序会从可运行的进程队列中选择一个RUNNABLE状态的进程并运行它。为了确保安全运行，系统必须确保： p-\u0026gt;context 中包含了进程的寄存器状态，因为进程的状态已经被保存，CPU 寄存器中不再有它的内容。 没有其他 CPU 在使用该进程的内核栈，这意味着该进程没有被任何其他 CPU 执行，从而避免栈冲突。 没有 CPU 的 c-\u0026gt;proc 引用该进程，表明它确实处于可运行状态，等待被分配 CPU。 为什么说“保持 p-\u0026gt;lock 时，这些属性通常不成立”，“这些属性”又是什么？\n当进程处于 RUNNING 状态时，计时器中断发生时，CPU 寄存器必须保持进程的寄存器值，c-\u0026gt;proc 必须指向该进程。 当进程处于 RUNNABLE 状态时，p-\u0026gt;context 必须保存进程的寄存器状态，没有 CPU 在进程的内核栈上运行，没有 CPU 的 c-\u0026gt;proc 指向它。 为什么保持 p-\u0026gt;lock 时，这些属性通常不成立：\n在持有p-\u0026gt;lock的过程中，系统可能正在进行上下文切换，此时进程的状态尚未完全确定。例如： 当进程正从 RUNNING 状态切换到 RUNNABLE 状态时，进程的寄存器可能还没有完全保存到 p-\u0026gt;context 中。 或者，进程的状态正在更新，可能还存在 CPU 在该进程的内核栈上运行，导致暂时的不一致。 因此，在上下文切换的过程中，这些不变量可能会短暂地被打破。这就是为什么在 sched 执行过程中，必须持有 p-\u0026gt;lock，确保在完成整个上下文切换前，其他 CPU 不会并发地修改该进程的状态。\n维护上述不变量是xv6经常在一个线程中获取p-\u0026gt;lock并在另一个线程中释放它的原因，例如在yield中获取并在scheduler中释放。\n一旦yield开始修改一个RUNNING进程的状态为RUNNABLE，锁必须保持被持有状态，直到不变量恢复：最早的正确释放点是scheduler（在其自身栈上运行）清除c-\u0026gt;proc之后。类似地，一旦scheduler开始将RUNNABLE进程转换为RUNNING，在内核线程完全运行之前（在swtch之后，例如在yield中）绝不能释放锁。\n在上下文切换和调度过程中，锁的释放时机非常关键。锁的作用是确保进程的状态在切换时不会被多个 CPU 并发修改或破坏，尤其是维护那些关键的不变量。具体来说，锁必须在整个上下文切换过程中保持，以防止状态不一致。下面详细解释锁的释放时机：\nyield 中获取锁的时机：\n当一个 RUNNING 状态的进程决定放弃 CPU（例如通过 yield），它需要先获取自己的锁（p-\u0026gt;lock）。\n获取锁后，进程的状态开始从 RUNNING 改为 RUNNABLE。这个过程非常关键，因为它影响到进程的执行状态，以及 CPU 寄存器中的内容如何保存。\n在这个状态转换过程中，锁必须保持，直到确认状态安全地转换为 RUNNABLE，并且不会有其他 CPU 并发地访问或修改该进程的状态。\n锁在 scheduler 中释放的时机：\n一旦 yield 把进程的状态标记为 RUNNABLE，接下来调度程序（scheduler）会从可运行的进程列表中选择下一个进程，并进行上下文切换。\n但是，在调度过程中，锁不能立即释放，因为此时进程的状态（p-\u0026gt;state 和 p-\u0026gt;context）还没有完全稳定。特别是，调度器还没有完成对 c-\u0026gt;proc 的清除（c-\u0026gt;proc 表示当前 CPU 上正在运行的进程）。具体来说：\n调度器会在其自身的栈上运行，选择下一个要执行的进程。 锁的释放时机：调度器必须等到它完全切换到另一个进程后，并且确保当前进程的 c-\u0026gt;proc 被清除（即当前 CPU 不再引用该进程）后，才可以释放 p-\u0026gt;lock。\n为何不能提前释放锁？\n如果在 yield 修改状态后、上下文切换前提前释放锁，可能会导致系统中的其他 CPU 错误地认为这个进程已经处于稳定的 RUNNABLE 状态，并尝试同时调度和运行它。这会导致多个 CPU 操作同一个内核栈，从而导致数据冲突和不一致性。\n锁必须确保状态转换和上下文切换在完成后，才释放锁给其他调度程序或 CPU，以免破坏不变量。\n调度器把 RUNNABLE 转换为 RUNNING 时的锁释放：\n当调度器选择一个 RUNNABLE 的进程运行时，它会将该进程的状态从 RUNNABLE 转为 RUNNING。\n在这之前，调度器会持有该进程的锁，确保没有其他 CPU 并发修改它。\n在上下文切换完成（即 swtch 切换到该进程）后，调度器必须继续持有锁，直到切换完成，并且进程真正开始运行。\n一旦切换到该进程，锁可以安全地释放，因为该进程已经完全运行，状态已经稳定。\n总结锁的释放时机：\nyield 后：锁不能立即释放，必须保持到调度器清理完 c-\u0026gt;proc，并安全地将控制权交给另一个进程。 scheduler 选择下一个进程后：当调度器把进程从 RUNNABLE 转换为 RUNNING 时，必须确保切换完成，才能释放锁。 总的来说，锁的释放时机是确保状态和上下文切换完成，进程状态稳定后，才能释放锁，否则可能导致状态混乱和并发冲突。\np-\u0026gt;lock还保护其他东西：exit和wait之间的相互作用，避免丢失wakeup的机制（参见第7.5节），以及避免一个进程退出和其他进程读写其状态之间的争用（例如，exit系统调用查看p-\u0026gt;pid并设置p-\u0026gt;killed(*kernel/proc.c*:611)）。\n为了清晰起见，也许为了性能起见，有必要考虑一下p-\u0026gt;lock的不同功能是否可以拆分。\n代码：mycpu和myproc Xv6通常需要指向当前进程的proc结构体的指针。\n在单处理器系统上，可以有一个指向当前proc的全局变量。\n但这不能用于多核系统，因为每个核执行的进程不同。\n解决这个问题的方法是基于每个核心都有自己的寄存器集，从而使用其中一个寄存器来帮助查找每个核心的信息。\nXv6为每个CPU维护一个struct cpu，它记录当前在该CPU上运行的进程（如果有的话），为CPU的调度线程保存寄存器，以及管理中断禁用所需的嵌套自旋锁的计数。\n函数mycpu (kernel/proc.c:60)返回一个指向当前CPU的struct cpu的指针。\nRISC-V给它的CPU编号，给每个CPU一个hartid。Xv6确保每个CPU的hartid在内核中存储在该CPU的tp寄存器中。\n这允许mycpu使用tp对一个cpu结构体数组（即cpus数组，kernel/proc.c:9）进行索引，以找到正确的那个。\n确保CPU的tp始终保存CPU的hartid有点麻烦。\nmstart在CPU启动次序的早期设置tp寄存器，此时仍处于机器模式（kernel/start.c:46）。\n因为用户进程可能会修改tp，usertrapret在蹦床页面（trampoline page）中保存tp。\n最后，uservec在从用户空间（kernel/trampoline.S:70）进入内核时恢复保存的tp。\n编译器保证永远不会使用tp寄存器。\n如果RISC-V允许xv6直接读取当前hartid会更方便，但这只允许在机器模式下，而不允许在管理模式下。\ncpuid和mycpu的返回值很脆弱：如果定时器中断并导致线程让步（yield），然后移动到另一个CPU，以前返回的值将不再正确。\nxv6要求调用者禁用中断，并且只有在使用完返回的struct cpu后才重新启用。\n函数myproc (kernel/proc.c:68)返回当前CPU上运行进程struct proc的指针。\nmyproc禁用中断，调用mycpu，从struct cpu中取出当前进程指针（c-\u0026gt;proc），然后启用中断。\n即使启用中断，myproc的返回值也可以安全使用：如果计时器中断将调用进程移动到另一个CPU，其struct proc指针不会改变。\n虽然在多核环境下多个 CPU 是并发执行的，但每次调用这些函数时，都会针对一个特定的 CPU 上的线程运行。\nXv6 操作系统在多核系统上无法使用单一的全局变量来指向当前运行的进程。为了区分每个处理器核上执行的进程，Xv6 为每个 CPU 维护了一个 struct cpu，记录了当前 CPU 上正在运行的进程，以及其他 CPU 状态信息。为了能够快速访问每个核心的 struct cpu，Xv6 使用了 RISC-V 的 tp 寄存器，该寄存器保存每个 CPU 的 hartid（硬件线程 ID）。这使得 Xv6 能通过 tp 对应 cpus 数组进行索引，从而获取每个核的 struct cpu。\nmycpu() 函数返回当前 CPU 的 struct cpu 指针。它通过 tp 寄存器找到对应的 CPU 结构体。 myproc() 函数返回当前 CPU 上运行的进程的 struct proc 指针。它通过禁用中断，调用 mycpu() 函数从当前 CPU 的 struct cpu 中取出正在运行的进程指针。 为了避免在中断或线程迁移（例如由于计时器中断导致线程切换到另一个 CPU）时返回值不再有效，Xv6 要求在获取这些信息时禁用中断，并在使用完后再启用。\n正课 线程概述 今天的课程会讨论线程以及XV6如何实现线程切换。\n今天这节课与之前介绍的系统调用，Interrupt，page table和锁的课程一样，都是有关XV6底层实现的课程。\n为什么计算机需要运行多线程？可以归结为以下原因：\n首先，人们希望他们的计算机在同一时间不是只执行一个任务。\n有可能计算机需要执行分时复用的任务，例如MIT的公共计算机系统Athena允许多个用户同时登陆一台计算机，并运行各自的进程。甚至在一个单用户的计算机或者在你的iphone上，你会运行多个进程，并期望计算机完成所有的任务而不仅仅只是一个任务。\n其次，多线程可以让程序的结构变得简单。\n线程在有些场合可以帮助程序员将代码以简单优雅的方式进行组织，并减少复杂度。实际上在第一个lab中prime number部分，通过多个进程可以更简单，方便，优雅的组织代码。\n最后，使用多线程可以通过并行运算，在拥有多核CPU的计算机上获得更快的处理速度。\n常见的方式是将程序进行拆分，并通过线程在不同的CPU核上运行程序的不同部分。如果你足够幸运的话，你可以将你的程序拆分并在4个CPU核上通过4个线程运行你的程序，同时你也可以获取4倍的程序运行速度。你可以认为XV6就是一个多CPU并行运算的程序。\n所以，线程可以认为是一种在有多个任务时简化编程的抽象。\n一个线程可以认为是串行执行代码的单元。\n如果你写了一个程序只是按顺序执行代码，那么你可以认为这个程序就是个单线程程序，这是对于线程的一种宽松的定义。\n虽然人们对于线程有很多不同的定义，在这里，我们认为线程就是单个串行执行代码的单元，它只占用一个CPU并且以普通的方式一个接一个的执行指令。\n除此之外，线程还具有状态，我们可以随时保存线程的状态并暂停线程的运行，并在之后通过恢复状态来恢复线程的运行。\n线程的状态包含了三个部分：\n程序计数器（Program Counter），它表示当前线程执行指令的位置。 保存变量的寄存器。 程序的Stack（注，详见5.5）。通常来说每个线程都有属于自己的Stack，Stack记录了函数调用的记录，并反映了当前线程的执行点。 操作系统中线程系统的工作就是管理多个线程的运行。\n我们可能会启动成百上千个线程，而线程系统的工作就是弄清楚如何管理这些线程并让它们都能运行。\n多线程的并行运行主要有两个策略：\n第一个策略是在多核处理器上使用多个CPU，每个CPU都可以运行一个线程，如果你有4个CPU，那么每个CPU可以运行一个线程。\n每个线程自动的根据所在CPU就有了程序计数器和寄存器。但是如果你只有4个CPU，却有上千个线程，每个CPU只运行一个线程就不能解决这里的问题了。\n第二个策略是一个CPU在多个线程之间来回切换。\n假设我只有一个CPU，但是有1000个线程，我们接下来将会看到XV6是如何实现线程切换使得XV6能够先运行一个线程，之后将线程的状态保存，再切换至运行第二个线程，然后再是第三个线程，依次类推直到每个线程都运行了一会，再回来重新执行第一个线程。\n实际上，与大多数其他操作系统一样，XV6结合了这两种策略。\n首先线程会运行在所有可用的CPU核上，其次每个CPU核会在多个线程之间切换，因为通常来说，线程数会远远多于CPU的核数。\n不同线程系统之间的一个主要的区别就是，线程之间是否会共享内存。\n一种可能是你有一个地址空间，多个线程都在这一个地址空间内运行，并且它们可以看到彼此的更新。\n比如说共享一个地址空间的线程修改了一个变量，共享地址空间的另一个线程可以看到变量的修改。\n所以当多个线程运行在一个共享地址空间时，我们需要用到上节课讲到的锁。\nXV6内核共享了内存，并且XV6支持内核线程的概念，对于每个用户进程都有一个内核线程来执行来自用户进程的系统调用。\n所有的内核线程都共享了内核内存，所以XV6的内核线程的确会共享内存。\n另一方面，XV6还有另外一种线程。\n每一个用户进程都有独立的内存地址空间（注，详见4.2），并且包含了一个线程，这个线程控制了用户进程代码指令的执行。\n所以XV6中的用户线程之间没有共享内存，你可以有多个用户进程，但是每个用户进程都是拥有一个线程的独立地址空间。XV6中的进程不会共享内存。\n在一些其他更加复杂的系统中，例如Linux，允许在一个用户进程中包含多个线程，进程中的多个线程共享进程的地址空间。\n当你想要实现一个运行在多个CPU核上的用户进程时，你就可以在用户进程中创建多个线程。\nLinux中也用到了很多我们今天会介绍的技术，但是在Linux中跟踪每个进程的多个线程比XV6中每个进程只有一个线程要复杂的多。\n还有一些其他的方式可以支持在一台计算机上交织的运行多个任务，我们不会讨论它们，但是如果你感兴趣的话，你可以去搜索event-driven programming或者state machine，这些是在一台计算机上不使用线程但又能运行多个任务的技术。\n在所有的支持多任务的方法中，线程技术并不是非常有效的方法，但是线程通常是最方便，对程序员最友好的，并且可以用来支持大量不同任务的方法。\nXV6线程调度 实现内核中的线程系统存在以下挑战：\n第一个是如何实现线程间的切换。\n这里停止一个线程的运行并启动另一个线程的过程通常被称为线程调度（Scheduling）。我们将会看到XV6为每个CPU核都创建了一个线程调度器（Scheduler）。\n第二个挑战是，当你想要实际实现从一个线程切换到另一个线程时，你需要保存并恢复线程的状态。\n所以需要决定线程的哪些信息是必须保存的，并且在哪保存它们。\n最后一个挑战是如何处理运算密集型线程（compute bound thread）。\n对于线程切换，很多直观的实现是由线程自己自愿的保存自己的状态，再让其他的线程运行。但是如果我们有一些程序正在执行一些可能要花费数小时的长时间计算任务，这样的线程并不能自愿的出让CPU给其他的线程运行。所以这里需要能从长时间运行的运算密集型线程撤回对于CPU的控制，将其放置于一边，稍后再运行它。\n接下来，将首先介绍如何处理运算密集型线程。\n这里的具体实现，就是利用定时器中断。\n在每个CPU核上，都存在一个硬件设备，它会定时产生中断。\nXV6与其他所有的操作系统一样，将这个中断传输到了内核中。\n所以即使我们正在用户空间计算π的前100万位，定时器中断仍然能在例如每隔10ms的某个时间触发，并将程序运行的控制权从用户空间代码切换到内核中的中断处理程序（注，因为中断处理程序优先级更高）。\n哪怕这些用户空间进程并不配合工作（注，也就是用户空间进程一直占用CPU），内核也可以从用户空间进程获取CPU控制权。\n位于内核的定时器中断处理程序，会自愿的将CPU出让（yield）给线程调度器，并告诉线程调度器说，你可以让一些其他的线程运行了。\n这里的出让其实也是一种线程切换，它会保存当前线程的状态，并在稍后恢复。\n这里的基本流程是，定时器中断将CPU控制权给到内核，内核再自愿的出让CPU。\n这样的处理流程被称为pre-emptive scheduling。\npre-emptive的意思是，即使用户代码本身没有出让CPU，定时器中断仍然会将CPU的控制权拿走，并出让给线程调度器。与之相反的是voluntary scheduling。\n在XV6和其他的操作系统中，线程调度是这么实现的：定时器中断会强制的将CPU控制权从用户进程给到内核，这里是pre-emptive scheduling，之后内核会代表用户进程（注，实际是内核中用户进程对应的内核线程会代表用户进程出让CPU），使用voluntary scheduling。\n在执行线程调度的时候，操作系统需要能区分几类线程：\n当前在CPU上运行的线程 一旦CPU有空闲时间就想要运行在CPU上的线程 以及不想运行在CPU上的线程，因为这些线程可能在等待I/O或者其他事件 这里不同的线程是由状态区分，但是实际上线程的完整状态会要复杂的多（注，线程的完整状态包含了程序计数器，寄存器，栈等等）。下面是我们将会看到的一些线程状态：\nRUNNING，线程当前正在某个CPU上运行 RUNABLE，线程还没有在某个CPU上运行，但是一旦有空闲的CPU就可以运行 SLEEPING，这个状态意味着线程在等待一些I/O事件，它只会在I/O事件发生了之后运行 这节课，我们主要关注RUNNING和RUNABLE这两类线程。\n前面介绍的定时器中断或者说pre-emptive scheduling，实际上就是将一个RUNNING线程转换成一个RUNABLE线程。\n通过出让CPU，pre-emptive scheduling将一个正在运行的线程转换成了一个当前不在运行但随时可以再运行的线程。\n因为当定时器中断触发时，这个线程还在好好的运行着。\n对于RUNNING状态下的线程，它的程序计数器和寄存器位于正在运行它的CPU硬件中。\n而RUNABLE线程，因为并没有CPU与之关联，所以对于每一个RUNABLE线程，当我们将它从RUNNING转变成RUNABLE时，我们需要将它还在RUNNING时位于CPU的状态拷贝到内存中的某个位置，注意这里不是从内存中的某处进行拷贝，而是从CPU中的寄存器拷贝。\n我们需要拷贝的信息就是程序计数器（Program Counter）和寄存器。\n当线程调度器决定要运行一个RUNABLE线程时，这里涉及了很多步骤，但是其中一步是将之前保存的程序计数器和寄存器拷贝回调度器对应的CPU中。\nXV6线程切换（一） 我们或许会运行多个用户空间进程，例如C compiler（CC），LS，Shell，它们或许会，也或许不会想要同时运行。\n在用户空间，每个进程有自己的内存，每个进程都包含了一个用户程序栈（user stack），并且当进程运行的时候，它在RISC-V处理器中会有程序计数器和寄存器。\n当用户程序在运行时，实际上是用户进程中的一个用户线程在运行。\n如果程序执行了一个系统调用或者因为响应中断走到了内核中，那么相应的用户空间状态会被保存在程序的trapframe中（注，详见lec06），同时属于这个用户程序的内核线程被激活。\n所以首先，用户的程序计数器，寄存器等等被保存到了trapframe中，之后CPU被切换到内核栈上运行，实际上会走到trampoline和usertrap代码中（注，详见lec06）。\n之后内核会运行一段时间处理系统调用或者执行中断处理程序。在处理完成之后，如果需要返回到用户空间，trapframe中保存的用户进程状态会被恢复。\n除了系统调用，用户进程也有可能是因为CPU需要响应类似于定时器中断走到了内核空间。\n上一节提到的pre-emptive scheduling，会通过定时器中断将CPU运行切换到另一个用户进程。\n在定时器中断程序中，如果XV6内核决定从一个用户进程切换到另一个用户进程，那么首先在内核中第一个进程的内核线程会被切换到第二个进程的内核线程。\n之后再在第二个进程的内核线程中返回到用户空间的第二个进程，这里返回也是通过恢复trapframe中保存的用户进程状态完成。\n当XV6从CC程序的内核线程切换到LS程序的内核线程时：\nXV6会首先会将CC程序的内核线程的内核寄存器保存在一个context对象中。 类似的，因为要切换到LS程序的内核线程，那么LS程序现在的状态必然是RUNABLE，表明LS程序之前运行了一半。这同时也意味着LS程序的用户空间状态已经保存在了对应的trapframe中，更重要的是，LS程序的内核线程对应的内核寄存器也已经保存在对应的context对象中。所以接下来，XV6会恢复LS程序的内核线程的context对象，也就是恢复内核线程的寄存器。 之后LS会继续在它的内核线程栈上，完成它的中断处理程序（注，假设之前LS程序也是通过定时器中断触发的pre-emptive scheduling进入的内核）。 然后通过恢复LS程序的trapframe中的用户进程状态，返回到用户空间的LS程序中。 最后恢复执行LS。 这里核心点在于，在XV6中，任何时候都需要经历：\n从一个用户进程切换到另一个用户进程，都需要从第一个用户进程接入到内核中，保存用户进程的状态并运行第一个用户进程的内核线程。 再从第一个用户进程的内核线程切换到第二个用户进程的内核线程。 之后，第二个用户进程的内核线程暂停自己，并恢复第二个用户进程的用户寄存器。 最后返回到第二个用户进程继续执行。 XV6线程切换（二） 实际的线程切换流程会复杂的多。\n假设我们有进程P1正在运行，进程P2是RUNABLE当前并不在运行。\n假设在XV6中我们有2个CPU核，这意味着在硬件层面我们有CPU0和CPU1。\n我们从一个正在运行的用户空间进程切换到另一个RUNABLE但是还没有运行的用户空间进程的更完整的故事是：\n首先与我之前介绍的一样，一个定时器中断强迫CPU从用户空间进程切换到内核，trampoline代码将用户寄存器保存于用户进程对应的trapframe对象中； 之后在内核中运行usertrap，来实际执行相应的中断处理程序。这时，CPU正在进程P1的内核线程和内核栈上，执行内核中普通的C代码； 假设进程P1对应的内核线程决定它想出让CPU，它会做很多工作，这个我们稍后会看，但是最后它会调用swtch函数（译注：switch 是C 语言关键字，因此这个函数命名为swtch 来避免冲突），这是整个线程切换的核心函数之一； swtch函数会保存用户进程P1对应内核线程的寄存器至context对象。所以目前为止有两类寄存器：用户寄存器存在trapframe中，内核线程的寄存器存在context中。 但是，实际上swtch函数并不是直接从一个内核线程切换到另一个内核线程。\nXV6中，一个CPU上运行的内核线程可以直接切换到的是这个CPU对应的调度器线程。\n所以如果我们运行在CPU0，swtch函数会恢复之前为CPU0的调度器线程保存的寄存器和stack pointer，之后就在调度器线程的context下执行schedulder函数中（注，后面代码分析有介绍）。\n在schedulder函数中会做一些清理工作，例如将进程P1设置成RUNABLE状态。\n之后再通过进程表单找到下一个RUNABLE进程。\n假设找到的下一个进程是P2（虽然也有可能找到的还是P1），schedulder函数会再次调用swtch函数，完成下面步骤：\n先保存自己的寄存器到调度器线程的context对象 找到进程P2之前保存的context，恢复其中的寄存器 因为进程P2在进入RUNABLE状态之前，如刚刚介绍的进程P1一样，必然也调用了swtch函数。所以之前的swtch函数会被恢复，并返回到进程P2所在的系统调用或者中断处理程序中（注，因为P2进程之前调用swtch函数必然在系统调用或者中断处理程序中）。 不论是系统调用也好中断处理程序也好，在从用户空间进入到内核空间时会保存用户寄存器到trapframe对象。所以当内核程序执行完成之后，trapframe中的用户寄存器会被恢复。 最后用户进程P2就恢复运行了。 每一个CPU都有一个完全不同的调度器线程。\n调度器线程也是一种内核线程，它也有自己的context对象。\n任何运行在CPU1上的进程，当它决定出让CPU，它都会切换到CPU1对应的调度器线程，并由调度器线程切换到下一个进程。\n学生提问：context保存在哪？\nRobert教授：\n每一个内核线程都有一个context对象。但是内核线程实际上有两类。\n每一个用户进程有一个对应的内核线程，它的context对象保存在用户进程对应的proc结构体中。\n每一个调度器线程，它也有自己的context对象，但是它却没有对应的进程和proc结构体，所以调度器线程的context对象保存在cpu结构体中。\n在内核中，有一个cpu结构体的数组，每个cpu结构体对应一个CPU核，每个结构体中都有一个context字段。\n学生提问：为什么不能将context对象保存在进程对应的trapframe中？\nRobert教授：context可以保存在trapframe中，因为每一个进程都只有一个内核线程对应的一组寄存器，我们可以将这些寄存器保存在任何一个与进程一一对应的数据结构中。\n对于每个进程来说，有一个proc结构体，有一个trapframe结构体，所以我们可以将context保存于trapframe中。\n但是或许出于简化代码或者让代码更清晰的目的，trapframe还是只包含进入和离开内核时的数据。\n而context结构体中包含的是在内核线程和调度器线程之间切换时，需要保存和恢复的数据。\n学生提问：出让CPU是由用户发起的还是由内核发起的？\nRobert教授：对于XV6来说，并不会直接让用户线程出让CPU或者完成线程切换，而是由内核在合适的时间点做决定。\n有的时候你可以猜到特定的系统调用会导致出让CPU，例如一个用户进程读取pipe，而它知道pipe中并不能读到任何数据，这时你可以预测读取会被阻塞，而内核在等待数据的过程中会运行其他的进程。\n内核会在两个场景下出让CPU。\n当定时器中断触发了，内核总是会让当前进程出让CPU，因为我们需要在定时器中断间隔的时间点上交织执行所有想要运行的进程。\n另一种场景就是任何时候一个进程调用了系统调用并等待I/O，例如等待你敲入下一个按键，在你还没有按下按键时，等待I/O的机制会触发出让CPU。\n学生提问：用户进程调用sleep函数是不是会调用某个系统调用，然后将用户进程的信息保存在trapframe，然后触发进程切换，这时就不是定时器中断决定，而是用户进程自己决定了吧？\nRobert教授：如果进程执行了read系统调用，然后进入到了内核中。\n而read系统调用要求进程等待磁盘，这时系统调用代码会调用sleep，而sleep最后会调用swtch函数。\nswtch函数会保存内核线程的寄存器到进程的context中，然后切换到对应CPU的调度器线程，再让其他的线程运行。\n这样在当前线程等待磁盘读取结束时，其他线程还能运行。\n所以，这里的流程除了没有定时器中断，其他都一样，只是这里是因为一个系统调用需要等待I/O（注，感觉答非所问）\n学生提问：每一个CPU的调度器线程有自己的栈吗？\nRobert教授：是的，每一个调度器线程都有自己独立的栈。\n实际上调度器线程的所有内容，包括栈和context，与用户进程不一样，都是在系统启动时就设置好了。\n如果你查看XV6的start.s（注：是entry.S和start.c）文件，你就可以看到为每个CPU核设置好调度器线程。\n这里有一个术语需要解释一下。\n当人们在说context switching，他们通常说的是从一个线程切换到另一个线程，因为在切换的过程中需要先保存前一个线程的寄存器，然后再恢复之前保存的后一个线程的寄存器，这些寄存器都是保存在context对象中。\n在有些时候，context switching也指从一个用户进程切换到另一个用户进程的完整过程。\n偶尔你也会看到context switching是指从用户空间和内核空间之间的切换。\n对于我们这节课来说，context switching主要是指一个内核线程和调度器线程之间的切换。\n这里有一些有用的信息可以记住。\n每一个CPU核在一个时间只会做一件事情，每个CPU核在一个时间只会运行一个线程，它要么是运行用户进程的线程，要么是运行内核线程，要么是运行这个CPU核对应的调度器线程。\n所以在任何一个时间点，CPU核并没有做多件事情，而是只做一件事情。\n线程的切换创造了多个线程同时运行在一个CPU上的假象。\n类似的每一个线程要么是只运行在一个CPU核上，要么它的状态被保存在context中。\n线程永远不会运行在多个CPU核上，线程要么运行在一个CPU核上，要么就没有运行。\n在XV6的代码中，context对象总是由swtch函数产生，所以context总是保存了内核线程在执行swtch函数时的状态。\n当我们在恢复一个内核线程时，对于刚恢复的线程所做的第一件事情就是从之前的swtch函数中返回。\n学生提问：我们这里一直在说线程，但是从我看来XV6的实现中，一个进程就只有一个线程，有没有可能一个进程有多个线程？\nRobert教授：我们这里的用词的确有点让人混淆。\n在XV6中，一个进程要么在用户空间执行指令，要么是在内核空间执行指令，要么它的状态被保存在context和trapframe中，并且没有执行任何指令。\n这里该怎么称呼它呢？你可以根据自己的喜好来称呼它，对于我来说，每个进程有两个线程，一个用户空间线程，一个内核空间线程，并且存在限制使得一个进程要么运行在用户空间线程，要么为了执行系统调用或者响应中断而运行在内核空间线程 ，但是永远也不会两者同时运行。\nXV6进程切换示例程序 我们先来看一下proc.h中的proc结构体，从结构体中我们可以看到很多之前介绍的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Per-process state struct proc { struct spinlock lock; // p-\u0026gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent\u0026#39;s wait int pid; // Process ID // these are private to the process, so p-\u0026gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) }; 首先是保存了用户空间线程寄存器的trapframe字段 其次是保存了内核线程寄存器的context字段 还有保存了当前进程的内核栈的kstack字段，这是进程在内核中执行时保存函数调用的位置 state字段保存了当前进程状态，要么是RUNNING，要么是RUNABLE，要么是SLEEPING等等 lock字段保护了很多数据，目前来说至少保护了对于state字段的更新。举个例子，因为有锁的保护，两个CPU的调度器线程不会同时拉取同一个RUNABLE进程并运行它 接下来会运行一个简单的演示程序，在这个程序中我们会从一个进程切换到另一个。\n这个程序中会创建两个进程，两个进程会一直运行。\n代码首先通过fork创建了一个子进程，然后两个进程都会进入一个死循环，并每隔一段时间生成一个输出表明程序还在运行。\n但是它们都不会很频繁的打印输出（注，每隔1000000次循环才打印一个输出），并且它们也不会主动出让CPU（注，因为每个进程都执行的是没有sleep的死循环）。\n所以我们这里有了两个运算密集型进程，并且因为我们接下来启动的XV6只有一个CPU核，它们都运行在同一个CPU上。\n为了让这两个进程都能运行，有必要让两个进程之间能相互切换。\n接下来运行spin程序，\n可以看到一直有字符在输出，一个进程在输出“/”，另一个进程在输出\u0026quot;\u0026quot;。\n从输出看，虽然现在XV6只有一个CPU核，但是每隔一会，XV6就在两个进程之间切换。\n“/”输出了一会之后，定时器中断将CPU切换到另一个进程运行然后又输出“\\”一会。所以可以看到定时器中断在起作用。\n接下来，在trap.c的devintr函数中的207行设置一个断点，这一行会识别出当前是在响应定时器中断。\n之后在gdb中continue。立刻会停在中断的位置，因为定时器中断还是挺频繁的。现在我们可以确认我们在usertrap函数中，并且usertrap函数通过调用devintr函数来处理这里的中断（注，从下图的栈输出可以看出）。\n因为devintr函数处理定时器中断的代码基本没有内容，接下来我在gdb中输入finish来从devintr函数中返回到usertrap函数。\n当我们返回到usertrap函数时，虽然我们刚刚从devintr函数中返回，但是我们期望运行到下面的yield函数。所以我们期望devintr函数返回2。\n可以从gdb中看到devintr的确返回的是2。\n在yield函数中，当前进程会出让CPU并让另一个进程运行。\n现在让我们看一下当定时器中断发生的时候，用户空间进程正在执行什么内容。\n我在gdb中输入print p来打印名称为p的变量。变量p包含了当前进程的proc结构体。\n学生提问：怎么区分不同进程的内核线程？\nRobert教授：每一个进程都有一个独立的内核线程。实际上有两件事情可以区分不同进程的内核线程。\n其中一件是，每个进程都有不同的内核栈，它由proc结构体中的kstack字段所指向；\n另一件就是，任何内核代码都可以通过调用myproc函数来获取当前CPU正在运行的进程。\n内核线程可以通过调用这个函数知道自己属于哪个用户进程。\nmyproc函数会使用tp寄存器来获取当前的CPU核的ID，并使用这个ID在一个保存了所有CPU上运行的进程的结构体数组中，找到对应的proc结构体。这就是不同的内核线程区分自己的方法。\n首先会打印p-\u0026gt;name来获取进程的名称，\n当前进程是spin程序，如预期一样。\n当前的进程ID是3，进程切换之后，我们预期进程ID会不一样。\n我们还可以通过打印变量p的trapframe字段获取表示用户空间状态的32个寄存器，这些都是我们在Lec06中学过的内容。这里面最有意思的可能是trapframe中保存的用户程序计数器。\n可以查看spin.asm文件来确定对应地址的指令。\n可以看到定时器中断触发时，用户进程正在执行死循环的加1，这符合我们的预期。\n学生提问：看起来所有的CPU核要能完成线程切换都需要有一个定时器中断，那如果硬件定时器出现故障了怎么办？\nRobert教授：是的，总是需要有一个定时器中断。\n用户进程的pre-emptive scheduling能工作的原因是，用户进程运行时，中断总是打开的。\nXV6会确保返回到用户空间时，中断是打开的。\n这意味着当代码在用户空间执行时，定时器中断总是能发生。\n在内核中会更加复杂点，因为内核中偶尔会关闭中断，比如当获取锁的时候，中断会被关闭，只有当锁被释放之后中断才会重新打开，所以如果内核中有一些bug导致内核关闭中断之后再也没有打开中断，同时内核中的代码永远也不会释放CPU，那么定时器中断不会发生。\n但是因为XV6是我们写的，所以它总是会重新打开中断。XV6中的代码如果关闭了中断，它要么过会会重新打开中断，然后内核中定时器中断可以发生并且我们可以从这个内核线程切换走，要么代码会返回到用户空间。我们相信XV6中不会有关闭中断然后还死循环的代码。\n同一个学生提问：我的问题是，定时器中断是来自于某个硬件，如果硬件出现故障了呢？\nRobert教授：那你的电脑坏了，你要买个新电脑了。\n这个问题是可能发生的，因为电脑中有上亿的晶体管，有的时候电脑会有问题，但是这超出了内核的管理范围了。\n所以我们假设计算机可以正常工作。\n有的时候软件会尝试弥补硬件的错误，比如通过网络传输packet，总是会带上checksum，这样如果某个网络设备故障导致某个bit反转了，可以通过checksum发现这个问题。\n但是对于计算机内部的问题，人们倾向于不用软件来尝试弥补硬件的错误。\n学生提问：当一个线程结束执行了，比如说在用户空间通过exit系统调用结束线程，同时也会关闭进程的内核线程。那么线程结束之后和下一个定时器中断之间这段时间，CPU仍然会被这个线程占有吗？还是说我们在结束线程的时候会启动一个新的线程？\nRobert教授：exit系统调用会出让CPU。\n尽管我们这节课主要是基于定时器中断来讨论，但是实际上XV6切换线程的绝大部分场景都不是因为定时器中断，比如说一些系统调用在等待一些事件并决定让出CPU。\nexit系统调用会做各种操作然后调用yield函数来出让CPU，这里的出让并不依赖定时器中断。\nXV6线程切换\u0026ndash;yield/sched函数 回到devintr函数返回到usertrap函数中的位置。\n在gdb里面输入几次step走到yield函数的调用。\nyield函数是整个线程切换的第一步，下面是yield函数的内容：\n1 2 3 4 5 6 7 8 9 10 // Give up the CPU for one scheduling round. void yield(void) { struct proc *p = myproc(); acquire(\u0026amp;p-\u0026gt;lock); p-\u0026gt;state = RUNNABLE; sched(); release(\u0026amp;p-\u0026gt;lock); } yield函数只做了几件事情，它首先获取了进程的锁。\n实际上，在锁释放之前，进程的状态会变得不一致，例如，yield将要将进程的状态改为RUNABLE，表明进程并没有在运行，但是实际上这个进程还在运行，代码正在当前进程的内核线程中运行。\n所以这里加锁的目的之一就是：即使我们将进程的状态改为了RUNABLE，其他的CPU核的调度器线程也不可能看到进程的状态为RUNABLE并尝试运行它。\n否则的话，进程就会在两个CPU核上运行了，而一个进程只有一个栈，这意味着两个CPU核在同一个栈上运行代码（注，因为XV6中一个用户进程只有一个用户线程）。\n接下来yield函数中将进程的状态改为RUNABLE。\n这里的意思是，当前进程要出让CPU，并切换到调度器线程。\n当前进程的状态是RUNABLE意味着它还会再次运行，因为毕竟现在是一个定时器中断打断了当前正在运行的进程。\n之后yield函数中调用了位于proc.c文件中的sched函数。\n我们进入到sched函数中，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // Switch to scheduler. Must hold only p-\u0026gt;lock // and have changed proc-\u0026gt;state. Saves and restores // intena because intena is a property of this // kernel thread, not this CPU. It should // be proc-\u0026gt;intena and proc-\u0026gt;noff, but that would // break in the few places where a lock is held but // there\u0026#39;s no process. void sched(void) { int intena; struct proc *p = myproc(); if(!holding(\u0026amp;p-\u0026gt;lock)) panic(\u0026#34;sched p-\u0026gt;lock\u0026#34;); if(mycpu()-\u0026gt;noff != 1) panic(\u0026#34;sched locks\u0026#34;); if(p-\u0026gt;state == RUNNING) panic(\u0026#34;sched running\u0026#34;); if(intr_get()) panic(\u0026#34;sched interruptible\u0026#34;); intena = mycpu()-\u0026gt;intena; swtch(\u0026amp;p-\u0026gt;context, \u0026amp;mycpu()-\u0026gt;context); mycpu()-\u0026gt;intena = intena; } 可以看出，sched函数基本没有干任何事情，只是做了一些合理性检查，如果发现异常就panic。\n为什么会有这么多检查？因为这里的XV6代码已经有很多年的历史了，这些代码经历过各种各样的bug，相应的这里就有各种各样的合理性检查和panic来避免可能的bug。\n我将跳过所有的检查，直接走到位于底部的swtch函数。\nXV6线程切换\u0026ndash;switch函数\nswtch函数会将当前的内核线程的寄存器保存到p-\u0026gt;context中。\nswtch函数的另一个参数c-\u0026gt;context，c表示当前CPU的结构体。\nCPU结构体中的context保存了当前CPU核的调度器线程的寄存器。\n所以swtch函数在保存完当前内核线程的内核寄存器之后，就会恢复当前CPU核的调度器线程的寄存器，并继续执行当前CPU核的调度器线程。\n接下来，我们快速的看一下我们将要切换到的context（注，也就是调度器线程的context）。\n因为我们只有一个CPU核，这里我在gdb中print cpus[0].context\n这里看到的就是之前保存的当前CPU核的调度器线程的寄存器。\n在这些寄存器中，最有趣的就是ra（Return Address）寄存器，因为ra寄存器保存的是当前函数的返回地址，所以调度器线程中的代码会返回到ra寄存器中的地址。\n通过查看kernel.asm，我们可以知道这个地址的内容是什么。也可以在gdb中输入“x/i 0x80001f2e”进行查看。\n输出中包含了地址中的指令和指令所在的函数名。\n所以我们将要返回到scheduler函数中。\n因为我们接下来要调用swtch函数，让我们来看看swtch函数的内容。swtch函数位于switch.s文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Context switch # # void swtch(struct context *old, struct context *new); # # Save current registers in old. Load from new.\t.globl swtch swtch: sd ra, 0(a0) sd sp, 8(a0) sd s0, 16(a0) sd s1, 24(a0) sd s2, 32(a0) sd s3, 40(a0) sd s4, 48(a0) sd s5, 56(a0) sd s6, 64(a0) sd s7, 72(a0) sd s8, 80(a0) sd s9, 88(a0) sd s10, 96(a0) sd s11, 104(a0) ld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1) ret 首先，ra寄存器被保存在了a0寄存器指向的地址。\na0寄存器对应了swtch函数的第一个参数，从前面可以看出这是当前线程的context对象地址 ；\na1寄存器对应了swtch函数的第二个参数，从前面可以看出这是即将要切换到的调度器线程的context对象地址。\n所以函数中上半部分是将当前的寄存器保存在当前线程对应的context对象中，函数的下半部分是将调度器线程的寄存器，也就是我们将要切换到的线程的寄存器恢复到CPU的寄存器中。\n之后函数就返回了。所以调度器线程的ra寄存器的内容才显得有趣，因为它指向的是swtch函数返回的地址，也就是scheduler函数。\nswtch函数的上半部分保存了ra，sp等等寄存器，但是并没有保存程序计数器pc（Program Counter），因为程序计数器不管怎样都会随着函数调用更新。\n程序计数器并没有有效信息，我们现在知道我们在swtch函数中执行，所以保存程序计数器并没有意义。\n但是我们关心的是我们是从哪调用进到swtch函数的，因为当我们通过switch恢复执行当前线程并且从swtch函数返回时，我们希望能够从调用点继续执行。\nra寄存器保存了swtch函数的调用点，所以这里保存的是ra寄存器。\n我们可以打印ra寄存器，如你们所预期的一样，它指向了sched函数。\n为什么RISC-V中有32个寄存器，但是swtch函数中只保存并恢复了14个寄存器？\n因为switch是按照一个普通函数来调用的，对于有些寄存器，swtch函数的调用者默认swtch函数会做修改，所以调用者已经在自己的栈上保存了这些寄存器，当函数返回时，这些寄存器会自动恢复。所以swtch函数里只需要保存Callee Saved Register就行。\n因为swtch函数是从C代码调用的，所以我们知道Caller Saved Register会被C编译器保存在当前的栈上。\nCaller Saved Register大概有15-18个，而我们在swtch函数中只需要处理C编译器不会保存，但是对于swtch函数又有用的一些寄存器。\n最后我想看的是sp（Stack Pointer）寄存器。\n从它的值很难看出它的意义是什么。它实际是当前进程的内核栈地址，它由虚拟内存系统映射在了一个高地址。\n现在，我们保存了当前的寄存器，并从调度器线程的context对象恢复了寄存器，我直接跳到swtch函数的最后，也就是ret指令的位置。\n在我们实际返回之前，我们再来打印一些有趣的寄存器。\n首先sp寄存器有了一个不同的值，sp寄存器的值现在在内存中的stack0区域中。\n这个区域实际上是在启动顺序中非常非常早的一个位置，start.s在这个区域创建了栈，这样才可以调用第一个C函数。\n所以调度器线程运行在CPU对应的bootstack上。\n其次是ra寄存器，现在指向了scheduler函数，因为我们恢复了调度器线程的context对象中的内容。\n现在，我们其实已经在调度器线程中了，这里寄存器的值与上次打印的已经完全不一样了。\n虽然我们还在swtch函数中，但是现在我们实际上位于调度器线程调用的swtch函数中。\n调度器线程在启动过程中调用的也是swtch函数。\n接下来通过执行ret指令，我们就可以返回到调度器线程中。\n学生提问：我不知道我们使用的RISC-V处理器是不是有一些其他的状态？但是我知道一些Intel的X86芯片有floating point unit state等其他的状态，我们需要处理这些状态吗？\nRobert教授：你的观点非常对。在一些其他处理器例如X86中，线程切换的细节略有不同，因为不同的处理器有不同的状态。\n所以我们这里介绍的代码非常依赖RISC-V。\n其他处理器的线程切换流程可能看起来会非常的不一样，比如说可能要保存floating point寄存器。我不知道RISC-V如何处理浮点数，但是XV6内核并没有使用浮点数，所以不必担心。但是是的，线程切换与处理器非常相关。\n学生提问：为什么swtch函数要用汇编来实现，而不是C语言？\nRobert教授：C语言中很难与寄存器交互。\n可以肯定的是C语言中没有方法能更改sp、ra寄存器。\n所以在普通的C语言中很难完成寄存器的存储和加载，唯一的方法就是在C中嵌套汇编语言。\n所以我们也可以在C函数中内嵌switch中的指令，但是这跟我们直接定义一个汇编函数是一样的。\n或者说swtch函数中的操作是在C语言的层级之下，所以并不能使用C语言。\nXV6线程切换\u0026ndash;scheduler 来看一下scheduler的完整代码，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // Per-CPU process scheduler. // Each CPU calls scheduler() after setting itself up. // Scheduler never returns. It loops, doing: // - choose a process to run. // - swtch to start running that process. // - eventually that process transfers control // via swtch back to the scheduler. void scheduler(void) { struct proc *p; struct cpu *c = mycpu(); c-\u0026gt;proc = 0; for(;;){ // Avoid deadlock by ensuring that devices can interrupt. intr_on(); int nproc = 0; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++) { acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state != UNUSED) { nproc++; } if(p-\u0026gt;state == RUNNABLE) { // Switch to chosen process. It is the process\u0026#39;s job // to release its lock and then reacquire it // before jumping back to us. p-\u0026gt;state = RUNNING; c-\u0026gt;proc = p; swtch(\u0026amp;c-\u0026gt;context, \u0026amp;p-\u0026gt;context); // Process is done running for now. // It should have changed its p-\u0026gt;state before coming back. c-\u0026gt;proc = 0; } release(\u0026amp;p-\u0026gt;lock); } if(nproc \u0026lt;= 2) { // only init and sh exist intr_on(); asm volatile(\u0026#34;wfi\u0026#34;); } } } 现在我们正运行在CPU拥有的调度器线程中，并且我们正好在之前调用swtch函数的返回状态。\n之前调度器线程调用switch是因为想要运行pid为3的进程，也就是刚刚被中断的spin程序。\n虽然pid为3的spin进程也调用了swtch函数，但是那个switch并不是当前返回的这个switch。\nspin进程调用的swtch函数还没有返回，而是保存在了pid为3的栈和context对象中。现在返回的是之前调度器线程对于swtch函数的调用。\n在scheduler函数中，因为我们已经停止了spin进程的运行，所以我们需要抹去对于spin进程的记录。\n我们接下来将c-\u0026gt;proc设置为0（c-\u0026gt;proc = 0;）。\n因为我们现在并没有在这个CPU核上运行这个进程，为了不让任何人感到困惑，我们这里将CPU核运行的进程对象设置为0。\n之前在yield函数中获取了进程的锁，因为yield不想进程完全进入到Sleep状态之前，任何其他的CPU核的调度器线程看到这个进程并运行它。\n而现在我们完成了从spin进程切换走，所以现在可以释放锁了。\n这就是release(\u0026amp;p-\u0026gt;lock)的意义。\n现在，我们仍然在scheduler函数中，但是其他的CPU核可以找到spin进程，并且因为spin进程是RUNABLE状态，其他的CPU可以运行它。\n这没有问题，因为我们已经完整的保存了spin进程的寄存器，并且我们不在spin进程的栈上运行程序，而是在当前CPU核的调度器线程栈上运行程序，所以其他的CPU核运行spin程序并没有问题。\n但是因为启动QEMU时我们只指定了一个核，所以在我们现在的演示中并没有其他的CPU核来运行spin程序。\n接下来我将简单介绍一下p-\u0026gt;lock。从调度的角度来说，这里的锁完成了两件事情。\n首先，出让CPU涉及到很多步骤，我们需要将进程的状态从RUNNING改成RUNABLE，我们需要将进程的寄存器保存在context对象中，并且我们还需要停止使用当前进程的栈。\n所以这里至少有三个步骤，而这三个步骤需要花费一些时间。\n所以锁的第一个工作就是在这三个步骤完成之前，阻止任何一个其他核的调度器线程看到当前进程。\n锁这里确保了三个步骤的原子性。从CPU核的角度来说，三个步骤要么全发生，要么全不发生。\n第二，当我们开始要运行一个进程时，p-\u0026gt;lock也有类似的保护功能。\n当我们要运行一个进程时，我们需要将进程的状态设置为RUNNING，我们需要将进程的context移到RISC-V的寄存器中。\n但是，如果在这个过程中，发生了中断，从中断的角度来说进程将会处于一个奇怪的状态。\n比如说进程的状态是RUNNING，但是又还没有将所有的寄存器从context对象拷贝到RISC-V寄存器中。\n所以，如果这时候有了一个定时器中断将会是个灾难，因为我们可能在寄存器完全恢复之前，从这个进程中切换走。\n而从这个进程切换走的过程中，将会保存不完整的RISC-V寄存器到进程的context对象中。\n所以我们希望启动一个进程的过程也具有原子性。\n在这种情况下，切换到一个进程的过程中，也需要获取进程的锁以确保其他的CPU核不能看到这个进程。\n同时在切换到进程的过程中，还需要关闭中断，这样可以避免定时器中断看到还在切换过程中的进程。\n现在我们在scheduler函数的循环中，代码会检查所有的进程并找到一个来运行。\n现在我们知道还有另一个进程，因为我们之前fork了另一个spin进程。\n这里我跳过进程检查，直接在找到RUNABLE进程的位置设置一个断点。\n在代码的468行，获取了进程的锁，所以现在我们可以进行切换到进程的各种步骤。\n在代码的473行，进程的状态被设置成了RUNNING。\n代码的474行将找到的RUNABLE进程记录为当前CPU执行的进程。\n代码的475行，又调用了swtch函数来保存调度器线程的寄存器，并恢复目标进程的寄存器（注，实际上恢复的是目标进程的内核线程）。\n我们可以打印新的进程的名字来查看新的进程。可以看到进程名还是spin，但是pid已经变成了4，而前一个进程的pid是3。\n我们还可以查看目标进程的context对象，其中ra寄存器的内容就是我们要切换到的目标线程的代码位置。\n虽然我们在代码475行调用的是swtch函数，但是我们前面已经看过了swtch函数会返回到即将恢复的ra寄存器地址，所以我们真正关心的就是ra指向的地址。\n通过打印这个地址的内容，可以看到swtch函数会返回到sched函数中。\n这完全在意料之中，因为可以预期的是，将要切换到的进程之前是被定时器中断通过sched函数挂起的，并且之前在sched函数中又调用了swtch函数。\n在swtch函数的最开始，我们仍然在调度器线程中，但是这一次是从调度器线程切换到目标进程的内核线程。\n所以从swtch函数内部将会返回到目标进程的内核线程的sched函数，通过打印backtrace，我们可以看到，之前有一个usertrap的调用，这必然是之前因为定时器中断而出现的调用。\n之后在中断处理函数中还调用了yield和sched函数，正如我们之前看到的一样。\n但是，这里调用yield和sched函数是在pid为4的进程调用的，而不是我们刚刚看的pid为3的进程。\n学生提问：如果不是因为定时器中断发生的切换，我们是不是可以期望ra寄存器指向其他位置，例如sleep函数？\nRobert教授：是的，我们之前看到了代码执行到这里会包含一些系统调用相关的函数。\n你基本上回答了自己的问题，如果我们因为定时器中断之外的原因而停止了执行当前的进程，switch会返回到一些系统调用的代码中，而不是我们这里看到sched函数。\n我记得sleep最后也调用了sched函数，虽然bracktrace可能看起来会不一样，但是还是会包含sched。\n所以我这里只介绍了一种进程间切换的方法，也就是因为定时器中断而发生切换。\n但是还有其他的可能会触发进程切换，例如等待I/O或者等待另一个进程向pipe写数据。\n这里有件事情需要注意，调度器线程调用了swtch函数，但是我们从swtch函数返回时，实际上是返回到了对于switch的另一个调用，而不是调度器线程中的调用。\n我们返回到的是pid为4的进程在很久之前对于switch的调用。这里可能会有点让人困惑，但是这就是线程切换的核心。\n另一件需要注意的事情是，swtch函数是线程切换的核心，但是swtch函数中只有保存寄存器，再加载寄存器的操作。\n线程除了寄存器以外的还有很多其他状态，它有变量，堆中的数据等等，但是所有的这些数据都在内存中，并且会保持不变。\n我们没有改变线程的任何栈或者堆数据。\n所以线程切换的过程中，处理器中的寄存器是唯一的不稳定状态，且需要保存并恢复。\n而所有其他在内存中的数据会保存在内存中不被改变，所以不用特意保存并恢复。\n我们只是保存并恢复了处理器中的寄存器，因为我们想在新的线程中也使用相同的一组寄存器。\nXV6线程第一次调用switch函数 学生提问Linux内一个进程多个线程的实现方式，因为在XV6中，一个进程只有一个用户线程\n学生提问：操作系统都带了线程的实现，如果想要在多个CPU上运行一个进程内的多个线程，那需要通过操作系统来处理而不是用户空间代码，是吧？那这里的线程切换是怎么工作的？是每个线程都与进程一样了吗？操作系统还会遍历所有存在的线程吗？比如说我们有8个核，每个CPU核都会在多个进程的更多个线程之间切换。同时我们也不想只在一个CPU核上切换一个进程的多个线程，是吧？\nRobert教授：Linux是支持一个进程包含多个线程，Linux的实现比较复杂，或许最简单的解释方式是：几乎可以认为Linux中的每个线程都是一个完整的进程。\nLinux中，我们平常说一个进程中的多个线程，本质上是共享同一块内存的多个独立进程。\n所以Linux中一个进程的多个线程仍然是通过一个内存地址空间执行代码。\n如果你在一个进程创建了2个线程，那基本上是2个进程共享一个地址空间。\n之后，调度就与XV6是一致的，也就是针对每个进程进行调度。\n学生提问：用户可以指定将线程绑定在某个CPU上吗？操作系统如何确保一个进程的多个线程不会运行在同一个CPU核上？要不然就违背了多线程的初衷了。\nRobert教授：这里其实与XV6非常相似，假设有4个CPU核，Linux会找到4件事情运行在这4个核上。\n如果并没有太多正在运行的程序的话，或许会将一个进程的4个线程运行在4个核上。\n或者如果有100个用户登录在Athena机器上，内核会随机为每个CPU核找到一些事情做。\n如果你想做一些精细的测试，有一些方法可以将线程绑定在CPU核上，但正常情况下人们不会这么做。\n学生提问：所以说一个进程中的多个线程会有相同的page table？\nRobert教授：是的，如果你在Linux上，你为一个进程创建了2个线程，我不确定它们是不是共享同一个的page table，还是说它们是不同的page table，但是内容是相同的。\n学生提问：有没有原因说这里的page table要是分开的？\nRobert教授：我不知道Linux究竟用了哪种方法。\n以下是线程第一次调用switch的过程\n学生提问：当调用swtch函数的时候，实际上是从一个线程对于switch的调用切换到了另一个线程对于switch的调用。所以线程第一次调用swtch函数时，需要伪造一个“另一个线程”对于switch的调用，是吧？因为也不能通过swtch函数随机跳到其他代码去。\nRobert教授：是的。我们来看一下第一次调用switch时，“另一个”调用swtch函数的线程的context对象。\nproc.c文件中的allocproc函数会被启动时的第一个进程和fork调用，allocproc会设置好新进程的context，如下所示：\n实际上大部分寄存器的内容都无所谓。\n但是ra很重要，因为这是进程的第一个switch调用会返回的位置。\n同时因为进程需要有自己的栈，所以ra和sp都被设置了。\n这里设置的forkret函数就是进程的第一次调用swtch函数会切换到的“另一个”线程位置。\n学生提问：所以当swtch函数返回时，CPU会执行forkret中的指令，就像forkret刚刚调用了swtch函数并且返回了一样？\nRobert教授：是的，从switch返回就直接跳到了forkret的最开始位置。\n学生提问：我们会在其他场合调用forkret吗？还是说它只会用在这？\nRobert教授：是的，它只会在启动进程的时候以这种奇怪的方式运行。下面是forkret函数的代码，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // A fork child\u0026#39;s very first scheduling by scheduler() // will swtch to forkret. void forkret(void) { static int first = 1; // Still holding p-\u0026gt;lock from scheduler. release(\u0026amp;myproc()-\u0026gt;lock); if (first) { // File system initialization must be run in the context of a // regular process (e.g., because it calls sleep), and thus cannot // be run from main(). first = 0; fsinit(ROOTDEV); } usertrapret(); } 从代码中看，它的工作其实就是释放调度器之前获取的锁。\n函数最后的usertrapret函数其实也是一个假的函数，它会使得程序表现的看起来像是从trap中返回，但是对应的trapframe其实也是假的，这样才能跳到用户的第一个指令中。\n学生提问：与之前的context对象类似的是，对于trapframe也不用初始化任何寄存器，因为我们要去的是程序的最开始，所以不需要做任何假设，对吧？\nRobert教授：我认为程序计数器还是要被初始化为0的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Set up first user process. void userinit(void) { struct proc *p; p = allocproc(); initproc = p; // allocate one user page and copy init\u0026#39;s instructions // and data into it. uvminit(p-\u0026gt;pagetable, initcode, sizeof(initcode)); p-\u0026gt;sz = PGSIZE; // prepare for the very first \u0026#34;return\u0026#34; from kernel to user. p-\u0026gt;trapframe-\u0026gt;epc = 0; // user program counter p-\u0026gt;trapframe-\u0026gt;sp = PGSIZE; // user stack pointer safestrcpy(p-\u0026gt;name, \u0026#34;initcode\u0026#34;, sizeof(p-\u0026gt;name)); p-\u0026gt;cwd = namei(\u0026#34;/\u0026#34;); p-\u0026gt;state = RUNNABLE; release(\u0026amp;p-\u0026gt;lock); } 因为fork拷贝的进程会同时拷贝父进程的程序计数器，所以我们唯一不是通过fork创建进程的场景就是创建第一个进程的时候。这时需要设置程序计数器为0。\n学生提问：在fortret函数中，if(first)是什么意思？\nRobert教授：文件系统需要被初始化，具体来说，需要从磁盘读取一些数据来确保文件系统的运行，比如说文件系统究竟有多大，各种各样的东西在文件系统的哪个位置，同时还需要有crash recovery log。\n完成任何文件系统的操作都需要等待磁盘操作结束，但是XV6只能在进程的context下执行文件系统操作，比如等待I/O。所以初始化文件系统需要等到我们有了一个进程才能进行。而这一步是在第一次调用forkret时完成的，所以在forkret中才有了if(first)判断。\n","date":"2024-09-21T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08111thread-switching/bg_hu14431857381732514318.jpg","permalink":"https://echudet.github.io/p/mit6s08111thread-switching/","title":"MIT6S081：11Thread switching"},{"content":"Lecture 10 Multiprocessors and locking 预习 为什么要用锁 大多数内核，包括xv6，交错执行多个活动。交错的一个来源是多处理器硬件：计算机的多个CPU之间独立执行，如xv6的RISC-V。多个处理器共享物理内存，xv6利用共享（sharing）来维护所有CPU进行读写的数据结构。\n这种共享增加了一种可能性，即一个CPU读取数据结构，而另一个CPU正在更新它，甚至多个CPU同时更新相同的数据；如果不仔细设计，这种并行访问可能会产生不正确的结果或损坏数据结构。\n即使在单处理器上，内核也可能在许多线程之间切换CPU，导致它们的执行交错。\n最后，如果中断发生在错误的时间，设备中断处理程序修改与某些可中断代码相同的数据，可能导致数据损坏。\n单词并发（concurrency）是指由于多处理器并行、线程切换或中断，多个指令流交错的情况。\n内核中充满了并发访问数据（concurrently-accessed data）。例如，两个CPU可以同时调用kalloc，从而从空闲列表的头部弹出。\n以并发下的正确性为目标的策略和支持它们的抽象称为并发控制技术（concurrency control techniques）。\n锁的作用 锁提供了互斥，确保一次只有一个CPU可以持有锁。如果程序员将每个共享数据项关联一个锁，并且代码在使用一个数据项时总是持有相关联的锁，那么该项一次将只被一个CPU使用。在这种情况下，我们说锁保护数据项。尽管锁是一种易于理解的并发控制机制，但锁的缺点是它们会扼杀性能，因为它们会串行化并发操作。\n竞态条件 考虑两个进程在两个不同的CPU上调用wait。wait释放了子进程的内存。因此，在每个CPU上，内核将调用kfree来释放子进程的页面。内核分配器维护一个链接列表：kalloc() 从空闲页面列表中取出（pop）一个内存页面；kfree()将一个内存页面添加（push）到空闲列表上。为了获得最佳性能，我们可能希望两个父进程的kfree可以并行执行，而不必等待另一个进程，但是考虑到xv6的kfree实现，这将导致错误。\n链表位于两个CPU共享的内存中，这两个CPU使用load和store指令操作链表。（实际上，每个处理器都有cache，但从概念上讲，多处理器系统的行为就像所有CPU共享一块单独的内存一样）如果没有并发请求，您可能以如下方式实现列表push操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct element { int data; struct element *next; }; struct element *list = 0; void push(int data) { struct element *l; l = malloc(sizeof *l); l-\u0026gt;data = data; l-\u0026gt;next = list; list = l; } 如果存在隔离性，那么这个实现是正确的。但是，如果多个副本并发执行，代码就会出错。如果两个CPU同时执行push，如图6.1所示，两个CPU都可能在执行第16行之前执行第15行，这会导致如图6.2所示的不正确的结果。然后会有两个类型为element的列表元素使用next指针设置为list的前一个值。当两次执行位于第16行的对list的赋值时，第二次赋值将覆盖第一次赋值；第一次赋值中涉及的元素将丢失。\n竞态条件是指多个进程读写某些共享数据（至少有一个访问是写入）的情况。竞争通常包含bug，要么丢失更新（如果访问是写入的），要么读取未完成更新的数据结构。竞争的结果取决于进程在处理器运行的确切时机以及内存系统如何排序它们的内存操作，这可能会使竞争引起的错误难以复现和调试。例如，在调试push时添加printf语句可能会改变执行的时间，从而使竞争消失。\n避免竞争的通常方法是使用锁。锁确保互斥，这样一次只有一个CPU可以执行push中敏感的代码行；这使得上述情况不可能发生。\n上面代码的正确上锁版本只添加了几行（用黄色突出显示）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct element { int data; struct element *next; }; struct element *list = 0; struct lock listlock; void push(int data) { struct element *l; l = malloc(sizeof *l); l-\u0026gt;data = data; acquire(\u0026amp;listlock); l-\u0026gt;next = list; list = l; release(\u0026amp;listlock); } acquire和release之间的指令序列通常被称为临界区域（critical section）。锁的作用通常被称为保护list。\n当我们说锁保护数据时，我们实际上是指锁保护适用于数据的某些不变量集合。不变量是跨操作维护的数据结构的属性。\n通常，操作的正确行为取决于操作开始时不变量是否为真。操作可能暂时违反不变量，但必须在完成之前重新建立它们。\n例如，在链表的例子中，不变量是list指向列表中的第一个元素，以及每个元素的next字段指向下一个元素。push的实现暂时违反了这个不变量：在第17行，l-\u0026gt;next指向list（注：则此时list不再指向列表中的第一个元素，即违反了不变量），但是list还没有指向l（在第18行重新建立）。我们上面检查的竞态条件发生了，因为第二个CPU执行了依赖于列表不变量的代码，而这些代码（暂时）被违反了。正确使用锁可以确保每次只有一个CPU可以对临界区域中的数据结构进行操作，因此当数据结构的不变量不成立时，将没有其他CPU对数据结构执行操作。\n不变量（invariant） 是在程序或数据结构的某些时刻始终必须保持为真的条件或规则。它们是系统运行过程中所必须遵循的“永恒真理”，确保数据结构处于正确的状态。如果不变量被违反，程序的行为可能会出现错误。\n不变量确保了数据结构的正确性，而在并发环境下（多CPU同时操作），如果没有锁的保护，某个CPU在不变量被暂时打破时访问数据结构，就可能导致不一致的情况。\n因此，锁的作用是确保数据结构的修改过程能够独占，防止其他CPU在不变量暂时不成立时访问。\n您可以将锁视为串行化（serializing）并发的临界区域，以便同时只有一个进程在运行这部分代码，从而维护不变量（假设临界区域设定了正确的隔离性）。您还可以将由同一锁保护的临界区域视为彼此之间的原子，即彼此之间只能看到之前临界区域的完整更改集，而永远看不到部分完成的更新。\n尽管正确使用锁可以改正不正确的代码，但锁限制了性能。\n如果两个进程并发调用kfree，锁将串行化这两个调用，我们在不同的CPU上运行它们没有任何好处。如果多个进程同时想要相同的锁或者锁经历了争用，则称之为发生冲突（conflict）。内核设计中的一个主要挑战是避免锁争用。\n锁的位置对性能也很重要。例如，在push中把acquire的位置提前也是正确的：将acquire移动到第13行之前完全没问题。但这样对malloc的调用也会被串行化，从而降低了性能。\n代码：Locks Xv6有两种类型的锁：自旋锁（spinlocks）和睡眠锁（sleep-locks）。\n从自旋锁（注：自旋，即循环等待）开始。\nXv6将自旋锁表示为struct spinlock。结构体中的重要字段是locked，当锁可用时为零，当它被持有时为非零。\n从逻辑上讲，xv6应该通过执行以下代码来获取锁\n1 2 3 4 5 6 7 8 9 10 void acquire(struct spinlock* lk) // does not work! { for(;;) { if(lk-\u0026gt;locked == 0) { lk-\u0026gt;locked = 1; break; } } } 不幸的是，这种实现不能保证多处理器上的互斥。\n可能会发生两个CPU同时到达第5行，看到lk-\u0026gt;locked为零，然后都通过执行第6行占有锁。此时就有两个不同的CPU持有锁，从而违反了互斥属性。我们需要的是一种方法，使第5行和第6行作为原子（即不可分割）步骤执行。\n因为锁被广泛使用，多核处理器通常提供实现第5行和第6行的原子版本的指令。\n在RISC-V上，这条指令是amoswap r, a。amoswap读取内存地址a处的值，将寄存器r的内容写入该地址，并将其读取的值放入r中。\n也就是说，它交换寄存器和指定内存地址的内容。\n它原子地执行这个指令序列，使用特殊的硬件来防止任何其他CPU在读取和写入之间使用内存地址。\nXv6的acquire使用可移植的C库调用归结为amoswap的指令__sync_lock_test_and_set；返回值是lk-\u0026gt;locked的旧（交换了的）内容。\nacquire函数将swap包装在一个循环中，直到它获得了锁前一直重试（自旋）。\n每次迭代将1与lk-\u0026gt;locked进行swap操作，并检查lk-\u0026gt;locked之前的值。\n如果之前为0，swap已经把lk-\u0026gt;locked设置为1，那么我们就获得了锁；如果前一个值是1，那么另一个CPU持有锁，我们原子地将1与lk-\u0026gt;locked进行swap的事实并没有改变它的值。\n获取锁后，用于调试，acquire将记录下来获取锁的CPU。lk-\u0026gt;cpu字段受锁保护，只能在保持锁时更改。\n函数release与acquire相反：它清除lk-\u0026gt;cpu字段，然后释放锁。\n从概念上讲，release只需要将0分配给lk-\u0026gt;locked。C标准允许编译器用多个存储指令实现赋值，因此对于并发代码，C赋值可能是非原子的。\n因此release使用执行原子赋值的C库函数__sync_lock_release。该函数也可以归结为RISC-V的amoswap指令。\n代码：使用锁 Xv6在许多地方使用锁来避免竞争条件（race conditions）。\n如上所述，kalloc(kernel/kalloc.c:69)和kfree(kernel/kalloc.c:47)就是一个很好的例子。\n使用锁的一个困难部分是决定要使用多少锁，以及每个锁应该保护哪些数据和不变量。\n有几个基本原则。\n首先，任何时候可以被一个CPU写入，同时又可以被另一个CPU读写的变量，都应该使用锁来防止两个操作重叠。\n其次，请记住锁保护不变量（invariants）：如果一个不变量涉及多个内存位置，通常所有这些位置都需要由一个锁来保护，以确保不变量不被改变。\n为了提高效率，不要向太多地方上锁是很重要的，因为锁会降低并行性。\n如果并行性不重要，那么可以安排只拥有一个线程，而不用担心锁。\n一个简单的内核可以在多处理器上做到这一点，方法是拥有一个锁，这个锁必须在进入内核时获得，并在退出内核时释放（尽管如管道读取或wait的系统调用会带来问题）。\n许多单处理器操作系统已经被转换为使用这种方法在多处理器上运行，有时被称为“大内核锁（big kernel lock）”。\n但是这种方法牺牲了并行性：一次只能有一个CPU运行在内核中。如果内核做一些繁重的计算，使用一组更细粒度的锁的集合会更有效率，这样内核就可以同时在多个处理器上执行。\n作为粗粒度锁的一个例子，xv6的kalloc.c分配器有一个由单个锁保护的空闲列表。\n如果不同CPU上的多个进程试图同时分配页面，每个进程在获得锁之前将必须在acquire中自旋等待。自旋会降低性能，因为它只是无用的等待。\n如果对锁的争夺浪费了很大一部分CPU时间，也许可以通过改变分配器的设计来提高性能，使其拥有多个空闲列表，每个列表都有自己的锁，以允许真正的并行分配。\n作为细粒度锁定的一个例子，xv6对每个文件都有一个单独的锁，这样操作不同文件的进程通常可以不需等待彼此的锁而继续进行。\n文件锁的粒度可以进一步细化，以允许进程同时写入同一个文件的不同区域。最终的锁粒度决策需要由性能测试和复杂性考量来驱动。\n作为预览，表6.3列出了xv6中的所有锁。\n锁 描述 bcache.lock 保护块缓冲区缓存项（block buffer cache entries）的分配 cons.lock 串行化对控制台硬件的访问，避免混合输出 ftable.lock 串行化文件表中文件结构体的分配 icache.lock 保护索引结点缓存项（inode cache entries）的分配 vdisk_lock 串行化对磁盘硬件和DMA描述符队列的访问 kmem.lock 串行化内存分配 log.lock 串行化事务日志操作 管道的pi-\u0026gt;lock 串行化每个管道的操作 pid_lock 串行化next_pid的增量 进程的p-\u0026gt;lock 串行化进程状态的改变 tickslock 串行化时钟计数操作 索引结点的 ip-\u0026gt;lock 串行化索引结点及其内容的操作 缓冲区的b-\u0026gt;lock 串行化每个块缓冲区的操作 Figure 6.3: Locks in xv6\n死锁和锁排序 如果在内核中执行的代码路径必须同时持有数个锁，那么所有代码路径以相同的顺序获取这些锁是很重要的。\n如果它们不这样做，就有死锁的风险。\n假设xv6中的两个代码路径需要锁A和B，但是代码路径1按照先A后B的顺序获取锁，另一个路径按照先B后A的顺序获取锁。假设线程T1执行代码路径1并获取锁A，线程T2执行代码路径2并获取锁B。接下来T1将尝试获取锁B，T2将尝试获取锁A。\n两个获取都将无限期阻塞，因为在这两种情况下，另一个线程都持有所需的锁，并且不会释放它，直到它的获取返回。\n为了避免这种死锁，所有代码路径必须以相同的顺序获取锁。全局锁获取顺序的需求意味着锁实际上是每个函数规范的一部分：调用者必须以一种使锁按照约定顺序被获取的方式调用函数。\n由于sleep的工作方式（见第7章），Xv6有许多包含每个进程的锁（每个struct proc中的锁）在内的长度为2的锁顺序链。\n例如，consoleintr (*kernel/console.c*:138)是处理键入字符的中断例程。当换行符到达时，任何等待控制台输入的进程都应该被唤醒。为此，consoleintr在调用wakeup时持有cons.lock，wakeup获取等待进程的锁以唤醒它。\n因此，全局避免死锁的锁顺序包括必须在任何进程锁之前获取cons.lock的规则。\n文件系统代码包含xv6最长的锁链。例如，创建一个文件需要同时持有目录上的锁、新文件inode上的锁、磁盘块缓冲区上的锁、磁盘驱动程序的vdisk_lock和调用进程的p-\u0026gt;lock。为了避免死锁，文件系统代码总是按照前一句中提到的顺序获取锁。\n遵守全局死锁避免的顺序可能会出人意料地困难。\n有时锁顺序与逻辑程序结构相冲突，例如，也许代码模块M1调用模块M2，但是锁顺序要求在M1中的锁之前获取M2中的锁。\n有时锁的身份是事先不知道的，也许是因为必须持有一个锁才能发现下一个要获取的锁的身份。\n这种情况在文件系统中出现，因为它在路径名称中查找连续的组件，也在wait和exit代码中出现，因为它们在进程表中寻找子进程。\n最后，死锁的危险通常是对细粒度锁定方案的限制，因为更多的锁通常意味着更多的死锁可能性。避免死锁的需求通常是内核实现中的一个主要因素。\n锁和中断处理函数 一些xv6自旋锁保护线程和中断处理程序共用的数据。\n例如，clockintr定时器中断处理程序在增加ticks(kernel/trap.c:163)的同时内核线程可能在sys_sleep(kernel/sysproc.c:64)中读取ticks。锁tickslock串行化这两个访问。\n自旋锁和中断的交互引发了潜在的危险。\n假设sys_sleep持有tickslock，并且它的CPU被计时器中断中断。clockintr会尝试获取tickslock，意识到它被持有后等待释放。在这种情况下，tickslock永远不会被释放：只有sys_sleep可以释放它，但是sys_sleep直到clockintr返回前不能继续运行。所以CPU会死锁，任何需要锁的代码也会冻结。\n为了避免这种情况，如果一个自旋锁被中断处理程序所使用，那么CPU必须保证在启用中断的情况下永远不能持有该锁。\nXv6更保守：当CPU获取任何锁时，xv6总是禁用该CPU上的中断。中断仍然可能发生在其他CPU上，此时中断的acquire可以等待线程释放自旋锁；由于不在同一CPU上，不会造成死锁。\n当CPU未持有自旋锁时，xv6重新启用中断；它必须做一些记录来处理嵌套的临界区域。\nacquire调用push_off (kernel/spinlock.c:89) 并且release调用pop_off (kernel/spinlock.c:100)来跟踪当前CPU上锁的嵌套级别。当计数达到零时，pop_off恢复最外层临界区域开始时存在的中断使能状态。intr_off和intr_on函数执行RISC-V指令分别用来禁用和启用中断。\n严格的在设置lk-\u0026gt;locked (kernel/spinlock.c:28)之前让acquire调用push_off是很重要的。\n如果两者颠倒，会存在一个既持有锁又启用了中断的短暂窗口期，不幸的话定时器中断会使系统死锁。同样，只有在释放锁之后，release才调用pop_off也是很重要的(kernel/spinlock.c:66)。\n指令和内存访问排序 人们很自然地会想到程序是按照源代码语句出现的顺序执行的。\n然而，许多编译器和中央处理器为了获得更高的性能而不按顺序执行代码。\n如果一条指令需要许多周期才能完成，中央处理器可能会提前发出指令，这样它就可以与其他指令重叠，避免中央处理器停顿。\n例如，中央处理器可能会注意到在顺序指令序列A和B中彼此不存在依赖。CPU也许首先启动指令B，或者是因为它的输入先于A的输入准备就绪，或者是为了重叠执行A和B。编译器可以执行类似的重新排序，方法是在源代码中一条语句的指令发出之前，先发出另一条语句的指令。\n编译器和CPU在重新排序时需要遵循一定规则，以确保它们不会改变正确编写的串行代码的结果。\n然而，规则确实允许重新排序后改变并发代码的结果，并且很容易导致多处理器上的不正确行为。\nCPU的排序规则称为内存模型（memory model）。\n例如，在push的代码中，如果编译器或CPU将对应于第4行的存储指令移动到第6行release后的某个地方，那将是一场灾难：\n1 2 3 4 5 6 l = malloc(sizeof *l); l-\u0026gt;data = data; acquire(\u0026amp;listlock); l-\u0026gt;next = list; list = l; release(\u0026amp;listlock); 如果发生这样的重新排序，将会有一个窗口期，另一个CPU可以获取锁并查看更新后的list，但却看到一个未初始化的list-\u0026gt;next。\n为了告诉硬件和编译器不要执行这样的重新排序，xv6在acquire(kernel/spinlock.c、:22) 和release(kernel/spinlock.c:47)中都使用了__sync_synchronize()。\n__sync_synchronize()是一个内存障碍：它告诉编译器和CPU不要跨障碍重新排序load或store指令。\n因为xv6在访问共享数据时使用了锁，xv6的acquire和release中的障碍在几乎所有重要的情况下都会强制顺序执行。\n睡眠锁 有时xv6需要长时间保持锁。\n例如，文件系统（第8章）在磁盘上读写文件内容时保持文件锁定，这些磁盘操作可能需要几十毫秒。如果另一个进程想要获取自旋锁，那么长时间保持自旋锁会导致获取进程在自旋时浪费很长时间的CPU。\n自旋锁的另一个缺点是，一个进程在持有自旋锁的同时不能让出（yield）CPU，然而我们希望持有锁的进程等待磁盘I/O的时候其他进程可以使用CPU。\n持有自旋锁时让步是非法的，因为如果第二个线程试图获取自旋锁，就可能导致死锁：因为acquire不会让出CPU，第二个线程的自旋可能会阻止第一个线程运行并释放锁。在持有锁时让步也违反了在持有自旋锁时中断必须关闭的要求。\n因此，我们想要一种锁，它在等待获取锁时让出CPU，并允许在持有锁时让步（以及中断）。\nXv6以睡眠锁（sleep-locks）的形式提供了这种锁。acquiresleep (kernel/sleeplock.c:22) 在等待时让步CPU。\n在更高层次上，睡眠锁有一个被自旋锁保护的锁定字段，acquiresleep对sleep的调用原子地让出CPU并释放自旋锁。结果是其他线程可以在acquiresleep等待时执行。\n因为睡眠锁保持中断使能，所以它们不能用在中断处理程序中。\n因为acquiresleep可能会让出CPU，所以睡眠锁不能在自旋锁临界区域中使用（尽管自旋锁可以在睡眠锁临界区域中使用）。\n因为等待会浪费CPU时间，所以自旋锁最适合短的临界区域；睡眠锁对于冗长的操作效果很好。\n自旋锁：当进程想要获取自旋锁时，如果锁被占用，进程不会暂停或让出CPU，而是不断地尝试（自旋），直到获取锁。这意味着自旋锁要求进程始终保持运行，不让出CPU，且在锁释放之前，系统无法调度其他进程来运行。 睡眠锁：当进程想要获取睡眠锁时，如果锁被占用，进程会暂停（睡眠），并让出CPU，使得其他进程可以被调度来运行。等到锁可用时，系统会重新唤醒这个进程。 当一个进程持有自旋锁时，中断是关闭的，这是为了确保在自旋锁保护的临界区内，系统不会发生上下文切换（即不会暂停这个进程去调度其他进程）。\n真实世界 尽管对并发原语和并行性进行了多年的研究，但使用锁进行编程仍然具有挑战性。\n通常最好将锁隐藏在更高级别的结构中，如同步队列，尽管xv6没有这样做。\n如果您使用锁进行编程，明智的做法是使用试图识别竞争条件（race conditions）的工具，因为很容易错过需要锁的不变量。\n大多数操作系统都支持POSIX线程（Pthread），它允许一个用户进程在不同的CPU上同时运行几个线程。\nPthread支持用户级锁（user-level locks）、障碍（barriers）等。\n支持Pthread需要操作系统的支持。\n例如，应该是这样的情况，如果一个Pthread在系统调用中阻塞，同一进程的另一个Pthread应当能够在该CPU上运行。\n另一个例子是，如果一个线程改变了其进程的地址空间（例如，映射或取消映射内存），内核必须安排运行同一进程下的线程的其他CPU更新其硬件页表，以反映地址空间的变化。\n没有原子指令实现锁是可能的，但是代价昂贵，并且大多数操作系统使用原子指令。\n如果许多CPU试图同时获取相同的锁，可能会付出昂贵的开销。\n如果一个CPU在其本地cache中缓存了一个锁，而另一个CPU必须获取该锁，那么更新保存该锁的cache行的原子指令必须将该行从一个CPU的cache移动到另一个CPU的cache中，并且可能会使cache行的任何其他副本无效。从另一个CPU的cache中获取cache行可能比从本地cache中获取一行的代价要高几个数量级。\n为了避免与锁相关的开销，许多操作系统使用无锁的数据结构和算法。\n例如，可以实现一个像本章开头那样的链表，在列表搜索期间不需要锁，并且使用一个原子指令在一个列表中插入一个条目。\n然而，无锁编程比有锁编程更复杂；例如，人们必须担心指令和内存重新排序。\n有锁编程已经很难了，所以xv6避免了无锁编程的额外复杂性。\n关于POSIX与Pthread 操作系统支持**POSIX线程（Pthread）**时的一些关键机制，尤其是在多核处理器环境下，如何管理多个线程和进程之间的资源共享和同步。\nPthread 是一种标准化的多线程编程接口，支持在同一个用户进程中同时运行多个线程。\n线程是轻量级进程，它们共享同一个进程的地址空间、文件描述符和其他资源，但可以并行执行。在多核处理器上，不同的线程可以分布在不同的CPU上并行执行。\n用户级锁，这些锁是专门用来控制并发的机制，用于防止多个线程同时修改共享数据，导致数据不一致。\n用户级锁的操作由应用程序控制，不涉及内核。\n例如，多个线程共享一个变量时，通过锁来确保一次只有一个线程能够访问或修改这个变量。\n在操作系统支持Pthread的情况下，用户进程可以同时运行多个线程。每个线程可以分布在不同的CPU上并行执行。\n因此，即使一个线程在执行系统调用时阻塞了（无法继续执行），同一个进程中的另一个线程仍然可以继续在该CPU上运行。\n这确保了即使有线程在系统调用中被阻塞，也不会影响同一进程中其他线程的运行。\n所有线程共享同一个进程的地址空间。\n如果一个线程改变了该进程的地址空间，比如映射（增加内存）或取消映射（释放内存），那么操作系统需要确保在其他CPU上运行的该进程的其他线程的硬件页表（CPU用于管理内存映射的机制）也要更新。\n否则，不同CPU上运行的线程可能会看到不一致的内存视图。\n正课 为什么要用锁 这节课更关注在内核和操作系统中使用的锁。\n为什么我们需要锁？\n故事要从应用程序想要使用多个CPU核开始。使用多个CPU核可以带来性能的提升，如果一个应用程序运行在多个CPU核上，并且执行了系统调用，那么内核需要能够处理并行的系统调用。\n如果系统调用并行的运行在多个CPU核上，那么它们可能会并行的访问内核中共享的数据结构。\nXV6有很多共享的数据结构，例如proc、ticks和我们之后会看到的buffer cache等等。\n当并行的访问数据结构时，例如一个核在读取数据，另一个核在写入数据，我们需要使用锁来协调对于共享数据的更新，以确保数据的一致性。\n但是实际的情况有些令人失望，因为我们想要通过并行来获得高性能，我们想要并行的在不同的CPU核上执行系统调用，但是如果这些系统调用使用了共享的数据，我们又需要使用锁，而锁又会使得这些系统调用串行执行，所以最后锁反过来又限制了性能。\n我们处于一个矛盾的处境，出于正确性，我们需要使用锁，但是考虑到性能，锁又是极不好的。\n这就是现实，我们接下来会看看如何改善这个处境。\n但是回到最开始，为什么应用程序一定要使用多个CPU核来提升性能呢？\n这个实际上与过去几十年技术的发展有关。\n大概从2000年开始：\nCPU的时钟频率就没有再增加过了（绿线）。 这样的结果是，CPU的单线程性能达到了一个极限并且也没有再增加过（蓝线）。 但是另一方面，CPU中的晶体管数量在持续的增加 （深红色线）。 所以现在不能通过使用单核来让代码运行的更快，要想运行的更快，唯一的选择就是使用多个CPU核。所以从2000年开始，处理器上核的数量开始在增加（黑线）。 所以现在如果一个应用程序想要提升性能，它不能只依赖单核，必须要依赖于多核。\n这也意味着，如果应用程序与内核交互的较为紧密，那么操作系统也需要高效的在多个CPU核上运行。\n这就是我们对内核并行的运行在多个CPU核上感兴趣的直接原因。\n那为什么要使用锁呢？前面我们已经提到了，是为了确保正确性。\n当一份共享数据同时被读写时，如果没有锁的话，可能会出现race condition，进而导致程序出错。\nrace condition可以有不同的表现形式，并且它可能发生，也可能不发生。\n锁如何避免竞争条件（race condition） 首先你们在脑海里应该有多个CPU核在运行，比如说CPU0在运行指令，CPU1也在运行指令，这两个CPU核都连接到同一个内存上。\n在前面的代码中，数据freelist位于内存中，它里面记录了2个内存page。\n假设两个CPU核在相同的时间调用kfree。\nkfree函数接收一个物理地址pa作为参数，freelist是个单链表，kfree中将pa作为单链表的新的head节点，并更新freelist指向pa（注，也就是将空闲的内存page加在单链表的头部）。\n当两个CPU都调用kfree时，CPU0想要释放一个page，CPU1也想要释放一个page，现在这两个page都需要加到freelist中。\nkfree中首先将对应内存page的变量r指向了当前的freelist（也就是单链表当前的head节点）。\n我们假设CPU0先运行，那么CPU0会将它的变量r的next指向当前的freelist。如果CPU1在同一时间运行，它可能在CPU0运行第二条指令（kmem.freelist = r）之前运行代码。\n所以它也会完成相同的事情，它会将自己的变量r的next指向当前的freelist。现在两个物理page对应的变量r都指向了同一个freelist（注，也就是原来单链表的head节点）。\n剩下的代码也会并行的执行（kmem.freelist = r），这行代码会更新freelist为r。因为我们这里只有一个内存，所以总是有一个CPU会先执行，另一个后执行。我们假设CPU0先执行，那么freelist会等于CPU0的变量r。之后CPU1再执行，它又会将freelist更新为CPU1的变量r。\n这样的结果是，我们丢失了CPU0对应的page。CPU0想要释放的内存page最终没有出现在freelist数据中。\n在代码中，用来解决这里的问题的最常见方法就是使用锁。\n锁就是一个对象，就像其他在内核中的对象一样。有一个结构体叫做lock，它包含了一些字段，这些字段中维护了锁的状态。\n锁有非常直观的API：\nacquire，接收指向lock的指针作为参数。acquire确保了在任何时间，只会有一个进程能够成功的获取锁。 release，也接收指向lock的指针作为参数。在同一时间尝试获取锁的其他进程需要等待，直到持有锁的进程对锁调用release。 锁的acquire和release之间的代码，通常被称为critical section。\n之所以被称为critical section，是因为通常会在这里以原子的方式执行共享数据的更新。\n所以基本上来说，如果在acquire和release之间有多条指令，它们要么会一起执行，要么一条也不会执行。\n现在的程序通常会有许多锁。实际上，XV6中就有很多的锁。\n因为锁序列化了代码的执行。\n如果两个处理器想要进入到同一个critical section中，只会有一个能成功进入，另一个处理器会在第一个处理器从critical section中退出之后再进入。所以这里完全没有并行执行。\n如果内核中只有一把大锁，我们暂时将之称为big kernel lock。\n基本上所有的系统调用都会被这把大锁保护而被序列化。\n系统调用会按照这样的流程处理：一个系统调用获取到了big kernel lock，完成自己的操作，之后释放这个big kernel lock，再返回到用户空间，之后下一个系统调用才能执行。\n这样的话，如果我们有一个应用程序并行的调用多个系统调用，这些系统调用会串行的执行，因为我们只有一把锁。\n所以通常来说，例如XV6的操作系统会有多把锁，这样就能获得某种程度的并发执行。如果两个系统调用使用了两把不同的锁，那么它们就能完全的并行运行。\n首先，并没有强制说一定要使用锁，锁的使用完全是由程序员决定的。如果你想要一段代码具备原子性，那么其实是由程序员决定是否增加锁的acquire和release。\n其次，代码不会自动加锁，程序员自己要确定好是否将锁与数据结构关联，并在适当的位置增加锁的acquire和release。\n什么时候使用锁 锁限制了并发性，也限制了性能。那这带来了一个问题，什么时候才必须要加锁呢？\n一个非常保守同时也是非常简单的规则：如果两个进程访问了一个共享的数据结构，并且其中一个进程会更新共享的数据结构，那么就需要对于这个共享的数据结构加锁。\n这是个保守的规则，如果一个数据结构可以被多个进程访问，其中一个进程会更新这个数据，那么可能会产生race condition，应该使用锁来确保race condition不会发生。\n但是同时，这条规则某种程度上来说又太过严格了。\n如果有两个进程共享一个数据结构，并且其中一个进程会更新这个数据结构，在某些场合不加锁也可以正常工作。\n不加锁的程序通常称为lock-free program，不加锁的目的是为了获得更好的性能和并发度，不过lock-free program比带锁的程序更加复杂一些。\n矛盾的是，有时候这个规则太过严格，而有时候这个规则又太过宽松了。\n除了共享的数据，在一些其他场合也需要锁，例如对于printf，如果我们将一个字符串传递给它，XV6会尝试原子性的将整个字符串输出，而不是与其他进程的printf交织输出。\n尽管这里没有共享的数据结构，但在这里锁仍然很有用处，因为我们想要printf的输出也是序列化的。\n这条规则并不完美，但是它已经是一个足够好的指导准则。\n因为有了race condition，所以需要锁。\n我们之前在kfree函数中构造的race condition是很容易被识别到的，实际上如果你使用race detection工具，就可以立即找到它。但是对于一些更复杂的场景，就不是那么容易探测到race condition。\n那么我们能通过自动的创建锁来自动避免race condition吗？\n如果按照刚刚的简单规则，一旦我们有了一个共享的数据结构，任何操作这个共享数据结构都需要获取锁，那么对于XV6来说，每个结构体都需要自带一个锁，当我们对于结构体做任何操作的时候，会自动获取锁。\n可是如果我们这样做的话，结果就太过严格了，所以不能自动加锁。\n假设我们有一个对于rename的调用，这个调用会将文件从一个目录移到另一个目录，我们现在将文件d1/x移到文件d2/y。\n如果我们按照前面说的，对数据结构自动加锁。\n现在我们有两个目录对象，一个是d1，另一个是d2，那么我们会先对d1加锁，删除x，之后再释放对于d1的锁；之后我们会对d2加锁，增加y，之后再释放d2的锁。这是我们在使用自动加锁之后的一个假设的场景。\n在我们完成了第一步，也就是删除了d1下的x文件，但是还没有执行第二步，也就是创建d2下的y文件时，其他的进程会看到文件完全不存在。\n这明显是个错误的结果，因为文件还存在只是被重命名了，文件在任何一个时间点都是应该存在的。\n所以这里正确的解决方法是，我们在重命名的一开始就对d1和d2加锁，之后删除x再添加y，最后再释放对于d1和d2的锁。\n在这个例子中，我们的操作需要涉及到多个锁，但是直接为每个对象自动分配一个锁会带来错误的结果。\n在这个例子中，锁应该与操作而不是数据关联，所以自动加锁在某些场景下会出问题。\n学生提问：可不可以在访问某个数据结构的时候，就获取所有相关联的数据结构的锁？\nFrans教授：这是一种实现方式。但是这种方式最后会很快演进成big kernel lock，这样你就失去了并发执行的能力，但是你肯定想做得更好。这里就是使用锁的矛盾点了，如果你想要程序简单点，可以通过coarse-grain locking（注，也就是大锁），但是这时你就失去了性能。\n锁的特性和死锁 通常锁有三种作用。\n锁可以避免丢失更新。\n如果你回想我们之前在kalloc.c中的例子，丢失更新是指我们丢失了对于某个内存page在kfree函数中的更新。如果没有锁，在出现race condition的时候，内存page不会被加到freelist中。但是加上锁之后，我们就不会丢失这里的更新。\n锁可以打包多个操作，使它们具有原子性。\n我们之前介绍了加锁解锁之间的区域是critical section，在critical section的所有操作会都会作为一个原子操作执行。\n锁可以维护共享数据结构的不变性。\n共享数据结构如果不被任何进程修改的话是会保持不变的。如果某个进程acquire了锁并且做了一些更新操作，共享数据的不变性暂时会被破坏，但是在release锁之后，数据的不变性又恢复了。你们可以回想一下之前在kfree函数中的freelist数据，所有的free page都在一个单链表上。但是在kfree函数中，这个单链表的head节点会更新。freelist并不太复杂，对于一些更复杂的数据结构可能会更好的帮助你理解锁的作用。\n接下来再来看一下锁可能带来的一些缺点。不恰当的使用锁，可能会带来一些锁特有的问题。最明显的一个例子就是死锁（Deadlock）。\n一个死锁的最简单的场景就是：首先acquire一个锁，然后进入到critical section；在critical section中，再acquire同一个锁；第二个acquire必须要等到第一个acquire状态被release了才能继续执行，但是不继续执行的话又走不到第一个release，所以程序就一直卡在这了。这就是一个死锁。\n这是死锁的一个最简单的例子，XV6会探测这样的死锁，如果XV6看到了同一个进程多次acquire同一个锁，就会触发一个panic。\n当有多个锁的时候，场景会更加有趣。\n假设现在我们有两个CPU，一个是CPU1，另一个是CPU2。CPU1执行rename将文件d1/x移到d2/y，CPU2执行rename将文件d2/a移到d1/b。这里CPU1将文件从d1移到d2，CPU2正好相反将文件从d2移到d1。\n我们假设我们按照参数的顺序来acquire锁，那么CPU1会先获取d1的锁，如果程序是真正的并行运行，CPU2同时也会获取d2的锁。之后CPU1需要获取d2的锁，这里不能成功，因为CPU2现在持有锁，所以CPU1会停在这个位置等待d2的锁释放。而另一个CPU2，接下来会获取d1的锁，它也不能成功，因为CPU1现在持有锁。\n这也是死锁的一个例子，有时候这种场景也被称为deadly embrace。这里的死锁就没那么容易探测了。\n这里的解决方案是，如果你有多个锁，你需要对锁进行排序，所有的操作都必须以相同的顺序获取锁。\n所以对于一个系统设计者，你需要确定对于所有的锁对象的全局的顺序。\n如在这里的例子中我们让d1一直在d2之前，这样我们在rename的时候，总是先获取排序靠前的目录的锁，再获取排序靠后的目录的锁。如果对于所有的锁有了一个全局的排序，这里的死锁就不会出现了。\n不过在设计一个操作系统的时候，定义一个全局的锁的顺序会有些问题。\n如果一个模块m1中方法g调用了另一个模块m2中的方法f，那么m1中的方法g需要知道m2的方法f使用了哪些锁。\n因为如果m2使用了一些锁，那么m1的方法g必须集合f和g中的锁，并形成一个全局的锁的排序，意味着在m2中的锁必须对m1可见，这样m1才能以恰当的方法调用m2。\n但是这样又违背了代码抽象的原则。在完美的情况下，代码抽象要求m1完全不知道m2是如何实现的。\n但是不幸的是，具体实现中，m2内部的锁需要泄露给m1，这样m1才能完成全局锁排序。所以当你设计一些更大的系统时，锁使得代码的模块化更加的复杂了。\n学生提问：有必要对所有的锁进行排序吗？\nFrans教授：在上面的例子中，这取决于f和g是否共用了一些锁。如果你看XV6的代码，你可以看到会有多种锁的排序，因为一些锁与其他的锁没有任何关系，它们永远也不会在同一个操作中被acquire。如果两组锁不可能在同一个操作中被acquire，那么这两组锁的排序是完全独立的。所以没有必要对所有的锁进行一个全局的排序，但是所有的函数需要对共同使用的一些锁进行一个排序。\n锁与性能 我们前面已经看过了两类锁带来的挑战，一个是死锁，另一个是破坏了程序的模块化。\n这一部分来看看第三个挑战，也就是锁与性能之间的权衡。\n基本上来说，如果你想获得更高的性能，你需要拆分数据结构和锁。\n如果你只有一个big kernel lock，那么操作系统只能被一个CPU运行。\n如果你想要性能随着CPU的数量增加而增加，你需要将数据结构和锁进行拆分。\n那怎么拆分呢？通常不会很简单，有的时候还有些困难。比如说，你是否应该为每个目录关联不同的锁？你是否应该为每个inode关联不同的锁？你是否应该为每个进程关联不同的锁？或者是否有更好的方式来拆分数据结构呢？\n如果你重新设计了加锁的规则，你需要确保不破坏内核一直尝试维护的数据不变性。\n如果你拆分了锁，你可能需要重写代码。如果你为了获得更好的性能，重构了部分内核或者程序，将数据结构进行拆分并引入了更多的锁，这涉及到很多工作，你需要确保你能够继续维持数据的不变性，你需要重写代码。\n所以这里就有矛盾点了。我们想要获得更好的性能，那么我们需要有更多的锁，但是这又引入了大量的工作。\n通常来说，开发的流程是：\n先以coarse-grained lock（注，也就是大锁）开始。 再对程序进行测试，来看一下程序是否能使用多核。 如果可以的话，那么工作就结束了，你对于锁的设计足够好了；如果不可以的话，那意味着锁存在竞争，多个进程会尝试获取同一个锁，因此它们将会序列化的执行，性能也上不去，之后你就需要重构程序。 在这个流程中，测试的过程比较重要。有可能模块使用了coarse-grained lock，但是它并没有经常被并行的调用，那么其实就没有必要重构程序，因为重构程序设计到大量的工作，并且也会使得代码变得复杂。\n所以如果不是必要的话，还是不要进行重构。\nXV6中UART模块对于锁的使用 接下来我们看一下XV6的代码，通过代码来理解锁是如何在XV6中工作的。首先查看一下uart.c。\n1 2 3 4 5 6 //the transmit output buffer. struct spinlock uart_tx_lock; #define UART_TX_BUF_SIZE 32 char uart_tx_buf [UART_TX_BUF_SIZE]; int uart_tx_w;//write next to uart_tx_buf[uart_tx_w++] int uart_tx_r;//read next from uart_tx_buf[uar_tx_r++] 所以你可以认为对于UART模块来说，现在是一个coarse-grained lock的设计。\n这个锁保护了UART的的传输缓存；写指针；读指针。当我们传输数据时，写指针会指向传输缓存的下一个空闲槽位，而读指针指向的是下一个需要被传输的槽位。\n这是我们对于并行运算的一个标准设计，它叫做消费者-生产者模式。\n所以现在有了一个缓存，一个写指针和一个读指针。\n读指针的内容需要被显示，写指针接收来自例如printf的数据。\n我们前面已经了解到了锁有多个角色。\n第一个是保护数据结构的特性不变，数据结构有一些不变的特性，例如读指针需要追赶写指针；从读指针到写指针之间的数据是需要被发送到显示端；从写指针到读指针之间的是空闲槽位，锁帮助我们维护了这些特性不变。\n接下来看一下uart.c中的uartputc函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // add a character to the output buffer and tell the // UART to start sending if it isn\u0026#39;t already. // blocks if the output buffer is full. // because it may block, it can\u0026#39;t be called // from interrupts; it\u0026#39;s only suitable for use // by write(). void uartputc(int c) { acquire(\u0026amp;uart_tx_lock); if(panicked){ for(;;) ; } while(1){ if(((uart_tx_w + 1) % UART_TX_BUF_SIZE) == uart_tx_r){ // buffer is full. // wait for uartstart() to open up space in the buffer. sleep(\u0026amp;uart_tx_r, \u0026amp;uart_tx_lock); } else { uart_tx_buf[uart_tx_w] = c; uart_tx_w = (uart_tx_w + 1) % UART_TX_BUF_SIZE; uartstart(); release(\u0026amp;uart_tx_lock); return; } } } 函数首先获得了锁，然后查看当前缓存是否还有空槽位，如果有的话将数据放置于空槽位中；写指针加1；调用uartstart；最后释放锁。\n如果两个进程在同一个时间调用uartputc，那么这里的锁会确保来自于第一个进程的一个字符进入到缓存的第一个槽位，接下来第二个进程的一个字符进入到缓存的第二个槽位。\n这就是锁帮助我们避免race condition的一个简单例子。如果没有锁的话，第二个进程可能会覆盖第一个进程的字符。\n接下来我们看一下uartstart函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // if the UART is idle, and a character is waiting // in the transmit buffer, send it. // caller must hold uart_tx_lock. // called from both the top- and bottom-half. void uartstart() { while(1){ if(uart_tx_w == uart_tx_r){ // transmit buffer is empty. return; } if((ReadReg(LSR) \u0026amp; LSR_TX_IDLE) == 0){ // the UART transmit holding register is full, // so we cannot give it another byte. // it will interrupt when it\u0026#39;s ready for a new byte. return; } int c = uart_tx_buf[uart_tx_r]; uart_tx_r = (uart_tx_r + 1) % UART_TX_BUF_SIZE; // maybe uartputc() is waiting for space in the buffer. wakeup(\u0026amp;uart_tx_r); WriteReg(THR, c); } } 如果uart_tx_w不等于uart_tx_r，那么缓存不为空，说明需要处理缓存中的一些字符。\n锁确保了我们可以在下一个字符写入到缓存之前，处理完缓存中的字符，这样缓存中的数据就不会被覆盖。\n最后，锁确保了一个时间只有一个CPU上的进程可以写入UART的寄存器，THR。\n所以这里锁确保了硬件寄存器只有一个写入者。\n当UART硬件完成传输，会产生一个中断。在前面的代码中我们知道了uartstart的调用者会获得锁以确保不会有多个进程同时向THR寄存器写数据。\n但是UART中断本身也可能与调用printf的进程并行执行。\n如果一个进程调用了printf，它运行在CPU0上；CPU1处理了UART中断，那么CPU1也会调用uartstart。\n因为我们想要确保对于THR寄存器只有一个写入者，同时也确保传输缓存的特性不变（注，这里指的是在uartstart中对于uart_tx_r指针的更新），我们需要在中断处理函数中也获取锁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // handle a uart interrupt, raised because input has // arrived, or the uart is ready for more output, or // both. called from trap.c. void uartintr(void) { // read and process incoming characters. while(1){ int c = uartgetc(); if(c == -1) break; consoleintr(c); } // send buffered characters. acquire(\u0026amp;uart_tx_lock); uartstart(); release(\u0026amp;uart_tx_lock); } 所以，在XV6中，驱动的bottom部分（注，也就是中断处理程序）和驱动的up部分（注，uartputc函数）可以完全的并行运行，所以中断处理程序也需要获取锁。\n学生提问：UART的缓存中，读指针是不是总是会落后于写指针？\nFrans教授：从读指针到写指针之间的字符是要显示的字符，UART会逐次的将读指针指向的字符在显示器上显示，同时printf可能又会将新的字符写入到缓存。读指针总是会落后于写指针直到读指针追上了写指针，这时两个指针相同，并且此时缓存中没有字符需要显示。\n自旋锁的实现（一） 接下来我们看一下如何实现自旋锁。\n锁的特性就是只有一个进程可以获取锁，在任何时间点都不能有超过一个锁的持有者。\n实现锁的主要难点在于锁的acquire接口，在acquire里面有一个死循环，循环中判断锁对象的locked字段是否为0，如果为0那表明当前锁没有持有者，当前对于acquire的调用可以获取锁。\n之后我们通过设置锁对象的locked字段为1来获取锁。最后返回。\n如果锁的locked字段不为0，那么当前对于acquire的调用就不能获取锁，程序会一直spin。\n也就是说，程序在循环中不停的重复执行，直到锁的持有者调用了release并将锁对象的locked设置为0。\n两个进程可能同时读到锁的locked字段为0。\n如果CPU0和CPU1同时到达A语句，它们会同时看到锁的locked字段为0，之后它们会同时走到B语句，这样它们都acquire了锁。这样我们就违背了锁的特性。\n为了解决这里的问题并得到一个正确的锁的实现方式，其实有多种方法，但是最常见的方法是依赖于一个特殊的硬件指令。\n这个特殊的硬件指令会保证一次test-and-set操作的原子性。\n在RISC-V上，这个特殊的指令就是amoswap（atomic memory swap）。\n这个指令接收3个参数，分别是address，寄存器r1，寄存器r2。\n这条指令会先锁定住address，将address中的数据保存在一个临时变量中（tmp），之后将r1中的数据写入到地址中，之后再将保存在临时变量中的数据写入到r2中，最后再对于地址解锁。\n通过这里的加锁，可以确保address中的数据存放于r2，而r1中的数据存放于address中，并且这一系列的指令打包具备原子性。\n大多数的处理器都有这样的硬件指令，因为这是一个实现锁的方便的方式。\n这里我们通过将一个软件锁转变为硬件锁最终实现了原子性。\n不同处理器的具体实现可能会非常不一样，处理器的指令集通常像是一个说明文档，它不会有具体实现的细节，具体的实现依赖于内存系统是如何工作的，比如说：\n多个处理器共用一个内存控制器，内存控制器可以支持这里的操作，比如给一个特定的地址加锁，然后让一个处理器执行2-3个指令，然后再解锁。因为所有的处理器都需要通过这里的内存控制器完成读写，所以内存控制器可以对操作进行排序和加锁。 如果内存位于一个共享的总线上，那么需要总线控制器（bus arbiter）来支持。总线控制器需要以原子的方式执行多个内存操作。 如果处理器有缓存，那么缓存一致性协议会确保对于持有了我们想要更新的数据的cache line只有一个写入者，相应的处理器会对cache line加锁，完成两个操作。 硬件原子操作的实现可以有很多种方法。\n但是基本上都是对于地址加锁，读出数据，写入新数据，然后再返回旧数据（注，也就是实现了atomic swap）。\n接下来我们看一下如何使用这条指令来实现自旋锁。让我们来看一下XV6中的acquire和release的实现。首先我们看一下spinlock.h。\n1 2 3 4 5 6 7 8 // Mutual exclusion lock. struct spinlock { uint locked; // Is the lock held? // For debugging: char *name; // Name of lock. struct cpu *cpu; // The cpu holding the lock. }; 里面有spinlock结构体的定义。内容也比较简单，包含了locked字段表明当前是否上锁，其他两个字段主要是用来输出调试信息，一个是锁的名字，另一个是持有锁的CPU。\n接下来我们看一下spinlock.c文件，先来看一下acquire函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Acquire the lock. // Loops (spins) until the lock is acquired. void acquire(struct spinlock *lk) { push_off(); // disable interrupts to avoid deadlock. if(holding(lk)) panic(\u0026#34;acquire\u0026#34;); // On RISC-V, sync_lock_test_and_set turns into an atomic swap: // a5 = 1 // s1 = \u0026amp;lk-\u0026gt;locked // amoswap.w.aq a5, a5, (s1) while(__sync_lock_test_and_set(\u0026amp;lk-\u0026gt;locked, 1) != 0) ; // Tell the C compiler and the processor to not move loads or stores // past this point, to ensure that the critical section\u0026#39;s memory // references happen strictly after the lock is acquired. // On RISC-V, this emits a fence instruction. __sync_synchronize(); // Record info about lock acquisition for holding() and debugging. lk-\u0026gt;cpu = mycpu(); } 在函数中有一个while循环，这就是我刚刚提到的test-and-set循环。实际上C的标准库已经定义了这些原子操作，所以C标准库中已经有一个函数__sync_lock_test_and_set，它里面的具体行为与我刚刚描述的是一样的。\n因为大部分处理器都有的test-and-set硬件指令，所以这个函数的实现比较直观。\n我们可以通过查看kernel.asm来了解RISC-V具体是如何实现的。\n这里比较复杂，总的来说，一种情况下我们跳出循环，另一种情况我们继续执行循环。C代码就要简单的多。\n如果锁没有被持有，那么锁对象的locked字段会是0，如果locked字段等于0，我们调用test-and-set将1写入locked字段，并且返回locked字段之前的数值0。\n如果返回0，那么意味着没有人持有锁，循环结束。如果locked字段之前是1，那么这里的流程是，先将之前的1读出，然后写入一个新的1，但是这不会改变任何数据，因为locked之前已经是1了。\n之后__sync_lock_test_and_set会返回1，表明锁之前已经被人持有了，这样的话，判断语句不成立，程序会持续循环（spin），直到锁的locked字段被设置回0。\n接下来我们看一下release的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Release the lock. void release(struct spinlock *lk) { if(!holding(lk)) panic(\u0026#34;release\u0026#34;); lk-\u0026gt;cpu = 0; // Tell the C compiler and the CPU to not move loads or stores // past this point, to ensure that all the stores in the critical // section are visible to other CPUs before the lock is released, // and that loads in the critical section occur strictly before // the lock is released. // On RISC-V, this emits a fence instruction. __sync_synchronize(); // Release the lock, equivalent to lk-\u0026gt;locked = 0. // This code doesn\u0026#39;t use a C assignment, since the C standard // implies that an assignment might be implemented with // multiple store instructions. // On RISC-V, sync_lock_release turns into an atomic swap: // s1 = \u0026amp;lk-\u0026gt;locked // amoswap.w zero, zero, (s1) __sync_lock_release(\u0026amp;lk-\u0026gt;locked); pop_off(); } 可以看出release也使用了atomic swap操作，将0写入到了s1。下面是对应的C代码，它基本确保了将lk-\u0026gt;locked中写入0是一个原子操作。\n自旋锁的实现（二） 有关spin lock的实现，有3个细节我想介绍一下。\n首先，为什么release函数中不直接使用一个store指令将锁的locked字段写为0?因为其他的处理器可能会向locked字段写入1，或者写入0。\n这里的问题是，对于很多人包括我自己来说，经常会认为一个store指令是一个原子操作，但实际并不总是这样，这取决于具体的实现。\n例如，对于CPU内的缓存，每一个cache line的大小可能大于一个整数，那么store指令实际的过程将会是：首先会加载cache line，之后再更新cache line。\n所以对于store指令来说，里面包含了两个微指令。\n这样的话就有可能得到错误的结果。所以为了避免理解硬件实现的所有细节，例如整数操作不是原子的，或者向一个64bit的内存值写数据是不是原子的，我们直接使用一个RISC-V提供的确保原子性的指令来将locked字段写为0。\namoswap并不是唯一的原子指令，下图是RISC-V的手册，它列出了所有的原子指令。\n(略)\n第二个细节是，在acquire函数的最开始，会先关闭中断。\n为什么会是这样呢？让我们回到uart.c中。我们先来假设acquire在一开始并没有关闭中断。在uartputc函数中，首先会acquire锁，如果不关闭中断会发生什么呢？\nuartputc函数会acquire锁，UART本质上就是传输字符，当UART完成了字符传输它会做什么？是的，它会产生一个中断之后会运行uartintr函数，在uartintr函数中，会获取同一把锁，但是这把锁正在被uartputc持有。\n如果这里只有一个CPU的话，那这里就是死锁。\n中断处理程序uartintr函数会一直等待锁释放，但是CPU不出让给uartputc执行的话锁又不会释放。在XV6中，这样的场景会触发panic，因为同一个CPU会再次尝试acquire同一个锁。\n所以spinlock需要处理两类并发，一类是不同CPU之间的并发，一类是相同CPU上中断和普通程序之间的并发。\n针对后一种情况，我们需要在acquire中关闭中断。中断会在release的结束位置再次打开，因为在这个位置才能再次安全的接收中断。\n第三个细节就是memory ordering。假设我们先通过将locked字段设置为1来获取锁，之后对x加1，最后再将locked字段设置0来释放锁。\n但是编译器或者处理器可能会重排指令以获得更好的性能。\n对于上面的串行指令流，如果将x\u0026lt;-x+1移到locked\u0026lt;-0之后可以吗？这会改变指令流的正确性吗？\n并不会，因为x和锁完全相互独立，它们之间没有任何关联。\n如果他们还是按照串行的方式执行，x\u0026lt;-x+1移到锁之外也没有问题。所以在一个串行执行的场景下是没有问题的。\n实际中，处理器在执行指令时，实际指令的执行顺序可能会改变。编译器也会做类似的事情，编译器可能会在不改变执行结果的前提下，优化掉一些代码路径并进而改变指令的顺序。\n但是对于并发执行，很明显这将会是一个灾难。如果我们将critical section与加锁解锁放在不同的CPU执行，将会得到完全错误的结果。所以指令重新排序在并发场景是错误的。\n为了禁止，或者说为了告诉编译器和硬件不要这样做，我们需要使用memory fence或者叫做synchronize指令，来确定指令的移动范围。\n对于synchronize指令，任何在它之前的load/store指令，都不能移动到它之后。锁的acquire和release函数都包含了synchronize指令。\n这样前面的例子中，x\u0026lt;-x+1就不会被移到特定的memory synchronization点之外。\n这就是为什么在acquire和release中都有__sync_synchronize函数的调用。\n学生提问：有没有可能在锁acquire之前的一条指令被移到锁release之后？或者说这里会有一个界限不允许这么做？\nFrans教授：在这里的例子中，acquire和release都有自己的界限（注，也就是__sync_synchronize函数的调用点）。所以发生在锁acquire之前的指令不会被移到acquire的__sync_synchronize函数调用之后，这是一个界限。在锁的release函数中有另一个界限。所以在第一个界限之前的指令会一直在这个界限之前，在两个界限之间的指令会保持在两个界限之间，在第二个界限之后的指令会保持在第二个界限之后。\n总结 让我来总结一下这节课的内容。\n首先，锁确保了正确性，但是同时又会降低性能，这是个令人失望的现实，我们是因为并发运行代码才需要使用锁，而锁另一方面又限制了代码的并发运行。\n其次锁会增加编写程序的复杂性，在我们的一些实验中会看到锁，我们需要思考锁为什么在这，它需要保护什么。如果你在程序中使用了并发，那么一般都需要使用锁。如果你想避免锁带来的复杂性，可以遵循以下原则：不到万不得已不要共享数据。如果你不在多个进程之间共享数据，那么race condition就不可能发生，那么你也就不需要使用锁，程序也不会因此变得复杂。但是通常来说如果你有一些共享的数据结构，那么你就需要锁，你可以从coarse-grained lock开始，然后基于测试结果，向fine-grained lock演进。\n最后，使用race detector来找到race condition，如果你将锁的acquire和release放置于错误的位置，那么就算使用了锁还是会有race。\n学生提问：在一个处理器上运行多个线程与在多个处理器上运行多个进程是否一样？\nFrans教授：差不多吧，如果你有多个线程，但是只有一个CPU，那么你还是会想要特定内核代码能够原子执行。所以你还是需要有critical section的概念。你或许不需要锁，但是你还是需要能够对特定的代码打开或者关闭中断。如果你查看一些操作系统的内核代码，通常它们都没有锁的acquire，因为它们假定自己都运行在单个处理器上，但是它们都有开关中断的操作。\n","date":"2024-09-17T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08110multiprocessors-and-locking/bg_hu17813658287347361090.png","permalink":"https://echudet.github.io/p/mit6s08110multiprocessors-and-locking/","title":"MIT6S081：10Multiprocessors and locking"},{"content":"Lecture 9 Interrupts 课程杂记 内存是如何被真实的操作系统（而不是像XV6这样的教学操作系统）所使用 首先是计算机中总共有多少内存（33048332），如果你再往后看的话，你会发现大部分内存都被使用了（4214604 + 26988148）。但是大部分内存并没有被应用程序所使用，而是被buff/cache用掉了。这在一个操作系统中还挺常见的，因为我们不想让物理内存就在那闲置着，我们想让物理内存被用起来，所以这里大块的内存被用作buff/cache。 大部分操作系统运行时几乎没有任何空闲的内存。这意味着，如果应用程序或者内核需要使用新的内存，那么我们需要丢弃一些已有的内容。\u0026mdash;\u0026ndash;现在的空闲内存（free）或许足够几个page用，但是在某个时间点如果需要大量内存的话，要么是从应用程序，要么是从buffer/cache中，需要撤回已经使用的一部分内存。 VIRT表示的是虚拟内存地址空间的大小，RES是实际使用的内存数量。实际使用的内存数量远小于地址空间的大小。所以，我们上节课讨论的基于虚拟内存和page fault提供的非常酷的功能在这都有使用。 中断对应的场景很简单，就是硬件想要得到操作系统的关注。（例如网卡收到了一个packet，网卡会生成一个中断；用户通过键盘按下了一个按键，键盘会产生一个中断。）\n操作系统需要做的是，保存当前的工作，处理中断，处理完成之后再恢复之前的工作。这里的保存和恢复工作，与我们之前看到的系统调用过程非常相似。（所以系统调用，page fault，中断，都使用相同的机制）\n但是中断又有一些不一样的地方\nasynchronous。当硬件生成中断时，Interrupt handler与当前运行的进程在CPU上没有任何关联。但如果是系统调用的话，系统调用发生在运行进程的context下。\nconcurrency。我们这节课会稍微介绍并发，在下一节课，我们会介绍更多并发相关的内容。对于中断来说，CPU和生成中断的设备是并行的在运行。网卡自己独立的处理来自网络的packet，然后在某个时间点产生中断，但是同时，CPU也在运行。所以我们在CPU和设备之间是真正的并行的，我们必须管理这里的并行。\nprogram device。我们这节课主要关注外部设备，例如网卡，UART，而这些设备需要被编程。每个设备都有一个编程手册，就像RISC-V有一个包含了指令和寄存器的手册一样。设备的编程手册包含了它有什么样的寄存器，它能执行什么样的操作，在读写控制寄存器的时候，设备会如何响应。不过通常来说，设备的手册不如RISC-V的手册清晰，这会使得对于设备的编程会更加复杂。\n中断是从哪里产生的？因为我们主要关心的是外部设备的中断，而不是定时器中断或者软件中断。外设中断来自于主板上的设备。\n主板可以连接以太网卡，MicroUSB，MicroSD等，主板上的各种线路将外设和CPU连接在一起。\n我们在之前的课程已经知道UART0会映射到内核内存地址的某处，类似于读写内存，通过向相应的设备地址执行load/store指令，我们就可以对例如UART的设备进行编程。\n所有的设备都连接到处理器上，处理器上是通过Platform Level Interrupt Control，简称PLIC来处理设备中断。PLIC会管理来自于外设的中断。\n我们有53个不同的来自于设备的中断。这些中断到达PLIC之后，PLIC会路由这些中断。图的右下角是CPU的核，PLIC会将中断路由到某一个CPU的核。如果所有的CPU核都正在处理中断，PLIC会保留中断直到有一个CPU核可以用来处理中断。所以PLIC需要保存一些内部数据来跟踪中断的状态。\n具体流程是：\nPLIC会通知当前有一个待处理的中断 其中一个CPU核会Claim接收中断，这样PLIC就不会把中断发给其他的CPU处理 CPU核处理完中断之后，CPU会通知PLIC PLIC将不再保存中断的信息 PLIC只是分发中断，而内核需要对PLIC进行编程来告诉它中断应该分发到哪。实际上，内核可以对中断优先级进行编程，这里非常的灵活。\n当UART触发中断的时候，所有的CPU核都能收到中断吗？取决于你如何对PLIC进行编程。对于XV6来说，所有的CPU都能收到中断，但是只有一个CPU会Claim相应的中断。\n通常来说，管理设备的代码称为驱动，所有的驱动都在内核中。\nxv6的示范是UART设备的驱动，代码在uart.c文件中。如果我们查看代码的结构，我们可以发现大部分驱动都分为两个部分，bottom/top。\nbottom部分通常是Interrupt handler。当一个中断送到了CPU，并且CPU设置接收这个中断，CPU会调用相应的Interrupt handler。Interrupt handler并不运行在任何特定进程的context中，它只是处理中断。 top部分，是用户进程，或者内核的其他部分调用的接口。对于UART来说，这里有read/write接口，这些接口可以被更高层级的代码调用。 通常情况下，驱动中会有一些队列（或者说buffer），top部分的代码会从队列中读写数据，而Interrupt handler（bottom部分）同时也会向队列中读写数据。这里的队列可以将并行运行的设备和CPU解耦开来。\n通常对于Interrupt handler来说存在一些限制，因为它并没有运行在任何进程的context中，所以进程的page table并不知道该从哪个地址读写数据，也就无法直接从Interrupt handler读写数据。驱动的top部分通常与用户的进程交互，并进行数据的读写。\n在很多操作系统中，驱动代码加起来可能会比内核还要大，主要是因为，对于每个设备，你都需要一个驱动，而设备又很多。\n通常来说，编程是通过memory mapped I/O完成的。\n在SiFive的手册中，设备地址出现在物理地址的特定区间内，这个区间由主板制造商决定。操作系统需要知道这些设备位于物理地址空间的具体/位置，然后再通过普通的load/store指令对这些地址进行编程。load/store指令实际上的工作就是读写设备的控制寄存器。\n例如，对网卡执行store指令时，CPU会修改网卡的某个控制寄存器，进而导致网卡发送一个packet。所以这里的load/store指令不会读写内存，而是会操作设备。并且你需要阅读设备的文档来弄清楚设备的寄存器和相应的行为，有的时候文档很清晰，有的时候文档不是那么清晰。\n例如，0x200_0000对应CLINT，0xC000000对应的是PLIC。在这个图中UART0对应的是0x1001*_*0000，但是在QEMU中，我们的UART0的地址略有不同，因为在QEMU中我们并不是完全的模拟SiFive主板，而是模拟与SiFive主板非常类似的东西。\n以上就是Memory-mapped IO。\n这是一个很简单的芯片，图中表明了芯片拥有的寄存器。例如对于控制寄存器000，如果写它会将数据写入到寄存器中并被传输到其他地方，如果读它可以读出存储在寄存器中的内容。UART可以让你能够通过串口发送数据bit，在线路的另一侧会有另一个UART芯片，能够将数据bit组合成一个个Byte。\n这里还有一些其他可以控制的地方，例如控制寄存器001，可以通过它来控制UART是否产生中断。实际上对于一个寄存器，其中的每个bit都有不同的作用。例如对于寄存器001，也就是IER寄存器，bit0-bit3分别控制了不同的中断。这个文档还有很多内容，但是对于我们这节课来说，上图就足够了。不过即使是这么简单的一个设备，它的文档也有很多页。\n当XV6启动时，Shell会输出提示符“$ ”\n对于“$ ”来说，实际上就是设备会将字符传输给UART的寄存器，UART之后会在发送完字符之后产生一个中断。在QEMU中，模拟的线路的另一端会有另一个UART芯片（模拟的），这个UART芯片连接到了虚拟的Console，它会进一步将“$ ”显示在console上。\n另一方面，对于“ls”，这是用户输入的字符。键盘连接到了UART的输入线路，当你在键盘上按下一个按键，UART芯片会将按键字符通过串口线发送到另一端的UART芯片。另一端的UART芯片先将数据bit合并成一个Byte，之后再产生一个中断，并告诉处理器说这里有一个来自于键盘的字符。之后Interrupt handler会处理来自于UART的字符。\nRISC-V有许多与中断相关的寄存器：\nSIE（Supervisor Interrupt Enable）寄存器。这个寄存器中有一个bit（E）专门针对例如UART的外部设备的中断；有一个bit（S）专门针对软件中断，软件中断可能由一个CPU核触发给另一个CPU核；还有一个bit（T）专门针对定时器中断。我们这节课只关注外部设备的中断。 SSTATUS（Supervisor Status）寄存器。这个寄存器中有一个bit来打开或者关闭中断。每一个CPU核都有独立的SIE和SSTATUS寄存器，除了通过SIE寄存器来单独控制特定的中断，还可以通过SSTATUS寄存器中的一个bit来控制所有的中断。 SIP（Supervisor Interrupt Pending）寄存器。当发生中断时，处理器可以通过查看这个寄存器知道当前是什么类型的中断。 SCAUSE寄存器，这个寄存器我们之前看过很多次。它会表明当前状态的原因是中断。 STVEC寄存器，它会保存当trap，page fault或者中断发生时，CPU运行的用户程序的程序计数器，这样才能在稍后恢复程序的运行。 我们第一个外设是console，这是我们print的输出位置。查看位于console.c的consoleinit函数。这里首先初始化了锁，我们现在还不关心这个锁。然后调用了uartinit，uartinit函数位于uart.c文件。这个函数实际上就是配置好UART芯片使其可以被使用。这里的流程是先关闭中断，之后设置波特率，设置字符长度为8bit，重置FIFO，最后再重新打开中断。以上就是uartinit函数，运行完这个函数之后，原则上UART就可以生成中断了。但是因为我们还没有对PLIC编程，所以中断不能被CPU感知。最终，在main函数中，需要调用plicinit函数。下图是plicinit函数。\nPLIC与外设一样，也占用了一个I/O地址（0xC000_0000）。代码的第一行使能了UART的中断，这里实际上就是设置PLIC会接收哪些中断，进而将中断路由到CPU。类似的，代码的第二行设置PLIC接收来自IO磁盘的中断。\nmain函数中，plicinit之后就是plicinithart函数。plicinit是由0号CPU运行，之后，每个CPU的核都需要调用plicinithart函数表明对于哪些外设中断感兴趣。\n所以在plicinithart函数中，每个CPU的核都表明自己对来自于UART和VIRTIO的中断感兴趣。因为我们忽略中断的优先级，所以我们将优先级设置为0。\n到目前为止，我们有了生成中断的外部设备，我们有了PLIC可以传递中断到单个的CPU。但是CPU自己还没有设置好接收中断，因为我们还没有设置好SSTATUS寄存器。在main函数的最后，程序调用了scheduler函数。scheduler函数主要是运行进程。但是在实际运行进程之前，会执行intr_on函数来使得CPU能接收中断。\nintr_on函数只完成一件事情，就是设置SSTATUS寄存器，打开中断标志位。\n在这个时间点，中断被完全打开了。如果PLIC正好有pending的中断，那么这个CPU核会收到中断。\n以上就是中断的基本设置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //init.c中的main函数，这是系统启动后运行的第一个进程。 int main(void)） { int pid,wpid; if(open(\u0026#34;console\u0026#34;,0 RDWR)\u0026lt;0){ mknod(\u0026#34;console\u0026#34;,CONSOLE,0); open(\u0026#34;console\u0026#34;,0 RDWR); } dup(0);/stdout dup(0);/stderr for(;;){ printf(\u0026#34;init:starting sh\\n\u0026#34;); pid fork(); if(pid 0){ printf(\u0026#34;init:fork failed\\n\u0026#34;); exit(1); } if(pid ==0){ exec(\u0026#34;sh\u0026#34;,argv); printf(\u0026#34;init:exec sh failed\\n\u0026#34;); exit(1); } 首先这个进程的main函数创建了一个代表Console的设备。这里通过mknod操作创建了console设备。因为这是第一个打开的文件，所以这里的文件描述符0。之后通过dup创建stdout和stderr。这里实际上通过复制文件描述符0，得到了另外两个文件描述符1，2。最终文件描述符0，1，2都用来代表Console。\nShell程序首先打开文件描述符0，1，2。之后Shell向文件描述符2打印提示符“$ ”。\n尽管Console背后是UART设备，但是从应用程序来看，它就像是一个普通的文件。Shell程序只是向文件描述符2写了数据，它并不知道文件描述符2对应的是什么。在Unix系统中，设备是由文件表示。\n假设键盘生成了一个中断并且发向了PLIC，PLIC会将中断路由给一个特定的CPU核，并且如果这个CPU核设置了SIE寄存器的E bit（注，针对外部中断的bit位），那么会发生以下事情：\n首先，会清除SIE寄存器相应的bit，这样可以阻止CPU核被其他中断打扰，该CPU核可以专心处理当前中断。处理完成之后，可以再次恢复SIE寄存器相应的bit。 之后，会设置SEPC寄存器为当前的程序计数器。我们假设Shell正在用户空间运行，突然来了一个中断，那么当前Shell的程序计数器会被保存。 之后，要保存当前的mode。在我们的例子里面，因为当前运行的是Shell程序，所以会记录user mode。 再将mode设置为Supervisor mode。 最后将程序计数器的值设置成STVEC的值。（注，STVEC用来保存trap处理程序的地址，详见lec06）在XV6中，STVEC保存的要么是uservec或者kernelvec函数的地址，具体取决于发生中断时程序运行是在用户空间还是内核空间。在我们的例子中，Shell运行在用户空间，所以STVEC保存的是uservec函数的地址。而从之前的课程我们可以知道uservec函数会调用usertrap函数。所以最终，我们在usertrap函数中。我们这节课不会介绍trap过程中的拷贝，恢复过程，因为在之前的课程中已经详细的介绍过了。 在trap.c的devintr函数中，首先会通过SCAUSE寄存器判断当前中断是否是来自于外设的中断。如果是的话，再调用plic_claim函数来获取中断。plic_claim函数位于plic.c文件中。在这个函数中，当前CPU核会告知PLIC，自己要处理中断，PLIC_SCLAIM会将中断号返回，对于UART来说，返回的中断号是10。\n学生提问： UART对于键盘来说很重要，来自于键盘的字符通过UART走到CPU再到我们写的代码。但是我不太理解UART对于Shell输出字符究竟有什么作用？因为在这个场景中，并没有键盘的参与。\nFrans教授：显示设备与UART也是相连的。所以UART连接了两个设备，一个是键盘，另一个是显示设备，也就是Console。QEMU也是通过模拟的UART与Console进行交互，而Console的作用就是将字符在显示器上画出来。\n学生提问：uartinit只被调用了一次，所以才导致了所有的CPU核都共用一个buffer吗？\nFrans教授：因为只有一个UART设备，一个buffer只针对一个UART设备，而这个buffer会被所有的CPU核共享，这样运行在多个CPU核上的多个程序可以同时向Console打印输出，而驱动中是通过锁来确保多个CPU核上的程序串行的向Console打印输出。\n学生提问：我们之所以需要锁是因为有多个CPU核，但是却只有一个Console，对吧？\nFrans教授：是的，如我们之前说的驱动的top和bottom部分可以并行的运行。所以一个CPU核可以执行uartputc函数，而另个一CPU核可以执行uartintr函数，我们需要确保它们是串行执行的，而锁确保了这一点。\n学生提问：那是不是意味着，某个时间，其他所有的CPU核都需要等待某一个CPU核的处理？\nFrans教授：这里并不是死锁。其他的CPU核还是可以在等待的时候运行别的进程。\n与中断相关的并发，并发加大了中断编程的难度。\n设备与CPU是并行运行的。例如当UART向Console发送字符的时候，CPU会返回执行Shell，而Shell可能会再执行一次系统调用，向buffer中写入另一个字符，这些都是在并行的执行。这里的并行称为producer-consumer并行。 中断会停止当前运行的程序。例如，Shell正在运行第212个指令，突然来了个中断，Shell的执行会立即停止。对于用户空间代码，这并不是一个大的问题，因为当我们从中断中返回时，我们会恢复用户空间代码，并继续执行执行停止的指令。我们已经在trap和page fault中看过了这部分内容。但是当内核被中断打断时，事情就不一样了。所以，代码运行在kernel mode也会被中断，这意味着即使是内核代码，也不是直接串行运行的。在两个内核指令之间，取决于中断是否打开，可能会被中断打断执行。对于一些代码来说，如果不能在执行期间被中断，这时内核需要临时关闭中断，来确保这段代码的原子性。 驱动的top和bottom部分是并行运行的。例如，Shell会在传输完提示符“$”之后再调用write系统调用传输空格字符，代码会走到UART驱动的top部分（注，uartputc函数），将空格写入到buffer中。但是同时在另一个CPU核，可能会收到来自于UART的中断，进而执行UART驱动的bottom部分，查看相同的buffer。所以一个驱动的top和bottom部分可以并行的在不同的CPU上运行。这里我们通过lock来管理并行。因为这里有共享的数据，我们想要buffer在一个时间只被一个CPU核所操作。 这里我将会关注在第一点，也就是producer/consumser并发。这是驱动中的非常常见的典型现象。如你们所见的，在驱动中会有一个buffer，在我们之前的例子中，buffer是32字节大小。并且有两个指针，分别是读指针和写指针。\nproducer可以一直写入数据，直到写指针 + 1等于读指针，因为这时，buffer已经满了。当buffer满了的时候，producer必须停止运行。我们之前在uartputc函数中看过，如果buffer满了，代码会sleep，暂时搁置Shell并运行其他的进程。\nInterrupt handler，也就是uartintr函数，在这个场景下是consumer，每当有一个中断，并且读指针落后于写指针，uartintr函数就会从读指针中读取一个字符再通过UART设备发送，并且将读指针加1。当读指针追上写指针，也就是两个指针相等的时候，buffer为空，这时就不用做任何操作。\n学生提问：这里的buffer对于所有的CPU核都是共享的吗？\nFrans教授：这里的buffer存在于内存中，并且只有一份，所以，所有的CPU核都并行的与这一份数据交互。所以我们才需要lock。\n学生提问：对于uartputc中的sleep，它怎么知道应该让Shell去sleep？\nFrans教授： sleep会将当前在运行的进程存放于sleep数据中。它传入的参数是需要等待的信号，在这个例子中传入的是uart_tx_r的地址。在uartstart函数中，一旦buffer中有了空间，会调用与sleep对应的函数wakeup，传入的也是uart_tx_r的地址。任何等待在这个地址的进程都会被唤醒。有时候这种机制被称为conditional synchronization。\n在UART的另一侧，会有类似的事情发生，有时Shell会调用read从键盘中读取字符。 在read系统调用的底层，会调用fileread函数。在这个函数中，如果读取的文件类型是设备，会调用相应设备的read函数。\n在我们的例子中，read函数就是console.c文件中的consoleread函数。\n这里与UART类似，也有一个buffer，包含了128个字符。其他的基本一样，也有producer和consumser。但是在这个场景下Shell变成了consumser，因为Shell是从buffer中读取数据。而键盘是producer，它将数据写入到buffer中。\n从consoleread函数中可以看出，当读指针和写指针一样时，说明buffer为空，进程会sleep。所以Shell在打印完“$ ”之后，如果键盘没有输入，Shell进程会sleep，直到键盘有一个字符输入。所以在某个时间点，假设用户通过键盘输入了“l”，这会导致“l”被发送到主板上的UART芯片，产生中断之后再被PLIC路由到某个CPU核，之后会触发devintr函数，devintr可以发现这是一个UART中断，然后通过uartgetc函数获取到相应的字符，之后再将字符传递给consoleintr函数。\n默认情况下，字符会通过consputc，输出到console上给用户查看。之后，字符被存放在buffer中。在遇到换行符的时候，唤醒之前sleep的进程，也就是Shell，再从buffer中将数据读出。\n所以这里也是通过buffer将consumer和producer之间解耦，这样它们才能按照自己的速度，独立的并行运行。如果某一个运行的过快了，那么buffer要么是满的要么是空的，consumer和producer其中一个会sleep并等待另一个追上来。\n当Unix刚被开发出来的时候，Interrupt处理还是很快的。这使得硬件可以很简单，当外设有数据需要处理时，硬件可以中断CPU的执行，并让CPU处理硬件的数据。\n而现在，中断相对处理器来说变慢了。从前面的介绍可以看出来这一点，需要很多步骤才能真正的处理中断数据。如果一个设备在高速的产生中断，处理器将会很难跟上。所以如果查看现在的设备，可以发现，现在的设备相比之前做了更多的工作。所以在产生中断之前，设备上会执行大量的操作，这样可以减轻CPU的处理负担。所以现在硬件变得更加复杂。\n如果你有一个高性能的设备，例如你有一个千兆网卡，这个网卡收到了大量的小包，网卡每秒可以生成1.5Mpps，这意味着每一个微秒，CPU都需要处理一个中断，这就超过了CPU的处理能力。那么当网卡收到大量包，并且处理器不能处理这么多中断的时候该怎么办呢？\n这里的解决方法就是使用polling。除了依赖Interrupt，CPU可以一直读取外设的控制寄存器，来检查是否有数据。对于UART来说，我们可以一直读取RHR寄存器，来检查是否有数据。现在，CPU不停的在轮询设备，直到设备有了数据。\n这种方法浪费了CPU cycles，当我们在使用CPU不停的检查寄存器的内容时，我们并没有用CPU来运行任何程序。在我们之前的例子中，如果没有数据，内核会让Shell进程sleep，这样可以运行另一个进程。\n所以，对于一个慢设备，你肯定不想一直轮询它来得到数据。我们想要在没有数据的时候切换出来运行一些其他程序。但是如果是一个快设备，那么Interrupt的overhead也会很高，那么我们在polling设备的时候，是经常能拿到数据的，这样可以节省进出中断的代价。\n所以对于一个高性能的网卡，如果有大量的包要传入，那么应该用polling。对于一些精心设计的驱动，它们会在polling和Interrupt之间动态切换（注，也就是网卡的NAPI）。\n内存在真实操作系统中的使用 在真实操作系统中，内存管理是一项复杂的任务。操作系统不仅要为应用程序分配内存，还需要管理内存的使用以确保系统高效运行。大部分内存并不只是简单地分配给应用程序，而是有很多部分被用作缓存或缓冲区（buff/cache）。这样可以确保物理内存不会闲置，而是被充分利用。\n现代操作系统采用了类似于虚拟内存和分页机制（Page Fault）的策略，虚拟内存为每个进程提供了一个独立的地址空间，即使物理内存并不足够。系统通过在需要时从buff/cache或者应用程序中回收内存来满足新的请求。这也是为什么当需要大块内存时，系统必须从已经使用的部分中回收一些内存。\n中断与并发的关系 中断是一种机制，用于设备通知操作系统它们需要处理的情况。例如，网卡接收数据包时，会向操作系统发送中断，或者当用户按下键盘时，键盘产生中断。在并发环境下，中断使得系统能够同时处理多个事件，而不必等待一个进程的结束。\n中断与并发之间的关系体现在设备和CPU之间的真正并行性。设备（如网卡或键盘）与CPU是并行运行的。例如，网卡接收数据包的同时，CPU也在运行其他任务。在设备产生中断时，操作系统需要中断当前的工作，保存状态，处理中断，之后恢复之前的工作。中断处理的这种机制和系统调用、页面错误（Page Fault）等有很大的相似性，但中断具有异步性和并发性，CPU和设备独立地执行任务。\n中断的处理可以分为两部分：Top Half和Bottom Half。\nTop Half：这是中断处理的第一阶段。Top Half负责快速响应中断，执行最紧急的任务，如确认中断来源、读取硬件寄存器等。Top Half的目标是尽可能快地完成，以减少对系统的干扰，因此它的执行时间要尽可能短。 Bottom Half：Bottom Half负责处理那些可以稍后执行的任务。由于Top Half不能处理复杂的任务，这些任务会被推迟到Bottom Half来完成。Bottom Half的处理时间相对较长，但它是在普通进程上下文中执行的，不会阻塞其他中断的处理。 关系：Top Half处理中断的紧急部分，而Bottom Half处理非紧急的部分。Top Half会通过某种机制（如标记位）通知Bottom Half继续处理剩余工作。通过这种分工，系统能迅速响应中断，同时保证复杂任务的处理不会影响系统的性能。\n通过UART的串口通信来具体说明中断机制中的并发关系：\n中断触发与Top Half\n当UART设备接收到一个字符时，它会生成一个中断。 操作系统进入Top Half，从UART的寄存器中读取字符并将其存入内存的缓冲区中。这一步必须快速完成，避免阻塞系统。 Bottom Half处理\n当有空闲时间或者需要时，系统进入Bottom Half，从缓冲区中取出字符，进一步处理或将其传递给用户进程（如Shell）。 在这个例子中并发体现在：\n系统正在执行某个任务（例如，运行一个用户进程），而此时UART设备接收到数据并触发中断，操作系统会暂停当前任务来处理中断。这是中断机制的并发性。 Top Half和Bottom Half之间的协作也是一种并发：Top Half处理了中断的紧急部分，将复杂工作留给Bottom Half。两者通过共享的缓冲区进行数据传递。 此外，Producer-Consumer模式也可以在这个例子中体现出来：\nProducer：UART设备产生的数据被存入缓冲区。 Consumer：用户进程（如Shell）从缓冲区读取数据进行处理。 缓冲区和锁机制可以使Producer和Consumer解耦，使它们能够按照各自的速度运行。这种并发处理通过缓冲区的读写操作来保证系统稳定、高效。\n如材料中提到的，中断处理增加了编程的复杂性，尤其是在并发环境下。设备和CPU是独立并行运行的。对于内核代码来说，中断可能会打断执行，这就要求在一些情况下，内核要临时关闭中断来确保代码执行的原子性。此外，驱动的Top Half和Bottom Half可以在不同的CPU上并行执行，这使得锁和同步机制变得非常重要，以确保不会出现竞态条件。\n总的来说：\n真实操作系统会充分利用内存，使其大部分被用于缓存和缓冲区。 中断机制通过异步处理实现了设备和CPU之间的并发性。 Top Half和Bottom Half分工协作，共同处理中断事件。 通过缓冲区和同步机制，操作系统能够有效地管理并发事件，例如在UART设备和用户进程之间的Producer-Consumer模式下运行。 这种架构在复杂的现代操作系统中，通过更高效的中断处理和并发管理，确保了系统的稳定性和性能。\nTop and Bottom Top Half是中断处理的第一阶段，主要特征是快速响应和处理紧急任务。它的核心任务是在中断发生时立即执行，以尽快恢复系统的正常运行。Top Half通常处理的工作包括：\n识别中断来源：当一个中断发生时，操作系统首先要知道哪个设备触发了中断。例如，是键盘输入了字符，还是网卡接收到了数据包。 保存重要数据：例如，从设备的硬件寄存器中读取数据并保存到一个临时缓冲区。这一步骤非常重要，因为它确保了中断源数据不会丢失。 清除或屏蔽中断：为了避免重复中断，Top Half通常会执行必要的操作以清除当前的中断状态，或暂时屏蔽相同的中断。 执行上下文：Top Half是在中断上下文中执行的。这意味着它并不运行在任何特定进程的上下文中，而是独立于进程执行。因此，Top Half不允许进行阻塞操作，因为它不能切换到其他任务。\n时间要求：由于中断的发生会打断系统的正常运行，Top Half的执行时间必须尽可能短，以便快速释放CPU资源，恢复系统的正常调度和任务执行。\nBottom Half是中断处理的第二阶段，它主要负责处理那些可以延迟执行的任务。由于Top Half必须尽快执行，复杂或耗时的操作会被推迟到Bottom Half中进行。这些任务可能包括：\n数据处理：例如，从Top Half中保存的缓冲区中读取数据并将其传递给用户进程或进行进一步处理。这个过程可能包括解析协议、与其他系统组件交互等复杂操作。 与用户进程交互：Bottom Half通常负责将处理后的数据传递给用户进程或内核的其他部分。比如，当一个文件系统接收到一个磁盘读请求时，Bottom Half可能会将数据从磁盘读取到内存，并准备好供用户进程访问。 执行上下文：与Top Half不同，Bottom Half在普通进程上下文中执行。这意味着它运行在系统的普通调度机制下，可以被中断，可以阻塞，可以与其他进程一起共享CPU时间。因为它在进程上下文中执行，所以Bottom Half可以访问系统的所有资源，并执行任何复杂的操作。\n时间要求：Bottom Half没有严格的时间要求，因为它的任务可以延迟执行，只要在合理的时间内完成即可。\nTop Half与Bottom Half的关系:\n协作与分工：Top Half和Bottom Half是通过一种协作分工的机制来共同完成中断处理的任务。Top Half快速响应中断并执行紧急处理，而Bottom Half则处理非紧急任务，确保系统的高效运行。这种分工允许系统能够同时响应多个中断，并在不中断系统正常运行的情况下处理复杂的任务。\n通信机制：在Top Half完成初步处理后，它会通过某种机制（如标志、队列、任务调度等）通知Bottom Half继续处理剩余的任务。Bottom Half通常在系统有空闲时间或需要时被调度执行。\n以UART串口通信为例来具体说明Top Half和Bottom Half的运作：\nTop Half:\n当UART设备接收到一个新的字符时，会触发一个中断。 操作系统立即响应这个中断，进入Top Half的处理程序。 Top Half会从UART的接收寄存器中读取字符，并将其保存到系统的一个缓冲区中（如环形缓冲区）。 读取完数据后，Top Half可能会清除中断标志，以防止再次触发相同的中断。 随后，Top Half结束，系统恢复执行被中断的任务。 Bottom Half:\n当系统空闲或特定条件满足时，Bottom Half被调度执行。 Bottom Half从Top Half保存的缓冲区中读取字符，并执行进一步处理，如将数据传递给用户进程。 如果是用户进程等待的数据（如Shell在等待用户输入），Bottom Half会将数据传递给进程，或者将处理结果显示在控制台上。 Bottom Half处理完成后，系统继续调度其他任务。 键盘输入与UART触发中断 当用户按下键盘上的某个键时，键盘会将该按键对应的字符通过UART串口传输到计算机主板上的UART设备。UART设备接收到来自键盘的字节数据，并将其存储在UART的接收寄存器中（Receive Holding Register, RHR）。与此同时，UART设备会触发一个硬件中断，通知操作系统有新的数据需要处理。\n当UART设备产生中断时，操作系统立即暂停当前正在执行的任务，进入中断处理程序的Top Half阶段。Top Half的主要任务是快速响应中断，将数据从UART设备中读取并保存到缓冲区中。详细过程如下：\n进入中断处理程序：操作系统通过查看UART中断状态，确认是键盘输入引发的中断。 读取UART寄存器数据：Top Half从UART的接收寄存器中读取一个字节的数据（即用户按下的字符），并将其存放到操作系统内存中的一个缓冲区。这个缓冲区通常是环形缓冲区（circular buffer），用于临时保存键盘输入的数据。 屏蔽/清除中断：为了避免重复中断，Top Half会清除中断标志或暂时屏蔽UART中断，防止再度中断。 结束Top Half：完成数据读取后，Top Half立即返回，恢复系统对其他任务的执行，确保中断处理时间尽可能短。 注意：在这个阶段，Top Half并不会进行复杂的处理任务，如将数据交给用户进程或显示在屏幕上。它只是确保数据从UART寄存器中取出，并保存到缓冲区中。\nTop Half完成后，操作系统会在合适的时机调用Bottom Half来处理剩下的任务。Bottom Half的主要职责是从缓冲区中读取数据，并将其传递给需要的进程（如Shell）。该阶段允许进行更复杂和耗时的操作，具体步骤如下：\n调度Bottom Half：系统在某个空闲时刻或当缓冲区中有足够的数据时，调度Bottom Half执行。 读取缓冲区数据：Bottom Half从Top Half保存的环形缓冲区中提取字符。这个字符是用户通过键盘输入的。 处理字符数据：读取到的字符可能会被进一步处理，例如检查是否有特殊控制字符（如回车或退格）。如果是普通字符，数据通常会被直接传递给需要处理的进程或设备。 用户进程交互：Bottom Half将从缓冲区中读取的字符提供给用户进程。例如，当前正在运行的Shell进程可能正在等待用户输入的命令。当Bottom Half处理完这些字符后，Shell可以继续执行用户的命令。 对于用户进程（如Shell）来说，键盘输入和UART设备的交互过程如下：\nShell等待输入：Shell进程通常会通过调用read系统调用，等待用户的输入。在这个过程中，Shell进程会进入休眠状态（sleep），直到有新的输入数据可用。 唤醒Shell进程：当用户输入字符并且Bottom Half完成了数据的处理后，Shell进程会被唤醒。具体来说，当环形缓冲区中有新的数据时，系统会通过wakeup机制唤醒之前休眠的Shell进程。 Shell读取数据：唤醒后的Shell进程通过read函数从缓冲区中读取用户输入的字符，并将其作为命令的一部分继续处理。 处理并显示：Shell读取并处理用户输入后，会在控制台上显示相应的反馈结果。例如，用户输入了命令“ls”，Shell读取该命令并将结果显示在屏幕上。 UART读取键盘输入的整个过程结合了中断机制和驱动程序的并发处理。操作系统通过中断管理器（如PLIC）响应外设的中断请求，并通过Top Half和Bottom Half机制高效地处理设备输入，同时保持系统的流畅运行。整个过程的并发管理依靠缓冲区和同步机制来解耦设备和用户进程的交互：\nTop Half快速响应中断并将键盘输入数据保存到缓冲区中。 Bottom Half在适当时机处理缓冲区中的数据，并将其交付给用户进程。 用户进程通过read函数获取输入数据，处理命令并显示结果。 这套机制使得操作系统能够并行处理键盘输入和其他任务，实现高效的多任务操作。\nTop Half与Bottom Half的接口 Top Half处理紧急的中断事件，如UART中断的情况。它的接口可以被内核或用户进程调用，典型的接口包括：\n读写接口：如read() 和 write()系统调用，用户进程可以调用这些接口从设备（如UART）读取数据或写入数据。这些接口的调用最终会触发Top Half处理输入/输出事件。 设备接口：在驱动程序中，Top Half直接与设备的寄存器和中断进行交互，它通过接口与硬件通信。例如，在UART驱动中，Top Half会读取UART的接收寄存器（如RHR寄存器）以获取数据，并将这些数据传递到内存缓冲区。 通常情况下，Bottom Half不会直接暴露给用户进程，因为它的工作是在内核中完成的。它的主要职责是处理Top Half推迟的任务，比如数据的进一步处理、协议解析等。虽然Bottom Half不直接提供传统意义上的接口，但它与Top Half及内核的其他部分之间会有某种隐式接口或协作机制。\nBottom Half的使用方式：\n延迟任务机制：Bottom Half通过某种延迟任务机制（如软中断softirq、任务队列tasklet、工作队列workqueue）被调度执行，这些机制不需要直接暴露给用户，而是在Top Half标记完成后，由操作系统自动安排执行。例如： 软中断（Softirq）：一种轻量级的延迟任务处理方式，用于处理网络数据包等延迟任务。 工作队列（Workqueue）：工作队列可以安排更复杂的任务，通常用于处理需要长时间执行的非紧急操作。 缓冲区的读写操作：Bottom Half主要通过访问共享的缓冲区进行操作。例如，Top Half将数据写入环形缓冲区，Bottom Half读取该缓冲区的数据并处理。虽然这些操作是通过内核函数调用完成的，但它们不直接作为接口提供给用户进程。 Top Half和Bottom Half的接口区别：\nTop Half的接口： 直接与用户进程交互：Top Half通常提供系统调用接口，如read() 和 write()，这些接口由用户进程调用，直接与外部设备（如UART）交互。 紧急处理：Top Half处理硬件中断的紧急部分，执行快速数据采集和传递任务，保证设备响应迅速。 Bottom Half的“接口”： 间接与内核交互：Bottom Half通过软中断、任务队列等机制被调度执行，不与用户进程直接交互。它主要处理延迟任务，如协议处理、数据打包等。 非紧急处理：Bottom Half处理不需要立即完成的任务，确保Top Half能够快速恢复正常工作，而不会因为复杂任务而延迟。 虽然Bottom Half不直接暴露给用户进程，开发者可以通过内核API间接与Bottom Half交互，例如：\n延迟任务调度：如果需要让某个任务在Bottom Half中处理，可以使用**任务队列（Workqueue）*或者*软中断（Softirq） API。例如，通过内核的queue_work()函数，将任务安排到工作队列中，等待系统空闲时执行。 数据传递：Bottom Half负责将处理后的数据传递给Top Half或内核的其他部分。举例来说，Bottom Half处理完某个网络数据包后，会通过某种内核回调机制将数据传递给网络协议栈，进一步传递到用户进程。 总结：\nTop Half的接口：直接暴露给用户进程或内核的其他部分，通常包括读写设备的系统调用接口以及硬件寄存器的访问接口。它通过这些接口处理设备的紧急任务。 Bottom Half的接口：Bottom Half不直接暴露给用户进程，而是通过内核的延迟任务机制（如软中断、工作队列）来处理非紧急任务。这些机制由Top Half或内核自动安排，开发者可以通过内核API使用这些机制，但并不会直接与用户进程交互。 Bottom Half的作用更多是通过内核机制间接使用，如果需要处理复杂的任务或与外部接口交互，通常是由Top Half处理好紧急任务后，Bottom Half通过缓冲区和延迟任务机制来完成实际的操作。\n控制台输入过程（详细分点说明） 初始化阶段：\nmain 函数调用 consoleinit: main 函数在系统启动时调用 consoleinit（kernel/console.c:184），该函数负责初始化UART硬件，使UART能够处理接收和发送字符的中断。\n在consoleinit中，UART硬件配置为：\n每当接收到一个字节时触发接收中断。 每当一个字节被发送完毕时触发发送完成中断。 UART等待输入：\nUART硬件等待输入字符:\nUART的FIFO（First In, First Out）缓冲区存储从外部接收到的字符。UART会将接收到的每个字符存放到 RHR（Receiver Holding Register）寄存器中，供软件读取。 LSR（Line Status Register）会更新状态位，指示UART接收到新字符，准备供软件读取。\n用户输入触发中断：\n用户按键输入传递到UART:\n当用户通过键盘输入字符时，输入会通过QEMU模拟的UART硬件发送到xv6中的UART设备。 当UART接收到字符时，触发接收中断，并向RISC-V内核发出中断请求。 RISC-V的陷阱处理程序接收到该中断后，调用 devintr（kernel/trap.c:177）处理。 处理中断：\ndevintr 确定中断源: devintr 从RISC-V的 scause 寄存器中读取中断信息，发现中断来自外部设备。 devintr 通过与PLIC（Platform-Level Interrupt Controller）的交互，确定中断来源于UART设备。 调用UART中断处理程序：\nuartintr 处理UART中断: devintr 确定中断来自UART后，调用 uartintr（kernel/uart.c:180）。 uartintr 从UART的RHR读取所有等待处理的输入字符。 这些输入字符被传递给 consoleintr（kernel/console.c:138）。 字符处理与缓冲：\nconsoleintr 处理用户输入字符: consoleintr 将接收到的字符积累到缓冲区 cons.buf 中。 该函数对特殊字符（如退格、Ctrl+U）进行处理。 如果用户输入了换行符（代表一整行输入完成），consoleintr 会唤醒等待的 consoleread 函数。 整行输入读取：\nconsoleread 等待整行输入: 当用户进程（如shell）调用 read 系统调用时，系统进入 consoleread 函数（kernel/console.c:82）。 consoleread 会检查 cons.buf 中的内容，如果未接收到整行输入，调用 sleep 使进程进入休眠状态，等待输入完成。 当整行输入完成并且 consoleintr 唤醒 consoleread 时，consoleread 会将 cons.buf 中的整行内容复制到用户空间。 返回输入结果： 返回用户输入数据:\nconsoleread 将缓冲区中的一整行数据复制到用户进程的缓冲区中，并通过 read 系统调用将该数据返回到用户进程。 用户进程（如shell）会继续处理该输入行，执行相应操作。\n总结：\n输入流程开始于用户按键输入，字符通过QEMU的UART硬件传递到xv6的UART设备，触发中断。 中断处理程序 devintr 和 uartintr 从UART硬件读取输入字符，consoleintr 将字符缓存在 cons.buf 中。 当整行输入完成后，consoleread 负责将输入行复制到用户空间，并返回给用户进程。 控制台输出过程（函数作用与调用关系） write 系统调用\n作用: 用户进程通过 write 系统调用向控制台输出数据。\n调用关系: write 系统调用从用户空间接收数据，并将其交给内核，由内核负责将数据写入到控制台设备的文件描述符。\nuartputc (kernel/uart.c:87)\n作用: 这是控制台输出的核心函数，用于将每个字符输出到UART设备。\n缓冲机制: 将输出字符添加到输出缓冲区 uart_tx_buf，这样写进程不需要等待UART硬件逐个字节传输数据。 调用 uartstart: 在字符被放入缓冲区后，uartputc 会调用 uartstart 来启动UART的输出传输。 等待情况: 如果缓冲区已满，uartputc 将等待直到缓冲区有空余空间再继续。 uartstart (kernel/uart.c:126)\n作用: 检查UART是否正在发送数据，如果没有则开始传输下一个字符。\n设备启动: uartstart 会从输出缓冲区中取出下一个要发送的字符，写入到 THR（Transmitter Holding Register） 中，启动UART的传输。\n调用关系:\nuartstart可以由两种方式调用：\n由 uartputc 调用，用于启动第一个字符的传输。 由 uartintr 调用，用于继续传输剩余缓冲区中的字符。 uartintr (kernel/uart.c:180)\n作用: UART中断处理程序，用于在传输完成时处理下一个字符。\n调用 uartstart: 当UART完成了一个字节的传输，会触发中断，uartintr 会调用 uartstart 来继续发送缓冲区中的下一个字节。 中断机制: UART每次传输完成一个字节，都会触发中断，这使得 uartintr 能够继续从 uart_tx_buf 缓冲区中读取字符并发送。\n控制台输出的调用关系图 用户进程调用 write 系统调用 用户进程通过 write 向控制台文件描述符写入多个字符。 write 调用 uartputc write 系统调用内部将每个字符传递给 uartputc。 uartputc 将每个字符加入到 uart_tx_buf 缓冲区中，确保写进程不必等待字符传输完成。 uartputc 调用 uartstart uartputc 启动传输第一个字符，将其交给UART硬件。 uartstart 检查UART是否空闲，如果空闲，则从 uart_tx_buf 中取出下一个字符，并写入 THR 寄存器，启动字符传输。 UART硬件发送完成，触发中断 UART每次完成一个字符传输时，会触发中断，通知CPU传输完成。 中断处理程序 uartintr 处理传输完成 中断触发后，RISC-V内核的陷阱处理程序调用 devintr，随后调用 uartintr。 uartintr 负责从 uart_tx_buf 中读取下一个字符，调用 uartstart 启动下一个字符的传输。 uartstart 启动下一字符传输 uartstart 再次从 uart_tx_buf 中取出下一个字符，写入UART的 THR 寄存器中，继续传输。 控制台输出的完整流程：\n用户进程通过 write 系统调用 向控制台写入多个字符。 write 调用 uartputc，将字符逐个加入输出缓冲区 uart_tx_buf。 uartputc 调用 uartstart，启动UART传输第一个字符。 UART硬件发送完一个字符后，触发中断。 中断处理程序 uartintr 调用 uartstart，继续发送输出缓冲区中的下一个字符。 UART持续传输，直到所有缓冲字符都被发送完毕。 控制台输出是通过将数据放入输出缓冲区并异步发送的过程。write 系统调用将数据交给 uartputc，它会立即返回，避免进程阻塞。通过 uartstart 和中断机制，UART硬件逐步发送缓冲区中的字符，最终完成数据传输。这种设计通过缓冲和中断解耦了设备的慢速I/O操作与进程的执行，从而提高了系统的效率和响应能力。\nUART（通用异步收发传输器）具有双向通信功能，既能接收字符，也能发送字符。这是UART作为一个串行通信设备的核心功能，它不仅用于从外部设备接收输入，还能将系统内部的数据发送到外部设备。下面我详细解释一下这两者的关系，以及发送字符到UART的作用。\n接收字符（输入）： UART负责从外部设备接收数据，比如从键盘输入或从其他串行设备接收到数据。 在你提到的控制台输入的过程中，UART通过硬件接收字符，触发中断，最终让xv6内核处理输入字符。 发送字符（输出）： UART也负责将系统内部的数据传输到外部设备。例如，操作系统需要把用户进程的输出（比如shell的输出）显示在控制台屏幕上，这时就需要通过UART发送数据。 当系统向控制台写入输出数据时，它通过UART传输这些字符，最终显示在终端或其他外部设备上。 定时器中断的作用 定时器中断在操作系统中起着至关重要的作用，它的主要功能是确保系统时钟的精确性，以及实现任务的调度。在xv6操作系统中，定时器中断同样是保持系统正常运行的关键机制，特别是在多任务处理和进程调度方面。下面我将详细说明定时器中断的具体作用。\n维持系统时钟\n定时器中断的首要功能是维持操作系统的时钟，这是所有现代操作系统的基础。操作系统通过定时器中断来跟踪系统的当前时间，并执行与时间相关的操作，比如：\n时间片（Time Slice）管理：在多任务系统中，每个进程或线程会分配一定的时间片进行执行。当时间片用完后，定时器中断会触发，从而通知操作系统进行任务切换。 系统时间维护：定时器中断以固定的间隔触发，操作系统利用这种固定周期的中断来更新系统的时间，使得系统可以精确地管理秒、分钟、小时等时间单位。 定时事件：定时器中断可以用于触发定时任务（比如操作系统内核中的心跳机制，定期检查系统健康状态），也可以用于实现用户进程中的定时功能，如sleep函数。 实现进程调度\n在xv6操作系统中，定时器中断起到了触发进程调度的作用。通过定时器中断，操作系统可以定期检查当前运行的进程，并决定是否切换到另一个进程：\n时间片轮转调度（Round-Robin Scheduling）：在多任务操作系统中，定时器中断用于实现时间片轮转调度算法。操作系统会为每个进程分配一个时间片（比如几毫秒），当时间片用完时，定时器中断会触发操作系统的调度器（scheduler）运行，选择另一个进程执行。\n例如，在xv6中，当定时器中断发生时，系统会调用 yield 函数来触发任务切换。yield 将当前进程的状态保存，并调度其他进程运行。如果当前进程是一个计算密集型进程（即需要大量CPU时间的进程），定时器中断确保该进程不会长期占用CPU，而是让其他进程也有机会执行。\n公平性：定时器中断通过时间片机制保证系统中每个进程能够公平地获取CPU资源。如果没有定时器中断，一个高优先级或耗时长的进程可能会长时间占用CPU，导致其他进程饥饿。\n打断无限循环或阻塞操作\n在一些情况下，进程可能会进入长时间的阻塞或无限循环。定时器中断的存在可以强制打断这些进程，从而避免系统陷入死锁或卡死状态：\n防止单一进程长期占用CPU：如果一个进程陷入无限循环，定时器中断会周期性触发进程切换，确保系统可以继续运行其他任务。 进程间的并发处理：定时器中断帮助操作系统在进程间实现并发运行，防止某个进程因长时间执行而阻止其他进程获取CPU。 保证系统响应性\n定时器中断还确保系统能够及时响应外部事件，例如I/O操作、用户输入等。即使系统正处于一个计算密集型任务中，定时器中断可以强制让操作系统暂停当前任务，检查是否有需要优先处理的事件：\nI/O并发：定时器中断可以在处理耗时的I/O操作时保证系统不会卡死。例如，当系统等待外部设备的输入时，定时器中断允许其他进程继续执行，而不是让整个系统停滞在I/O操作上。 定期触发系统任务\n定时器中断不仅用于任务调度，还用于定期触发系统级任务。这些任务通常是在操作系统后台运行，用于维护系统的健康状态或执行周期性的操作。例如：\n垃圾回收：操作系统可能会在定时器中断的帮助下，定期清理不再使用的资源或内存。 系统监控：操作系统会使用定时器中断定期执行监控任务，检查系统负载、资源使用情况，或者检查进程是否存在异常行为。 定时器中断的典型过程：\n设置定时器：在xv6启动时，start.c 中的代码设置了定时器硬件，通过编程CLINT（Core-Local Interruptor）来生成定期的中断信号。 定时器中断触发：定时器达到设定的时间后，触发中断信号。这个中断会导致CPU暂停当前的任务，跳转到定时器中断处理程序。 处理定时器中断： 定时器中断触发后，处理程序 timervec 会保存CPU的寄存器状态，设置下一个定时器中断，并请求RISC-V发起一个软件中断。 软件中断交给xv6的内核处理，执行内核的任务调度机制。 任务调度：通过中断的方式，xv6可以决定是否需要切换到另一个进程，确保所有进程都能公平地获取CPU时间。 定时器中断是xv6以及所有现代操作系统的关键组成部分。它不仅维持了系统的时间管理，还通过触发进程切换来实现多任务处理。定时器中断确保了系统的并发性、响应性和公平性，在用户进程之间高效调度资源，同时保持系统流畅运行。\nCLINT 的详细解释 CLINT（Core-Local Interruptor） 是 RISC-V 架构中的一个硬件模块，负责生成核局部中断（Local Interrupts），包括定时器中断和软件中断。它在每个 CPU 核上单独存在，用于处理与该核相关的中断，主要包括定时器中断和软件中断。\nCLINT 在 RISC-V 系统中的主要作用是提供一种机制，使每个 CPU 核能够：\n生成定时器中断：CLINT 包含一个计时器寄存器，通过编程这些寄存器，操作系统可以设置一个定时器。定时器到达指定时间后，CLINT 会向相应的 CPU 核发出一个定时器中断信号。 触发软件中断：CLINT 可以通过写入寄存器，向本地 CPU 核发出一个软件中断。这种中断通常用于进程间通信（IPC），比如操作系统内核通知某个核执行特定的任务。 管理中断：CLINT 负责处理与核本地相关的中断，它允许每个 CPU 核独立处理自己的定时器中断和软件中断。 CLINT 的工作范围在 CPU 核本地，特别是与定时器和软件中断相关的控制操作。在以下几个方面，CLINT 发挥了关键作用：\n定时器中断管理： CLINT 内部有一个定时器寄存器 mtime，用于存储当前的时间计数值，还有一个 mtimecmp 寄存器，表示下一个定时器中断的时间。 当 mtime 增加到等于或超过 mtimecmp 的值时，CLINT 生成一个定时器中断，通知 CPU 核中断处理程序（如 timervec）。 软件中断管理： 操作系统可以通过向 CLINT 的软件中断寄存器写入特定的值来触发一个软件中断。这个中断可以用于通知 CPU 执行特定的内核任务或进程间通信。 核本地中断控制： CLINT 在每个 CPU 核上独立运行，管理与该核相关的本地中断，而不影响其他 CPU 核。这样可以实现并行的中断处理，确保多核系统的高效运行。 要编程 CLINT，我们需要与其提供的寄存器进行交互。通常，这涉及对 mtime 和 mtimecmp 寄存器的读写操作。\n初始化定时器中断\n在 xV6 中，定时器中断的设置是在系统初始化阶段完成的，通常在 start.c 文件中实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void start() { // ... 初始化代码 ... // 设置定时器：编程 CLINT int interval = 1000000; // 定时器间隔，时间单位依赖于时钟频率 *(uint64*)CLINT_MTIMECMP(hartid) = *(uint64*)CLINT_MTIME + interval; // 启用定时器中断 w_mie(r_mie() | MIE_MTIE); // 允许机器模式定时器中断 // 设置 mtvec 指向定时器中断处理程序 timervec w_mtvec((uint64)timervec); // 进入内核主程序 main(); } CLINT 寄存器\nmtime 寄存器：用于计时，它会不断递增，表示当前的时间。\nmtimecmp 寄存器：用于设定定时器中断的触发时间。操作系统将 mtimecmp 设为一个未来的时间值，当 mtime 达到该值时，CLINT 触发定时器中断。\n处理定时器中断\n当 mtime 达到或超过 mtimecmp 的值时，CLINT 触发一个定时器中断。定时器中断会导致 CPU 跳转到中断处理向量（timervec），在此基础上进行以下步骤：\n保存当前状态：timervec 保存当前寄存器状态，以免打断正在执行的任务。 重新编程定时器：将 mtimecmp 更新到一个新的未来时间，以便设置下一个定时器中断。 触发软件中断：timervec 请求生成一个软件中断，通过正常的陷阱机制交给内核进一步处理。 在 devintr 中处理软件中断\n软件中断是由定时器中断处理程序触发的。devintr 函数（kernel/trap.c:204）处理这种软件中断，具体工作如下：\n识别中断类型：通过检查 RISC-V 的 scause 寄存器，devintr 确定中断是由定时器引发的。 执行调度程序：如果定时器中断触发了软件中断，devintr 会调用内核的调度函数 yield，从而可能将 CPU 切换到另一个进程。 总结：\nCLINT 是 RISC-V 系统中的一个核心组件，负责处理核本地的中断，尤其是定时器中断和软件中断。 定时器中断的编程 通过设置 mtime 和 mtimecmp 寄存器来完成，定期生成中断信号，通知操作系统进行调度和其他周期性任务。 定时器中断的作用 是维持系统时钟、调度进程、打断无限循环或阻塞操作，确保系统响应性和公平性。 CLINT 通过提供定时器和软件中断功能，在多核系统中帮助操作系统高效地管理和调度任务。\n","date":"2024-09-16T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0819interrupts/bg_hu4564447995448469058.jpg","permalink":"https://echudet.github.io/p/mit6s0819interrupts/","title":"MIT6S081：9Interrupts"},{"content":"Lecture 8 Page Fault 课程杂记 在XV6中，一旦用户空间进程触发了page fault，会导致进程被杀掉。目前为止，内存地址映射相对来说比较静态。不管是user page table还是kernel page table，都是在最开始的时候设置好，之后就不会再做任何变动。 page fault可以让这里的地址映射关系变得动态起来。通过page fault，内核可以更新page table，这是一个非常强大的功能。 内核需要什么样的信息才能够响应page fault 需要出错的虚拟地址，或者是触发page fault的源。(引起page fault的内存地址) 出错的原因。(引起page fault的原因类型) 触发page fault的指令的地址。(引起page fault时的程序计数器值)\u0026ndash;\u0026gt;因为在page fault handler中我们或许想要修复page table，并重新执行对应的指令。 Lazy page allocation。在XV6中，sbrk的实现默认是eager allocation。这表示了，一旦调用了sbrk，内核会立即分配应用程序所需要的物理内存。 sbrk是XV6提供的系统调用，它使得用户应用程序能扩大自己的heap。当调用sbrk时，它的参数是整数，代表了你想要申请的page数量。sbrk会扩展heap的上边界（也就是会扩大heap）。 当一个应用程序启动的时候，sbrk指向的是heap的最底端，同时也是stack的最顶端。这个位置通过代表进程的数据结构中的sz字段表示，这里以p-\u0026gt;sz表示。 当sbrk实际发生或者被调用的时候，内核会分配一些物理内存，并将这些内存映射到用户应用程序的地址空间，然后将内存内容初始化为0，再返回sbrk系统调用。应用程序还可以通过给sbrk传入负数作为参数，来减少或者压缩它的地址空间。 利用lazy allocation。核心思想非常简单，sbrk系统调基本上不做任何事情，唯一需要做的事情就是提升p-\u0026gt;sz，将p-\u0026gt;sz增加n，其中n是需要新分配的内存page数量。但是内核在这个时间点并不会分配任何物理内存。之后在某个时间点，应用程序使用到了新申请的那部分内存，这时会触发page fault，因为我们还没有将新的内存映射到page table。所以，如果我们解析一个大于旧的p-\u0026gt;sz，但是又小于新的*p-\u0026gt;sz（注，也就是旧的p-\u0026gt;sz + n）*的虚拟地址，我们希望内核能够分配一个内存page，并且重新执行指令。 当你查看一个用户程序的地址空间时，存在text区域，data区域，同时还有一个BSS区域（注，BSS区域包含了未被初始化或者初始化为0的全局或者静态变量）。当编译器在生成二进制文件时，编译器会填入这三个区域。text区域是程序的指令，data区域存放的是初始化了的全局变量，BSS包含了未被初始化或者初始化为0的全局变量。\n在一个正常的操作系统中，如果执行exec，exec会申请地址空间，里面会存放text和data。因为BSS里面保存了未被初始化的全局变量，这里或许有许多许多个page，但是所有的page内容都为0。\n通常可以调优的地方是，我有如此多的内容全是0的page，在物理内存中，我只需要分配一个page，这个page的内容全是0。然后将所有虚拟地址空间的全0的page都map到这一个物理page上。这样至少在程序启动的时候能节省大量的物理内存分配。\n因为所有的虚拟地址空间page都期望page的内容是全0，所以这里的PTE都是只读的。\n之后在某个时间点，应用程序尝试写BSS中的一个page时，比如说需要更改一两个变量的值，我们会得到page fault。那么，对于这个特定场景中的page fault,我们应该创建一个新的page，将其内容设置为0，并重新执行指令。之后我们需要更新这个page的mapping关系，首先PTE要设置成可读可写，然后将其指向新的物理page。这里相当于更新了PTE，之后我们可以重新执行指令。\ncopy-on-write fork，有时也称为COW fork。当我们创建子进程时，与其创建，分配并拷贝内容到新的物理内存，其实我们可以直接共享父进程的物理内存page。我们可以设置子进程的PTE指向父进程对应的物理内存page。子进程想要修改这些内存的内容，相应的更新应该对父进程不可见。为了确保进程间的隔离性，我们可以将这里的父进程和子进程的PTE的标志位都设置成只读的。\n父进程和子进程都会继续运行，而父进程或者子进程都可能会执行store指令来更新一些全局变量，这时就会触发page fault，因为现在在向一个只读的PTE写数据。\n新分配的物理内存page只对子进程的地址空间可见，所以我们可以将相应的PTE设置成可读写，并且我们可以重新执行store指令。实际上，对于触发刚刚page fault的物理page，因为现在只对父进程可见，相应的PTE对于父进程也变成可读写的了。\n当内核在管理这些page table时，对于copy-on-write相关的page，内核可以设置相应的bit位，这样当发生page fault时，我们可以发现如果copy-on-write bit位设置了，我们就可以执行相应的操作了。否则的话，比如说lazy allocation，我们就做一些其他的处理操作。\n学生提问：当发生page fault时，我们其实是在向一个只读的地址执行写操作。内核如何能分辨现在是一个copy-on-write fork的场景，而不是应用程序在向一个正常的只读地址写数据。是不是说默认情况下，用户程序的PTE都是可读写的，除非在copy-on-write fork的场景下才可能出现只读的PTE？\nFrans教授：内核必须要能够识别这是一个copy-on-write场景。几乎所有的page table硬件都支持了这一点。我们之前并没有提到相关的内容，下图是一个常见的多级page table。对于PTE的标志位，我之前介绍过第0bit到第7bit，但是没有介绍最后两位RSW。这两位保留给supervisor software使用，supervisor softeware指的就是内核。内核可以随意使用这两个bit位。所以可以做的一件事情就是，将bit8标识为当前是一个copy-on-write page。\n我们回到exec，在未修改的XV6中，操作系统会加载程序内存的text，data区域，并且以eager的方式将这些区域加载进page table。但是根据我们在lazy allocation和zero-filled on demand的经验，为什么我们要以eager的方式将程序加载到内存中？为什么不再等等，直到应用程序实际需要这些指令的时候再加载内存？程序的二进制文件可能非常的巨大，将它全部从磁盘加载到内存中将会是一个代价很高的操作。又或者data区域的大小远大于常见的场景所需要的大小，我们并不一定需要将整个二进制都加载到内存中。\n对于exec，在虚拟地址空间中，我们为text和data分配好地址段，但是相应的PTE并不对应任何物理内存page。对于这些PTE，我们只需要将valid bit位设置为0即可。\n在最坏的情况下，用户程序使用了text和data中的所有内容，那么我们将会在应用程序的每个page都收到一个page fault。但是如果我们幸运的话，用户程序并没有使用所有的text区域或者data区域，那么我们一方面可以节省一些物理内存，另一方面我们可以让exec运行的更快（注，因为不需要为整个程序分配内存）。\n我们将要读取的文件，它的text和data区域可能大于物理内存的容量。又或者多个应用程序按照demand paging的方式启动，它们二进制文件的和大于实际物理内存的容量。对于demand paging来说，假设内存已经耗尽了或者说OOM了，这个时候如果得到了一个page fault，需要从文件系统拷贝中拷贝一些内容到内存中，但这时你又没有任何可用的物理内存page。一个选择是撤回page（evict page）。比如说将部分内存page中的内容写回到文件系统再撤回page。一旦你撤回并释放了page，那么你就有了一个新的空闲的page，你可以使用这个刚刚空闲出来的page，分配给刚刚的page fault handler，再重新执行指令。\n什么样的page可以被撤回？并且该使用什么样的策略来撤回page？\n最常用的策略，Least Recently Used，或者叫LRU。没有被Access过的page可以直接撤回，定时的将Access bit恢复成0。\n一个学生提问：为什么需要恢复这个bit？\nFrans教授：如果你想知道page最近是否被使用过，你需要定时比如每100毫秒或者每秒清除Access bit，如果在下一个100毫秒这个page被访问过，那你就知道这个page在上一个100毫秒中被使用了。而Access bit为0的page在上100毫秒未被使用。这样你就可以统计每个内存page使用的频度，这是一个成熟的LRU实现的基础。（注，可以通过Access bit来决定内存page 在LRU中的排名）\n这节课最后要讨论的内容，也是后面的一个实验，就是memory mapped files。这里的核心思想是，将完整或者部分文件加载到内存中，这样就可以通过内存地址相关的load或者store指令来操纵文件。\n为了支持这个功能，一个现代的操作系统会提供一个叫做mmap的系统调用。这个系统调用会接收一个虚拟内存地址（VA），长度（len），protection，一些标志位，一个打开文件的文件描述符，和偏移量（offset）。\n这里的语义就是，从文件描述符对应的文件的偏移量的位置开始，映射长度为len的内容到虚拟内存地址VA，同时我们需要加上一些保护，比如只读或者读写。\n假设文件内容是读写并且内核实现mmap的方式是eager方式（不过大部分系统都不会这么做），内核会从文件的offset位置开始，将数据拷贝到内存，设置好PTE指向物理内存的位置。之后应用程序就可以使用load或者store指令来修改内存中对应的文件内容。当完成操作之后，会有一个对应的unmap系统调用，参数是虚拟地址（VA），长度（len）。来表明应用程序已经完成了对文件的操作，在unmap时间点，我们需要将dirty block写回到文件中。我们可以很容易的找到哪些block是dirty的，因为它们在PTE中的dirty bit为1。\n","date":"2024-09-15T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0818page-fault/bg_hu2818399105440696351.jpg","permalink":"https://echudet.github.io/p/mit6s0818page-fault/","title":"MIT6S081：8Page Fault"},{"content":"Lecture 6 Isolation \u0026amp; system call entry/exit 课程杂记 Traps and system calls\n有三种事件会导致 CPU 搁置普通指令的执行，强制将控制权转移给处理该事件的特殊代码。\n系统调用-\u0026gt;用户程序执行 ecall 指令要求内核为其做某事时 异常-\u0026gt;一条指令（用户或内核）做了一些非法的事情，如除以零 中断-\u0026gt;当一个设备发出需要注意的信号时，例如当磁盘硬件完成一个读写 请求时 代码在执行时发生 trap，之后都会被恢复，而且不需要意识到发生了什么特殊的事情\n执行顺序：trap 迫使控制权转移到内核；内核保存寄存器和其他状态，以便恢复执行；内核执行适当的处理程序代码（例如，系统调用实现或设备驱动程序）；内核恢复保存的状态，并从 trap 中 返回；代码从原来的地方恢复。\nXv6 trap 处理分为四个阶段：RISC-V CPU 采取的硬件行为，为内核 C 代码准备的汇编入口，处理 trap的 C 处理程序，以及系统调用或设备驱动服务\n三种 trap 类型之间的 共性表明，内核可以用单一的代码入口处理所有的 trap，但事实证明，为三种不同的情况， 即来自用户空间的 trap、来自内核空间的 trap 和定时器中断，设置单独的汇编入口和 C trap 处理程序是很方便的。\n每个 RISC-V CPU都有一组控制寄存器，内核写入这些寄存器来告诉CPU 如何处理 trap， 内核可以通过读取这些寄存器来发现已经发生的 trap。这里是最重要的寄存器的概述。\nstvec：内核在这里写下 trap 处理程序的地址；RISC-V 到这里来处理 trap。\nsepc：当 trap 发生时，RISC-V 会将程序计数器保存在这里（因为 PC 会被 stvec 覆 盖）。sret(从 trap 中返回)指令将 sepc 复制到 pc 中。内核可以写 sepc 来控制 sret 的返回到哪里。\nscause：RISC -V 在这里放了一个数字，描述了 trap 的原因。\nsscratch：内核在这里放置了一个值，这个值会方便 trap 恢复/储存用户上下文。\nsstatus：sstatus 中的 SIE 位控制设备中断是否被启用，如果内核清除 SIE，RISCV 将推迟设备中断，直到内核设置 SIE。SPP 位表示 trap 是来自用户模式还是监督者模式，并控制 sret 返回到什么模式。\n当需要执行 trap 时，RISC-V 硬件对所有的 trap 类型（除定时器中断外）进行以下操作：\n如果该 trap 是设备中断，且 sstatus SIE 位为 0，则不要执行以下任何操作。 通过清除 SIE 来禁用中断。 复制 pc 到 sepc 34 将当前模式(用户或监督者)保存在 sstatus 的 SPP 位。 在 scause 设置该次 trap 的原因。 将模式转换为监督者。 将 stvec 复制到 pc。 执行新的 pc。 因为 RISC-V 硬件在 trap 过程中不切换页表，所以用户页表必须包含 uservec 的映射， 即 stvec 指向的 trap 处理程序地址。uservec 必须切换 satp，使其指向内核页表；为了在切 换后继续执行指令，uservec 必须被映射到内核页表与用户页表相同的地址。\nXv6 用一个包含 uservec 的 trampoline 页来满足这些条件。Xv6 在内核页表和每个用户 页表中的同一个虚拟地址上映射了 trampoline 页。这个虚拟地址就是 TRAMPOLINE （如我 们在图 2.3 和图 3.3 中看到的）。trampoline.S 中包含 trampoline 的内容，（执行用户代码 时）stvec 设置为 uservec（kernel/trampoline.S:16）。\ntrapframe 包含指向当前进程的内核栈、当前 CPU 的 hartid、usertrap 的地址和内核页 表的地址的指针，uservec 将这些值设置到相应的寄存器中，并将 satp 切换到内核页表和 刷新 TLB，然后调用 usertrap。\n用户空间 trap 处理流程详解\n当 RISC-V 系统中用户空间程序发生 trap（如系统调用、非法操作或设备中断）时，处理流程如下：\n1. Trap 发生时的初始状态 用户程序在用户模式下运行，并使用用户页表。 程序可能触发 trap，原因可能是系统调用（ecall 指令）、非法指令或硬件中断。 2. 切换到 uservec（kernel/trampoline.S:16） CPU 检测到 trap 并自动跳转到 stvec 指向的地址，即 uservec，这是一个位于内核中的 trap 处理程序的入口地址。 问题：此时仍在使用用户页表，用户页表可能无法映射内核的所有区域，直接访问内核会导致失败。 3. uservec 的操作 uservec 是一个位于 trampoline 页的代码，trampoline 页同时映射在用户页表和内核页表中的相同虚拟地址上。 uservec 通过 csrrw 指令将寄存器 a0 与 sscratch 互换。此时，a0 变为可用，并指向了 sscratch 中的内核数据（通常为进程的 trapframe）。 uservec 保存用户程序的寄存器到进程的 trapframe 中。trapframe 是专门为每个进程分配的一页内存，映射在用户地址空间中（虚拟地址 TRAPFRAME 下）。 4. 切换到内核页表 uservec 设置寄存器，使得 satp 寄存器指向内核页表，并刷新 TLB（Translation Lookaside Buffer，地址转换缓冲）。 切换页表后，系统安全地进入内核地址空间，可以正常访问内核的所有内存和功能。 5. 调用 usertrap（kernel/trap.c:37） uservec 完成初步处理后，跳转到 usertrap，它负责进一步处理 trap。 usertrap 首先更改 stvec 指向 kernelvec，以处理在内核模式下可能发生的其他 trap。 保存 sepc（用户程序计数器），因为在内核中可能会发生进程切换。 根据 trap 的类型进行处理： 如果是系统调用，调用 syscall 处理。 如果是设备中断，调用 devintr 处理。 如果是异常，可能会杀死当前进程。 6. 准备返回用户空间 usertrap 完成处理后，准备返回用户空间。\n调用\n1 usertrapret （kernel/trap.c:90），它负责恢复用户空间的运行环境：\n设置 stvec 重新指向 uservec，以准备下一次 trap。 恢复 sepc 到保存的用户程序计数器。 跳转到 trampoline 页的 userret，准备切换回用户页表。 7. userret 的操作 1 userret 负责最后的状态恢复和切换回用户页表：\nuserret 将 satp 切换回用户页表，确保系统回到用户模式。 恢复用户寄存器，包括 a0 和 sscratch 的最终交换，确保在下一次 trap 发生时这些寄存器可用。 使用 sret 指令返回用户空间，从保存的用户程序计数器处继续执行用户代码。 关键点总结 Trampoline 页：这是内核和用户页表中映射的特殊区域，确保了在页表切换前后的代码能正常运行。 sscratch 寄存器：用于在 trap 处理过程中保存和交换关键的寄存器值。 trapframe：保存所有用户寄存器的状态，便于在用户和内核模式之间切换时进行恢复和保存。 当系统调用函数返回时，syscall 将其返回值记录在 p-\u0026gt;trapframe-\u0026gt;a0 中。用户空间 的 exec()将会返回该值，因为 RISC-V 上的 C 调用通常将返回值放在 a0 中。系统调用返回 负数表示错误，0 或正数表示成功。如果系统调用号无效，syscall 会打印错误并返回 1。\n内核 trap 代码将用 户寄存器保存到当前进程的 trapframe 中，内核代码可以在那里找到它们。函数 argint、 argaddr 和 argfd 从 trapframe 中以整数、指针或文件描述符的形式检索第 n 个系统调用参 数。它们都调用 argraw 在 trapframe 中检索相应的数据(kernel/syscall.c:35)。\nXv6 对异常的响应是相当固定：如果一个异常发生在用户空间，内核就会杀死故障进程。 如果一个异常发生在内核中，内核就会 panic。真正的操作系统通常会以更有趣的方式进行 响应。\n当 CPU 不能将虚拟地址翻译成物理地址时，CPU 会产生一个页面故障异常(page-fault exception)。 RISC-V 有三种不同的页故障：load 页故障（当加载指令不能翻译其虚拟地址 时）、stote 页故障（当存储指令不能翻译其虚拟地址时）和指令页故障（当指令的地址不能 翻译时）。scause 寄存器中的值表示页面故障的类型，stval 寄存器中包含无法翻译的地址。\n写时复制（Copy-On-Write, COW）Fork\n在传统的 fork 操作中，父进程的整个内存空间（包括数据段、堆和栈）都会被复制给子进程。这种方法非常消耗资源，因为即使子进程只使用了父进程的一小部分内存，内核仍然需要进行全量的内存复制。\nCOW fork 提出了一个更高效的解决方案。通过在初始时不复制物理内存，而是让父进程和子进程共享同一块物理内存，操作系统可以大幅节省内存和时间。但这样做有一个问题：如果父进程和子进程都试图写入同一块内存，可能会互相干扰。为了避免这种情况，操作系统将这块共享的内存页标记为只读。\n当父进程或子进程试图写入这块内存时，CPU 会检测到这个写操作违反了只读权限，进而触发一个**页面故障（page fault）**异常。操作系统的页面故障处理程序会意识到这个异常来自于一个 COW 页面，并且会执行以下操作：\n分配一个新的物理内存页。 将原来共享的页面内容复制到新分配的页面。 更新进程的页表，将这个新的页面映射为可写。 重新执行引发页面故障的指令。 这样，父进程和子进程各自获得了独立的、可写的内存，而这一切都是透明的，对应用程序来说是无感知的。\n懒分配（Lazy Allocation）\n懒分配是一种内存管理优化技术，它基于这样一个事实：应用程序往往请求比实际需要更多的内存。在懒分配的实现中，当应用程序调用 sbrk（或其他类似的内存分配系统调用）时，操作系统不会立即分配实际的物理内存，而只是扩大了进程的虚拟地址空间，并将新地址标记为无效。\n当应用程序尝试访问这些新地址时，会因为没有对应的物理内存而触发页面故障。此时，操作系统会为该页面实际分配物理内存，并将其映射到进程的页表中。这种按需分配的策略，确保了物理内存只在应用程序真正需要时才被分配，从而节省了内存资源。\n从磁盘分页（Paging from Disk）\n当系统内存不足时，操作系统可能需要将不常使用的内存页暂时移出物理内存，写入磁盘。这个过程叫做交换（swapping）。当一个被交换到磁盘的内存页再次被访问时，CPU 会触发一个页面故障，因为页表中的对应条目已经被标记为无效。\n操作系统会检查页面故障发生的地址，并判断该地址是否属于一个被换出的页面。如果是这样，内核会从磁盘中读取该页面，加载到物理内存，并更新页表，使其重新有效。随后，操作系统会恢复应用程序的执行，重新执行触发页面故障的指令。\n自动扩展堆栈：如果进程需要更多的栈空间，而它当前的栈已经耗尽，操作系统可以通过页面故障来检测到这一情况，然后分配新的内存页来扩展栈。\n内存映射文件：文件的内容可以直接映射到进程的内存空间中。如果映射的部分文件被访问而未加载，操作系统会通过页面故障来加载相应的文件部分。\n","date":"2024-09-14T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0816isolation-system-call-entry/exit/bg_hu9738515208017608039.jpg","permalink":"https://echudet.github.io/p/mit6s0816isolation-system-call-entry/exit/","title":"MIT6S081：6Isolation \u0026 system call entry/exit"},{"content":"Lecture 5 Convention for RISC-V \u0026amp; ASM 课程杂记 C-\u0026gt;ASM\n处理器有一套指令集ISA C -\u0026gt; ASM (.S files)-\u0026gt; binary (obj/o files) RISC-V(Reduced ISA, simple Instr) open source Simple Instr Integrated Devices x86-64(ISA-\u0026gt;PC[Intel, AMP]) Complex ISA, 15k Instr backwards compatible \u0026amp; cmd demand high efficient ARM(RISC) Andriod IOS(Apple) gdb\nRISC-V\n寄存器是用来进行任何运算和数据读取的最快的方式，这就是为什么使用它们很重要，也是为什么我们更喜欢使用寄存器而不是内存。当我们调用函数时，你可以看到这里有a0 - a7寄存器。通常我们在谈到寄存器的时候，我们会用它们的ABI名字。不仅是因为这样描述更清晰和标准，同时也因为在写汇编代码的时候使用的也是ABI名字。第一列中的寄存器名字并不是超级重要，它唯一重要的场景是在RISC-V的Compressed Instruction中。基本上来说，RISC-V中通常的指令是64bit，但是在Compressed Instruction中指令是16bit。在Compressed Instruction中我们使用更少的寄存器，也就是x8 - x15寄存器。我猜你们可能会有疑问，为什么s1寄存器和其他的s寄存器是分开的，因为s1在Compressed Instruction是有效的，而s2-11却不是。除了Compressed Instruction，寄存器都是通过它们的ABI名字来引用。\na0到a7寄存器是用来作为函数的参数。如果一个函数有超过8个参数，我们就需要用内存了。从这里也可以看出，当可以使用寄存器的时候，我们不会使用内存，我们只在不得不使用内存的场景才使用它。\n对于Stack来说，是从高地址开始向低地址使用。所以栈总是向下增长。当我们想要创建一个新的Stack Frame的时候，总是对当前的Stack Pointer做减法。一个函数的Stack Frame包含了保存的寄存器，本地变量，并且，如果函数的参数多于8个，额外的参数会出现在Stack中。所以Stack Frame大小并不总是一样，即使在这个图里面看起来是一样大的。不同的函数有不同数量的本地变量，不同的寄存器，所以Stack Frame的大小是不一样的。但是有关Stack Frame有两件事情是确定的：\nReturn address总是会出现在Stack Frame的第一位 指向前一个Stack Frame的指针也会出现在栈中的固定位置 有关Stack Frame中有两个重要的寄存器，第一个是SP（Stack Pointer），它指向Stack的底部并代表了当前Stack Frame的位置。第二个是FP（Frame Pointer），它指向当前Stack Frame的顶部。因为Return address和指向前一个Stack Frame的的指针都在当前Stack Frame的固定位置，所以可以通过当前的FP寄存器寻址到这两个数据。\n","date":"2024-09-08T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0815convention-for-risc-v-asm/bg_hu15301645053803154401.jpg","permalink":"https://echudet.github.io/p/mit6s0815convention-for-risc-v-asm/","title":"MIT6S081：5Convention for RISC-V \u0026 ASM"},{"content":"Lecture 4 Page Tables(vm) 课程杂记 vm -\u0026gt; virtual memory ，map virtual to physical 隔离性\n正确设置的VM可以实现隔离性\ndefault下，不存在memory isolation\naddress spaces，地址空间 给包括内核所有程序专属空间，这些空间相互独立\n如何在物理内存上划分虚拟空间\n通过page table实现，kalloc储存余有空间的地址\n而page table 页表由处理器硬件或内存管理单元MMU实现，映射内存的VA-\u0026gt;PG表存储在MMU中，也有表存储在内存中用来找到MMU中的表（e.g. SATP寄存器）\nMMU是不定存储的，必要时读取表\n改写SATP寄存器是内核的权利，用户无法改写\npaging hw(risc-v)\nSATP-64bits,可存储2^64个地址，过大\n实际上，它仅为page创建地址，一个page为4kb,\nindex和offset，index指page序号（指向内存中的4kb），offset指page中具体的偏移\n实际上，高25bit并未使用（在RISC-V中)，中27位用于index，低12位用于offset(2^12 bits = 4 kb)，而物理内存可以比虚拟内存大，XV86为44位address + 12 offset，此处56为设计师设计的，并无具体含义\n虽然page table可存储2^27个条目，但它并非一次性存储在各个程序中，而是多级结构，先将27位划分为三个9bit，再指向不同的page directory（4096bytes），其中一个条目为8bytes，故其中含有512个PTE，VM的PT中最高级的9bit指向PD，得到一个物理page号，其指向中级PD，再得到一个PPN指向低级PD，再得到真正的物理地址\nPTE低十位一般用于标志位，\nRSW , Dirty, Accessed , Global, User, Execute, Write, Read, Valid\n9 \u0026mdash;-\u0026gt; 0\n当无法翻译的时候，会出现page fault\nsatp在此多级过程中，会提供最高级PD的地址，然后高位9bit会在PD中寻找相应的PPN\n这三个PD都存放于物理地址\n从PD得到的PPN（非目标物理地址）是44bit的，会在后面补上12个0，再指向下一个PD\n读一次地址要访问三次地址，代价较高，故有页表缓存（TLB），又称PTE缓存。TLB会存储VA-\u0026gt;PQ的映射\n当os切换page table时，会清空TLB缓存\n在RISC-V中，清空TLB指令为sfence_vma\n在RISC-V中，walk函数实现了与硬件MMU相同的功能\n三级页表一般是由硬件实现，而非操作系统\npage table 提供了一层抽象，这里的映射完全由操作系统控制，可实现多种功能，如page fault，os可以更新page table，并重新加载指令\nCPU每个核都有MMU和TLB\nxv6 vm code + layout kernelbase地址于0x800000时（kernel虚拟地址下的），产生的信息若地址低于它，则流向不同io设备，否则流向dram （当然可能也是unused，因为没那么多设备）\n总体物理地址的layout，是由主板决定的，并非仅是CPU决定\nGuardPage 不会映射，起保护Kernel Stack的作用，\nkernel stack 与 kernel data 映射于一处\n每一个用户进程都有一个kernel stack\n用户进程会占用kernel的虚拟内存的free memory，\n用户的虚拟内存和kernel的一样大，但用户的空间利用率低\nkernel中大部分虚拟内存和真实内存是一一对应的，因此在启动虚拟内存映射时不会对kernel 的运行造成影响\n每个进程都有用户栈和内核栈（p-\u0026gt;kstack）。当进程运行用户指令时，只有其用户栈被使用，其内核栈则是空的。然而当进程（通过系统调用或中断）进入内核时，内核代码就在进程的内核栈中执行；进程处于内核中时，其用户栈仍然保存着数据，只是暂时处于不活跃状态。进程的线程交替地使用着用户栈和内核栈。要注意内核栈是用户代码无法使用的，这样即使一个进程破坏了自己的用户栈，内核也能保持运行。\nxv6 使用结构体 struct proc 来维护一个进程的状态，其中最为重要的状态是进程的页表，内核栈，当前运行状态。我们接下来会用 p-\u0026gt;xxx 来指代 proc 结构中的元素。 当进程使用系统调用时，处理器转入内核栈中，提升硬件的特权级，然后运行系统调用对应的内核代码。当系统调用完成时，又从内核空间回到用户空间：降低硬件特权级，转入用户栈，恢复执行系统调用指令后面的那条用户指令。线程可以在内核中“阻塞”，等待 I/O, 在 I/O 结束后再恢复运行。 p-\u0026gt;state 指示了进程的状态：新建、准备运行、运行、等待 I/O 或退出状态中。 p-\u0026gt;pgdir 以 x86 硬件要求的格式保存了进程的页表。xv6 让分页硬件在进程运行时使用 p-\u0026gt;pgdir。进程的页表还记录了保存进程内存的物理页的地址。 当 PC 开机时，它会初始化自己然后从磁盘中载入 boot loader 到内存并运行。然后，boot loader 把 xv6 内核从磁盘中载入并从 entry（1040）开始运行。x86 的分页硬件在此时还没有开始工作；所以这时的虚拟地址是直接映射到物理地址上的。\nboot loader 把 xv6 内核装载到物理地址 0x100000 处。之所以没有装载到内核指令和内核数据应该出现的 0x80100000，是因为小型机器上很可能没有这么大的物理内存。而之所以在 0x100000 而不是 0x0 则是因为地址 0xa0000 到 0x100000 是属于 I/O 设备的。\n为了让内核的剩余部分能够运行，entry 的代码设置了页表，将 0x80000000（称为 KERNBASE（0207））开始的虚拟地址映射到物理地址 0x0 处。注意，页表经常会这样把两段不同的虚拟内存映射到相同的一段物理内存，我们将会看到更多类似的例子。\n“页表经常会这样把两段不同的虚拟内存映射到相同的一段物理内存”指的是以下两段虚拟内存：\n虚拟地址 0x0 到 0x400000：这段虚拟地址范围直接映射到物理地址 0x0 到 0x400000。在 entry 代码开始运行时，为了能够正常执行，这个映射是必要的，因为在系统启动时，虚拟地址和物理地址是直接对应的。 虚拟地址 KERNBASE 到 KERNBASE + 0x400000：这段虚拟地址范围对应的是高地址的内核空间，KERNBASE 通常为 0x80000000。为了让内核在其预期的高虚拟地址空间运行，entry 代码将这一段虚拟地址也映射到了物理地址 0x0 到 0x400000。 这两段虚拟地址虽然不同，但它们都映射到了同一段物理内存，即物理地址 0x0 到 0x400000。这样做的目的是为了在内核加载的早期阶段能够在物理内存较小的情况下让内核正常工作。随着系统的进一步初始化和内存管理机制的完善，低地址的映射会被移除，内核最终会运行在高地址的虚拟内存空间中。\n在xv6的启动过程中，分页硬件起到了至关重要的作用。简单来说，分页硬件负责将程序使用的虚拟地址翻译成实际的物理地址，这是实现虚拟内存管理的核心机制。分页硬件的作用：\n地址翻译：分页硬件的主要任务是将CPU在执行指令时产生的虚拟地址，按照页表的规定，转换为实际的物理地址。这使得每个进程可以拥有一个独立的虚拟地址空间，而不需要关心实际的物理内存布局。 内存保护：分页硬件可以通过页表中的权限位，控制对内存的访问权限。例如，它可以设置某些内存区域只读、不可执行，或者仅允许内核访问，从而提供内存保护机制，防止进程之间互相干扰。 支持进程隔离：分页硬件使得不同进程可以拥有各自的虚拟地址空间，这些空间彼此独立。即使多个进程使用相同的虚拟地址，它们在物理内存中映射到的实际地址也不同，从而实现了进程间的隔离。 分页硬件的启动步骤如下：\n设置页表：在启动分页硬件之前，内核首先要设置好页表。页表是一种数据结构，它记录了虚拟地址到物理地址的映射关系。xv6在启动过程中会建立一个初始的页表，这个页表会映射内核代码和数据到合适的物理地址。 加载页表地址到控制寄存器 %cr3：内核通过将页表的物理地址加载到控制寄存器 %cr3 中，告诉CPU当前要使用哪一个页表来进行地址翻译。这个步骤是让分页硬件知道应该使用哪一张表来进行虚拟地址到物理地址的转换。 启用分页：在设置好页表并将其地址加载到 %cr3 后，内核会设置控制寄存器 %cr0 中的分页启用位（CR0_PG），这一步骤正式启动了分页硬件。此时，CPU开始使用页表进行地址翻译，系统的内存管理机制进入正常工作状态。 切换到高地址执行：启用分页后，内核可以安全地从低地址跳转到高地址继续执行。由于页表已经将高地址映射到正确的物理内存位置，内核可以在它的预期虚拟地址空间中正常工作。 在整个启动过程中，除了分页硬件外，还有几个关键的硬件组件也在发挥重要作用。这些硬件协同工作，确保操作系统能够成功启动并运行：\nCPU（中央处理器）\n指令执行：CPU是整个启动过程的核心，它负责执行从boot loader到内核的所有指令。启动过程开始时，CPU从固化在硬件中的地址（通常是BIOS或EFI）开始执行，然后逐步加载和运行boot loader，再到加载和执行内核。\n模式切换：CPU在启动过程中会从实模式切换到保护模式，最后进入分页模式。在实模式下，CPU只处理简单的地址计算，但在保护模式和分页模式下，CPU可以利用复杂的内存管理和保护机制。\nBIOS/UEFI\n硬件初始化：BIOS（基本输入输出系统）或UEFI（统一可扩展固件接口）是计算机开机时首先运行的固件，它负责初始化计算机的硬件组件，如CPU、内存、硬盘和I/O设备。\n引导加载器加载：BIOS/UEFI在完成硬件初始化后，会将控制权交给boot loader，这通常是通过读取特定存储设备上的引导扇区来实现的。\n内存（RAM）\n代码和数据存储：内存用于存储启动过程中加载的所有代码和数据，包括boot loader、内核代码、页表以及其他重要的数据结构。在内核启动并运行时，内存还会被用来存储进程的代码、数据和栈等。\n栈操作：CPU在执行指令时使用内存中的栈来保存函数调用的信息、局部变量和返回地址。栈的正确初始化和管理对于顺利执行程序至关重要。\n硬盘控制器\n加载内核：硬盘控制器负责从硬盘中读取boot loader和内核。boot loader在启动时会通过硬盘控制器将内核从存储设备加载到内存的指定位置，以便CPU能够执行内核代码。 I/O设备\n基本输入输出：在启动过程中，I/O设备（如键盘、显示器、串口等）通常会被初始化，以便在启动过程的早期阶段，系统可以输出调试信息或接受输入。 中断控制器\n中断管理：中断控制器负责管理和处理硬件中断。在启动过程中，操作系统会设置中断向量表，以确保系统能够正确处理来自外部设备的中断请求。虽然在启动过程的早期阶段，系统可能不会处理复杂的中断，但中断控制器的初始化和配置是操作系统启动的一个重要步骤。 创建第一个进程的过程可以分为几个重要步骤，以下是通俗但详细的讲解：\n初始化阶段：首先，在系统启动时，main 函数会初始化一些必要的设备和子系统。然后，它调用了 userinit 函数来创建系统中的第一个进程。 分配进程槽位：userinit 函数调用了 allocproc，这是一个用于在内核中分配新进程的函数。allocproc 的任务是找到一个未被使用的进程槽位（结构体 struct proc），然后将其状态从未使用（UNUSED）标记为胚胎状态（EMBRYO），表示它正在被使用并分配给一个新进程。 设置内核栈：接下来，allocproc 为新进程分配一个内核栈，并设置好内核栈和一系列内核寄存器，使得当进程第一次运行时，它会“返回”到用户空间。这个过程通过 forkret 函数实现，该函数的作用是让内核线程从某个点开始执行，最后跳转到 trapret，准备好用户态的寄存器并跳转到用户代码执行。 初始化页表：为了让进程能够执行自己的代码，userinit 还需要为进程创建一个页表，这个页表最初只映射内核的内存区域。然后，userinit 调用了 inituvm，为进程分配了一页物理内存，并将初始化代码（initcode.S）拷贝到这段内存中。这个代码是用来启动进程的。 设置用户模式寄存器：在这个阶段，userinit 设置了进程的陷阱帧（trapframe），这是一个用于保存用户模式寄存器的结构。这里的关键是确保进程以用户模式（而不是内核模式）启动，并且能够处理硬件中断。 进程变为可运行状态：一旦所有这些设置完成，userinit 将进程的状态设置为 RUNNABLE，这意味着该进程现在可以被调度器选中运行了。 调度和运行：在 main 函数调用了 userinit 之后，系统调用 scheduler 函数来开始运行进程。调度器会找到一个处于 RUNNABLE 状态的进程，将其状态设为 RUNNING，然后通过 swtch 函数切换上下文，开始运行这个进程的内核线程。 用户代码的执行：最终，这个第一个进程的执行从 initcode.S 的第一个指令开始，该指令位于虚拟地址0。因为之前已经为该进程建立了页表，这些虚拟地址会被翻译为实际的物理内存地址，从而顺利执行进程的用户代码。 通过这些步骤，系统成功创建并运行了第一个用户进程，这也是系统启动后的第一个进程。\n新进程分配的内核栈最初包含一些关键内容，这些内容是为了使新进程能够正确地启动和运行。具体来说，内核栈的初始内容包括：\n上下文信息（context）：这是用于保存进程的寄存器状态的结构，包括指令指针（eip）和栈指针（esp）等。这些信息是为了保证进程能够从正确的位置继续执行。 返回地址：allocproc 会将返回地址设置为 forkret，这意味着当进程第一次运行时，它将从 forkret 开始执行。forkret 函数会进行一些初始化工作，然后返回到 trapret 函数。 trapret 地址：这是栈上的另一个重要内容，它在 forkret 之后被调用，用来从用户态进入内核态时恢复用户寄存器，并最终返回到用户代码的执行。 这些内容的作用是确保当新进程第一次被调度器选中运行时，它能够正确地进行从内核态到用户态的过渡，并开始执行用户代码。这种设置使得创建新进程和通过 fork 复制进程的过程可以共用同样的机制，从而简化了进程管理的逻辑。\n在一个进程中，除了内核栈以外，还包括以下重要内容：\n用户栈（User Stack）： 作用：用户栈用于存储用户态下的函数调用、局部变量和函数参数。每当进程进入用户态并执行用户代码时，它会使用用户栈。用户栈在每次系统调用或中断发生时会被保存，以便在返回到用户态时恢复。 进程控制块（Process Control Block, PCB）： 作用：这是内核用于管理和调度进程的一个重要数据结构。PCB 包含了进程的各种状态信息，如进程ID（PID）、进程状态（如 RUNNING, SLEEPING, ZOMBIE 等）、程序计数器（指示下一条指令的位置）、寄存器内容、打开的文件描述符列表、优先级、调度信息、以及内存管理信息（如页表指针）。 关键作用：PCB 是内核追踪进程的核心结构，它保存了进程执行所需的所有上下文信息。当进程被切换时，PCB 是用来保存和恢复上下文的关键。 虚拟地址空间（Virtual Address Space）： 作用：每个进程都有自己独立的虚拟地址空间。这个空间通常包括代码段（Text Segment）、数据段（Data Segment）、堆（Heap）和栈（Stack）。代码段包含可执行代码；数据段存放已初始化和未初始化的全局变量；堆用于动态内存分配；栈用于函数调用和局部变量。 关键作用：虚拟地址空间为每个进程提供了一个独立的运行环境，确保进程之间的内存隔离和安全。 页表（Page Table）： 作用：页表是内存管理单元（MMU）使用的数据结构，用于将进程的虚拟地址映射到物理地址。每个进程有自己的页表，内核利用它来管理内存和地址转换。 关键作用：页表实现了虚拟内存，使得进程可以假装拥有连续的内存地址，同时内核可以灵活地分配实际的物理内存。 文件描述符表（File Descriptor Table）： 作用：这是进程打开的所有文件的列表。每个打开的文件（或其他 I/O 资源，如管道、网络连接等）都有一个与之关联的文件描述符。 关键作用：文件描述符表使得进程可以进行文件 I/O 操作，如读写文件、网络通信等。每个文件描述符对应一个内核对象，内核通过这个对象管理文件操作。 信号处理表（Signal Handling Table）： 作用：用于定义进程如何处理各种信号。信号是操作系统向进程传递事件通知的一种方式（如中断、非法操作等）。 关键作用：信号处理表指定了进程遇到特定信号时要采取的操作（例如，忽略信号、执行默认操作或调用用户定义的处理函数）。 进程工作目录和根目录： 作用：进程有其当前工作目录（Current Working Directory, CWD）和根目录（Root Directory）。这些目录用于文件系统操作，指定文件路径是相对还是绝对。 关键作用：管理文件系统路径的上下文，使得进程能够访问和操作文件系统中的文件和目录。 共享内存和映射区域： 作用：这是进程可能使用的共享内存区域或通过内存映射文件的区域（如动态库映射）。 关键作用：提供进程间通信（IPC）机制，并允许进程高效访问大文件或共享数据段。 这些组件共同组成了一个进程的完整运行环境。它们协作使得操作系统可以多任务运行，安全地管理多个进程，以及在不同进程之间进行上下文切换。\n在创建第一个进程的过程中，初始化页表是至关重要的，因为页表用于将虚拟地址映射到物理地址。每个进程都有自己的虚拟地址空间，操作系统使用页表来管理这些地址与实际物理内存的对应关系。\n初始化页表的主要目的是为新进程提供一个独立的虚拟地址空间，这样进程可以运行在它自己的地址空间中，而不会干扰其他进程。同时，页表的设置允许操作系统通过分页机制有效管理内存，确保安全性和内存隔离。\n页表的重要性：在系统中，所有进程都运行在各自的虚拟地址空间中，而操作系统通过页表将这些虚拟地址转换为实际的物理地址。初始化页表为进程的运行提供了正确的地址映射，从而确保了进程能够访问内核和用户空间的必要资源。 当我们说“这个页表最初只映射内核的内存区域”时，指的是在初始阶段，页表只设置了内核空间的映射，而没有映射用户空间的地址。内核的地址空间包括内核代码、数据、堆、栈（包括内核栈）等内核所需的内存区域。\n内核栈：内核栈是内核在执行时使用的栈，是每个进程在内核态运行时的重要部分。在进程执行系统调用或发生中断时，CPU 会从用户模式切换到内核模式，使用内核栈来处理内核态代码。这部分内存是内核地址空间的一部分，因此会被映射。 所以，初始页表仅映射了内核的内存区域，意味着进程在最初的设置中只能访问内核空间的内存。这是为了保证进程一开始可以正常地在内核中执行。\n这里提到的“代码是用来启动进程的”指的是 initcode.S 这段汇编代码，它是用于初始化并启动第一个用户进程的。\ninitcode.S 的来源：initcode.S 是一个小型的汇编程序，它作为内核的一部分编译并链接到内核映像中。在构建内核时，链接器会将这段代码作为二进制数据嵌入到内核映像中。内核通过两个符号 _binary_initcode_start 和 _binary_initcode_size 标识这段代码的起始位置和大小。 作用：这段代码的主要作用是为第一个进程（initproc）提供一段简单的用户态代码。当第一个进程开始执行时，它会运行这段代码来初始化用户态的环境。其目的是确保进程能够顺利启动并进入用户态执行。这是内核为启动第一个进程提供的基础代码。\n如何加载：在创建第一个进程时，内核会通过 inituvm 函数为该进程分配一页物理内存，并将 initcode.S 的二进制代码拷贝到这段内存中。这段内存被映射到进程的虚拟地址空间的地址0。当进程第一次运行时，处理器会从地址0开始执行 initcode.S 的指令。 总结一下，初始化页表是为了设置进程的虚拟地址空间，而最初的页表只映射内核区域（包括内核栈），以便在内核态能够正常工作。initcode.S 是一段汇编代码，它从内核编译时嵌入内核中，被加载到第一个进程的内存中，用于启动并运行该进程。\n每个进程的内核栈不是相通的，而是独立的。但是，内核空间在所有进程之间是共享的，但各进程对其访问是受控的。\n内核栈：\n独立性：每个进程都有自己独立的内核栈。当一个进程从用户态切换到内核态（例如执行系统调用或处理中断）时，它使用自己的内核栈来保存调用信息、局部变量以及处理中断或系统调用所需的其他数据。由于每个进程的内核栈是独立的，进程之间不会互相影响。这种独立性确保了进程的内核态执行环境的安全性和稳定性。 切换上下文时的保存与恢复：当操作系统在多个进程之间切换时（即进行上下文切换），它会保存当前进程的内核栈状态，并恢复下一个要运行的进程的内核栈。这种机制保证了每个进程可以独立执行，而不会干扰其他进程的内核栈。 内核空间：\n共享性：内核空间是所有进程共享的地址空间的一部分。它包含了内核代码、全局数据结构、设备驱动程序、内核栈等。这意味着所有进程都可以访问相同的内核代码和全局内核数据。内核空间通常位于每个进程的虚拟地址空间的高地址部分。 安全性：虽然内核空间在所有进程之间是共享的，但普通用户进程无法直接访问内核空间。这是通过硬件机制和内核态/用户态的权限控制来实现的。只有在进程运行于内核态时（例如在系统调用或中断处理期间），进程才能访问内核空间的内容。 统一的映射：所有进程的页表都会映射到相同的内核空间区域。这意味着当系统切换进程时，尽管进程的用户空间地址不同，但内核空间地址是一致的。这使得内核能够高效地管理和调度多个进程，而不必为每个进程维护独立的内核空间。 总结：\n内核栈：每个进程有自己的内核栈，互不相通，保证了内核态的独立性和安全性。 内核空间：内核空间在所有进程之间是共享的，但受权限控制，只有在内核态下进程才能访问内核空间。 这种设计确保了系统的安全性和稳定性，同时也保证了操作系统的效率。\np-\u0026gt;context-\u0026gt;eip 是进程控制块（struct proc）中的一个字段，用于保存进程在内核态的执行状态，特别是保存进程执行时的程序计数器（即指令指针）。p-\u0026gt;context-\u0026gt;eip` 指的是什么？\neip：eip 是 x86 架构中的指令指针寄存器（也称为程序计数器）。它保存了 CPU 下一条将要执行的指令的地址。在上下文切换时，操作系统会保存当前进程的 eip 值，以便在恢复这个进程时，它可以从之前中断的地方继续执行。 p-\u0026gt;context-\u0026gt;eip：p-\u0026gt;context 是一个指向进程上下文结构的指针，这个结构保存了内核态下的 CPU 寄存器状态，其中包括 eip。当进程被切换出去时，eip 会被保存到 p-\u0026gt;context-\u0026gt;eip，当进程被切换回来的时候，eip 会从 p-\u0026gt;context-\u0026gt;eip 中恢复，这样进程就能继续从上次停止的地方执行。 p-\u0026gt;context 是一个用于保存进程在内核态执行时 CPU 寄存器状态的结构。它通常包含以下内容：\neip（指令指针）：保存下一条将要执行的指令的地址。 esp（栈指针）：保存当前的栈指针，用于指示当前的栈顶位置。 ebx、ecx、edx、esi、edi、ebp：这些都是通用寄存器，用于保存进程执行时的寄存器状态。 eflags：保存 CPU 的状态标志寄存器，用于记录当前进程执行时的状态（如中断是否启用、运算结果的标志等）。 当操作系统进行上下文切换时，会将当前进程的所有 CPU 寄存器状态保存到 p-\u0026gt;context 中。然后操作系统会加载下一个进程的 p-\u0026gt;context，恢复所有寄存器状态，使 CPU 继续执行该进程。这种机制确保了多任务操作系统可以高效地在多个进程之间切换，而不丢失每个进程的执行状态。\n指令指针 eip 的作用尤其重要，因为它决定了进程接下来会执行哪条指令。没有正确保存和恢复 eip，进程可能会从错误的地址继续执行，导致系统不稳定甚至崩溃。\n总的来说，p-\u0026gt;context 结构中的内容是进程上下文切换的核心，它记录了进程在内核态下所有关键的寄存器状态，以确保进程可以在被中断后正确恢复并继续执行。\nLab时遇到的问题 “通过一个没有映射的页保护用户栈”是什么意思，怎么做到的\n这句话的意思是：xv6 通过在用户栈的末尾放置一个没有映射的页（即一个虚拟地址空间内的页表条目不指向任何物理内存），来保护用户栈免受溢出错误的影响。\n具体来说，当程序运行时，用户栈是从高地址向低地址方向增长的。如果程序意外地超出了栈的范围（比如递归太深或者分配了过多的局部变量），它可能会尝试访问栈的下方区域。如果这块区域没有得到适当保护，程序就可能覆盖掉其他内存区域的数据，导致不可预测的行为。\n为了防止这种情况发生，xv6 在用户栈的下方添加了一个“没有映射”的页。这意味着该页的页表条目（PTE）没有指向任何实际的物理内存。如果程序尝试访问这个没有映射的页（例如由于栈溢出），CPU 的分页硬件就会检测到这个访问，并产生一个页面错误（page fault），从而终止程序或让操作系统采取相应的处理措施。这就能有效地防止栈溢出带来的潜在危害。\n建立一个地址空间的过程可以分为以下几个步骤：\n调用 kvmalloc()\n位置：main() 调用 kvmalloc，位于代码的第1757行。\n作用：kvmalloc() 是用于创建并切换到一个拥有内核所需映射的页表。它确保在操作系统启动时，内核可以正常运行。\n关键操作：该函数调用 setupkvm() 来完成大部分工作。\n调用 setupkvm()\n位置：第1737行。\n作用：setupkvm() 是负责实际创建内核所需的页表映射的函数。\n具体步骤：\n分配页目录页：setupkvm() 首先会分配一页内存来存储页目录，这是分页的最高层结构，用于指向页表。 调用 mappages()：使用 mappages() 函数，创建映射表，这些映射表可以在 kmap 数组中找到。kmap 数组中包含了各种需要被映射的内存区域。 调用 mappages()\n位置：第1679行。\n作用：mappages() 用来在页表中建立虚拟内存到物理内存的映射。\n具体步骤：\n逐页映射：mappages() 是逐页处理的，即一页一页地创建映射。对于每个虚拟地址，都需要建立相应的物理内存映射。 调用 walkpgdir()：mappages() 通过调用 walkpgdir() 来找到该虚拟地址对应的页表项（PTE）。 设置 PTE：在页表项中记录物理页号、访问权限（如 PTE_W、PTE_U）和 PTE_P 标志位来表示该页表项是否有效。 调用 walkpgdir()\n位置：第1654行。\n作用：walkpgdir() 模拟 x86 硬件分页机制，找到虚拟地址对应的页表项（PTE）。\n具体步骤：\n定位页目录项：walkpgdir() 使用虚拟地址的前10位来查找页目录中对应的条目（位于第1659行）。 检查是否需要分配页表页：如果页目录条目不存在且 alloc 参数为真，则分配一个新的页表页，并将其物理地址存入页目录中。 定位页表项：然后使用虚拟地址的中间10位，找到该地址在页表中的对应条目（PTE）。 返回PTE地址：最后，walkpgdir() 返回该PTE的地址，供 mappages() 初始化。 初始化页表项 (PTE)\n位置：在 mappages() 的第1691行。\n作用：mappages() 在 walkpgdir() 返回的 PTE 地址处，初始化 PTE 条目。\n具体内容：\n保存物理页号：PTE 中记录相应的物理内存页号。 设置权限标志：根据需要设置 PTE 的权限标志（如 PTE_W 表示可写，PTE_U 表示用户访问权限）。 标记页表项有效：通过设置 PTE_P 标志位，表明该页表项有效。 完成内核的基础映射\nsetupkvm()完成了kmap数组中内核地址空间的映射，包括： 内核指令和数据段的映射。 PHYSTOP 以下的物理内存的映射。 I/O 设备地址空间的映射。 未建立用户空间映射\n注意：在这个阶段，setupkvm() 仅仅完成了内核所需的映射，并未为用户空间内存建立映射。用户空间的映射将在稍后的执行过程中进行。 切换到新页表\n当 kvmalloc() 完成时，它会切换到新创建的页表，这确保了内核可以使用新建立的映射来执行代码。 如何管理空闲物理内存页的分配与释放。\n物理内存分配器中的数据结构\n空闲链表：分配器使用一个由空闲的物理内存页构成的链表来管理内存。链表中的每个元素是一个struct run结构体，用于表示一个空闲页。\n自包含的结构：为了节省空间，分配器将struct run结构体存放在每个空闲页的开头部分，因为这些页本身是空闲的，没有其他数据。\n内存的安全保护\nSpin L\tock：分配器使用自旋锁（spin lock）来保护空闲链表的操作，防止并发操作引起的数据不一致问题。这个锁和链表一起封装在一个结构体中，以确保锁可以保护到相关的所有数据。 分配器的初始化\nkinit1和kinit2函数：分配器初始化分两步完成，kinit1用于在系统启动初期分配前4MB的内存，因为此时不支持锁和高于4MB的内存操作。kinit2允许使用锁，并解锁了更多的内存供系统使用。\nPHYSTOP假设：在x86架构上，操作系统假设系统有240MB的物理内存（PHYSTOP），从内核结束地址到PHYSTOP之间的内存作为空闲内存池。freerange函数通过调用kfree将这部分内存页逐一加入空闲链表。\n地址对齐和释放\n对齐处理：freerange函数使用PGROUNDUP确保内存页是以4096字节对齐的地址进行释放的，因为PTE（页表项）只能指向对齐的物理地址。\nkfree函数：kfree负责将物理页重新加入空闲链表。它首先将释放的内存页内容设置为全1，以确保访问已释放内存时立刻崩溃，而不是继续使用无效数据。\n地址映射\n虚拟地址和物理地址：分配器通过虚拟地址找到对应的物理页。这是因为在系统的内存管理中，物理内存通常被映射到高地址的虚拟内存空间中。 类型转换\n双重用途的地址：分配器在操作地址时，既将地址当作整数进行运算，也将其当作指针来操作内存中的数据。这种用法导致了大量的类型转换，同时也说明了分配器在释放或分配内存时可能会隐式地改变内存的类型。 用户地址空间中的栈与堆\n堆与栈的布局：堆位于栈的上方，栈本身占用一页内存，包含进程启动时的初始数据。栈的顶部包含命令行参数及其指针数组，并模仿函数main(argc, argv)的调用环境。\n保护页：为了防止栈的溢出，栈的下方有一个未映射的保护页。如果栈增长超过这一页，会触发异常，从而保护系统不受非法访问的影响。\n操作系统中 exec 系统调用的工作原理，exec 用于创建和初始化用户进程的地址空间。\nELF 格式与 exec 的基本流程\nELF 文件：exec 通过读取并解析 ELF (Executable and Linkable Format) 文件来初始化用户地址空间。ELF 是一种通用的二进制文件格式，exec 首先检查文件的 ELF 头部以验证文件格式是否正确。\n魔法数字检查：ELF 文件以 4 个“魔法数字”（0x7F, 'E', 'L', 'F'）开头。exec 首先通过检查这四个字节来确定文件是否是有效的 ELF 文件。如果魔法数字匹配，exec 继续处理文件，否则放弃执行。\n内存分配与段加载\n页表设置：exec 通过 setupkvm 函数创建一个新的页表，其中不包含用户部分的映射。这为用户地址空间的初始化做好了准备。\n内存分配与加载：exec 使用 allocuvm 为 ELF 文件的每个段分配内存。接着，loaduvm 将段的内容从文件中加载到内存中。loaduvm 通过 walkpgdir 找到内存中相应的物理地址，并通过 readi 从文件读取数据。\n处理 Program Header\n段头解释：ELF 文件包含多个段头（Program Header），每个段头描述了一个需要载入内存的程序段。exec 依次处理每个段头，分配相应的内存并加载数据。\nfilesz 与 memsz：段头中的 filesz 是从文件中读取的字节数，memsz 是分配的内存大小。如果 filesz 小于 memsz，表示从文件读取的数据不足以填满分配的内存，未填充的部分通常用于存储未初始化的全局变量，并被填充为 0。\n用户栈的分配与初始化\n栈内存分配：exec 为用户栈分配一页内存，并在栈顶放置传入的参数字符串。参数字符串的指针被保存在 ustack 中，最后在参数列表 (argv) 的末尾放置一个空指针，以便程序知道参数列表的结束。\n伪造的返回地址：在栈上，ustack 的前三个位置依次存放伪造的返回地址、参数个数 (argc)、以及参数列表的指针 (argv)。这使得当程序从 main 函数开始执行时，能够正确访问这些参数。\n栈保护页\n防止栈溢出：在分配栈页下方，exec 会设置一个无法访问的保护页。这个保护页用于防止栈意外增长超出分配的页数，从而引发内存访问错误。\n处理大参数：如果传入的参数过大以至于超出栈的页，exec 通过 copyout 函数检测并返回错误，以避免栈溢出。\n错误处理与内存释放\n错误处理：如果 exec 在创建新的内存映像过程中发现错误（例如无效的程序段），它会跳转到 bad 标签处，释放已经分配的内存，并返回 -1，表示调用失败。\n内存映像切换：只有在确认新内存映像已经正确构建后，exec 才会释放旧的内存映像。这是为了确保如果 exec 失败时，能够安全地返回错误代码 -1。成功完成后，exec 返回 0，表示执行成功。\n总结\nexec 是一个关键的系统调用，用于将用户进程的地址空间初始化为一个新的执行映像。它通过加载 ELF 文件、分配内存、设置栈空间等操作，确保用户进程可以正确启动并运行。 栈只分配一页内存，这在某些情况下可能会显得太小，特别是当程序需要处理大量的参数或者深度递归时。这种设计是为了简化内存管理和避免过度浪费，但也带来了某些局限性。如果需要处理超过一页内存的大参数，系统有以下几种方法来应对：\n增加栈的大小\n在某些系统中，可以动态增加栈的大小。具体方法包括：\n手动扩展：程序可以通过系统调用（例如 mmap）手动分配更多的栈空间。\n自动扩展：某些操作系统会在检测到栈溢出时自动扩展栈空间。这个过程通常是由内核负责的，通过捕获页面错误并分配新页面来实现栈的自动扩展。\n使用堆内存\n堆内存分配：程序可以使用堆（heap）来处理大数据，而不是依赖栈。通过调用 malloc 或者 calloc 这样的标准库函数，程序可以在堆上分配足够大的内存块来存储参数或其他数据。\n传递指针：对于大参数，可以将参数放在堆中，然后在栈上只传递一个指向堆中数据的指针。这样可以有效减轻栈的压力，同时仍然能够处理大数据。\n参数从文件读取\n文件输入：如果参数数据特别大，可以将其放在文件中，程序启动后从文件中读取。这种方式避免了将大量数据直接传递给程序，而是通过文件 I/O 进行处理，灵活性较高。 分块处理\n分块加载：对于过大的参数，可以考虑将其分块加载到内存中。每次只处理其中的一部分数据，处理完后再读取下一部分。这样可以降低对栈的要求，同时保持程序对大数据的处理能力。 环境变量\n环境变量传递：有时可以通过环境变量传递一些参数，环境变量不受栈大小的限制，且可以在程序运行时随时读取。 系统配置\n修改系统配置：某些操作系统允许通过修改配置文件或编译参数来增加默认的栈大小。这种方法可以在需要处理大量参数的应用中增大栈空间，避免因栈空间不足导致的错误。 “超级页”的特点\n“超级页”（Superpages）指的是在内存管理中使用较大的页面大小，而不是标准的4KB页。超级页通常为4MB或更大，具体特点包括：\n减少页表开销：使用超级页可以显著减少页表的大小和复杂度，因为每个超级页覆盖的地址空间比标准页大得多。这意味着需要维护的页表项（PTE）更少，从而减轻了页表的管理负担。 提高TLB（Translation Lookaside Buffer）效率：TLB 是一个缓存，用于加速虚拟地址到物理地址的转换。由于超级页减少了页表项的数量，TLB 更有可能缓存所有需要的页表项，从而减少TLB缺失的几率，提高内存访问效率。 适合大内存应用：在拥有大量内存的机器上，使用超级页非常有效，因为它们可以减少页表的复杂性，提高内存管理的整体性能。 浪费内存的风险：当应用程序的内存需求较小时，超级页可能会导致内存浪费。例如，一个只需要8KB内存的程序，如果被分配了4MB的超级页，将会浪费大量未使用的内存。 ","date":"2024-09-07T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0814page-tablesvm/124146068_hu6653000053672877102.jpg","permalink":"https://echudet.github.io/p/mit6s0814page-tablesvm/","title":"MIT6S081：4Page Tables(vm)"},{"content":"Lecture 3 OS Organization \u0026amp; System Calls OS 隔离性、防御性 为了不变成一个恶意程序，Shell在发现自己运行了一段时间之后，需要让别的程序也有机会能运行。这种机制有时候称为协同调度（Cooperative Scheduling）。 这个特性是非常有用的，不论应用程序在执行什么操作，multiplexing都会迫使应用程序时不时的释放CPU，这样其他的应用程序才能运行。 使用操作系统的一个原因，甚至可以说是主要原因就是为了实现multiplexing和内存隔离。如果你不使用操作系统，并且应用程序直接与硬件交互，就很难实现这两点。所以，将操作系统设计成一个库，并不是一种常见的设计。你或许可以在一些实时操作系统中看到这样的设计，因为在这些实时操作系统中，应用程序之间彼此相互信任。但是在大部分的其他操作系统中，都会强制实现硬件资源的隔离。 需要通过硬件来实现这的强隔离性。这里的硬件支持包括了两部分，第一部分是user/kernel mode，kernel mode在RISC-V中被称为Supervisor mode但是其实是同一个东西；第二部分是page table或者虚拟内存（Virtual Memory）。 处理器会有两种操作模式，第一种是user mode，第二种是kernel mode。当运行在kernel mode时，CPU可以运行特定权限的指令（privileged instructions）；当运行在user mode时，CPU只能运行普通权限的指令（unprivileged instructions）。 实际上RISC-V还有第三种模式称为machine mode。在大多数场景下，我们会忽略这种模式，所以我也不太会介绍这种模式。 在处理器的一个bit，当它为1的时候是user mode，当它为0时是kernel mode。当处理器在解析指令时，如果指令是特殊权限指令，并且该bit被设置为1，处理器会拒绝执行这条指令，就像在运算时不能除以0一样。 让整个操作系统代码都运行在kernel mode。大多数的Unix操作系统实现都运行在kernel mode。比如，XV6中，所有的操作系统服务都在kernel mode中，这种形式被称为Monolithic Kernel Design（宏内核 ）。 另一种设计主要关注点是减少内核中的代码，它被称为Micro Kernel Design（微内核 ）。在这种模式下，希望在kernel mode中运行尽可能少的代码。所以这种设计下还是有内核，但是内核只有非常少的几个模块，例如，内核通常会有一些IPC的实现或者是Message passing；非常少的虚拟内存的支持，可能只支持了page table；以及分时复用CPU的一些支持。微内核的目的在于将大部分的操作系统运行在内核之外。因为在内核中的代码的数量较小，更少的代码意味着更少的Bug。 在实际中，两种内核设计都会出现，出于历史原因大部分的桌面操作系统是宏内核，如果你运行需要大量内核计算的应用程序，例如在数据中心服务器上的操作系统，通常也是使用的宏内核，主要的原因是Linux提供了很好的性能。但是很多嵌入式系统，例如Minix，Cell，这些都是微内核设计。这两种设计都很流行，如果你从头开始写一个操作系统，你可能会从一个微内核设计开始。 System Calls 有一种方式能够让应用程序可以将控制权转移给内核（Entering Kernel）。在RISC-V中，有一个专门的指令用来实现这个功能，叫做ECALL。ECALL接收一个数字参数，当一个用户程序想要将程序执行的控制权转移到内核，它只需要执行ECALL指令，并传入一个数字。这里的数字参数代表了应用程序想要调用的System Call。\n不论是Shell还是其他的应用程序，当它在用户空间执行fork时，它并不是直接调用操作系统中对应的函数，而是调用ECALL指令，并将fork对应的数字作为参数传给ECALL。之后再通过ECALL跳转到内核。\n左边是用户空间，右边是内核空间。在内核侧，有一个位于syscall.c的函数syscall，每一个从应用程序发起的系统调用都会调用到这个syscall函数，syscall函数会检查ECALL的参数，通过这个参数内核可以知道需要调用的是fork\n内核有时候也被称为可被信任的计算空间（Trusted Computing Base），在一些安全的术语中也被称为TCB。\n在这段代码中，p-\u0026gt;trapframe 和 p-\u0026gt;trapframe-\u0026gt;a0 是与系统调用处理相关的两个重要部分。为了理解它们的意义，我们需要明确 trapframe 是什么，以及 a0 在其中的作用。\np-\u0026gt;trapframe 的意义\ntrapframe 结构：trapframe 是一个结构体，它保存了当进程从用户态切换到内核态时，CPU 寄存器的状态。这通常发生在进程进行系统调用、发生中断或异常时。操作系统会在内核栈中创建一个 trapframe，用于保存当前的 CPU 状态（包括通用寄存器、指令指针、栈指针等）。 p-\u0026gt;trapframe：这里的 p 是指向进程控制块（struct proc）的指针，而 p-\u0026gt;trapframe 则是该进程的 trapframe。当进程进入内核态时，操作系统会使用 p-\u0026gt;trapframe 来保存进程的 CPU 寄存器状态。 p-\u0026gt;trapframe-\u0026gt;a0 的意义\na0：在 trapframe 结构体中，a0 是保存系统调用返回值的寄存器。通常在 xv6 操作系统中，a0 是一个寄存器，用于保存从系统调用返回时的结果。每个系统调用执行完后，返回的结果会存放在 trapframe-\u0026gt;a0 中，然后在从内核返回用户态时，这个值会被传递给用户进程。 p-\u0026gt;trapframe-\u0026gt;a0 = syscall[num]()：这行代码表示执行一个系统调用，并将系统调用的返回值保存到 p-\u0026gt;trapframe-\u0026gt;a0 中。syscall[num]() 是一个函数指针数组，num 是系统调用号，根据这个号找到对应的系统调用函数并执行。执行后的返回值被存入 a0 中，稍后当进程返回用户态时，这个值将被返回给进程的用户代码。 总结\np-\u0026gt;trapframe：保存了当前进程的 CPU 寄存器状态，当进程从用户态进入内核态时使用。 p-\u0026gt;trapframe-\u0026gt;a0：用于保存系统调用的返回值，这个值会在系统调用执行完毕后传递回用户态的进程。 这种机制保证了用户进程在进行系统调用时能够正确地与内核交互，并获取系统调用的结果。\np-\u0026gt;context 和 p-\u0026gt;trapframe 是两种不同的数据结构，它们分别用于保存不同阶段的进程状态，具有不同的用途和作用。不过，它们也有相似之处，都是为了实现进程的正确执行和状态保存。\np-\u0026gt;context 与 p-\u0026gt;trapframe 的不同之处\n用途和作用不同: p-\u0026gt;context： 作用：p-\u0026gt;context 主要用于保存进程在 内核态 的上下文（即寄存器状态），特别是当进程在内核态运行且发生上下文切换时（例如调度器将进程挂起以运行另一个进程）。它保存的是内核态寄存器的状态，包括程序计数器（eip）、栈指针（esp）等。这个上下文会在进程被重新调度运行时恢复，以确保进程从上次被挂起的地方继续执行。 使用场景：p-\u0026gt;context 的保存与恢复主要发生在进程的上下文切换过程中，即当操作系统在多个进程之间切换时。p-\u0026gt;context 保存的是内核态的运行状态，而不是用户态的状态。 p-\u0026gt;trapframe： 作用：p-\u0026gt;trapframe 则主要用于保存进程从 用户态 切换到 内核态 时的 CPU 寄存器状态。它保存的寄存器状态包括通用寄存器、程序计数器、栈指针、段寄存器等，基本上就是用户态的所有寄存器状态。这是为了当系统调用、中断或异常发生时，能够保存用户态的状态，然后在处理完毕后可以恢复并返回用户态继续执行。 使用场景：p-\u0026gt;trapframe 的保存与恢复发生在用户态和内核态之间的切换过程中。例如，当进程执行系统调用时，CPU 的寄存器状态会被保存到 trapframe，以便在系统调用完成后正确返回用户态。 保存内容不同： p-\u0026gt;context：保存的是内核态的寄存器状态，特别是与内核栈相关的状态信息，例如 eip（指向内核代码中的某个位置）、esp（指向内核栈中的某个位置）等。 p-\u0026gt;trapframe：保存的是用户态的寄存器状态以及一些与系统调用和中断相关的状态信息，例如用户态的 eip、esp 以及系统调用号、参数等。 使用的时机不同： p-\u0026gt;context：在进程调度时使用，当系统切换到其他进程时，当前进程的内核态上下文会被保存到 p-\u0026gt;context，并在下次调度回该进程时恢复。 p-\u0026gt;trapframe：在系统调用或中断发生时使用，当进程从用户态进入内核态时，trapframe 用于保存当前的用户态寄存器状态，以便在系统调用或中断处理完毕后能够正确地返回用户态。 p-\u0026gt;context 与 p-\u0026gt;trapframe 的相同之处\n都是用来保存状态的： 无论是 p-\u0026gt;context 还是 p-\u0026gt;trapframe，它们的主要作用都是为了保存进程在某个时刻的 CPU 状态，以便在之后能够正确恢复进程的执行。 都与进程切换有关： 虽然 p-\u0026gt;context 和 p-\u0026gt;trapframe 用于不同的切换场景（进程上下文切换 vs 用户态和内核态切换），但它们都涉及到进程在不同执行阶段的切换。 总结\np-\u0026gt;context 保存的是进程在 内核态 下的上下文信息，主要用于进程的上下文切换（即调度器在不同进程之间切换时）。 p-\u0026gt;trapframe 保存的是进程在 用户态 和 内核态 切换时的状态信息，主要用于处理系统调用、中断或异常时的状态保存和恢复。 二者的作用和使用场景不同，但它们的共同目标都是为了确保进程的执行能够在不同的切换过程中被正确保存和恢复。\nxv6的组织结构 xv6 使用了传统的内核概念 - 一个向其他运行中程序提供服务的特殊程序。每一个运行中程序（称之为进程）都拥有包含指令、数据、栈的内存空间。指令实现了程序的运算，数据是用于运算过程的变量，栈管理了程序的过程调用。 内核使用了 CPU 的硬件保护机制来保证用户进程只能访问自己的内存空间。内核拥有实现保护机制所需的硬件权限(hardware privileges)，而用户程序没有这些权限。当一个用户程序进行一次系统调用时，硬件会提升特权级并且开始执行一些内核中预定义的功能。 ","date":"2024-09-01T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0813os-organization-system-calls/2ed7cfb8882411ebb6edd017c2d2eca2_hu13301721337528563113.png","permalink":"https://echudet.github.io/p/mit6s0813os-organization-system-calls/","title":"MIT6S081：3OS Organization \u0026 System Calls"},{"content":"Lecture 1 OS Interface Inode 是什么 Inode 是 UNIX 类文件系统中一个非常核心的概念，负责管理文件和目录的元数据。\nInode 是文件系统中的一个数据结构，用来存储文件的元信息（metadata），而不包含文件的内容和文件名。每一个文件或者目录都会分配一个唯一的 Inode，Inode 中存储了与文件或目录相关的以下信息：\n文件类型：是普通文件、目录还是设备文件（在 stat 结构体中的 type 字段表示）。 文件的大小：以字节为单位存储（在 stat 结构体中的 size 字段表示）。 硬链接数：有多少个不同的文件名指向这个 Inode（在 stat 结构体中的 nlink 字段表示）。 文件的访问权限：谁可以读取、写入或执行这个文件。 文件的时间戳：包括文件的创建时间、最后访问时间、最后修改时间等。 数据块指针：指向文件内容在硬盘上存储位置的指针。 Inode 在文件系统中起着关键的作用，因为它连接了文件名和文件内容之间的关系。理解这一点可以从以下几个方面来说明：\n文件与文件名的分离：文件名只是一个指向 Inode 的引用，这意味着在文件系统中，多个不同的文件名可以指向同一个 Inode（也就是硬链接）。这就是为什么可以使用 link 系统调用创建多个文件名指向同一个文件。 文件的元数据管理：所有与文件相关的元数据都存储在 Inode 中。这包括文件类型、大小、权限、所有者等。当你需要获取文件的这些信息时，系统会查找文件的 Inode，而不需要访问文件的内容。 文件内容的定位：Inode 包含指向文件内容在磁盘上位置的指针。当你打开一个文件时，系统通过文件名查找到对应的 Inode，然后通过 Inode 中的数据块指针找到文件的实际内容。 删除文件：当你使用 unlink 系统调用删除文件时，系统只删除了文件名与 Inode 的关联。如果这个 Inode 没有其他链接（即 nlink 为 0），并且没有进程在使用这个文件，系统才会真正释放与该 Inode 相关联的磁盘空间。 比如运行了下面这段代码：\n1 2 open(\u0026#34;a\u0026#34;, O_CREATE|O_WRONLY); link(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); 这个例子创建了一个文件 a，并且为这个文件创建了一个硬链接 b。在这两行代码执行之后，文件 a 和 b 都指向同一个 Inode，它们的内容完全相同，任何一个文件的修改都会反映在另一个文件中。\n通过 Inode，你可以理解文件系统如何管理文件的元数据以及如何实现硬链接、文件删除等功能。简而言之，Inode 是文件系统中的核心结构，负责文件元数据的管理和文件内容的定位。\n特殊文件和普通文件的区别 指向设备的特殊文件和普通文件在文件系统中的处理方式和用途上有很大的不同。它们的主要区别如下：\n文件类型和作用\n普通文件：这是最常见的文件类型，用于存储用户的数据，例如文本文件、图像文件、程序文件等。普通文件包含实际的数据内容，文件系统通过 Inode 和数据块将文件名映射到硬盘上的实际数据。\n特殊文件（设备文件）：特殊文件不包含用户数据，而是用于与硬件设备交互。设备文件通过文件系统接口将文件操作（如读写）映射到设备驱动程序。例如，/dev/sda 可能是一个硬盘设备文件，通过对这个文件进行操作，实际上是对对应的硬盘设备进行操作。\nInode 中的差异\n普通文件的 Inode：普通文件的 Inode 包含文件的元数据（如文件类型、大小、权限等）和指向数据块的指针。这些指针指向存储在硬盘上文件内容的具体位置。\n特殊文件的 Inode：设备文件的 Inode 也包含元数据，但它的指针部分不指向存储在硬盘上的数据块，而是包含与设备相关的标识信息，即主设备号（major number）和次设备号（minor number）。这些编号告诉内核应该使用哪个设备驱动程序以及如何与具体的设备进行通信。\n操作方式\n普通文件操作：当你对普通文件进行读写操作时，文件系统会通过 Inode 查找文件内容在磁盘上的位置，并执行实际的数据读写。\n特殊文件操作：当你对设备文件进行读写操作时，文件系统并不直接处理磁盘数据，而是将这些操作请求转发给对应的设备驱动程序。设备驱动程序根据操作类型（读或写）与实际的硬件设备进行交互。比如，对 /dev/null 文件的写操作，实际上什么也不做，而对 /dev/sda 的写操作，可能会导致硬盘上的数据被写入。\n使用场景\n普通文件：用于存储和读取用户的应用数据，如文档、图像、程序代码等。\n特殊文件：用于操作和控制硬件设备，比如：\n字符设备文件（如 /dev/tty，对应终端设备）：处理字符流设备的读写，通常一次一个字符进行操作。 块设备文件（如 /dev/sda，对应硬盘设备）：处理块设备的读写操作，通常一次一个块（通常为512字节或更多）进行操作。 系统调用的区别\n普通文件：可以通过常规的系统调用如 open、read、write、close 等操作普通文件。\n特殊文件：虽然设备文件也可以通过 open、read、write 等系统调用操作，但这些调用最终会被内核处理，并转交给设备的驱动程序，由驱动程序与硬件直接交互。\n普通文件用于存储数据，文件系统会直接对其内容进行操作。而设备文件是与硬件交互的接口，操作这些文件时，实际上是在与对应的设备驱动程序交互，进而与实际的硬件设备交互。这种设计允许设备在 UNIX 文件系统中的统一表示，使得设备的操作与文件操作类似，保持了 UNIX 的一贯简洁和一致性。\n设备文件 设备文件本身不能被修改，因为它不包含实际的数据内容。设备文件只是一个接口，通过它可以对硬件设备进行操作。例如，当你写数据到一个设备文件时，这些数据实际上是发送到对应的硬件设备（如硬盘）上，而不是存储在设备文件本身中。\n设备文件在硬盘上几乎不占用存储空间。因为设备文件只是一个特殊的文件系统条目，主要存储了设备的标识信息（如主设备号和次设备号），以及一些基础的元数据（例如文件权限、创建时间等）。这些信息占用的空间非常小，通常只需要几个字节到几十个字节。\n与普通文件不同，设备文件没有指向实际数据块的指针，也没有存储用户数据的空间。所以，尽管设备文件存在于文件系统中，但它们在硬盘上占用的存储空间是微乎其微的。\n","date":"2024-08-31T00:00:00Z","image":"https://echudet.github.io/p/mit6s0811os-interface/56d1b86047724f56a2793bda4a87bccb_hu12603671851868893776.jpg","permalink":"https://echudet.github.io/p/mit6s0811os-interface/","title":"MIT6S081：1OS Interface"}]