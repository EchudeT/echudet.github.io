[{"content":" OS火热招新 http://oslabs.hitsz.edu.cn/oscomp/oscomp.html 大二寒假前看到校内有操作系统比赛的招新通知，又恰逢大段空闲时间，于是便抱着前来积累经验的想法，拉着什么都还没学的友人A，与主动加入我们的学长B组起了一个队伍，由我带队来参加操作系统比赛。最终凭借着运气+努力，取得了国一，可喜可贺。\n赛前积累 在比赛报名之前，我个人在操作系统方面的知识积累是mit6s081的xv6整个lab+南大的pa lab，学长B是学完了大三的整个操作系统课程，友人A则是还没有这方面的知识。\n在比赛之前，寒假期间，确定好比赛初期的时间安排，以及内核编写语言之后，我们开始组团进行rust语言的学习，时长为3周。关于rust方面的学习，主要是依赖以下几个内容。\n首先是阅读Rust Course(https://course.rs/about-book.html )，学习其中的Rust 语言基础学习和Rust 语言进阶学习两个大章，掌握rust语言的基本和进阶内容。\n其次是在Rust Practice(https://practice-zh.course.rs/why-exercise.html )里面进行简单的练习，做其中的习题，加深印象。\n最后是在CodeCrafters(https://app.codecrafters.io/catalog )里面写一些简单的项目熟悉rust。我是选了shell和interpreter两个当时免费的项目来练习，写了shell和部分interpreter。这里面的项目分成多个块组成，并且内有自动判错，可以及时发现我们代码中隐含的错误，能让我们更熟练地用rust写项目。\n完成对rust的学习之后，在寒假的剩余时间里面，我们开始对rcore内核(https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter0/index.html )的学习。\n这一阶段比较简单，主要是跟着rCore-Tutorial-Book手册学习。\n新年之前学习手册的0~4章，包括环境配置、批处理系统、分时任务、地址空间四章，新年之后学习手册的5~8章，包括进程、文件系统、进程通信、并发四章，最后剩余的第九章则是留于之后写驱动的同学来看。\n完成每章的学习指看完各章的内容+完成章末的实验要求（貌似5~8某章实验有一个bug），在阅读各章内容的时候，要主动结合内核里面相应的代码来看，不能干看文档内容，这样难以深入理解。\n完成上述的内容后，寒假应该仅剩余一两周的时间了，最后的这段时间就是阅读我校往届的优秀内核，总结经验。\n我们阅读了MankorOS, TitanixOS, MinotaurOS, PhoenixOS的内容，然后在返校之后聚团讨论了一番，对内核的整体架构有了一定的设想，这为我们之后构建内核的工作打下了坚实的基础。同时，我们也简单阅读了一些linux的资料，对于内核设计有了更深的见解。\n比赛过程 我们在开学的几周后才开始正式构建内核，而同实验室的第一组则是早早开始了构建内核，这导致了我们的进度落后，再加上初赛结束在期末月，为我们初赛的落后埋下了伏笔\u0026hellip;.\n我们的内核是从零开始构建的，主要参考的是PhoenixOS的架构，以其他内核作为优化点和来部分借鉴。花了几天时间构造内核的雏形之后，我们开了一个简单的会议划分了内核的分工。\n我将内核的工作划分为进程管理，内存，trap转换，文件系统，信号，驱动，网络，双架构。\n在最初的两周，我们需要实现进程管理，内存，trap转换这三个基本的功能，我负责进程，学长B负责内存，友人A负责trap。这一阶段比较简单，我们在计划时间内完美完成了这一阶段的内容，并尝试将basic测例的内容以嵌入内核的方式来运行。\n下一个阶段是文件系统，信号。文件系统方面比较繁杂，由我和学长B一起写，我主要写了文件系统的大体架构，学长B在这基础上进行优化和创新，之后我则侧重于扩展文件系统于OS侧的更多详细的功能。友人A则是在写信号方面的机制。这一阶段原本的计划时间也是两周，但是工作比想象中更繁杂，最后是花了三周的时间才完成，完成这一阶段之后，busybox测例集成功通过。\n第三阶段是驱动，网络，双架构+过更多的测例集。双架构方面文档比较多，需要阅读大量的文档，于是由学长B和友人A来一起写，我则是简单地了解loongArch方面的内容，对内核架构有大体的掌握。另一边，驱动和网络则是由我来写，网络方面参考ArceOS+PhoenixOS的实现，驱动方面则参考了MankorOS来写。而比赛测例集方面，则是由我来跑测例，如果遇到错误，我能解决就由我来解决，不能解决则是通知特定模块的负责人员来解决，或是合作debug。这一阶段一直持续到6月初，即期末月，也是初赛结束月。\n在期末月的时候，我们在复习期末的内容，没有再推进工作，在这之前我们基本完成了除了ltp和iperf以外的其他测例集，还存在少量的bug在修。完成期末考之后，忽然发现其他队伍跑ltp大大拉分，至少拉开了其他测例的总和分一倍，但鉴于我们要准备文档，ppt，视频，我们就没再修ltp，这导致了我们初赛落后于其他队伍。（我们是没跑ltp队伍的第一名）\n最后初赛综合排名第九。\n然后是决赛阶段，我们痛定思痛，将ltp测例集用linux系统跑了一遍，统计其中的高分测例并制定成一个表格，决定集中突破里面的高分测例。\n决赛分工比较简单，我来搞内核的驱动、网络、上板，完善内核以通过更多的测例。学长B是集中突破ltp的fanotify等方面的高分测例，以及完善内核和文件系统的各个方面，友人A则是掌握交叉编译的知识，交叉编译了vim、gcc这两程序在内核上面跑（运气撞题）。最后我们成功上了riscv的星光板和loongArch的星云板(仍有点bug)，并且通过了大量的ltp测例，取得了决赛线上测例的高分和线下决赛的高分。\n最后的最后，在答辩之前的一个夜晚，我们内核赛四组在线上分别模拟答辩，由老师进行评价。在老师评价完我们的ppt之后，学长B和我一番讨论，决定完全推翻原来的ppt，重构大部分内容。接下来由学长B操刀ppt，一直忙到了凌晨4点，我则是3点上床养精蓄锐，准备同一天的八点半开始的答辩。我们的答辩是第3组，在答辩之前我简单过了一遍ppt就到时间上场，准备十分紧迫。但最终运气不错，算是流畅地完成了答辩，不过想来也是非常惊险的一个过程，日后应该早早做好准备，切忌临时发挥，临阵磨枪还是欠缺很多内容。\n比赛过程还是挺长的，从寒假到暑假，基本是半年的比赛流程，并且工作量也不少，除去校内课程的学习，就我个人，基本把剩余的课外时间都花在这上面，少有时间搞其他的活动。若要参加要做好忙起来的准备。\n内核组成 具体的内核文档见此 调试经验 一般使用二分法进行调试，通过log输出大量的内容来确定上下文，一方面能够知晓发生bug的位置，另一方面能够获取其上下文的参数信息，方便调bug。如果log输出过多时，可以设置动态开关log，和过滤部分log的内容，从而筛选出关键信息。\n如果bug发生在库的深处，或是用户态，或是地址读写卡死崩溃问题，则考虑使用gdb进行跟踪调试，先通过log确定bug大致位置，在附近打断点，然后步进跟踪各个寄存器，或特权寄存器，或相关地址的值等，来确定bug发生的原因。不过对板子就不行了，因为板子上面没有gdb，就只能用二分法调bug了。（关于gdb，听说gdb-dashboard挺好用的，可以尝试使用）\n除此外，调试用户态的程序还可以使用qemu-riscv64或qemu-loongarch64(这个需要额外配置)，通过输出它们系统调用的日志，来对比自己的内核和标准的内核之间的差异，从而找出bug。\n遗憾 多核仍有死锁情况，并发支持不完善。 git本地支持正常，说明文件系统比较完善。但是git clone缺乏https，这可能是交叉编译缺失，或是文件系统还有缺陷，或是网络方面有bug，现场赛时间过短未能调出来。 文件系统的挂载功能不完善，需要实现挂载队列，但这会导致预估3k+的代码重构，这个功能应该在构建内核的时候一起实现，在内核比较完善之后去改容易造成大规模的影响，积重难返。 龙芯qemu的中断驱动仍存在问题，不能正常响应。并且龙芯板子上面存在奇怪的内存问题，当内核以release编译的时候会产生巨量的bug，而以debug模式编译则是少量bug，debug模式产生的未对齐处理多于release方面，怀疑是此方面问题，但是具体原因未知，未能debug出来。 内核中枢多时间系统todo，但是没时间写。 反思 内核构建需要尽早开始，最后要留出一个月的时间给ltp，因为ltp拉分最甚。 内存、arch、lock、log等模块尽量解耦合，放在外部，其他模块都需要依赖它们。 尽早完善调试架构，完善log机制，多进程记录机制，方便之后的调试。 文件系统的各个接口都需要留出扩展接口，用于未来更多的功能扩展。 注重实现更多的功能，在保证实现充足的功能的基础上考虑创新点（实际上很多创新点都是说出来的，内核本身实现就包含着很多特点，选取一些包装一下就是创新点了）。 在明确一个函数的内部逻辑，输入输出的时候可以用AI代写，省时省力，但不要让AI写你不理解的内容，这会导致石山代码。 明确队伍的git commit规范和注释规范，合适的开发规范能够让维护成本降低。 ","date":"2025-09-03T00:00:00Z","image":"https://echudet.github.io/p/oscomp2025-experience/bg_hu_2c4b9aee836e5ee8.jpg","permalink":"https://echudet.github.io/p/oscomp2025-experience/","title":"oscomp2025 experience"},{"content":"在参加OS内核比赛的时候，我们队最终项目是~1000 commits和10+ branches，可见使用git管理之麻烦。\n但也由此机会积累了不少使用git的经验，故此处记录下来，以便日后参考。\n虽然完全可以通过git --help或是git \u0026lt;cmd\u0026gt; --help来看如何使用。\n初始化仓库\n首先，使用git init可以创建一个git仓库，这个指令会在当前目录下面创建一个.git目录，并在里面存放git仓库管理的各类元文件，可以说有了它，当前目录才标识为git管理。如果你移除.git目录，需要重新执行git init指令才能重新将当前目录归于git管理，不过你之前的.git所含有的历史信息会全部丢失。\nwhat is .git? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // .git的构成 .git ├── branches ├── hooks ├── info ├── logs │ └── refs │ ├── heads │ └── remotes │ └── origin ├── objects └── refs ├── heads ├── remotes │ └── origin └── tags branches: 早期 Git 的残留目录，现在基本不用了。以前用于实现“非参考方式”下的分支管理。现今版本几乎始终为空，可以忽略。\nhooks:包含客户端或服务端的钩子脚本（shell、python等），这些脚本在特定事件（如提交、合并、推送前后）时自动执行。\ninfo:存放一些辅助信息，比如 .git/info/exclude 文件，可以设置本地忽略文件（类似 .gitignore，但只对本地有效，不会提交）。\nlogs:用于保存对分支和引用（ref）的操作日志。可用于 git reflog 命令查看 HEAD 和各分支的历史指向。例如logs/refs/heads/记录本地分支操作历史，logs/refs/remotes/origin/记录远程分支操作历史。\nobjects:存储所有 Git 对象（数据的“内容”和历史），包括 commit（提交）、tree（目录）、blob（文件内容）、tag（标签）等对象。每个对象都是用 SHA-1 哈希命名的子目录。这是 Git 核心存储：所有历史内容都在这里！\nrefs:保存所有引用（ref，即对提交对象的指针）的信息，这些实际控制了你看到的分支和标签。\nrefs/heads/：本地分支，每个分支一个文件，内容是 commit 的 SHA-1 值。 refs/remotes/origin/：远程分支跟踪仓库的分支。 refs/tags/：标签指向的 commit。 仓库管理\ngit仓库有三个主要的管理区域。\n第一个是工作区，指当前在电脑上面修改文件的区域，这个区域里面文件的修改信息还没有记录在git里面。\n第二个是暂存区。暂存区是工作区和版本区过渡的一个区域，便于临时舍弃某个文件的修改，或是控制某些修改，可以通过git status来查看状态。想让文件的修改进入这个区域，需要通过git add指令来进行。git add \u0026lt;file\u0026gt;能够让特定文件的修改进入暂存区，git add . 能够让当前仓库内所有文件的修改进入暂存区。\n第三个是版本区。这个区域就是所有通过git commit -m \u0026quot;\u0026lt;update info\u0026gt;\u0026quot;提交的信息所存放的地方。使用git log可以查看这个区域的信息。\n撤销操作\n实际操作中，我们可能会有很多误操作，这个时候需要我们进行撤销。\n在工作区对某文件进行错误修改时，可以使用git checkout -- \u0026lt;filename\u0026gt;来撤销指定文件的修改，还原为上一次commit的状态。 在误提交错误文件到git管理时(例如某个超大文件)，可以使用git rm \u0026lt;filename\u0026gt; \u0026amp;\u0026amp; git commit -m \u0026quot;rm file\u0026quot;来删除工作区+版本区的文件，若不想在工作区删除该文件，可以使用git rm --cached \u0026lt;filename\u0026gt;来代替。 在错误commit，想回退版本时，可以使用git reset HEAD~1，这将让HEAD指向上一个提交，但不会删除修改。如果想去掉修改的话，可以使用git reset --hard HEAD~1来代替。 git reset？ git reset在排查错误的时候是非常有用的，能够帮我们定位bug发生在哪个版本里面。此处简单介绍git reset的一些功能。\n简单来说，git reset是使工作区的指针HEAD指向版本仓库里面的特定版本，也就是之前某次提交后的项目。\nHEAD表示当前版本，HEAD^表示上一个版本，HEAD^^表示上上版本，以此类推。前100个版本就写不过来了，当然这还有更简单的方法HEAD~x，例如HEAD~100能够表示前100版本。\n也可以返回特定版本号git reset 1a2b3d。\ngit reset --hard: 工作区和缓存区的内容全部扔掉！然后返回指定版本。\ngit reset --soft: 保留工作区内容，并且已提交的内容于当前版本和返回版本的差异会存进缓存区。Attention:存进缓存区的方式是覆盖而不是追加！这意味着你之前存进缓存区的内容会丢失，所以不要连续使用两次git reset --soft。\ngit reset (--mixed): 保留工作区内容，并且当前版本和返回版本的差异会追加存入工作区。\n查看日志，之前的日志git log，之后的日志git reflog。\n新的分支\n创建新的分支时，可以使用git branch \u0026lt;name\u0026gt; 查看所有分支时，可以使用git branch -a 创建新的分支+切到此分支时，可以使用git checkout -b \u0026lt;name\u0026gt; 切到某个分支时，可以使用git checkout \u0026lt;name\u0026gt; 分支命名？ 规范的分支命名在一个项目中非常重要，保证了项目的稳定性和多人开发合作潜力。\n1.主分支(main/master): 存放稳定的、可发布的代码。不应该直接在主分支上开发，合并其他分支的代码进行更新。\n2.开发分支(develop): 用于集成开发中的功能分支。\n3.功能分支(feat):通常命名为feature/\u0026lt;feat-name\u0026gt;或feat/\u0026lt;feat-name\u0026gt;，用于开发新的功能，完成后可合并于develop分支，或是直接抛弃。\n4.修复分支(fix):通常命名为bugfix/\u0026lt;...\u0026gt;或fix/\u0026lt;...\u0026gt;，从develop分支创建，修复完成后合并回develop分支。\n5.热修复分支(hotfix):通常命名为hotfix/\u0026lt;...\u0026gt;，用于紧急修复生成环境的bug，从main分支创建，修复完成后合并回main和develop分支。\n6.发布分支(release):通常命名为release/v1.0.0或release/2025-09-02，从develop分支创建、发布完成后，合并到develop和main分支。\n7.支持分支(support):通常命名为support/v1.0.x，从main分支创建，用于维护旧版本。\n合并\n最简单的合并两个分支，git merge \u0026lt;sub-branch\u0026gt; 合并两个分支，但保留合并基分支的历史的线性化，不零碎掺杂合并从分支的提交历史，git merge --no-ff \u0026lt;sub-branch\u0026gt;。推荐主分支使用。 远程拉取合并分支，git pull origin \u0026lt;branch\u0026gt;相当于git fetch + git merge，此指令如果远程和本地有不同的提交历史，会创建一个新的合并提交，导致提交历史出现分叉。 远程拉取合并分支but rebase，git pull --rebase origin \u0026lt;branch\u0026gt;能够把远程分支的更改拉取到本地，然后将本地的提交一个个重新应用到远程分支的最新提交上，保持提交历史的线性化。推荐使用。 远程管理\n添加远程仓库的URL， git remote add \u0026lt;remote-name\u0026gt; \u0026lt;remote-url\u0026gt;，并通过git remote -v确认是否添加。 推送当前分支到远程仓库，并与远程分支关联，git push -u origin \u0026lt;branch\u0026gt;，其中origin 为\u0026lt;remote-name\u0026gt;。 推送当前分支到远程仓库，并与远程分支合并，git push origin \u0026lt;branch\u0026gt;，其中origin 为\u0026lt;remote-name\u0026gt;。 推送当前分支到远程仓库，并将远程分支覆盖，git push -f origin \u0026lt;branch\u0026gt;，其中origin 为\u0026lt;remote-name\u0026gt;。 删除远程分支，git push --delete origin \u0026lt;branch\u0026gt; 抓取远程分支，不合并，git fetch origin \u0026lt;branch\u0026gt;，一次性拉取远程所有分支，git fetch --all commit规范\n事实上，我们的内核项目commit注释一直没有一个统一的规范，进行的比较随意，甚至当成了ctrl+s功能，想保存的时候就提交一下，这导致了我们项目后期debug的一些困难。因此，这里总结一些commit的注释规范，以为后来的项目做准备。\n一个简单的例子，表明增加的功能以及实现了哪些相关的内容。\n1 2 3 4 5 6 7 feat(search): add autocomplete suggestions Implement real-time suggestions for search input field. Fetch top 5 matching results from backend API as user types. Enhance UX with keyboard navigation support. Closes #101 抽象出来，由以下几部分构成：\n类型（type）: 表示提交的类别。 范围（scope，可选）: 指定更改影响的模块或范围，用括号包裹。 主题（subject）: 简短描述更改内容，通常以动词开头，50-72 字符以内。 正文（body，可选）: 详细说明更改的原因和细节，每行不超过 72 字符。 脚注（footer，可选）: 关联问题或其他元信息，例如 issue ID。 1 2 3 4 5 \u0026lt;类型\u0026gt;(\u0026lt;范围\u0026gt;(可选)): \u0026lt;主题\u0026gt; \u0026lt;空行\u0026gt; \u0026lt;正文\u0026gt;(可选) \u0026lt;空行\u0026gt; \u0026lt;脚注\u0026gt;(可选) 那么最简单的commit注释至少应该为\nfeat: add autocomplete suggestions\n","date":"2025-09-02T00:00:00Z","permalink":"https://echudet.github.io/p/git-commands/","title":"git commands"},{"content":"时隔一年没有记过博客，很多东西都遗忘了。上个学期一直在忙OS比赛的事情，现在终于结束了，于是又考虑整理起博客来。之前所写的博客大多AI辅助，现在想来略有不妥，博客理应是个人的经验收获，而过于借助外力写出来的博客与个人的经验收获毫无关系，它是属于AI的，而不是个人的。因此，此篇博客记录了我思考如何撰写个人博客的内容，以供之后借鉴。\n写博客，第一要点是考虑博客所记录的内容。有了可写的内容，一篇博客才有写的价值，没有内容的博客是空洞而无意义的，应该避免无内容的书写。并且，在内容不够丰富的时候花少量的文笔记录，或是去搜索扩充自己的知识面合并着一起记录，而不应该对内容泛泛而言，强行氵内容，不然只是在写流水账。\n写博客，第二要点是环绕主题，有清晰的书写脉络，起承转合过度自然，适当扩展而不离题。在针对自己所想的内容拟定好标题之后，需要围绕着这个主题来整理内容，大篇幅围绕着主题来展开。当扩展的内容过多时，可以适当分割为其他博客并以超链接的形式呈现，而不是全部堆在一篇文章里面。\n写博客，第三要点是有良好的呈现格式。对于内容进行重点标注，表格划分，图像呈现\u0026hellip;.能够使博客的表达更为形象生动，内含的知识也更容易理解。\n最后，对于我个人而言，目前阶段需要尽量避免使用AI，过度地使用AI产出内容其实是急功近利的倾向，正确的做法是回归自我，以人为本，可以用AI来查阅资料，但不能完全的照搬，要有个人自己的理解，否则仅仅是AI的搬运工，这就丧失了博客的初心。因此，之后的博客将采取本篇博客的书写方式来写，不照搬AI，写出自己的理解与思考。\n","date":"2025-09-02T00:00:00Z","permalink":"https://echudet.github.io/p/how-to-write-blogs/","title":"how to write blogs"},{"content":"基于概率论排队理论的路由器缓冲区研究\n摘要\n随着计算机网络规模的不断扩大和数据流量的急剧增加，路由器作为网络核心设备，其性能直接影响整个网络的效率和可靠性。缓冲区作为路由器的重要组成部分，承担着临时存储和调度数据包的关键任务。本文通过应用运筹学中的排队理论，特别是M/M/c模型，对路由器缓冲区的设计进行深入研究。通过概率论的基本原理，分析系统利用率、缓冲区容量和丢包率等关键性能指标，探讨如何优化路由器的缓冲区设计，以提高网络性能并降低数据包丢失率。\n关键词：概率论；排队理论；M/M/c模型；路由器；缓冲区设计\n1. 引言 在现代计算机网络中，路由器作为连接不同网络段的关键设备，负责数据包的转发与调度。随着互联网应用的多样化和数据流量的爆发式增长，路由器的性能需求日益提高。\n缓冲区作为路由器的重要组成部分，主要用于暂存待处理的数据包，缓解数据流量的瞬时冲击。然而，缓冲区的设计不仅影响数据包的处理效率，还直接关系到网络的延迟和丢包率。因此，如何科学地设计路由器的缓冲区，成为提升网络性能的关键问题之一。\n排队理论，作为运筹学的重要分支，提供了分析和优化服务系统的强大工具。通过建立数学模型，排队理论能够有效描述数据包到达、等待和服务的随机过程，为缓冲区设计提供理论支持。\n本文将重点探讨M/M/c排队模型在路由器缓冲区设计中的应用，通过概率论的基本原理，分析关键性能指标，并提出优化设计的方法。\n2. 排队理论与概率论基础 2.1 排队理论概述 排队理论研究的是顾客（在本文中指数据包）在获得服务（即在路由器中被处理）过程中产生的等待和排队现象。其核心在于分析系统中的到达过程、服务过程、服务台数量及排队规则等因素，进而计算系统的性能指标，如平均等待时间、排队长度和系统利用率等。\n2.2 概率论在排队模型中的应用 概率论为排队理论提供了理论基础。通过描述顾客到达和服务过程的概率分布，概率论能够精确计算系统在不同状态下的概率，从而推导出系统的整体性能指标。常用的概率分布包括泊松分布（用于描述到达过程）和指数分布（用于描述服务时间）。\n2.3 M/M/c排队模型 M/M/c模型是排队理论中广泛应用的一个模型，其特点如下：\nM（Markovian）：到达过程服从泊松分布。 M（Markovian）：服务时间服从指数分布。 c：系统中有c个服务台（在本研究中指路由器的处理单元数量）。 该模型适用于描述多个并行处理单元的系统，能够有效分析路由器在多核处理器下的性能表现。\n3. 路由器缓冲区设计中的M/M/c模型应用 3.1 系统参数定义 在M/M/c模型中，关键参数包括：\n到达率（λ）：单位时间内到达路由器的数据包数量。 服务率（μ）：单位时间内每个处理单元能够处理的数据包数量。 服务台数量（c）：路由器中并行处理单元的数量。 系统利用率（ρ）：系统的繁忙程度，定义为 $rho = \\frac{\\lambda}{c\\mu}$。 3.2 系统利用率（ρ）的计算与意义 系统利用率ρ表示路由器处理单元的负载情况。其计算公式为：\n$$ \\rho = \\frac{\\lambda}{c\\mu} $$ 当ρ接近1时，表示系统接近饱和状态，处理单元几乎被完全利用，可能导致排队长度和等待时间急剧增加，增加丢包的风险。反之，较低的ρ则表明系统资源利用不足，可能造成资源浪费。\n3.3 无阻塞概率（P₀）的计算 无阻塞概率P₀是指系统中没有数据包等待处理的概率。其计算公式为：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c\\mu}{c\\mu - \\lambda} \\right]^{-1} $$ 该公式综合考虑了系统中0到c-1个数据包的状态，以及c个或更多数据包的状态。\n3.4 平均排队长度（Lq）和平均等待时间（Wq） 平均排队长度Lq表示在系统中等待处理的数据包的平均数量，其计算公式为：\n$$ L_q = \\frac{P_0 \\cdot (\\lambda/\\mu)^c \\cdot \\rho}{c! \\cdot (1 - \\rho)^2} $$平均等待时间Wq表示数据包在队列中等待处理的平均时间，通过Little定理可得：\n$$ W_q = \\frac{L_q}{\\lambda} $$3.5 平均系统内数据包数（L）和平均逗留时间（W） 平均系统内数据包数L包括正在处理和等待的数据包，其计算公式为：\n$$ L = L_q + \\frac{\\lambda}{\\mu} $$平均逗留时间W表示数据包在系统中的总时间，即等待时间与服务时间之和：\n$$ W = W_q + \\frac{1}{\\mu} $$4. 应用案例分析 4.1 案例背景 假设某路由器的到达率λ为200数据包/秒，每个处理单元的服务率μ为250数据包/秒，路由器配备了4个处理单元（c=4）。\n4.2 计算系统利用率（ρ） $$ \\rho = \\frac{200}{4 \\times 250} = \\frac{200}{1000} = 0.2 $$系统利用率为20%，表明处理单元处于较低负载状态。\n4.3 计算无阻塞概率（P₀） 首先计算：\n$$ \\lambda/\\mu = \\frac{200}{250} = 0.8 $$ 然后计算各项和：\n$$ \\sum_{n=0}^{3} \\frac{0.8^n}{n!} = \\frac{0.8^0}{0!} + \\frac{0.8^1}{1!} + \\frac{0.8^2}{2!} + \\frac{0.8^3}{3!} = 1 + 0.8 + 0.32 + 0.0853 = 2.2053 $$接着计算：\n$$ \\frac{0.8^4}{4!} \\cdot \\frac{4 \\times 250}{4 \\times 250 - 200} = \\frac{0.4096}{24} \\cdot \\frac{1000}{800} = 0.01707 \\cdot 1.25 = 0.02134 $$总和为：\n$$ 2.2053 + 0.02134 = 2.2266 $$ 因此：\n$$ P_0 = \\frac{1}{2.2266} \\approx 0.449 $$无阻塞概率约为44.9%。\n4.4 计算平均排队长度（Lq） $$ L_q = \\frac{0.449 \\times 0.8^4 \\times 0.2}{4! \\times (1 - 0.2)^2} = \\frac{0.449 \\times 0.4096 \\times 0.2}{24 \\times 0.64} = \\frac{0.0368}{15.36} \\approx 0.0024 \\text{ 数据包} $$平均排队长度约为0.0024数据包，表明队列中几乎没有数据包等待。\n4.5 计算平均等待时间（Wq）和平均逗留时间（W） $$ W_q = \\frac{0.0024}{200} = 0.000012 \\text{ 秒} = 0.012 \\text{ 毫秒} $$$$ W = \\frac{0.0024}{200} \\text{ 秒} + \\frac{1}{250} \\text{ 秒} = 0.012 \\text{ 毫秒} + 4 \\text{ 毫秒} \\approx 4.012 \\text{ 毫秒} $$5. 路由器决策中的排队理论应用 5.1 缓冲区大小的确定 通过计算平均排队长度Lq和平均等待时间Wq，可以确定路由器缓冲区的合理大小。在上述案例中，Lq接近于零，表明当前缓冲区容量充足，且处理单元具有较低负载，可以适当减少缓冲区资源，降低成本。\n5.2 处理单元数量的优化 系统利用率ρ为20%，处理单元有较大的冗余。为了提高资源利用率，可考虑减少处理单元数量，确保ρ维持在合理范围（如40%-70%），以平衡资源利用与服务质量。例如，将c减少到3：\n$$ \\rho = \\frac{200}{3 \\times 250} = 0.267 $$重新计算P₀、Lq、Wq和W，若性能指标仍满足需求，则可以实现资源优化。\n5.3 负载均衡与动态调整 在高流量环境下，路由器可能需要动态调整处理能力或负载分配策略，以应对突发流量。通过实时监控系统利用率和排队长度，路由器可以动态增加处理单元数量或调整缓冲区容量，确保网络性能稳定。\n5.4 优先级队列与服务质量保障 对于不同优先级的数据包，路由器可以采用优先级队列策略，确保关键数据包（如实时视频、语音）获得更快的处理速度，减少其等待时间。排队模型可扩展为具有优先级的M/M/c模型，以计算不同优先级队列的性能指标，指导优先级策略的实施。\n6. 结论 排队理论，特别是M/M/c模型，为路由器缓冲区的设计与优化提供了科学的数学工具。通过分析系统利用率、无阻塞概率、平均排队长度和等待时间等关键性能指标，能够合理配置缓冲区大小和处理单元数量，优化路由器性能，提升网络整体效率和可靠性。\n本文通过具体案例展示了排队理论在路由器缓冲区设计中的应用过程，强调了概率论在模型推导中的重要性。未来的研究可进一步拓展至更复杂的排队模型，如考虑数据包优先级、非指数服务时间分布等，以适应更为多样化和动态的网络环境需求。\n当系统中顾客数量 $ n \u0026lt; c $ 时，每个顾客都有专属的服务器，因此系统的状态概率 $ P_n $ 由独立的泊松到达过程决定，公式为 $ P_n = \\frac{(\\lambda/\\mu)^n}{n!} P_0 $，其中 $ (\\lambda/\\mu)^n $ 表示到达率与服务率的比率，$ n! $ 反映了 $ n $ 个顾客的排列组合。而当 $ n \\geq c $ 时，所有 $ c $ 个服务器都在忙碌，额外的 $ n - c $ 个顾客需要排队等待，此时状态概率 $ P_n $ 需要考虑服务器全部占用的排列方式和排队的组合，因此公式为 $ P_n = \\frac{(\\lambda/\\mu)^n}{c! \\cdot c^{n - c}} P_0 $。这个分段公式分别反映了在不同顾客数量下系统的服务和排队情况，从而准确描述了 M/M/c 排队模型的概率分布。\n正在思考 纠正公式不准确 首先，指出用户提供的公式可能有错误，如大括号不匹配或其他问题。然后，详细分析公式的每个部分，确保准确无误。 思考，持续 几秒\n抱歉，之前的公式在 LaTeX 格式上确实有错误。让我为你更正并简洁地推导 M/M/c 排队模型中的 平均排队长度 $ L_q $。\n重新推导平均排队长度 $ L_q $ 在 M/M/c 模型中，定义如下参数：\n到达率（$ \\lambda $）：单位时间内到达系统的顾客数量。 服务率（$ \\mu $）：每个服务器单位时间内能服务的顾客数量。 服务器数量（$ c $）。 系统利用率（$ \\rho $）： $$ \\rho = \\frac{\\lambda}{c\\mu} $$ 1. 无阻塞概率 $ P_0 $ 无阻塞概率 $ P_0 $ 表示系统中没有顾客的概率，其公式为： $$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{1}{1 - \\rho} \\right]^{-1} $$2. 有顾客需要等待的概率 $ P_{\\text{wait}} $（Erlang C 公式） 有顾客需要等待的概率 $ P_{\\text{wait}} $ 表示系统中所有 $ c $ 个服务器都在忙碌，导致新到的顾客需要排队等待。根据 Erlang C 公式： $$ P_{\\text{wait}} = \\frac{ \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{1}{1 - \\rho} }{ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{1}{1 - \\rho} } $$ 简化后，可以表示为： $$ P_{\\text{wait}} = \\frac{ (\\lambda/\\mu)^c \\cdot \\frac{1}{1 - \\rho} }{ c! \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{1}{1 - \\rho} \\right] } = \\frac{ (\\lambda/\\mu)^c }{ c! } \\cdot \\frac{\\rho}{1 - \\rho} \\cdot P_0 $$ 其中，$\\rho = \\frac{\\lambda}{c\\mu}$。\n3. 平均排队长度 $ L_q $ 根据 Little 定理，平均排队长度 $ L_q $ 可以表示为： $$ L_q = \\frac{ P_{\\text{wait}} \\cdot \\rho }{ 1 - \\rho } $$ 将 $ P_{\\text{wait}} $ 代入得： $$ L_q = \\frac{ P_{\\text{wait}} \\cdot \\rho }{ 1 - \\rho } = \\frac{ \\left( \\frac{ (\\lambda/\\mu)^c }{ c! } \\cdot \\frac{ \\rho }{ 1 - \\rho } \\cdot P_0 \\right) \\cdot \\rho }{ 1 - \\rho } = \\frac{ (\\lambda/\\mu)^c \\cdot \\rho^2 }{ c! \\cdot (1 - \\rho)^2 } \\cdot P_0 $$ 简化后，通常表示为： $$ L_q = \\frac{ (\\lambda/\\mu)^c \\cdot \\rho }{ c! \\cdot (1 - \\rho)^2 } \\cdot P_0 $$最终公式 综上所述，M/M/c 排队模型中 平均排队长度 $ L_q $ 的公式为： $$ L_q = \\frac{ P_0 \\cdot (\\lambda/\\mu)^c \\cdot \\rho }{ c! \\cdot (1 - \\rho)^2 } $$ 其中： $$ \\rho = \\frac{\\lambda}{c\\mu} $$ $$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{1}{1 - \\rho} \\right]^{-1} $$公式解释 $ (\\lambda/\\mu)^c / c! $：表示当有 $ c $ 个顾客在系统中时的基本概率部分。 $ \\rho / (1 - \\rho)^2 $：反映了系统利用率对排队长度的影响，特别是当 $ \\rho $ 接近 1 时，排队长度会迅速增加。 $ P_0 $：系统完全空闲的概率，用于归一化整个概率分布。 结论 通过以上推导，我们得出了 M/M/c 排队模型中 平均排队长度 $ L_q $ 的准确公式。这个公式展示了系统参数（到达率、服务率、服务器数量）如何共同影响排队的平均长度，帮助我们在实际应用中优化资源配置，提升服务效率。\n","date":"2024-12-02T23:19:30+08:00","permalink":"https://echudet.github.io/p/netmath2/","title":"Netmath2"},{"content":"目录 基本概念回顾 排队模型分类 主要排队模型及其推导 3.1 M/M/1 模型 3.2 M/M/c 模型 3.3 M/G/1 模型 3.4 G/M/1 模型 3.5 M/D/1 模型 3.6 G/G/1 模型 关键性能指标的推导 概率论原理在推导中的应用 排队理论在计算机网络中路由器节点的应用 6.1 路由器缓冲区设计 6.2 数据包排队与丢弃策略 6.3 负载均衡与资源分配 6.4 路由器决策中的排队理论应用实例 6.5 优先级排队策略 6.6 排队理论在路由器中的综合应用 6.7 路由器缓冲区扩展与拥塞控制 基本概念回顾 在深入模型推导之前，简要回顾一些基本概念：\n顾客（Customer）：需要服务的实体，如人、数据包等。 服务台（Server）：提供服务的设施或人员，如计算机网络中的路由器。 到达过程（Arrival Process）：顾客到达系统的方式，通常使用泊松过程描述。 服务过程（Service Process）：服务时间的分布，常见为指数分布（Markovian）、确定分布等。 排队规则（Queue Discipline）：如先到先服务（FCFS）。 系统容量（System Capacity）：系统能够容纳的最大顾客数。 稳态（Steady State）：系统在长期运行后的平衡状态。 排队模型分类 排队模型通常使用 Kendall 表示法 表示，形式为 A/S/c/K/N/D，其中：\nA（Arrival process）：到达过程的概率分布 S（Service process）：服务过程的概率分布 c（Number of servers）：服务台的数量 K（System capacity）：系统容量（可选） N（Population size）：顾客源的容量（可选） D（Queue discipline）：排队规则（可选） 常见的概率分布表示：\nM（Markovian）：泊松过程（指数分布） D（Deterministic）：确定性分布 G（General）：一般分布 简化形式通常只写前三项，如 M/M/1、M/M/c 等。\n主要排队模型及其推导 下面将详细推导几个主要的排队模型，重点介绍其公式的来源和概率论原理的应用。\nM/M/1 模型 定义 到达过程：泊松过程（λ），即顾客到达的间隔时间服从指数分布。 服务过程：单服务台，服务时间服从指数分布（μ）。 服务台数量：1 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 λ \u0026lt; μ。\n关键性能指标 系统利用率（ρ）：ρ = λ / μ 平均系统内顾客数（L）：L = ρ / (1 - ρ) 平均排队长度（Lq）：Lq = ρ² / (1 - ρ) 平均等待时间（Wq）：Wq = ρ / (μ - λ) 平均逗留时间（W）：W = 1 / (μ - λ) 推导过程 1. 到达过程与服务过程\n到达过程：假设顾客到达服从泊松过程，参数为 λ。这意味着在时间间隔 t 内，顾客到达的概率为：\n$$ P(\\text{n 个到达}) = \\frac{(\\lambda t)^n e^{-\\lambda t}}{n!} $$ 服务过程：服务时间服从指数分布，参数为 μ，概率密度函数为：\n$$ f_S(s) = μ e^{-μ s} $$ 2. 马尔可夫性质\n由于到达和服务过程均为无记忆的（Markovian），系统状态只依赖于当前状态，与过去无关。这使得排队过程成为 马尔可夫链。\n3. 平稳状态下的平衡方程\n设 P_n 为系统中有 n 个顾客的稳态概率。根据平稳状态下的流入率和流出率相等，得到：\n$$ λ P_n = μ P_{n+1}, \\quad \\text{对于 } n \\geq 0 $$从这个关系中，可以递归地求得 P_n：\n$$ P_{n+1} = \\frac{λ}{μ} P_n = ρ P_n $$其中，ρ = λ / μ。\n4. 求解稳态概率\n考虑系统的总概率：\n$$ \\sum_{n=0}^{\\infty} P_n = 1 $$利用递推关系：\n$$ P_n = ρ^n P_0 $$代入总概率：\n$$ P_0 \\sum_{n=0}^{\\infty} ρ^n = 1 $$这是一个等比数列，收敛条件是 ρ \u0026lt; 1（即 λ \u0026lt; μ）。\n$$ P_0 \\cdot \\frac{1}{1 - ρ} = 1 \\Rightarrow P_0 = 1 - ρ $$因此，\n$$ P_n = (1 - ρ) ρ^n $$5. 计算性能指标\n系统利用率（ρ）：\n$$ ρ = \\frac{λ}{μ} $$ 平均系统内顾客数（L）：\n$$ L = \\sum_{n=0}^{\\infty} n P_n = (1 - ρ) \\sum_{n=0}^{\\infty} n ρ^n $$使用概率论中的 期望公式：\n$$ \\sum_{n=0}^{\\infty} n ρ^n = \\frac{ρ}{(1 - ρ)^2} $$因此，\n$$ L = (1 - ρ) \\cdot \\frac{ρ}{(1 - ρ)^2} = \\frac{ρ}{1 - ρ} $$ 平均排队长度（Lq）：\n$$ Lq = L - \\rho^0 P_0 = L - P_0 = \\frac{ρ}{1 - ρ} - (1 - ρ) = \\frac{ρ^2}{1 - ρ} $$ 平均等待时间（Wq）：\n由 Little 定理：\n$$ Wq = \\frac{Lq}{λ} = \\frac{ρ^2}{λ(1 - ρ)} = \\frac{ρ}{μ - λ} $$ 平均逗留时间（W）：\n$$ W = \\frac{L}{λ} = \\frac{1}{μ - λ} $$ 6. 使用 Little 定理\nLittle 定理表述如下：\n$$ L = λ W $$ $$ Lq = λ Wq $$该定理适用于广义的排队系统，只要满足稳定状态和某些基本假设。\n通过上述步骤，我们从基本的泊松到达过程和指数服务时间出发，利用马尔可夫链的平稳状态平衡方程，推导出 M/M/1 模型的各项性能指标。关键使用的概率论原理包括：\n泊松过程和指数分布的无记忆性（Markov性质） 递推关系和稳态平衡方程 期望的线性性和等比数列求和公式 Little 定理连接系统状态与时间性能指标 M/M/c 模型 定义 到达过程：泊松过程（λ） 服务过程：c 个服务台，服务时间独立同分布，服从指数分布（μ） 服务台数量：c 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 λ \u0026lt; cμ。\n关键性能指标 系统利用率（ρ）：ρ = λ / (cμ) 无阻塞概率（P₀）：详见下文 平均系统内顾客数（L） 平均排队长度（Lq） 平均等待时间（Wq） 平均逗留时间（W） 推导过程 1. 系统状态\n设系统中有 n 个顾客（n ≥ 0）。因为有 c 个服务台，系统的状态可以分为：\nn \u0026lt; c：所有顾客都在接受服务，没有排队。 n ≥ c：有 (n - c) 个顾客在排队。 2. 平衡方程\n在稳态下，流入速率等于流出速率。对于各状态 n：\nn \u0026lt; c：\n$$ λ P_n = μ P_{n+1} $$ n ≥ c：\n$$ λ P_n = c μ P_{n+1} $$ 3. 求解 P₀\n利用递推关系，可以表达 P_n 为 P₀ 的函数。\n对于 n \u0026lt; c：\n$$ P_n = \\frac{λ^n}{μ^n n!} P_0 = \\frac{ρ^n}{n!} P_0 $$ 对于 n ≥ c：\n$$ P_n = \\frac{λ^n}{c^c μ^n c!} P_0 = \\frac{ρ^n}{c! c^{n - c}} P_0 = \\frac{ρ^c}{c!} \\left(\\frac{λ}{c μ}\\right)^{n - c} P_0 = \\frac{ρ^c}{c!} \\cdot ρ^{n - c} P_0 = \\frac{ρ^n}{c!} P_0 $$ 其中，ρ = λ / μ。\n系统的总概率为 1：\n$$ \\sum_{n=0}^{\\infty} P_n = 1 $$将 P_n 表达式代入：\n$$ \\sum_{n=0}^{c-1} \\frac{ρ^n}{n!} P_0 + \\sum_{n=c}^{\\infty} \\frac{ρ^n}{c!} P_0 = 1 $$分开求和：\n$$ P_0 \\left( \\sum_{n=0}^{c-1} \\frac{ρ^n}{n!} + \\frac{ρ^c}{c!} \\sum_{n=0}^{\\infty} \\frac{ρ^n}{c^n} \\right) = 1 $$简化第二个求和：\n$$ \\sum_{n=0}^{\\infty} \\frac{ρ^n}{c^n} = \\sum_{n=0}^{\\infty} \\left(\\frac{ρ}{c}\\right)^n = \\frac{1}{1 - \\frac{ρ}{c}} = \\frac{c}{c - ρ}, \\quad \\text{当 } ρ \u003c c $$因此，\n$$ P_0 \\left( \\sum_{n=0}^{c-1} \\frac{ρ^n}{n!} + \\frac{ρ^c}{c!} \\cdot \\frac{c}{c - ρ} \\right) = 1 $$整理得：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{ρ^n}{n!} + \\frac{ρ^c}{c!} \\cdot \\frac{c}{c - ρ} \\right]^{-1} $$这被称为 Erlang C 公式 的一部分。\n4. 计算性能指标\n无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{ρ^n}{n!} + \\frac{ρ^c}{c!} \\cdot \\frac{c}{c - ρ} \\right]^{-1} $$ 平均排队长度（Lq）：\n使用 Erlang C 公式：\n$$ Lq = \\frac{P_0 \\cdot \\frac{ρ^{c+1}}{c! (c - ρ)}}{(1 - ρ)^2} = \\frac{P_0 \\cdot \\rho^{c+1}}{c! (c - \\rho)^2} $$ 平均系统内顾客数（L）：\n$$ L = Lq + \\frac{λ}{μ} = Lq + ρ $$ 平均等待时间（Wq）：\n由 Little 定理：\n$$ Wq = \\frac{Lq}{λ} = \\frac{ρ^{c+1} P_0}{c! (c - \\rho)^2 λ} = \\frac{Lq}{λ} $$ 平均逗留时间（W）：\n$$ W = Wq + \\frac{1}{μ} = \\frac{Lq}{λ} + \\frac{1}{μ} $$ 5. 概率论原理应用\n在 M/M/c 模型的推导中，应用了以下概率论原理：\n泊松过程的性质：独立性和无记忆性。 马尔可夫链的稳态平衡方程。 递推关系：通过递推关系求解稳态概率。 等比数列求和：用于简化无限级数求和。 生成函数方法（间接使用）：在某些推导中用于处理复杂的级数求和。 Little 定理：连接时间和人数的期望值。 6. 例子\n假设有 3 个服务台（c = 3），到达率 λ = 2 顾客/分钟，服务率 μ = 1 顾客/分钟。\n计算：\nρ = λ / μ = 2 / 1 = 2\n稳态条件：λ \u0026lt; cμ ⇒ 2 \u0026lt; 3×1 ⇒ 成立。\n计算 P₀：\n$$ P_0 = \\left[ \\sum_{n=0}^{2} \\frac{2^n}{n!} + \\frac{2^3}{3!} \\cdot \\frac{3}{3 - 2} \\right]^{-1} = \\left[1 + 2 + 2 + \\frac{8}{6} \\times 3 \\right]^{-1} = \\left[5 + 4\\right]^{-1} = \\frac{1}{9} $$ 计算 Lq：\n$$ Lq = \\frac{P_0 \\cdot 2^{4}}{3! (3 - 2)^2} = \\frac{\\frac{1}{9} \\times 16}{6 \\times 1} = \\frac{16}{54} = \\frac{8}{27} \\approx 0.296 $$ 计算 L：\n$$ L = Lq + \\frac{λ}{μ} = \\frac{8}{27} + 2 \\approx 2.296 $$ 计算 Wq：\n$$ Wq = \\frac{Lq}{λ} = \\frac{8/27}{2} = \\frac{4}{27} \\approx 0.148 \\text{ 分钟} $$ 计算 W：\n$$ W = Wq + \\frac{1}{μ} = \\frac{4}{27} + 1 \\approx 1.148 \\text{ 分钟} $$ M/G/1 模型 定义 到达过程：泊松过程（λ） 服务过程：单服务台，服务时间服从一般分布（G），具有已知的均值 E[S] 和方差 Var(S) 服务台数量：1 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 λ E[S] \u0026lt; 1，即 ρ = λ E[S] \u0026lt; 1\n关键性能指标 平均等待时间（Wq）：\n$$ Wq = \\frac{λ E[S^2]}{2(1 - \\rho)} $$ 平均排队长度（Lq）：\n$$ Lq = \\lambda Wq = \\frac{\\lambda^2 E[S^2]}{2(1 - \\rho)} $$ 平均系统内顾客数（L）：\n$$ L = Lq + \\rho = \\frac{\\lambda^2 E[S^2]}{2(1 - \\rho)} + \\rho $$ 平均逗留时间（W）：\n$$ W = Wq + E[S] = \\frac{\\lambda E[S^2]}{2(1 - \\rho)} + E[S] $$ 推导过程 1. 到达过程与服务过程\n到达过程：泊松过程，λ。 服务过程：一般分布，具有均值 E[S] 和二阶矩 E[S²]。 2. 利用 Pollaczek-Khinchin (P-K) 公式\nM/G/1 模型的等待时间和排队长度可通过 P-K 公式推导得出。\n3. P-K 公式推导\n步骤概述：\n构建平稳状态下的生成函数。 利用泊松到达过程的特点。 应用概率论中的条件期望。 具体推导较为复杂，涉及到生成函数和复合泊松过程的性质，这里简要概述核心思想。\n4. 计算 Wq 和 Lq\n通过 P-K 公式，可以得到：\n$$ Wq = \\frac{λ E[S^2]}{2(1 - \\rho)} $$ $$ Lq = \\lambda Wq = \\frac{\\lambda^2 E[S^2]}{2(1 - \\rho)} $$其中，E[S^2] 是服务时间的二阶矩。\n5. 使用 Little 定理\nLittle 定理：\n$$ Lq = \\lambda Wq $$适用于广义的排队系统。\n概率论原理应用 泊松过程的独立增量性质 期望的线性性 条件期望 生成函数的使用 P-K 公式：结合概率论和随机过程理论得出。 G/M/1 模型 定义 到达过程：一般分布（G） 服务过程：单服务台，服务时间服从指数分布（μ） 服务台数量：1 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 λ \u0026lt; μ\n关键性能指标 由于服务过程为 Markovian，尽管到达过程是一般分布，但可以通过嵌入 Markov 链的方法进行分析。具体性能指标的表达式较为复杂，通常使用近似方法或数值解。\n推导过程 G/M/1 模型的详细推导涉及到：\n构建系统状态的转移概率 利用嵌入 Markov 链的方法 求解稳态概率 由于到达过程不是 Markovian，这使得分析复杂化。通常，性能指标需要借助数值方法或仿真进行估计。\n概率论原理应用 嵌入 Markov 链：将非 Markovian过程嵌入到 Markov 框架中。 稳态平衡方程。 条件期望。 M/D/1 模型 定义 到达过程：泊松过程（λ） 服务过程：单服务台，服务时间为确定常数 D 服务台数量：1 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 λ D \u0026lt; 1，即 ρ = λ D \u0026lt; 1\n关键性能指标 平均等待时间（Wq）：\n$$ Wq = \\frac{λ D^2}{2(1 - ρ)} $$ 平均排队长度（Lq）：\n$$ Lq = \\lambda Wq = \\frac{\\lambda^2 D^2}{2(1 - ρ)} $$ 平均系统内顾客数（L）：\n$$ L = Lq + \\rho = \\frac{\\lambda^2 D^2}{2(1 - ρ)} + \\rho $$ 平均逗留时间（W）：\n$$ W = Wq + D = \\frac{\\lambda D^2}{2(1 - ρ)} + D $$ 推导过程 1. 到达过程与服务过程\n到达过程：泊松过程，λ。 服务过程：确定的服务时间 D。 2. 计算 Wq 和 Lq\n由于服务时间确定，可以使用 M/D/1 模型的特定公式：\n$$ Wq = \\frac{λ D^2}{2(1 - ρ)} $$ $$ Lq = \\lambda Wq = \\frac{\\lambda^2 D^2}{2(1 - ρ)} $$3. 使用 Little 定理\n$$ Lq = \\lambda Wq $$4. 概率论原理应用\n泊松过程的无记忆性 期望的线性性 确定性服务时间的影响：减少了服务时间的不确定性，导致排队长度和等待时间的计算有所不同。 G/G/1 模型 定义 到达过程：一般分布（G） 服务过程：一般分布（G） 服务台数量：1 系统容量：无限 顾客源：无限 排队规则：先到先服务（FCFS） 稳态条件 系统达到稳态的条件是 ρ = λ E[S] \u0026lt; 1\n关键性能指标 G/G/1 模型是最一般的单服务器排队模型，通常不具有闭式解。性能指标通常通过近似公式或仿真方法获得。\n主要近似公式\nKendall\u0026rsquo;s 表达式：\n$$ Wq \\approx \\frac{ρ}{1 - ρ} \\left( \\frac{C_a^2 + C_s^2}{2} \\right) \\cdot \\frac{1}{μ} $$其中：\nC_a：到达过程的变异系数（标准差 / 均值） C_s：服务过程的变异系数 μ：服务率 推导过程 G/G/1 模型的详细推导涉及到复合随机过程和复杂的概率论技术，通常不适合手工推导。性能指标需要通过仿真或近似方法获得。\n概率论原理应用 复合随机过程 条件期望 变异系数 近似方法 关键性能指标的推导 关键性能指标包括：\n系统利用率（ρ）：表示系统的繁忙程度。 平均系统内顾客数（L）：系统中平均存在的顾客数量。 平均排队长度（Lq）：排队中顾客的平均数量。 平均等待时间（Wq）：顾客在队列中等待的平均时间。 平均逗留时间（W）：顾客在系统中的平均总时间，包括等待和服务时间。 服务水平（Service Level）：如等待时间小于某值的概率。 Little 定理\nLittle 定理是连接人数与时间的重要工具，表述如下：\n$$ L = λ W $$ $$ Lq = λ Wq $$其中：\nL：系统内平均顾客数 Lq：排队中平均顾客数 λ：到达率 W：系统内平均逗留时间 Wq：排队中平均等待时间 Little 定理的推导基于稳态假设和基本概率原理，它适用于广泛的排队系统。\n概率论原理在推导中的应用 在排队模型的推导过程中，广泛应用了概率论的多个核心原理：\n泊松过程：用于描述到达过程，具有独立增量和无记忆性。 指数分布：服务时间的概率分布，具有无记忆性，使得系统具有 Markov 性质。 马尔可夫链：用于描述系统状态的转移，尤其在 M/M/c 模型中。 稳态平衡方程：确保在稳态条件下，流入率等于流出率。 递推关系：通过递推求解稳态概率。 等比数列求和：在求解 P₀ 时，利用了等比数列的求和公式。 生成函数：在一些高级模型（如 M/G/1）中，用于处理复杂的级数和期望计算。 条件期望：在 P-K 公式的推导中应用，用于处理随机变量的条件期望。 Little 定理：连接人数与时间的期望值，广泛应用于各种排队系统的性能指标计算。 排队理论在计算机网络中路由器节点的应用 排队理论在计算机网络中，特别是在路由器节点的设计与优化中，起着至关重要的作用。通过数学模型，网络工程师能够分析和预测路由器在不同负载条件下的性能，进而做出优化决策。以下将详细介绍排队理论在路由器节点中的具体应用，并结合相关公式进行说明。\n路由器缓冲区设计 应用模型：M/M/1 或 M/M/c 模型\n应用说明：\n路由器的缓冲区（Buffer）用于临时存储等待处理的数据包。缓冲区的设计直接影响路由器的吞吐量、延迟和丢包率。通过应用排队理论中的M/M/1或M/M/c模型，可以分析和优化缓冲区的大小及路由器的服务能力。\nM/M/1模型适用于单处理单元的路由器。 M/M/c模型适用于具有多个处理单元或多核处理器的路由器。 关键参数：\n到达率（λ）：单位时间内到达路由器的数据包数量。 服务率（μ）：单位时间内路由器处理数据包的能力（数据包/秒）。 服务台数量（c）：路由器的处理单元数量。 公式应用：\nM/M/1 模型应用 系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda}{\\mu} $$ρ 表示路由器的繁忙程度。当 ρ 接近 1 时，路由器接近饱和状态，可能导致高延迟和丢包。\n平均系统内数据包数（L）：\n$$ L = \\frac{\\rho}{1 - \\rho} $$ 平均排队长度（Lq）：\n$$ Lq = \\frac{\\rho^2}{1 - \\rho} $$ 平均等待时间（Wq）：\n$$ Wq = \\frac{\\rho}{\\mu - \\lambda} $$ 平均逗留时间（W）：\n$$ W = \\frac{1}{\\mu - \\lambda} $$ 决策影响：\n缓冲区大小：通过计算平均排队长度 Lq，可以确定需要多少缓冲空间以容纳平均等待的数据包数量。例如，如果期望 Lq ≤ 10 数据包，可以通过调整 λ 和 μ 使得 \\(\\frac{\\rho^2}{1 - \\rho} ≤ 10\\)。\n服务能力（μ）：提高路由器的处理能力 μ 可以降低系统利用率 ρ，从而减少平均等待时间和排队长度。可以通过升级硬件或优化处理算法来实现。\nM/M/c 模型应用 对于具有多个处理单元的路由器，M/M/c 模型更为适用。\n系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda}{c \\mu} $$ 无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c \\mu}{c \\mu - \\lambda} \\right]^{-1} $$ 平均排队长度（Lq）：\n$$ Lq = \\frac{P_0 \\cdot (\\lambda/\\mu)^c \\cdot \\rho}{c! (1 - \\rho)^2} $$ 平均系统内数据包数（L）：\n$$ L = Lq + \\frac{\\lambda}{\\mu} $$ 平均等待时间（Wq）：\n$$ Wq = \\frac{Lq}{\\lambda} $$ 平均逗留时间（W）：\n$$ W = Wq + \\frac{1}{\\mu} $$ 决策影响：\n处理单元数量（c）：通过增加处理单元数量 c，可以提高服务率 cμ，从而降低系统利用率 ρ，减少排队长度和等待时间。\n缓冲区优化：计算无阻塞概率 P₀ 和平均排队长度 Lq，可以合理设计缓冲区大小，避免过大或过小带来的资源浪费或高丢包率。\n数据包排队与丢弃策略 应用模型：M/M/1/K 模型（有限容量）\n应用说明：\n在实际网络中，路由器的缓冲区是有限的。当缓冲区满时，新到达的数据包将被丢弃。M/M/1/K模型用于分析有限容量系统下的性能，包括丢包率和系统利用率。\n关键参数：\n系统容量（K）：包括正在处理和排队的数据包总数。 丢包率（P_loss）：新到达数据包被丢弃的概率。 公式应用：\n无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{K} \\frac{(\\lambda/\\mu)^n}{n!} \\right]^{-1} $$ 丢包率（P_{K+1}}）：\n当系统达到容量 K 时，任何新到达的数据包都会被丢弃。丢包率 P_loss 即为系统处于容量 K 时的概率：\n$$ P_{K+1} = P_K \\cdot \\frac{\\lambda}{c \\mu} $$其中 P_K 是系统处于容量 K 时的概率。\n决策影响：\n缓冲区容量（K）：通过分析丢包率与缓冲区容量的关系，确定一个合适的缓冲区大小。例如，若目标是 P_loss ≤ 1%，可通过调整 K 使得 P_K ≤ 1%。\n丢包策略选择：选择合适的数据包丢弃策略（如 FIFO、优先级排队、RED）可以优化网络性能，减少关键数据包的丢失。\n示例计算：\n假设路由器的到达率 λ = 100 数据包/秒，服务率 μ = 120 数据包/秒，系统容量 K = 50。\n计算 P₀：\n$$ P_0 = \\left[ \\sum_{n=0}^{50} \\frac{(100/120)^n}{n!} \\right]^{-1} $$由于 K 很大，近似为 P₀ ≈ 0。\n计算丢包率 P_loss：\n$$ P_loss \\approx P_K = \\frac{(\\lambda/\\mu)^K}{K!} P_0 $$由于 P₀ ≈ 0，P_loss 也非常小，表示缓冲区容量 K = 50 足以满足低丢包率需求。\n负载均衡与资源分配 应用模型：M/M/c 模型\n应用说明：\n在多路由器或多处理单元的网络中，负载均衡策略决定了如何将流量分配到不同的路由器。M/M/c模型帮助分析多服务台系统的性能，如多个处理单元或多个路由器节点的整体性能。\n关键参数：\n到达率（λ）：每个路由器的数据包到达率。 服务率（μ）：每个路由器的服务能力。 服务台数量（c）：处理单元数量。 公式应用：\n使用M/M/c模型中的公式，可以计算系统利用率、平均排队长度等关键性能指标，进而指导负载均衡策略的制定。\n系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda}{c \\mu} $$ 无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c \\mu}{c \\mu - \\lambda} \\right]^{-1} $$ 平均排队长度（Lq）：\n$$ Lq = \\frac{P_0 \\cdot (\\lambda/\\mu)^c \\cdot \\rho}{c! (1 - \\rho)^2} $$ 平均等待时间（Wq）：\n$$ Wq = \\frac{Lq}{\\lambda} $$ 决策影响：\n负载分配策略：根据各路由器的利用率和排队长度，动态调整数据包的分配，避免部分路由器过载而其他路由器闲置。\n资源扩展：通过分析系统利用率，决定是否需要增加处理单元或路由器节点，以提高整体网络的处理能力。\n示例应用：\n假设有 4 个路由器，每个路由器的到达率 λ = 50 数据包/秒，服务率 μ = 100 数据包/秒，服务台数量 c = 2。\n计算系统利用率（ρ）：\n$$ \\rho = \\frac{50}{2 \\times 100} = 0.25 $$ 计算无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{2-1} \\frac{(50/100)^n}{n!} + \\frac{(50/100)^2}{2!} \\cdot \\frac{2 \\times 100}{2 \\times 100 - 50} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.5 + \\frac{0.25}{2} \\cdot \\frac{200}{150} \\right]^{-1} = \\left[ 1 + 0.5 + \\frac{0.125 \\times 200}{150} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.5 + \\frac{25}{150} \\right]^{-1} = \\left[ 1.5 + 0.1667 \\right]^{-1} = \\frac{1}{1.6667} \\approx 0.6 $$ 计算平均排队长度（Lq）：\n$$ Lq = \\frac{0.6 \\times (0.5)^2 \\times 0.25}{2! \\times (1 - 0.25)^2} = \\frac{0.6 \\times 0.25 \\times 0.25}{2 \\times 0.5625} = \\frac{0.0375}{1.125} \\approx 0.033 $$ 计算平均等待时间（Wq）：\n$$ Wq = \\frac{0.033}{50} \\approx 0.00066 \\text{ 秒} = 0.66 \\text{ 毫秒} $$ 决策影响：\n负载均衡：每个路由器的平均等待时间极低，表明当前配置下路由器处理能力充足，可以维持现有负载。\n资源分配：若某个路由器的负载增加，需重新分配数据包流量或增加处理单元，以保持低延迟和高吞吐量。\n路由器决策中的排队理论应用实例 实例：动态调整路由器服务速率以适应流量变化\n应用模型：M/G/1 模型\n应用说明：\n在高动态网络环境中，数据包的服务时间可能变化较大，例如不同类型的数据包可能需要不同的处理时间。M/G/1模型适用于这种情况，可以帮助路由器动态调整服务速率，以适应不同流量条件下的性能需求。\n关键参数：\n到达率（λ）：每秒到达的数据包数量。 服务时间（S）：数据包处理时间，具有一般分布，具有均值 E[S] 和二阶矩 E[S²]。 服务率（μ）：μ = 1 / E[S] 公式应用：\n系统利用率（ρ）：\n$$ \\rho = \\lambda E[S] $$ 平均等待时间（Wq）（通过 Pollaczek-Khinchin 公式）：\n$$ Wq = \\frac{\\lambda E[S^2]}{2(1 - \\rho)} $$ 平均排队长度（Lq）：\n$$ Lq = \\lambda Wq = \\frac{\\lambda^2 E[S^2]}{2(1 - \\rho)} $$ 决策影响：\n动态调整服务速率（μ）：根据当前流量情况，调整处理能力。例如，在高流量时提升 μ 以降低 ρ，减少延迟；在低流量时降低 μ 以节省能源。\n队列管理策略：根据等待时间和排队长度，调整数据包的优先级或采取不同的排队策略，以优化网络性能。\n示例计算：\n假设路由器当前的到达率 λ = 80 数据包/秒，服务时间 S 服从一般分布，E[S] = 0.01 秒，Var(S) = 0.002 秒²。\n计算系统利用率（ρ）：\n$$ \\rho = \\lambda E[S] = 80 \\times 0.01 = 0.8 $$系统处于高负载状态，接近饱和。\n计算二阶矩 E[S²]：\n$$ E[S^2] = Var(S) + (E[S])^2 = 0.002 + 0.0001 = 0.0021 \\text{ 秒}² $$ 计算平均等待时间（Wq）：\n$$ Wq = \\frac{80 \\times 0.0021}{2(1 - 0.8)} = \\frac{0.168}{0.4} = 0.42 \\text{ 秒} $$ 计算平均排队长度（Lq）：\n$$ Lq = 80 \\times 0.42 = 33.6 \\text{ 数据包} $$ 决策措施：\n提升服务速率：由于 ρ = 0.8，路由器接近饱和状态，建议提升处理能力，例如增加处理单元或优化算法，使 μ 增大，降低 ρ 至 0.7 以下。\n优化队列管理：根据高排队长度 Lq，可以引入优先级队列或数据包调度算法，优先处理关键数据包，减少关键数据包的延迟。\n优先级排队策略 应用模型：M/M/1 with Priority Queues 或 M/M/c with Priority Queues\n应用说明：\n在实际网络中，不同类型的数据包可能具有不同的优先级（如实时视频数据包优先于普通网页数据包）。通过引入优先级排队策略，可以优化关键数据包的传输性能。\n关键参数：\n高优先级数据包到达率（λ₁） 低优先级数据包到达率（λ₂） 服务率（μ） 公式应用：\n系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda₁ + \\lambda₂}{\\mu} $$ 高优先级数据包的平均等待时间（Wq₁）：\n$$ Wq₁ = \\frac{\\rho}{\\mu - \\lambda₁ - \\lambda₂} $$ 低优先级数据包的平均等待时间（Wq₂）：\n$$ Wq₂ = \\frac{\\lambda₁}{\\mu (\\mu - \\lambda₁ - \\lambda₂)} + \\frac{\\rho}{\\mu - \\lambda₁ - \\lambda₂} $$ 决策影响：\n数据包优先级设置：通过设定不同的数据包优先级，可以确保关键数据包（如语音、视频）的低延迟传输。\n资源分配：根据优先级调整资源分配，例如为高优先级数据包分配更多的处理资源或更快的服务速率。\n示例应用：\n假设路由器处理两类数据包：\n高优先级：λ₁ = 30 数据包/秒，μ = 100 数据包/秒 低优先级：λ₂ = 50 数据包/秒 计算系统利用率（ρ）：\n$$ \\rho = \\frac{30 + 50}{100} = 0.8 $$ 计算高优先级数据包的平均等待时间（Wq₁）：\n$$ Wq₁ = \\frac{0.8}{100 - 30 - 50} = \\frac{0.8}{20} = 0.04 \\text{ 秒} $$ 计算低优先级数据包的平均等待时间（Wq₂）：\n$$ Wq₂ = \\frac{30}{100 \\times (100 - 30 - 50)} + \\frac{0.8}{100 - 30 - 50} = \\frac{30}{100 \\times 20} + \\frac{0.8}{20} = 0.015 + 0.04 = 0.055 \\text{ 秒} $$ 决策措施：\n优化优先级策略：确保高优先级数据包具有较低的等待时间，通过调整处理速率或增加高优先级数据包的处理资源。\n监控系统利用率：保持系统利用率在合理范围（如 ρ ≤ 0.7），以避免过高的等待时间和排队长度。\n排队理论在路由器中的综合应用 综合应用：\n在实际网络环境中，路由器节点需要同时考虑缓冲区设计、数据包排队与丢弃策略、负载均衡与资源分配以及优先级排队策略。以下是一个综合应用的示例：\n实例：设计一个多核路由器的缓冲区与负载均衡策略\n场景描述：\n路由器：4核处理器（c = 4），每核服务率 μ = 150 数据包/秒。 数据包到达率：高优先级 λ₁ = 200 数据包/秒，低优先级 λ₂ = 300 数据包/秒。 缓冲区容量：K = 1000 数据包。 目标：确保高优先级数据包的丢包率 ≤ 0.1%，低优先级数据包的丢包率 ≤ 1%。 步骤：\n计算系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda₁ + \\lambda₂}{c \\mu} = \\frac{200 + 300}{4 \\times 150} = \\frac{500}{600} \\approx 0.833 $$系统处于高负载状态，需优化。\n计算无阻塞概率（P₀）（使用 M/M/c/K 模型）：\n$$ P_0 = \\left[ \\sum_{n=0}^{4-1} \\frac{(500/600)^n}{n!} + \\frac{(500/600)^4}{4!} \\cdot \\frac{4 \\times 150}{4 \\times 150 - 500} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + \\frac{5}{6} + \\frac{(5/6)^2}{2} + \\frac{(5/6)^3}{6} + \\frac{(5/6)^4}{24} \\cdot \\frac{600}{100} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.833 + 0.347 + 0.115 + \\frac{0.482}{24} \\times 6 \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.833 + 0.347 + 0.115 + 0.120 \\right]^{-1} = \\left[ 2.415 \\right]^{-1} \\approx 0.414 $$ 计算丢包率：\n丢包率（P_loss）：\n$$ P_loss = P_K = P_0 \\cdot \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c \\mu}{c \\mu - \\lambda} = 0.414 \\times \\frac{(500/600)^4}{24} \\times \\frac{600}{100} $$ $$ P_loss = 0.414 \\times \\frac{0.482}{24} \\times 6 \\approx 0.414 \\times 0.120 \\approx 0.0497 \\approx 4.97\\% $$目前丢包率超过目标，需要优化。\n优化措施：\n增加服务率（μ）：提高每核处理能力，降低 ρ。 增加服务台数量（c）：增加处理单元，降低系统利用率。 扩展缓冲区容量（K）：减少丢包概率。 调整参数：\n假设通过优化，增加每核服务率至 μ = 200 数据包/秒。\n新的系统利用率（ρ）：\n$$ \\rho = \\frac{200 + 300}{4 \\times 200} = \\frac{500}{800} = 0.625 $$ 重新计算无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{3} \\frac{(500/800)^n}{n!} + \\frac{(500/800)^4}{4!} \\cdot \\frac{800}{800 - 500} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.625 + \\frac{0.3906}{2} + \\frac{0.2441}{6} + \\frac{0.1526}{24} \\times \\frac{800}{300} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.625 + 0.1953 + 0.0407 + 0.1526 \\right]^{-1} \\approx \\left[ 2.013 \\right]^{-1} \\approx 0.497 $$ 重新计算丢包率（P_loss）：\n$$ P_loss = 0.497 \\times \\frac{(500/800)^4}{24} \\times \\frac{800}{300} \\approx 0.497 \\times \\frac{0.2441}{24} \\times 2.666 \\approx 0.497 \\times 0.0102 \\times 2.666 \\approx 0.0135 \\approx 1.35\\% $$仍高于高优先级目标，进一步优化。\n进一步调整：增加服务台数量 c = 5，保持 μ = 200 数据包/秒。\n新的系统利用率（ρ）：\n$$ \\rho = \\frac{200 + 300}{5 \\times 200} = \\frac{500}{1000} = 0.5 $$ 重新计算无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{4} \\frac{(500/1000)^n}{n!} + \\frac{(500/1000)^5}{5!} \\cdot \\frac{1000}{1000 - 500} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 0.5 + 0.125 + 0.0208 + 0.0026 + \\frac{0.03125}{120} \\times 2 \\right]^{-1} \\approx \\left[ 1 + 0.5 + 0.125 + 0.0208 + 0.0026 + 0.00052 \\right]^{-1} \\approx \\left[ 1.648 \\right]^{-1} \\approx 0.607 $$ 重新计算丢包率（P_loss）：\n$$ P_loss = 0.607 \\times \\frac{(500/1000)^5}{120} \\times \\frac{1000}{500} = 0.607 \\times \\frac{0.03125}{120} \\times 2 = 0.607 \\times 0.0002604 \\times 2 \\approx 0.000315 \\approx 0.0315\\% $$ 最终结果：丢包率 P_loss ≈ 0.0315%，满足高优先级数据包的丢包率目标（≤ 0.1%），同时低优先级数据包的丢包率也得到了有效控制。\n决策措施：\n增加处理单元数量（c）：通过增加服务台数量，降低系统利用率 ρ，显著减少丢包率。\n优化服务速率（μ）：提升处理速率，进一步降低 ρ，优化系统性能。\n缓冲区管理：根据调整后的丢包率，确认缓冲区容量是否满足需求，确保系统在高负载下依然稳定运行。\n路由器缓冲区扩展与拥塞控制 应用模型：M/M/c/K 模型与 RED（Random Early Detection）策略\n应用说明：\n在高流量条件下，路由器缓冲区可能会出现拥塞，导致高延迟和丢包。通过结合M/M/c/K模型与拥塞控制策略（如RED），可以有效管理网络流量，预防拥塞发生。\n关键参数：\n缓冲区容量（K） 丢包概率（P_loss） 拥塞阈值（minimum threshold, maximum threshold） 公式应用：\n系统利用率（ρ）：\n$$ \\rho = \\frac{\\lambda}{c \\mu} $$ 无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{c-1} \\frac{(\\lambda/\\mu)^n}{n!} + \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c \\mu}{c \\mu - \\lambda} \\right]^{-1} $$ 丢包率（P_loss）：\n$$ P_loss = P_K = P_0 \\cdot \\frac{(\\lambda/\\mu)^c}{c!} \\cdot \\frac{c \\mu}{c \\mu - \\lambda} $$ 决策影响：\n缓冲区容量调整：根据流量预测和系统利用率，调整缓冲区容量以平衡延迟和丢包率。\n拥塞控制策略：采用RED等策略，提前检测和防止拥塞，通过随机丢包降低系统负载，保持系统稳定。\n示例应用：\n假设路由器有 3 个处理单元（c = 3），服务率 μ = 100 数据包/秒，数据包到达率 λ = 250 数据包/秒，缓冲区容量 K = 100。\n计算系统利用率（ρ）：\n$$ \\rho = \\frac{250}{3 \\times 100} \\approx 0.833 $$ 计算无阻塞概率（P₀）：\n$$ P_0 = \\left[ \\sum_{n=0}^{2} \\frac{(250/100)^n}{n!} + \\frac{(250/100)^3}{3!} \\cdot \\frac{300}{300 - 250} \\right]^{-1} = \\left[ 1 + 2.5 + \\frac{6.25}{2} + \\frac{15.625}{6} \\cdot \\frac{300}{50} \\right]^{-1} $$ $$ P_0 = \\left[ 1 + 2.5 + 3.125 + \\frac{15.625 \\times 6}{6} \\right]^{-1} = \\left[ 1 + 2.5 + 3.125 + 15.625 \\right]^{-1} = \\left[ 22.25 \\right]^{-1} \\approx 0.045 $$ 计算丢包率（P_loss）：\n$$ P_loss = P_K = 0.045 \\times \\frac{(2.5)^3}{6} \\times \\frac{300}{50} = 0.045 \\times \\frac{15.625}{6} \\times 6 = 0.045 \\times 15.625 = 0.703 \\approx 70.3\\% $$丢包率过高，需优化。\n采用RED策略：\n设置最小阈值（min_threshold）：例如，当缓冲区中数据包数超过 50 时，开始随机丢包。 设置最大阈值（max_threshold）：例如，当缓冲区中数据包数超过 80 时，大幅增加丢包概率。 调整缓冲区容量：\n若缓冲区容量增加至 K = 200：\n重新计算 P₀ 和 P_loss：\n$$ P_0 = \\left[ \\sum_{n=0}^{2} \\frac{2.5^n}{n!} + \\frac{2.5^3}{6} \\cdot \\frac{300}{50} + \\sum_{n=4}^{200} \\frac{2.5^n}{6} \\cdot \\frac{300}{50} \\right]^{-1} $$由于K增加，P_loss 将显著降低，满足较低的丢包率需求。\n决策措施：\n合理设置RED阈值：通过流量分析，确定合适的最小和最大阈值，以有效预防拥塞。\n优化缓冲区容量：在保证低丢包率的前提下，尽量减少缓冲区容量，以节省资源。\nOther Link\nhttps://zhuanlan.zhihu.com/p/375681567 https://zhuanlan.zhihu.com/p/90923312 https://blog.csdn.net/qq_39543783/article/details/107915998 https://blog.csdn.net/Douglassssssss/article/details/134063217 ","date":"2024-12-02T22:52:25+08:00","permalink":"https://echudet.github.io/p/netmath/","title":"Netmath"},{"content":"目录 引言 *PyTorch 模型的基本组成 2.1 模型架构（Architecture） 主干网络（Backbone） 特征金字塔（Feature Pyramid） 特征融合模块（Neck） 头部网络（Head） 2.2 模型参数（Parameters） 2.3 模型配置 Swin Transformer 的详细分析 3.1 *模型概述 3.2 *关键名词解释 3.3 Patch Embedding 3.4 Swin Transformer Block 3.4.1 多头注意力机制（Multi-Head Self-Attention） 3.4.2 窗口注意力（Window Attention） 3.4.3 移动窗口（Shifted Window）机制 3.4.4 前馈网络（Feed-Forward Network, FFN） 3.5 *Stage 和 Block 的层次结构 3.6 模型参数的调整 3.7 *模型的层级结构和通道的关系 MiDaS 模型中的应用 4.1 模型结构图 4.2 特征提取与融合 4.3 深度估计的实现 *模型调用流程 5.1 模型加载与初始化 5.2 数据加载与预处理 5.3 前向传播过程 5.4 后处理与输出 *关键名词解释 6.1 Block（块） 6.2 Patch（补丁） 6.3 Depth（深度） 6.4 Head（注意力头） 6.5 embed_dim（嵌入维度） 6.6 Channels（通道数） 6.7 Hooks（特征提取点） 6.8 .pt（模型文件） 模型训练与参数调整 7.1 损失函数与优化器 7.2 训练循环 7.3 超参数的影响 引言 PyTorch 是一个广泛使用于搞研究的深度学习框架，提供了灵活的模型定义和训练方式。理解 PyTorch 模型的结构和工作原理对于赶项目出成果来说至关重要。本分析将深入探讨 PyTorch 模型的组成部分，以 Swin Transformer 和 MiDaS 模型为例，尝试阐述各部分的功能和相互作用，并提供模型的调用流程和关键名词的解释。\nPyTorch 模型的基本组成 PyTorch 模型通常由以下几个主要组件组成：\n主干网络（Backbone）：负责从输入中提取特征。 特征金字塔（Feature Pyramid）：在不同尺度上提取特征，捕获多尺度信息。 特征融合模块（Neck）：融合不同层次的特征。 头部网络（Head）：根据融合后的特征进行预测。 模型架构（Architecture） 主干网络（Backbone） 定义：主干网络是模型的核心部分，包含多个层和块（blocks），负责从输入数据中提取初级和高级特征。 常用模型：ResNet、VGG、DenseNet、Transformer 等。 在 Swin Transformer 中：\nSwin Transformer 作为主干网络，利用 Transformer 的自注意力机制进行特征提取。 特征金字塔（Feature Pyramid） 定义：特征金字塔网络（FPN）在不同的尺度上提取特征，以捕获多尺度信息并提取不同分辨率的特征图。 作用：帮助模型同时关注细节和全局信息。 在 Swin Transformer 中：\n模型通过多个阶段（Stage）形成特征金字塔，每个阶段提取不同尺度的特征。 特征融合模块（Neck） 定义：特征融合模块将来自不同层次的特征进行整合。 作用：增强特征表示，提高模型的预测能力。 在 MiDaS 中：\n使用特征融合网络，将来自不同 Stage 的特征进行融合，以生成丰富的特征表示。 头部网络（Head） 定义：根据融合后的特征进行最终的预测任务，如分类、检测、分割等。 结构：通常由一系列卷积层或全连接层组成。 在 MiDaS 中：\n头部网络根据融合后的特征，输出深度估计结果，将特征转换为最终输出（深度图）。 模型参数（Parameters） 权重（Weights）： 每一层的变换矩阵 通过预训练获得 偏置（Biases）： 每一层的偏置项 归一化参数： 批归一化（Batch Norm）的均值和方差 层归一化（Layer Norm）的参数 模型配置 超参数： 输入大小：384x384 补丁大小：4x4 窗口大小：12-\u0026gt;24 头数：多头注意力中的头数 隐藏维度：特征维度 训练配置： 预训练数据集：ImageNet-22K 微调数据集：ImageNet-1K 学习率、批量大小等 Swin Transformer 的详细分析 模型概述 Swin Transformer 是一种用于计算机视觉的 Transformer 模型，具有以下特点：\n层次化设计：通过分层结构，逐步提取高层次特征。 窗口注意力机制：在局部窗口内计算注意力，降低计算复杂度。 移动窗口机制：通过移动窗口，捕获跨窗口的全局信息。 模型结构图：\n下图中对于stage中的输入显示的是序列形式 [B, N, C]，但也存在另一种形式，空间形式 [B, C, H, W]。\n1 2 3 4 5 6 7 8 9 10 11 12 13 特征表示的两种形式： 1. 序列形式 [B, N, C]： stage1: [B, 96*96, 128] # B=批次大小, N=序列长度, C=特征维度 stage2: [B, 48*48, 256] stage3: [B, 24*24, 512] stage4: [B, 12*12, 1024] 2. 空间形式 [B, C, H, W]： stage1: [B, 128, 96, 96] # B=批次大小, C=通道数, H=高度, W=宽度 stage2: [B, 256, 48, 48] stage3: [B, 512, 24, 24] stage4: [B, 1024, 12, 12] 它们的对应关系：\n1 2 3 4 5 6 7 8 # 序列形式 -\u0026gt; 空间形式的转换 [B, H*W, C] -\u0026gt; [B, C, H, W] # 具体例子： stage1: [1, 96*96, 128] -\u0026gt; [1, 128, 96, 96] # 重排序列为96x96的特征图 stage2: [1, 48*48, 256] -\u0026gt; [1, 256, 48, 48] # 重排序列为48x48的特征图 stage3: [1, 24*24, 512] -\u0026gt; [1, 512, 24, 24] # 重排序列为24x24的特征图 stage4: [1, 12*12, 1024] -\u0026gt; [1, 1024, 12, 12] # 重排序列为12x12的特征图 在代码中的转换：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 在swin_common.py中的实现 class Transpose(nn.Module): def __init__(self, dim0, dim1): super().__init__() self.dim0 = dim0 self.dim1 = dim1 def forward(self, x): # 转换维度顺序 x = x.transpose(self.dim0, self.dim1) return x # 特征后处理 pretrained.act_postprocess1 = nn.Sequential( Transpose(1, 2), # [B, N, C] -\u0026gt; [B, C, N] nn.Unflatten(2, torch.Size([96, 96])) # [B, C, N] -\u0026gt; [B, C, H, W] ) 修改后的完整结构图（添加两种表示形式）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Stage 1: 输入 -\u0026gt; PatchEmbed -\u0026gt; Transformer Blocks -\u0026gt; 输出 [1, 3, 384, 384] -\u0026gt; [1, 96*96, 128] -\u0026gt; 处理 -\u0026gt; [1, 96*96, 128] -\u0026gt; [1, 128, 96, 96] Stage 2: 输入 -\u0026gt; Transformer Blocks -\u0026gt; 输出 [1, 128, 96, 96] -\u0026gt; [1, 48*48, 256] -\u0026gt; 处理 -\u0026gt; [1, 48*48, 256] -\u0026gt; [1, 256, 48, 48] Stage 3: 输入 -\u0026gt; Transformer Blocks -\u0026gt; 输出 [1, 256, 48, 48] -\u0026gt; [1, 24*24, 512] -\u0026gt; 处理 -\u0026gt; [1, 24*24, 512] -\u0026gt; [1, 512, 24, 24] Stage 4: 输入 -\u0026gt; Transformer Blocks -\u0026gt; 输出 [1, 512, 24, 24] -\u0026gt; [1, 12*12, 1024] -\u0026gt; 处理 -\u0026gt; [1, 12*12, 1024] -\u0026gt; [1, 1024, 12, 12] 关键点：\nTransformer内部使用序列形式 [B, N, C] 卷积操作使用空间形式 [B, C, H, W] 两种形式可以相互转换 特征维度（C）和空间分辨率（H,W）在两种形式中都保持不变 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 MidasNet (整体模型) [B=batch_size] │ ├── 1. Backbone (SwinTransformer) │ │ │ ├── patch_embed (PatchEmbedding) │ │ ├── 输入图像: [B, 3, 384, 384] │ │ ├── patch划分: 4x4, 得到96x96个patch │ │ ├── proj (Linear) │ │ │ ├── weight: [128, 48] # 48=4x4x3 │ │ │ └── bias: [128] │ │ └── 输出: [B, 96*96, 128] │ │ │ ├── stage1 (BasicLayer) - 特征金字塔第一层 │ │ ├── 输入: [B, 96*96, 128] │ │ ├── blocks[0] (SwinTransformerBlock) │ │ │ ├── norm1 (LayerNorm) │ │ │ │ ├── weight: [128] │ │ │ │ └── bias: [128] │ │ │ ├── attn (WindowAttention) - 4个头 │ │ │ │ ├── qkv (Linear) │ │ │ │ │ ├── weight: [384, 128] # 384=128*3(Q,K,V) │ │ │ │ │ └── bias: [384] │ │ │ │ ├── 4个头每个处理: [B, window_size^2, 32] │ │ │ │ ├── proj (Linear) │ │ │ │ │ ├── weight: [128, 128] │ │ │ │ │ └── bias: [128] │ │ │ │ └── relative_position_bias: [169, 4] │ │ │ ├── norm2 (LayerNorm): [128] │ │ │ └── mlp (MLP) │ │ │ ├── fc1: [128] -\u0026gt; [512] │ │ │ └── fc2: [512] -\u0026gt; [128] │ │ ├── blocks[1]: 结构同上 │ │ ├── downsample │ │ │ ├── reduction: [512] -\u0026gt; [256] │ │ │ └── norm: [512] │ │ └── 输出: [B, 48*48, 256] │ │ │ ├── stage2 (BasicLayer) - 特征金字塔第二层 │ │ ├── 输入: [B, 48*48, 256] │ │ ├── blocks[0,1] - 结构同stage1 │ │ │ ├── 8个注意力头 │ │ │ ├── 每个头: [B, window_size^2, 32] │ │ │ └── mlp维度: 256 -\u0026gt; 1024 -\u0026gt; 256 │ │ ├── downsample: [256] -\u0026gt; [512] │ │ └── 输出: [B, 24*24, 512] │ │ │ ├── stage3 (BasicLayer) - 特征金字塔第三层 │ │ ├── 输入: [B, 24*24, 512] │ │ ├── blocks[0-5] - 6个Block │ │ │ ├── 16个注意力头 │ │ │ ├── 每个头: [B, window_size^2, 32] │ │ │ └── mlp维度: 512 -\u0026gt; 2048 -\u0026gt; 512 │ │ ├── downsample: [512] -\u0026gt; [1024] │ │ └── 输出: [B, 12*12, 1024] │ │ │ └── stage4 (BasicLayer) - 特征金字塔第四层 │ ├── 输入: [B, 12*12, 1024] │ ├── blocks[0-1] - 2个Block │ │ ├── 32个注意力头 │ │ ├── 每个头: [B, window_size^2, 32] │ │ └── mlp维度: 1024 -\u0026gt; 4096 -\u0026gt; 1024 │ └── 输出: [B, 6*6, 1024] │ ├── 2. Neck (特征融合网络) │ ├── 输入特征维度: │ │ ├── feat1: [B, 96*96, 128] │ │ ├── feat2: [B, 48*48, 256] │ │ ├── feat3: [B, 24*24, 512] │ │ └── feat4: [B, 12*12, 1024] │ ├── 融合层1: feat4 [1024] + feat3 [512] -\u0026gt; [512] │ ├── 融合层2: feat3 [512] + feat2 [256] -\u0026gt; [256] │ └── 融合层3: feat2 [256] + feat1 [128] -\u0026gt; [256] │ └── 3. Head (输出头) ├── 输入: [B, H, W, 256] ├── Conv2d: [256] -\u0026gt; [256] ├── ReLU ├── Conv2d: [256] -\u0026gt; [1] └── 输出: [B, H, W, 1] (深度图) 特征金字塔在这个结构中体现为：\nStage1: 96x96, 128维 (4个头) Stage2: 48x48, 256维 (8个头) Stage3: 24x24, 512维 (16个头) Stage4: 12x12, 1024维 (32个头) 每个Stage都是特征金字塔的一层，随着层数增加：\n空间分辨率降低（图像尺寸变小） 通道数增加（特征维度增加） 注意力头数量增加（更多的\u0026quot;专家\u0026quot;视角） 所以：\nBackbone就是这整个SwinTransformer结构 特征金字塔就是这四个Stage形成的多尺度特征 注意力头在每个Stage的WindowAttention模块中 特征融合在Neck部分进行 这就像一个层层递进的系统：\n底层Stage：大量简单特征，少量注意力头 高层Stage：少量复杂特征，大量注意力头 Neck：将各层特征智能融合 Head：生成最终输出 关键名词解释 在深入分析模型之前，先对一些关键名词进行解释：\nPatch（补丁）：将输入图像划分为固定大小的小块，每个小块称为一个补丁（Patch）。 Embed_dim（嵌入维度）：每个补丁被映射为高维特征向量，嵌入维度指这个特征向量的维度。 Block（块）：Swin Transformer 的基本组成单元，包括注意力机制和前馈网络等。 Depth（深度）：每个 Stage 中包含的 Block 数量，决定了模型的深度。 Head（注意力头）：在多头注意力机制中，多个独立的注意力计算单元称为注意力头。 Patch Embedding 功能：\n将输入图像划分为固定大小的补丁（Patch）。 将每个补丁映射为高维特征向量。 实现：\n1 2 3 4 5 6 7 8 9 10 11 class PatchEmbed(nn.Module): def __init__(self, patch_size=4, in_channels=3, embed_dim=128): super().__init__() self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) self.norm = nn.LayerNorm(embed_dim) def forward(self, x): x = self.proj(x) # 卷积映射 x = x.flatten(2).transpose(1, 2) # 展平并交换维度 x = self.norm(x) return x 解释：\nPatch（补丁）：将输入图像按照 patch_size 划分为多个小块。例如，patch_size=4 时，输入 384x384 的图像会被划分为 96x96 个补丁。 Embed_dim（嵌入维度）：每个补丁通过卷积映射，转换为 embed_dim=128 维的特征向量。 Swin Transformer Block Swin Transformer Block 是模型的基本组成单元，包含以下组件：\nLayerNorm 层：归一化输入特征。 多头自注意力层（使用窗口注意力）。 残差连接：保留输入特征，促进梯度传播。 前馈网络（FFN）：进一步处理特征。 多头注意力机制（Multi-Head Self-Attention） 功能：\n捕获输入特征中不同位置之间的关联关系。 不同的注意力头可以关注不同的特征模式。 实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class MultiHeadSelfAttention(nn.Module): def __init__(self, dim, num_heads): super().__init__() self.num_heads = num_heads self.qkv = nn.Linear(dim, dim * 3) self.proj = nn.Linear(dim, dim) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads) q, k, v = qkv.unbind(dim=2) # 计算注意力得分并应用于值向量 # ... x = self.proj(attention_output) return x 解释：\nHead（注意力头）：将特征维度 C 分为 num_heads 个部分，每个部分独立计算注意力。 多头的优势：并行处理，可以关注不同的特征模式，增强模型的表达能力。 窗口注意力（Window Attention） 功能：\n在局部窗口内计算自注意力，降低计算复杂度。 每个窗口独立计算，不同窗口之间不共享信息。 实现：\n1 2 3 4 5 6 7 8 9 10 11 class WindowAttention(nn.Module): def __init__(self, dim, window_size, num_heads): super().__init__() self.window_size = window_size self.attn = MultiHeadSelfAttention(dim, num_heads) def forward(self, x): # 将输入特征划分为窗口 # 对每个窗口应用多头自注意力 # ... return x 解释：\n窗口大小（window_size）：定义局部计算注意力的范围。例如，window_size=7 时，每个 7x7 的区域作为一个窗口。 优势：降低计算复杂度，使得模型能够处理高分辨率图像。 移动窗口（Shifted Window）机制 功能：\n通过在不同的 Swin Transformer Block 中移动窗口，实现跨窗口的信息交流。 增强模型的全局建模能力。 实现：\n标准窗口划分：在偶数层，直接按照窗口大小划分。 移动窗口划分：在奇数层，将特征图平移半个窗口大小，然后再进行窗口划分。 解释：\n移动窗口的目的：让相邻窗口之间的信息能够互相交流，克服窗口注意力的局部性限制。 前馈网络（Feed-Forward Network, FFN） 功能：\n对注意力层的输出进行进一步处理。 通常由两个线性层和一个激活函数组成。 实现：\n1 2 3 4 5 6 7 8 9 10 11 12 class FeedForwardNetwork(nn.Module): def __init__(self, dim, hidden_dim): super().__init__() self.fc1 = nn.Linear(dim, hidden_dim) self.act = nn.GELU() self.fc2 = nn.Linear(hidden_dim, dim) def forward(self, x): x = self.fc1(x) x = self.act(x) x = self.fc2(x) return x 解释：\n扩展维度：通常将特征维度扩大 4 倍（hidden_dim = 4 * dim），然后再缩小回原来的维度。 作用：增加模型的非线性表达能力。 Stage 和 Block 的层次结构 整体结构：\nStage：模型被划分为多个阶段，每个阶段处理不同尺度的特征。 Block（块）：每个 Stage 包含多个 Swin Transformer Block。 示例（以 Swin V2-Base 为例）：\nEmbed_dim：128。 Depths（深度）：[2, 2, 18, 2]，表示每个 Stage 包含的 Block 数量。 Stage 1：2 个 Block。 Stage 2：2 个 Block。 Stage 3：18 个 Block。 Stage 4：2 个 Block。 Num_heads（注意力头数量）：[4, 8, 16, 32]，表示每个 Stage 中的注意力头数量。 解释：\nDepth（深度）：决定了每个 Stage 的深度，Block 数量越多，模型可以提取更复杂的特征。\ndepths设置的原理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026#34;\u0026#34;\u0026#34; 为什么Stage3的depth最大（6）？ 1. 计算复杂度考虑： - Stage1: 96x96分辨率，depth小可以节省计算 - Stage3: 24x24分辨率，可以承担更多计算 2. 特征提取需求： - Stage1: 主要处理低级特征，不需要太多block - Stage3: 需要提取复杂的语义特征，需要更多block 3. 信息流动： Stage1 (2 blocks) ↓ [降采样] Stage2 (2 blocks) ↓ [降采样] Stage3 (6 blocks) ← 关键特征提取阶段 ↓ [降采样] Stage4 (2 blocks) \u0026#34;\u0026#34;\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # 1. 浅层配置 shallow_depths = [2, 2, 2, 2] shallow_effects = { \u0026#39;优点\u0026#39;: { \u0026#39;计算速度\u0026#39;: \u0026#39;快\u0026#39;, \u0026#39;内存占用\u0026#39;: \u0026#39;小\u0026#39;, \u0026#39;训练简单\u0026#39;: True }, \u0026#39;缺点\u0026#39;: { \u0026#39;特征提取\u0026#39;: \u0026#39;不够深入\u0026#39;, \u0026#39;模型表达能力\u0026#39;: \u0026#39;有限\u0026#39;, \u0026#39;复杂场景处理\u0026#39;: \u0026#39;欠佳\u0026#39; } } # 2. 深层配置 deep_depths = [2, 2, 18, 2] deep_effects = { \u0026#39;优点\u0026#39;: { \u0026#39;特征提取\u0026#39;: \u0026#39;非常深入\u0026#39;, \u0026#39;模型表达能力\u0026#39;: \u0026#39;强\u0026#39;, \u0026#39;复杂场景处理\u0026#39;: \u0026#39;优秀\u0026#39; }, \u0026#39;缺点\u0026#39;: { \u0026#39;计算速度\u0026#39;: \u0026#39;慢\u0026#39;, \u0026#39;内存占用\u0026#39;: \u0026#39;大\u0026#39;, \u0026#39;训练难度\u0026#39;: \u0026#39;高\u0026#39; } } # 3. 平衡配置（MiDaS使用） balanced_depths = [2, 2, 6, 2] balanced_effects = { \u0026#39;优点\u0026#39;: { \u0026#39;特征提取\u0026#39;: \u0026#39;充分\u0026#39;, \u0026#39;计算效率\u0026#39;: \u0026#39;适中\u0026#39;, \u0026#39;内存占用\u0026#39;: \u0026#39;合理\u0026#39; }, \u0026#39;缺点\u0026#39;: { \u0026#39;可能不适合\u0026#39;: \u0026#39;特别复杂的场景\u0026#39; } } Head（注意力头）：注意力头数量随着 Stage 的深入而增加，能够捕获更丰富的特征模式。\n注意力头的工作机制：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class WindowAttention: def __init__(self, dim=128, num_heads=4): # dim: 输入特征维度（例如128） # num_heads: 注意力头数量（例如4） self.head_dim = dim // num_heads # 每个头的维度（128/4=32） def forward(self, x): # x: [B, N, C] # 1. 将输入分给各个头 qkv = self.qkv(x) # [B, N, 3*C] qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim) # 2. 每个头独立工作 \u0026#34;\u0026#34;\u0026#34; 以4个头为例： 头1：专注于边缘特征 - 查询：这个位置有强边缘吗？ - 键值对：其他位置的边缘信息 头2：专注于纹理模式 - 查询：这里的纹理是什么样的？ - 键值对：相似纹理的区域 头3：专注于色彩分布 - 查询：这个区域的颜色特征？ - 键值对：相似颜色的区域 头4：专注于形状结构 - 查询：这里是什么形状？ - 键值对：相似形状的区域 \u0026#34;\u0026#34;\u0026#34; 头数增加的影响：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Stage1: 4个头 attention_stage1 = { \u0026#39;head_dim\u0026#39;: 128 // 4, # 每个头32维 \u0026#39;capacity\u0026#39;: \u0026#39;基础特征提取\u0026#39;, \u0026#39;complexity\u0026#39;: \u0026#39;低\u0026#39;, \u0026#39;computation\u0026#39;: \u0026#39;少\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;小\u0026#39; } # Stage4: 32个头 attention_stage4 = { \u0026#39;head_dim\u0026#39;: 1024 // 32, # 每个头32维 \u0026#39;capacity\u0026#39;: \u0026#39;复杂特征提取\u0026#39;, \u0026#39;complexity\u0026#39;: \u0026#39;高\u0026#39;, \u0026#39;computation\u0026#39;: \u0026#39;多\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;大\u0026#39; } \u0026#34;\u0026#34;\u0026#34; 头数增加的效果： 1. 优点： - 可以同时关注更多不同类型的特征 - 提高模型的表达能力 - 增强特征的多样性 2. 缺点： - 计算量增加 - 内存消耗增加 - 可能过度拟合（如果数据不足） \u0026#34;\u0026#34;\u0026#34; Block\n关键点：\nBlock不需要手动写：\n使用timm库提供的预定义模型 block的结构已经在模型中定义好了 Block的数量由depths控制：\ndepths=[2,2,6,2]表示每个stage的block数量\n这些block会自动创建\nBlock的参数会随着stage变化：\n维度会翻倍：128-\u0026gt;256-\u0026gt;512-\u0026gt;1024\n注意力头数会增加：4-\u0026gt;8-\u0026gt;16-\u0026gt;32\n特征提取通过hooks实现：\n指定要提取特征的block位置\n自动注册hook函数来获取中间特征\nBlock的创建：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 在timm库中，Swin Transformer的block是这样定义的： class SwinTransformerBlock(nn.Module): def __init__(self, dim, num_heads, window_size=7): super().__init__() # 1. 注意力层 self.attn = WindowAttention( dim=dim, # 特征维度 num_heads=num_heads, # 注意力头数 window_size=window_size # 窗口大小 ) # 2. MLP层 self.mlp = Mlp( in_features=dim, hidden_features=dim * 4, act_layer=nn.GELU ) # 3. 层归一化 self.norm1 = nn.LayerNorm(dim) self.norm2 = nn.LayerNorm(dim) Block的组织：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 在Swin Transformer中，blocks是通过depths参数组织的 def create_model(depths=[2, 2, 6, 2]): # Stage 1 self.stage1 = nn.ModuleList([ SwinTransformerBlock(dim=128, num_heads=4) for _ in range(depths[0]) # 创建2个block ]) # Stage 2 self.stage2 = nn.ModuleList([ SwinTransformerBlock(dim=256, num_heads=8) for _ in range(depths[1]) # 创建2个block ]) # Stage 3 (最深的stage) self.stage3 = nn.ModuleList([ SwinTransformerBlock(dim=512, num_heads=16) for _ in range(depths[2]) # 创建6个block ]) # Stage 4 self.stage4 = nn.ModuleList([ SwinTransformerBlock(dim=1024, num_heads=32) for _ in range(depths[3]) # 创建2个block ]) 在MiDaS中的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 # 在MiDaS中，通过timm库加载预训练模型 model = timm.create_model( \u0026#34;swinv2_base_window12to24_192to384_22kft1k\u0026#34;, pretrained=True ) # 然后通过hooks获取中间特征 hooks = [1, 1, 17, 1] # 指定每个stage中要提取特征的block索引 pretrained.model.layers[0].blocks[hooks[0]] # Stage 1的第1个block pretrained.model.layers[1].blocks[hooks[1]] # Stage 2的第1个block pretrained.model.layers[2].blocks[hooks[2]] # Stage 3的第17个block pretrained.model.layers[3].blocks[hooks[3]] # Stage 4的第1个block 模型参数的调整 主要参数：\nEmbed_dim（嵌入维度）：影响特征表示的丰富程度。 Depths（深度）：每个 Stage 中的 Block 数量，影响模型的深度和表达能力。 Num_heads（注意力头数量）：影响模型捕获不同特征模式的能力。 调整策略：\n计算资源有限时：减小 embed_dim、depths 和 num_heads。 需要更强的特征表示时：增大 embed_dim 和 depths。 平衡性能和效率：根据任务需求选择合适的参数配置。 模型的层级结构和通道的关系 以Swin V2-Base为例：\n模型的基本组成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class SwinV2Base(nn.Module): def __init__(self): # 1. Patch Embedding层 self.patch_embed = PatchEmbed( patch_size=4, # 将图像分成4x4的patch in_channels=3, # 输入RGB三通道 embed_dim=128 # 每个patch嵌入维度为128 ) # 2. 四个Transformer Stage self.stage1 = BasicLayer(...) # 处理1/4分辨率特征 self.stage2 = BasicLayer(...) # 处理1/8分辨率特征 self.stage3 = BasicLayer(...) # 处理1/16分辨率特征 self.stage4 = BasicLayer(...) # 处理1/32分辨率特征 通道数在各层级的变化：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 输入图像: [384, 384, 3] # ↓ Patch Embedding patch_tokens = [96, 96, 128] # 96x96个patch，每个128维 # Stage1: 保持通道数 stage1_out = [96, 96, 128] # 维持空间和通道维度 # Stage2: 通道数翻倍，空间分辨率减半 stage2_out = [48, 48, 256] # 48x48特征图，256通道 # Stage3: 再次翻倍 stage3_out = [24, 24, 512] # 24x24特征图，512通道 # Stage4: 最终特征 stage4_out = [12, 12, 1024] # 12x12特征图，1024通道 每个Stage的内部结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class BasicLayer(nn.Module): def __init__(self): # 1. Window Attention块 self.window_attn = WindowAttention( dim=256, # 特征维度 num_heads=8, # 注意力头数 window_size=12 # 窗口大小 ) # 2. Feed Forward Network (FFN) self.ffn = FFN( in_features=256, hidden_features=1024, # 内部扩展维度 out_features=256 ) # 3. Layer Norm层 self.norm1 = nn.LayerNorm(256) self.norm2 = nn.LayerNorm(256) 通道数增加的原因和作用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Stage 1: 128通道 # 每个通道关注不同特征： channels_stage1 = { 1-32: \u0026#34;边缘和纹理特征\u0026#34;, 33-64: \u0026#34;颜色和亮度模式\u0026#34;, 65-96: \u0026#34;局部形状特征\u0026#34;, 97-128: \u0026#34;基本结构特征\u0026#34; } # Stage 2: 256通道 # 更复杂的特征组合： channels_stage2 = { 1-64: \u0026#34;组合边缘特征\u0026#34;, 65-128: \u0026#34;复杂纹理模式\u0026#34;, 129-192: \u0026#34;部件级特征\u0026#34;, 193-256: \u0026#34;中等规模结构\u0026#34; } # Stage 3: 512通道 # 高级语义特征： channels_stage3 = { 1-128: \u0026#34;物体部件关系\u0026#34;, 129-256: \u0026#34;场景组成模式\u0026#34;, 257-384: \u0026#34;空间排布特征\u0026#34;, 385-512: \u0026#34;上下文信息\u0026#34; } 具体示例：处理一张图片的过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 假设输入图片: [1, 3, 384, 384] class SwinV2Process: def process_image(self, image): # 1. Patch Embedding patches = self.patch_embed(image) # [1, 96*96, 128] - 9216个patch，每个128维 # 2. Stage 1处理 stage1_out = self.stage1(patches) # 保持维度但增强特征表示 # 仍然是[1, 96*96, 128]但包含更丰富的特征 # 3. Stage 2处理 stage2_out = self.stage2(stage1_out) # 降采样+通道扩展 # [1, 48*48, 256] # 4. Stage 3处理 stage3_out = self.stage3(stage2_out) # [1, 24*24, 512] # 5. Stage 4处理 stage4_out = self.stage4(stage3_out) # [1, 12*12, 1024] 关键理解：\n层级的作用： 每个Stage负责特定尺度的特征提取 随着层级加深，感受野变大 特征从具体到抽象 通道的作用： 每个通道像一个\u0026quot;专家\u0026quot; 通道数增加=更多\u0026quot;专家\u0026quot;观点 高层通道组合低层特征 为什么需要这种结构： 渐进式特征提取 多尺度信息捕获 局部到全局的理解 这就像是一个层级化的观察系统：\n底层：大量简单\u0026quot;专家\u0026quot;看细节 中层：较少但更专业的\u0026quot;专家\u0026quot;归纳特征 高层：精英\u0026quot;专家\u0026quot;总结全局信息 MiDaS 模型中的应用 MiDaS 是一种用于单目深度估计的模型，利用了 Swin Transformer 作为主干网络。\n模型结构图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 MidasNet (整体模型) │ ├── 1. Backbone (SwinTransformer) │ │ │ ├── patch_embed (PatchEmbedding) │ │ └── 将图像分成patches │ │ │ ├── stage1 (特征金字塔第一层) │ │ ├── blocks[0] (SwinTransformerBlock) │ │ │ ├── norm1 │ │ │ ├── attn (WindowAttention) ← 这里有4个注意力头 │ │ │ │ ├── head1: 32维特征 │ │ │ │ ├── head2: 32维特征 │ │ │ │ ├── head3: 32维特征 │ │ │ │ └── head4: 32维特征 │ │ │ ├── norm2 │ │ │ └── mlp │ │ │ │ │ ├── blocks[1] (结构同上) │ │ └── downsample │ │ │ ├── stage2 (特征金字塔第二层) │ │ ├── blocks[0] (SwinTransformerBlock) │ │ │ ├── norm1 │ │ │ ├── attn (WindowAttention) ← 这里有8个注意力头 │ │ │ │ ├── head1: 32维特征 │ │ │ │ ├── head2: 32维特征 │ │ │ │ └── ... (共8个) │ │ │ ├── norm2 │ │ │ └── mlp │ │ │ │ │ ├── blocks[1] (结构同上) │ │ └── downsample │ │ │ ├── stage3 (特征金字塔第三层) │ │ ├── blocks[0-5] (6个Block) │ │ │ └── 每个block有16个注意力头 │ │ └── downsample │ │ │ └── stage4 (特征金字塔第四层) │ └── blocks[0-1] (2个Block) │ └── 每个block有32个注意力头 │ ├── 2. Neck (特征融合网络) │ ├── 融合层1 (feat4 + feat3) │ ├── 融合层2 (feat3 + feat2) │ └── 融合层3 (feat2 + feat1) │ └── 3. Head (输出头) ├── Conv2d ├── ReLU └── Conv2d (最终深度图) 特征提取与融合 特征提取：\nBackbone 提取不同尺度的特征，形成特征金字塔。 各 Stage 输出的特征： Stage 1：高分辨率，低级特征。 Stage 4：低分辨率，高级语义特征。 特征融合（Neck）：\n目的：将不同尺度的特征进行融合，整合局部和全局信息。 过程： 从高层特征开始，逐步上采样。 与下一层特征融合（通常通过加法或连接）。 重复步骤，直到融合所有层的特征。 实现：\n1 2 3 4 5 6 7 8 9 10 11 class FeatureFusionBlock(nn.Module): def __init__(self, features): super().__init__() self.res_conv = ResidualConvUnit(features) self.up = nn.Upsample(scale_factor=2, mode=\u0026#39;bilinear\u0026#39;, align_corners=False) def forward(self, x, skip_connection): x = self.up(x) x = x + skip_connection x = self.res_conv(x) return x 深度估计的实现 头部网络（Head）：\n结构：由一系列卷积层和激活函数组成。 作用：将融合后的特征映射为深度估计结果。 实现：\n1 2 3 4 5 6 7 8 9 10 11 12 class DepthEstimationHead(nn.Module): def __init__(self, in_channels): super().__init__() self.conv = nn.Sequential( nn.Conv2d(in_channels, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(128, 1, kernel_size=1) ) def forward(self, x): x = self.conv(x) return x 完整的前向传播过程：\n输入图像经过 Backbone，提取多尺度特征。 Neck 模块融合特征，生成综合特征表示。 Head 输出深度图。 模型调用流程 以Swin V2-Base为例，详细解释MiDaS的完整工作流程：\n模型初始化和加载：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 1. 创建MiDaS模型实例 model = MidasNet( model_path=\u0026#34;weights/dpt_swin2_base_384.pt\u0026#34;, features=256, # 特征维度 non_negative=True # 确保深度为非负 ) # 2. 加载Swin V2-Base backbone def _make_pretrained_swin2b24_384(pretrained): # 加载预训练的Swin V2-Base模型 model = timm.create_model( \u0026#34;swinv2_base_window12to24_192to384_22kft1k\u0026#34;, pretrained=pretrained ) return model 图像预处理和特征提取：\n1 2 3 4 5 6 7 8 9 10 # 1. 输入图像预处理 # 原始图像: [H, W, 3] -\u0026gt; [1, 3, 384, 384] input_image = preprocess(image) # 归一化、调整大小等 # 2. Swin V2-Base特征提取 # Swin V2-Base有4个stage，每个stage提取不同尺度的特征 stage1_feature = model.layers[0](x) # [1, 128, 96, 96] stage2_feature = model.layers[1](stage1) # [1, 256, 48, 48] stage3_feature = model.layers[2](stage2) # [1, 512, 24, 24] stage4_feature = model.layers[3](stage3) # [1, 1024, 12, 12] 特征重映射和统一：\n1 2 3 4 5 # 将不同stage的特征映射到统一维度（256通道） layer1_rn = self.scratch.layer1_rn(stage1_feature) # [1, 256, 96, 96] layer2_rn = self.scratch.layer2_rn(stage2_feature) # [1, 256, 48, 48] layer3_rn = self.scratch.layer3_rn(stage3_feature) # [1, 256, 24, 24] layer4_rn = self.scratch.layer4_rn(stage4_feature) # [1, 256, 12, 12] 特征融合过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 自底向上的特征融合 # 1. 最深层特征处理 path_4 = self.scratch.refinenet4(layer4_rn) # [1, 256, 12, 12] # 2. 融合第三层 path_3 = self.scratch.refinenet3(path_4, layer3_rn) # path_4上采样到[1, 256, 24, 24] # 与layer3_rn [1, 256, 24, 24]融合 # 3. 融合第二层 path_2 = self.scratch.refinenet2(path_3, layer2_rn) # path_3上采样到[1, 256, 48, 48] # 与layer2_rn [1, 256, 48, 48]融合 # 4. 融合第一层 path_1 = self.scratch.refinenet1(path_2, layer1_rn) # path_2上采样到[1, 256, 96, 96] # 与layer1_rn [1, 256, 96, 96]融合 深度图生成：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 通过输出头生成深度图 self.scratch.output_conv = nn.Sequential( # 1. 256-\u0026gt;128通道，保持空间分辨率 nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1), # [1, 128, 96, 96] # 2. 上采样2倍 Interpolate(scale_factor=2, mode=\u0026#34;bilinear\u0026#34;), # [1, 128, 192, 192] # 3. 128-\u0026gt;32通道 nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(True), # [1, 32, 192, 192] # 4. 32-\u0026gt;1通道（最终深度图） nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0), nn.ReLU(True) # 确保深度非负 # [1, 1, 192, 192] ) 后处理和输出：\n1 2 3 4 5 6 7 8 9 10 11 # 1. 调整深度图大小到原始图像尺寸 depth = F.interpolate( depth, size=(original_height, original_width), mode=\u0026#34;bilinear\u0026#34;, align_corners=False ) # 2. 深度图归一化（可选） depth = depth - depth.min() depth = depth / depth.max() 关键特点：\nSwin V2-Base的优势： 动态窗口大小（12-\u0026gt;24） 高效的层次化特征提取 预训练在ImageNet-22K和1K上 特征提取的层次性： Stage 1：捕获局部纹理和边缘 Stage 2：感知基本形状 Stage 3：理解物体部件 Stage 4：把握场景结构 特征融合的策略： 自底向上融合 残差连接保持信息 逐步整合全局和局部信息 深度估计的精确性： 多尺度特征利用 渐进式信息融合 高分辨率输出 数据流动过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 1. 输入图像 x = [1, 3, 384, 384] # [批次, 通道, 高, 宽] # 2. Patch Embedding x = [1, 96*96, 128] # [批次, patch数, 特征维度] # 3. Stage 1 x = [1, 48*48, 256] # 空间降采样，通道数增加 # 4. Stage 2 x = [1, 24*24, 512] # 继续降采样和增加通道 # 5. Stage 3 x = [1, 12*12, 1024] # 继续降采样和增加通道 # 6. Stage 4 x = [1, 6*6, 1024] # 最终特征图 这个过程就像是：\n先用不同\u0026quot;专家\u0026quot;（通道）从不同角度观察图像 将这些观察逐层提炼（特征提取） 整合不同层次的理解（特征融合） 最终得出深度判断（深度图） 关键名词解释 本节对文中提到的关键名词进行详细解释，帮助读者更好地理解模型的工作原理。\nBlock（块） 定义：模型的基本组成单元，包含多个层和操作。 在 Swin Transformer 中： Swin Transformer Block：包括 LayerNorm、多头注意力机制、残差连接和前馈网络等组件。 作用：在每个 Stage 中，多个 Block 依次堆叠，逐步提取更高级的特征。 Patch（补丁） 定义：将输入图像划分为固定大小的小块，每个小块称为一个补丁（Patch）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 假设输入图像: [1, 3, 384, 384] # patch_size = 4，意味着将图像切分成4x4的小块 # 图像切分过程： \u0026#34;\u0026#34;\u0026#34; 原始图像(384x384) ┌────────────────┐ │ ┌──┐┌──┐┌──┐ │ │ │P1││P2││P3│ │ │ └──┘└──┘└──┘ │ │ ┌──┐┌──┐┌──┐ │ │ │P4││P5││P6│ │ │ └──┘└──┘└──┘ │ └────────────────┘ 每个Patch(4x4): P1 = ┌────┐ │RGB │ │RGB │ │RGB │ └────┘ \u0026#34;\u0026#34;\u0026#34; # 384/4 = 96，所以水平和垂直方向各有96个patch # 因此总共有96x96=9216个patch 在 Swin Transformer 中：\nPatch Embedding：将每个补丁转换为高维特征向量，作为模型的输入。 优势：减少计算复杂度，将二维图像数据转换为一维序列，方便 Transformer 处理。 Depth（深度） 在模型结构中： Depth（深度）：指每个 Stage 中包含的 Block 数量，影响模型的深度和特征提取能力。 在深度估计任务中： Depth（深度）：指场景中物体距离相机的距离，深度估计的目标就是预测每个像素的深度值。 depths参数的选择建议： 任务复杂度： 简单任务：使用较小的depths 复杂任务：增加关键stage的depth 计算资源： 限制严格：使用较小的depths 资源充足：可以使用更大的depths 数据规模： 数据少：避免使用过大的depths（防止过拟合） 数据充足：可以使用更大的depths 实时性要求： 要求高：使用较小的depths 要求低：可以使用更大的depths Head（注意力头） 定义：在多头注意力机制中，多个独立的注意力计算单元称为注意力头（Head）。 在 Swin Transformer 中： 多头注意力机制：将特征维度分割成多个子空间，每个 Head 在各自的子空间中计算注意力，增强模型的特征表示能力。 embed_dim（嵌入维度） 定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 每个4x4的patch包含： patch_pixels = 4 * 4 * 3 # 48个值(4x4个像素，每个像素3个RGB通道) # 嵌入过程： \u0026#34;\u0026#34;\u0026#34; 一个Patch(48个值) -\u0026gt; 嵌入向量(128维) [R1,G1,B1, -\u0026gt; [e1, R2,G2,B2, e2, R3,G3,B3, e3, ... ... R16,G16,B16] e128] 这是一个从48维到128维的转换 就像用128个词来描述一个4x4的图像块 \u0026#34;\u0026#34;\u0026#34; # 这个转换通过线性投影实现： patch_embed = nn.Linear(patch_pixels, embed_dim) # 48 -\u0026gt; 128 Channels（通道数） 在MiDaS这个具体场景中的通道数：\n1 2 3 4 5 6 7 8 9 10 # 输入图像的通道数 input_image = torch.zeros(1, 3, 384, 384) # [batch_size, channels, height, width] # channels = 3: RGB三个颜色通道 # Backbone提取的特征通道数 layer_1 = self.pretrained.layer1(x) # [1, 256, H/4, W/4] layer_2 = self.pretrained.layer2(layer_1) # [1, 512, H/8, W/8] layer_3 = self.pretrained.layer3(layer_2) # [1, 1024, H/16, W/16] layer_4 = self.pretrained.layer4(layer_3) # [1, 2048, H/32, W/32] 在这里，通道数的变化表示：\n开始：3个通道（RGB） 第一层：扩展到256个通道（256个不同的特征检测器） 第二层：512个通道（更多的特征检测器） 第三层：1024个通道 第四层：2048个通道 在抽象意义上的通道数： 想象你在看一幅画：\nA. 单通道（Grayscale）：\n1 2 grayscale_image = torch.zeros(1, 1, 100, 100) # 就像黑白照片，只有明暗信息 每个像素只有一个值 类比：用铅笔画素描，只有深浅变化 B. 三通道（RGB）：\n1 2 color_image = torch.zeros(1, 3, 100, 100) # 像彩色照片，有RGB三个颜色通道 每个像素有三个值（红、绿、蓝） 类比：用三支彩色笔画画，可以混合出各种颜色 C. 多通道特征图：\n1 2 feature_map = torch.zeros(1, 256, 100, 100) # 256个不同的\u0026#34;视角\u0026#34;看同一张图 类比：想象256个不同的专家同时在观察同一个物体：\n专家1关注边缘 专家2关注颜色 专家3关注纹理 专家4关注形状 \u0026hellip;以此类推 通道数增加的意义：\n1 2 3 # 1x1卷积调整通道数 conv1x1 = nn.Conv2d(3, 64, kernel_size=1) # 从3通道扩展到64通道 就像：\n一个人看到一个苹果：只能说\u0026quot;这是个红色的圆形物体\u0026quot; 64个专家看同一个苹果： 专家1：\u0026ldquo;表面有光泽\u0026rdquo; 专家2：\u0026ldquo;有细微的斑点\u0026rdquo; 专家3：\u0026ldquo;果皮有特定纹理\u0026rdquo; 专家4：\u0026ldquo;边缘有特定曲率\u0026rdquo; \u0026hellip;更多细节描述 通道数在特征融合中的作用：\n1 2 3 4 class FeatureFusionBlock(nn.Module): def __init__(self, features=256): # features=256意味着256个\u0026#34;专家\u0026#34;的意见 self.conv = nn.Conv2d(features, features, 3, 1, 1) 就像：\n不同层的256个专家各自提供观点 通过融合得到更全面的理解 保持通道数相同确保\u0026quot;专家数量\u0026quot;一致，便于信息整合 实际应用中的通道数变化：\n1 2 3 4 5 6 # 在MiDaS中的输出层 self.scratch.output_conv = nn.Sequential( nn.Conv2d(256, 128, 3, 1, 1), # 256-\u0026gt;128通道 nn.Conv2d(128, 32, 3, 1, 1), # 128-\u0026gt;32通道 nn.Conv2d(32, 1, 1, 1, 0) # 32-\u0026gt;1通道（深度图） ) 这就像：\n先有256个专家的详细分析 整合为128个关键观点 进一步提炼为32个核心发现 最终得出一个统一的结论（深度值） 通道数的本质是特征维度，它决定了模型能够同时关注多少种不同的特征模式。就像人类观察事物时，可以同时注意到形状、颜色、纹理、边缘等多个方面，深度学习模型通过多通道来实现这种多角度的特征提取和分析。\nHooks（特征提取点） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 hooks = [1, 1, 17, 1] # 在不同层设置特征提取点 # - 第1层：第1个block的输出 # - 第2层：第1个block的输出 # - 第3层：第17个block的输出 # - 第4层：第1个block的输出 \u0026#39;\u0026#39;\u0026#39; 这些组件协同工作的过程： 输入图像首先被分割成补丁并嵌入 通过多个Transformer块进行特征提取 在指定的hook点提取多尺度特征 特征被送入头部网络进行处理 最终输出深度估计结果 \u0026#39;\u0026#39;\u0026#39; .pt（模型文件） .pt文件通常包含： 模型参数（state_dict） 模型配置（config） 优化器状态（如果保存） 训练元数据（如果保存） 访问特定部分： Backbone: model.pretrained或state_dict['pretrained.*'] 特征提取器: model.scratch或state_dict['scratch.*'] 特征融合块: model.scratch.refinenet* 特征的处理流程： 输入图像 → Backbone提取特征 → 特征融合块处理 → 输出深度图 实际示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 加载并查看模型结构 import torch def inspect_model(model_path): \u0026#34;\u0026#34;\u0026#34;检查模型结构和参数\u0026#34;\u0026#34;\u0026#34; model = torch.load(model_path) print(\u0026#34;模型的主要组成部分：\u0026#34;) if isinstance(model, dict): for key in model.keys(): print(f\u0026#34;- {key}\u0026#34;) print(\u0026#34;\\n模型层的结构：\u0026#34;) if \u0026#39;state_dict\u0026#39; in model: state_dict = model[\u0026#39;state_dict\u0026#39;] for name, param in state_dict.items(): print(f\u0026#34;层名称: {name}\u0026#34;) print(f\u0026#34;参数形状: {param.shape}\u0026#34;) print(\u0026#34;---\u0026#34;) # 使用示例 # inspect_model(\u0026#39;path_to_your_model.pt\u0026#39;) 模型训练与参数调整 损失函数与优化器 损失函数：\nL1 损失：计算预测深度与真实深度之间的绝对误差。 L2 损失：计算预测深度与真实深度之间的平方误差。 自定义损失：可以结合结构相似性（SSIM）等指标，提升模型性能。 优化器：\nAdamW：适用于 Transformer 结构的优化器，具有权重衰减的功能。 学习率：需要根据数据集和模型复杂度进行调整。 训练循环 代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 num_epochs = 10 for epoch in range(num_epochs): model.train() for images, depths in train_loader: images = images.to(device) depths = depths.to(device) # 前向传播 outputs = model(images) loss = criterion(outputs, depths) # 反向传播和优化 optimizer.zero_grad() loss.backward() optimizer.step() # 可选：验证模型性能，调整学习率等 注意事项：\n梯度裁剪：防止梯度爆炸，特别是在深度模型中。 学习率调度：使用 CosineAnnealingLR 或 ReduceLROnPlateau 等调度策略。 超参数的影响 模型参数：\nEmbed_dim（嵌入维度）： 影响：决定了特征向量的维度，增大 embed_dim 可以提升特征表示能力，但会增加计算量。 Depths（深度）： 影响：每个 Stage 中的 Block 数量，增加 Block 数量可以提取更深层次的特征，但也会增加模型复杂度。 Num_heads（注意力头数量）： 影响：决定了多头注意力机制的并行度，增加注意力头数量可以捕获更多特征模式，但会增加内存占用。 训练参数：\n学习率：需要根据模型和数据集进行调整，过大会导致训练不稳定，过小会导致收敛缓慢。 批次大小：需要根据 GPU 内存大小进行设置，较大的批次大小可以稳定梯度，但会占用更多内存。 权重衰减：防止模型过拟合，通常设置为 1e-4 或 1e-5。 参考 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows 作者：Ze Liu, Yutong Lin, Yue Cao, et al. 链接：https://arxiv.org/abs/2103.14030 MiDaS: Robust Monocular Depth Estimation with Mix and Match of Dense and Sparse Depth Samples 作者：René Ranftl, Katrin Lasinger, David Hafner, et al. 链接：https://arxiv.org/abs/1907.01341 PyTorch 官方文档 链接：https://pytorch.org/docs/ timm: PyTorch Image Models 作者：Ross Wightman 链接：https://github.com/rwightman/pytorch-image-models ","date":"2024-12-01T00:00:00Z","permalink":"https://echudet.github.io/p/aimodel/","title":"aimodel"},{"content":"数据仓库与商业智能 在当今信息爆炸的时代，企业面临着海量数据的收集与分析挑战。数据仓库与商业智能（Business Intelligence, BI）作为解决这一问题的核心技术，扮演着至关重要的角色。数据仓库通过系统化的存储与管理，为企业提供了一个整合、多维度的数据视图，而商业智能则利用这些数据进行深入分析，辅助决策者做出明智的商业决策。\n数据仓库的概念 数据仓库是一种专门设计用于分析和报告的数据库系统，与传统的事务性数据库不同，它主要关注于历史数据的存储和管理。数据仓库的核心在于其集成性、主题性、非易失性和时变性。\n集成性指的是数据仓库将来自不同来源的数据进行整合，消除数据冗余和不一致性；主题性则强调数据仓库围绕特定的业务主题进行组织，如销售、财务、人力资源等；非易失性意味着一旦数据被加载到数据仓库中，就不会被频繁修改；时变性则允许数据仓库记录数据的历史变化，支持时间序列分析。\n例如，一家零售企业可能会建立一个销售数据仓库，将来自不同门店、不同地区的销售数据集中存储，方便进行全局销售趋势分析和市场策略调整。\nETL过程 数据仓库的构建离不开ETL（Extract, Transform, Load）的过程，这是数据仓库数据处理的核心步骤。\n提取（Extract）是指从各种数据源中抽取所需的数据。这些数据源可能包括企业内部的事务数据库、外部的市场数据源、日志文件等。提取过程需要确保数据的完整性和准确性，同时尽量减少对源系统的影响。\n转换（Transform）涉及对提取的数据进行清洗、整合和转换，以适应数据仓库的要求。这一步骤可能包括数据格式转换、数据去重、数据聚合以及根据业务规则进行计算。例如，将不同格式的日期统一为标准格式，或者将销售数据按月汇总。\n加载（Load）则是将转换后的数据加载到数据仓库中。加载过程需要考虑数据的分区、索引和存储优化，以确保数据仓库的查询性能和存取效率。\n一个具体的例子是，银行在构建客户分析数据仓库时，通过ETL过程从多个分支机构的数据库中提取客户交易数据，经过清洗和转换后，统一加载到中央数据仓库中，供高层管理者进行客户行为分析和市场细分。\n数据建模 数据建模是数据仓库设计的重要环节，通过构建合理的数据模型，确保数据仓库能够高效支持复杂的查询和分析需求。常见的数据建模方法包括星型模型和雪花模型。\n星型模型是一种简单且高效的数据模型，核心是一个事实表（Fact Table）和多个维度表（Dimension Tables）。事实表存储业务过程中的度量数据，如销售金额、交易数量等，而维度表则描述业务实体的属性，如时间、地点、产品等。星型模型的优势在于查询性能高，结构直观，适合大多数数据分析需求。\n例如，在销售数据仓库中，事实表可能记录每笔交易的金额和数量，而维度表则分别描述时间（年、季度、月）、地点（国家、城市、门店）、产品（类别、品牌、型号）等信息。\n雪花模型是在星型模型基础上的进一步规范化，维度表被拆分为多个子维度表，以减少数据冗余。虽然雪花模型的结构更为复杂，但在某些情况下能够更有效地利用存储空间，并提升数据的一致性。\n例如，在上述销售数据仓库中，地点维度表可以进一步拆分为国家、城市和门店三级表结构，每一级表都存储更详细的地理信息。\nOLAP与数据挖掘 OLAP（联机分析处理）和数据挖掘是商业智能中的两大关键技术，前者侧重于多维数据分析，后者则致力于发现数据中的潜在模式和规律。\nOLAP通过多维数据模型，支持用户以不同维度和层次对数据进行切片、切块和旋转等操作，帮助用户从多个角度深入分析业务数据。常见的OLAP操作包括钻取（drill-down）、上卷（roll-up）和切片（slice）等。例如，企业管理者可以使用OLAP工具查看不同地区、不同时间段的销售业绩，识别出表现优异或滞后的区域和时期。\n数据挖掘则利用统计学、机器学习和人工智能技术，从大量数据中自动发现隐藏的模式和关系，支持预测分析和决策支持。数据挖掘应用广泛，包括客户细分、市场篮分析、预测建模等。例如，零售商可以通过数据挖掘分析顾客的购买行为，识别出常一起购买的产品组合，从而优化产品摆放和促销策略。\n商业智能工具与应用 商业智能工具是实现数据仓库价值的关键，它们提供了数据分析、报表生成、可视化展示等功能，帮助用户高效地从数据中提取洞察。常见的商业智能工具包括：\nTableau：以其强大的数据可视化能力著称，支持用户通过拖拽操作创建交互式报表和仪表盘。 Microsoft Power BI：集成度高，易于与Microsoft生态系统中的其他工具（如Excel、Azure）无缝衔接，适合企业级应用。 QlikView：强调数据关联分析，用户可以通过简单的操作探索数据之间的关系，发现潜在的业务机会。 SAP BusinessObjects：提供全面的BI解决方案，适用于大型企业复杂的分析需求。 这些工具在各个行业中都有广泛的应用。例如，金融机构利用商业智能工具进行风险管理和客户分析，制造企业通过数据分析优化生产流程和供应链管理，零售商则通过销售数据分析提升库存管理和市场营销效果。\n大数据与数据库技术 随着信息技术的迅猛发展，数据的生成速度和规模呈指数级增长，催生了“大数据”这一概念。大数据不仅改变了企业的运营方式，也对传统数据库技术提出了新的挑战与机遇。本章将深入探讨大数据的基本概念、Hadoop生态系统、分布式存储与计算，以及新兴的数据库技术，如NewSQL和时序数据库，阐明它们在现代数据管理中的重要性与应用。\n大数据概述 大数据指的是规模庞大、类型多样且生成速度极快的数据集合，通常被描述为“4V”：Volume（大量）、Velocity（高速）、Variety（多样）和Veracity（真实性）。这些数据不仅来源于传统的结构化数据，如数据库记录，还包括非结构化和半结构化数据，如社交媒体帖子、传感器数据、日志文件等。\n大数据的应用广泛且深远。在商业领域，企业通过分析大数据可以洞察消费者行为、优化运营流程、提升产品质量。例如，电子商务平台利用大数据分析用户的浏览和购买记录，进行个性化推荐，提高销售转化率；在医疗领域，研究人员通过分析海量的基因数据，推动精准医学的发展。\n然而，大数据也带来了诸多挑战。传统的数据库系统在处理大规模、多样化和高速度的数据时往往力不从心，需要新的技术和架构来应对这些挑战。此外，数据隐私与安全、数据治理等问题也亟需解决，以确保大数据的有效利用和合法合规。\nHadoop生态系统 为了应对大数据带来的挑战，Apache Hadoop生态系统应运而生，成为大数据处理的核心框架。Hadoop最初由Apache Software Foundation开发，旨在通过分布式计算和存储来处理大规模数据集。Hadoop生态系统包含多个组件，每个组件在数据存储、处理和管理中扮演着关键角色。\nHadoop的核心组件包括：\nHadoop分布式文件系统（HDFS）：HDFS是Hadoop的存储层，设计用于在廉价的硬件上存储海量数据。它通过数据块的复制和分布式存储，确保数据的高可用性和容错性。 MapReduce：MapReduce是Hadoop的计算模型，分为Map阶段和Reduce阶段。Map阶段负责数据的分割和初步处理，Reduce阶段则进行汇总和结果合并。这种并行处理模式极大地提升了数据处理效率。 Hadoop生态系统的扩展组件：\nYARN（Yet Another Resource Negotiator）：YARN是Hadoop的资源管理层，负责管理和调度集群资源，支持多种数据处理框架的运行，如Spark、Tez等。 Hive：Hive提供了一种类似SQL的查询语言（HiveQL），将结构化数据转换为MapReduce任务，简化了大数据查询和分析的过程。 Pig：Pig Latin是Pig的脚本语言，用于编写数据转换和分析任务，适合处理复杂的数据处理流程。 HBase：HBase是一个分布式的、面向列的NoSQL数据库，构建在HDFS之上，适用于实时读写操作和大规模数据存储。 Spark：虽然最初不属于Hadoop生态系统，但Spark已成为Hadoop的重要补充，提供了更高效的内存计算能力，支持流处理、机器学习和图计算等多种数据处理任务。 示例应用：一家互联网公司利用Hadoop生态系统构建了数据湖，通过HDFS存储用户行为日志，使用Spark进行实时数据分析，结合Hive进行数据仓库的查询与报表生成，从而实现了用户行为的全面监控和精准营销。\n分布式存储与计算 大数据的特性决定了单机存储和计算已无法满足需求，分布式存储与计算成为必然选择。分布式系统通过将数据和计算任务分散到多个节点上，实现高效的并行处理和资源利用。\n分布式存储的关键在于数据的分片与复制。数据分片将大规模数据集拆分成较小的块，分布存储在不同的节点上，提升存储容量和并行访问能力。数据复制则通过在多个节点上存储数据副本，确保数据的高可用性和容错性。例如，HDFS通过将每个数据块复制到多个节点上，防止单点故障导致的数据丢失。\n分布式计算依赖于并行处理框架，如MapReduce和Spark。MapReduce通过将任务分解为Map和Reduce阶段，利用集群中的多个节点同时处理不同的数据块，显著提高了计算速度。Spark进一步优化了分布式计算，通过内存计算和任务调度优化，实现了比MapReduce更高的性能，特别适合需要迭代计算的机器学习和图计算任务。\n分布式系统的挑战包括数据一致性、网络延迟、节点故障处理等。CAP定理指出，在分布式系统中，无法同时保证一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance），系统设计需在这三者之间做出权衡。例如，HBase在设计时选择了CP（Consistency and Partition Tolerance），确保数据的一致性，但在网络分区时可能会牺牲部分可用性。\n新兴数据库技术 随着大数据需求的不断演进，传统的关系型数据库（RDBMS）逐渐显现出局限性，催生了诸多新兴数据库技术。这些新技术在性能、可扩展性和功能性上对传统数据库进行了补充和优化，满足了现代应用的多样化需求。\nNewSQL NewSQL是对传统关系型数据库的创新和扩展，旨在提供与RDBMS相同的ACID事务保障，同时具备NoSQL数据库的高性能和可扩展性。NewSQL通过分布式架构和优化的存储引擎，实现了高并发处理和低延迟查询，适用于需要高一致性和实时性的应用场景。\n代表性产品：\nGoogle Spanner：Google的分布式数据库，结合了关系型数据库的结构和NoSQL的可扩展性，支持全球范围内的数据分布和强一致性。 CockroachDB：开源的NewSQL数据库，具备自动分片、故障恢复和水平扩展能力，适用于分布式应用和微服务架构。 VoltDB：内存优先的NewSQL数据库，针对高吞吐量和低延迟需求进行了优化，广泛应用于金融交易和电信领域。 应用示例：一家全球电商平台采用CockroachDB作为核心数据库，利用其强一致性和高可用性特性，确保跨区域订单处理的准确性和实时性，同时通过水平扩展应对高峰期的流量压力。\n时序数据库 时序数据库（Time Series Database, TSDB）专门用于存储和处理时间序列数据，即按照时间顺序排列的数据点。时序数据库在物联网、金融市场监控、日志分析等领域有广泛应用，因其在处理高频率数据和实时分析方面具备显著优势。\n关键特性：\n高效的数据压缩和存储：时序数据库采用专门的压缩算法，优化存储空间，提升数据写入和读取效率。 实时数据处理：支持快速的实时查询和分析，适用于需要即时响应的应用场景。 内置的时间序列分析功能：提供聚合、下采样、异常检测等专用功能，简化数据分析流程。 代表性产品：\nInfluxDB：开源的时序数据库，具备高性能的数据写入和查询能力，支持复杂的时间序列分析。 TimescaleDB：基于PostgreSQL构建，结合了关系型数据库的功能和时序数据库的性能，适用于需要丰富查询能力的应用。 Prometheus：专为监控和报警设计的时序数据库，广泛应用于云原生环境和微服务架构的性能监控。 应用示例：一家智能制造企业利用InfluxDB收集和存储来自生产线传感器的实时数据，通过内置的分析功能监控设备运行状态，及时发现和预防潜在故障，提升生产效率和设备利用率。\n现代数据库技术与趋势 随着信息技术的不断进步和业务需求的多样化，数据库技术也在迅速演变，以适应日益复杂的应用场景和数据管理需求。现代数据库技术不仅涵盖了传统关系型数据库的优化与扩展，还引入了云计算、自动化、智能化管理以及边缘计算等新兴领域。本章将全面探讨这些现代数据库技术与趋势，深入分析其原理、应用场景及未来发展方向。\n云数据库 云数据库是现代数据库技术的重要组成部分，它将数据库服务迁移到云端，利用云计算的弹性、可扩展性和高可用性，满足企业对数据存储和管理的需求。云数据库通常分为三种服务模式：软件即服务（SaaS）、平台即服务（PaaS）和基础设施即服务（IaaS）。\nSaaS（Software as a Service）：在SaaS模式下，数据库软件作为一种服务提供给用户，用户无需管理底层基础设施，只需通过互联网访问和使用数据库。例如，Salesforce提供的数据库服务允许企业管理客户关系数据，而无需自行部署和维护数据库服务器。\nPaaS（Platform as a Service）：PaaS模式提供了一个完整的平台，包括操作系统、数据库、中间件等，开发者可以在此平台上构建、部署和管理应用程序。PaaS数据库服务如Google Cloud SQL和Microsoft Azure SQL Database，简化了数据库的配置、管理和扩展过程，开发者可以专注于应用开发，而无需担心底层数据库的运维。\nIaaS（Infrastructure as a Service）：IaaS模式提供基础计算资源，如虚拟机、存储和网络，用户可以在此基础上自行安装和配置数据库软件。Amazon Web Services（AWS）的Amazon RDS（Relational Database Service）和Oracle Cloud Infrastructure提供的数据库服务，允许用户根据需求选择数据库引擎、配置实例规格，并灵活管理数据库环境。\n主要云数据库提供商及其服务：\n提供商 服务名称 特点及优势 Amazon Web Services (AWS) Amazon RDS, Amazon Aurora 高可用性、自动备份与恢复、支持多种数据库引擎（MySQL, PostgreSQL, Oracle, SQL Server等） Microsoft Azure Azure SQL Database, Cosmos DB 高度集成微软生态系统、全球分布、多模型数据库支持 Google Cloud Platform (GCP) Cloud SQL, Cloud Spanner 全球分布式架构、高可用性、强一致性和高扩展性 Oracle Cloud Oracle Autonomous Database 自动化管理、高安全性、优化的性能和可扩展性 应用示例：一家跨国电商企业利用AWS的Amazon Aurora作为其主数据库，享受其高可用性和自动扩展的优势，确保在全球各地的用户访问时都能获得低延迟和高性能的数据库服务。同时，企业通过AWS的自动备份和恢复功能，提升了数据安全性和业务连续性。\n数据库即服务（DBaaS） 数据库即服务（Database as a Service, DBaaS）是云数据库服务的一个具体实现，它提供了全面的数据库管理功能，使用户能够以按需、按使用量计费的方式使用数据库资源。DBaaS不仅简化了数据库的部署和管理，还提供了自动化的运维功能，如备份、恢复、监控和安全管理。\nDBaaS的优势：\n弹性扩展：用户可以根据业务需求动态调整数据库资源，轻松应对流量波动和数据增长。 成本效益：按需计费模式降低了初始投资和运维成本，特别适合中小企业和初创公司。 高可用性和容错性：DBaaS提供商通常具备多区域部署和自动故障切换能力，确保数据库的高可用性。 自动化运维：自动化的备份、恢复、更新和监控功能，减少了人工干预，提高了运维效率。 主要DBaaS产品：\n产品名称 提供商 特点及优势 Amazon RDS AWS 支持多种数据库引擎、自动备份与恢复、可扩展性强 Azure SQL Database Microsoft Azure 高度集成微软生态系统、内置智能性能优化、全球分布 Google Cloud SQL GCP 完全托管、自动化维护、与Google生态系统无缝集成 MongoDB Atlas MongoDB Inc. 专为NoSQL数据库设计、全球分布、自动扩展与备份 Oracle Autonomous DB Oracle 自主优化与修复、自动化管理、强大的安全特性 应用示例：一家移动应用开发公司采用MongoDB Atlas作为其NoSQL数据库解决方案，通过其自动扩展和全球分布功能，确保应用在全球用户访问量激增时仍能保持高性能和低延迟。同时，MongoDB Atlas的自动备份和恢复功能，提升了数据的安全性和可靠性。\n自动化与智能化数据库管理 随着数据库规模和复杂性的增加，传统的手动管理方式已经难以满足高效、可靠的需求。自动化与智能化数据库管理技术应运而生，通过机器学习和人工智能算法，提升数据库的运维效率和智能决策能力。\n自动化管理：\n自动化管理涵盖数据库的部署、配置、监控、备份、恢复和优化等多个方面。通过自动化工具，企业可以显著减少人为错误，提高数据库管理的效率和一致性。\n智能化管理：\n智能化管理进一步利用数据分析和机器学习技术，提供自适应优化和预测性维护。数据库系统可以自动分析性能瓶颈，优化查询计划，甚至预测潜在故障并提前采取措施。\n关键技术与工具：\n自动化配置与部署工具：如Terraform、Ansible和Chef，帮助企业实现基础设施即代码（Infrastructure as Code），自动化数据库的部署与配置。 智能性能优化：数据库管理系统如Microsoft Azure SQL Database和Oracle Autonomous Database，内置了智能性能监控和自动调优功能，能够根据实际负载自动调整索引、缓存和查询计划。 预测性维护：利用机器学习算法，分析数据库日志和性能指标，预测可能的故障和性能下降，提前进行维护和优化。 应用示例：一家金融服务公司采用Oracle Autonomous Database，通过其自动化管理功能，减少了数据库管理员的工作负担，同时利用智能性能优化功能，提升了查询效率和系统响应速度。此外，预测性维护功能帮助公司在潜在故障发生前进行预防性维护，确保了系统的高可用性和数据安全。\n边缘计算与数据库 随着物联网（IoT）和移动设备的普及，数据生成的地点逐渐向网络边缘移动。边缘计算将计算和存储资源部署在离数据源更近的位置，以减少延迟、提高带宽利用率和增强数据隐私保护。在这一背景下，边缘数据库应运而生，成为现代数据管理的重要趋势。\n边缘数据库的特点：\n低延迟：通过在本地部署数据库，显著减少数据传输时间，适用于实时应用场景。 高可用性：边缘数据库能够在网络断开或不稳定的情况下继续运行，保证业务连续性。 数据隐私与安全：敏感数据可以在本地处理和存储，减少了数据传输过程中的泄露风险。 资源受限优化：边缘设备通常具备有限的计算和存储资源，边缘数据库需要在性能和资源利用之间取得平衡。 代表性边缘数据库：\n数据库名称 特点及优势 SQLite 轻量级、嵌入式数据库，适用于资源受限的设备 Couchbase Lite 支持离线操作和数据同步，适合移动和物联网设备 Redis Edge 内存数据库，提供快速的数据访问和处理能力 Apache Ignite 分布式内存数据库，支持实时数据处理和分析 FaunaDB 分布式数据库，支持全球一致性和高可用性，适用于分布式应用 应用示例：一家智能家居公司在其设备中嵌入了Couchbase Lite数据库，实现本地数据存储和离线操作。用户的设备能够在没有网络连接的情况下继续运行，数据在网络恢复后自动与云端同步，确保了用户体验的连续性和数据的一致性。\n未来发展趋势 数据库技术的未来发展将围绕以下几个关键方向展开，不断推动数据管理的效率、智能化和广泛应用。\n云原生数据库 云原生数据库专为云环境设计，充分利用云计算的弹性和分布式特性。它们支持自动扩展、分布式存储和无服务器架构，能够动态适应业务需求的变化。云原生数据库如Google Cloud Spanner和Amazon Aurora，正在成为企业首选的数据库解决方案，特别适用于需要高可用性和全球分布的应用场景。\n多模数据库 多模数据库支持多种数据模型（如关系型、文档型、图型等）在同一数据库中共存，提供更高的灵活性和性能优化。通过统一的数据平台，开发者可以在一个系统中处理不同类型的数据，简化数据管理和应用开发流程。ArangoDB和OrientDB是典型的多模数据库，广泛应用于复杂数据关系和多样化应用需求的场景。\n人工智能与机器学习驱动的数据库 人工智能（AI）和机器学习（ML）技术正在深度融入数据库管理，提供更智能的优化和自动化功能。数据库系统能够通过学习历史数据和使用模式，自动优化查询性能、调整存储策略和预测系统需求。这不仅提升了数据库的性能和效率，也降低了运维成本和复杂性。\n应用示例：一家在线广告公司使用基于AI的数据库系统，自动优化广告投放查询，实时分析广告效果数据，提升了广告精准度和投放效率。同时，AI驱动的预测功能帮助公司提前识别流量高峰，动态调整资源配置，确保系统的稳定性和响应速度。\n无服务器数据库 无服务器数据库（Serverless Database）是一种新兴的数据库架构，用户无需管理服务器资源，只需按使用量付费。这种模式极大地简化了数据库的管理和扩展过程，适合于弹性需求和快速开发的应用场景。无服务器数据库如Firebase Firestore和Amazon Aurora Serverless，提供了高度的自动化和灵活性，使开发者能够专注于应用逻辑的实现。\n应用示例：一家初创公司开发了一款移动应用，通过Firebase Firestore实现了无服务器的数据存储和实时同步功能。应用能够根据用户需求自动扩展数据库资源，支持高并发访问，同时开发团队无需担心数据库的运维和管理，显著提升了开发效率和产品上线速度。\n数据隐私与安全 随着数据隐私法规的日益严格和用户对数据安全的关注，数据库技术在隐私保护和安全性方面也在不断进步。加密技术、访问控制、数据脱敏和隐私计算等手段被广泛应用，以确保数据在存储、传输和处理过程中的安全性和合规性。\n关键技术：\n数据加密：对静态数据和传输中的数据进行加密，防止未授权访问和数据泄露。 访问控制：基于角色的访问控制（RBAC）和细粒度权限管理，确保只有授权用户能够访问和操作敏感数据。 数据脱敏：在数据分析和共享过程中，对敏感信息进行脱敏处理，保护用户隐私。 隐私计算：通过同态加密、多方安全计算等技术，实现数据在加密状态下的计算和分析，保障数据隐私。 应用示例：一家医疗健康公司采用数据脱敏和隐私计算技术，在不暴露患者敏感信息的情况下，进行大规模的医疗数据分析，推动精准医疗和公共健康研究，同时严格遵守数据隐私法规，保护患者隐私。\n","date":"2024-11-27T00:00:00Z","image":"https://echudet.github.io/p/intro-database2/bg_hu_5a60ed7a17bff46e.jpg","permalink":"https://echudet.github.io/p/intro-database2/","title":"Intro Database（2）"},{"content":" ​\t粗略的数据库知识引入，速览可以大致了解一下数据库的基本知识。具体课程可以看清华大学李国良 老师讲授的数据库课程：Guoliang Li @ Tsinghua ，据说可与cmu15445一比高下。\n引言 数据库（Database）是一个系统化组织、存储和管理数据的集合，旨在支持高效的数据检索、更新和管理。它不仅是信息存储的基础设施，也是各种应用系统运作的核心。数据库通过结构化的方式将数据存储在表格、图形或文档中，并提供强大的查询和操作能力，使用户和应用程序能够快速、准确地获取所需信息。\n数据库的定义 数据库是一个有组织的数据集合，通常由数据库管理系统（DBMS）来管理。DBMS 提供了数据的存储、检索、更新和管理功能，确保数据的一致性、安全性和完整性。数据库的设计旨在减少数据冗余，提高数据的可用性和可靠性。例如，企业资源规划（ERP）系统利用数据库整合财务、供应链和人力资源等多个模块的数据，实现业务流程的优化和自动化。\n数据库的发展历史 数据库技术的发展经历了多个重要阶段，每一阶段的进步都显著提升了数据管理的效率和能力。\n早期阶段：层次模型与网状模型\n20世纪60年代，层次模型（Hierarchical Model）和网状模型（Network Model）是最早的数据库模型。层次模型以树状结构组织数据，适用于具有明确层级关系的数据存储，如IBM的IMS系统。网状模型则允许更复杂的多对多关系，适用于需要灵活数据关系的应用，如CODASYM和IDMS系统。然而，这些模型在灵活性和扩展性方面存在一定局限，难以应对日益复杂的数据需求。\n关系模型的兴起\n1970年代，埃德加·F·科德（Edgar F. Codd）提出了关系模型（Relational Model），彻底改变了数据库设计与管理的方式。关系模型以表格（关系）的形式组织数据，通过行和列表示记录和字段，极大地简化了数据操作和查询。1979年，IBM推出的SQL语言成为关系数据库的标准查询语言，使得数据操作更加直观和高效。此后，关系数据库管理系统（RDBMS）如Oracle、MySQL、PostgreSQL和Microsoft SQL Server迅速普及，成为主流数据库技术。\n面向对象与NoSQL数据库\n随着互联网和大数据时代的到来，传统的关系数据库在处理大规模、非结构化数据方面遇到挑战。1990年代末至2000年代初，面向对象数据库（OODBMS）开始出现，试图将面向对象编程的优势引入数据库管理。然而，真正推动数据库技术革新的是NoSQL数据库（如MongoDB、Cassandra和Redis），它们通过键值存储、文档存储、列存储和图数据库等不同的数据模型，提供了更高的扩展性和灵活性，满足了大数据和实时应用的需求。\n现代数据库技术\n进入21世纪，数据库技术持续演进，融合了分布式计算、云计算和人工智能等前沿技术。分布式数据库和NewSQL数据库结合了NoSQL的扩展性和关系数据库的ACID特性，提供了高性能和强一致性的解决方案。云数据库服务（如Amazon RDS、Google Cloud Spanner和Microsoft Azure SQL Database）使得数据库部署和管理更加便捷，支持按需扩展和高可用性。与此同时，时序数据库（如InfluxDB）和图数据库（如Neo4j）等新兴数据库类型，进一步拓展了数据库技术的应用范围。\n数据库系统的组成 一个完整的数据库系统通常由以下几个主要部分组成：\n数据库管理系统（DBMS）：DBMS 是数据库系统的核心，负责数据的存储、检索、更新、权限管理和备份恢复等功能。常见的DBMS包括MySQL、PostgreSQL、Oracle和Microsoft SQL Server等。 数据库：数据库是由实际存储的数据构成的集合，按照特定的模式组织起来。数据可以以表格、文档、图形等形式存在，具体取决于所使用的数据库模型。 用户接口：用户接口允许用户与数据库系统进行交互。接口可以是图形用户界面（GUI），如phpMyAdmin，或是命令行界面（CLI），如MySQL Shell，甚至是通过应用程序编程接口（API）进行的程序化访问。 应用程序：应用程序利用数据库提供的数据支持各种业务逻辑和功能实现。例如，电子商务网站通过数据库管理商品信息、用户账户和订单处理，社交媒体平台通过数据库存储用户发布的内容和互动记录。 硬件与网络：数据库系统依赖于底层的硬件和网络基础设施，确保数据存储的可靠性和访问的高效性。现代数据库系统通常运行在高性能服务器上，并通过高速网络实现分布式访问和数据同步。 数据库的重要性 数据库在现代信息系统中占据核心地位，支撑着各行各业的运作。例如，银行系统依赖数据库管理账户信息和交易记录，确保金融交易的准确和安全；医疗机构通过数据库存储患者的病历和治疗方案，提升医疗服务的效率和质量；电商平台利用数据库处理海量的用户订单和库存信息，支持高效的商品交易和供应链管理。数据库不仅提高了数据处理的效率，还通过数据分析和挖掘，助力企业做出更明智的决策，推动业务创新和增长。\n数据库模型 数据库模型是用于定义、描述和组织数据以及数据之间关系的框架。不同的数据库模型提供了不同的数据组织方式和操作机制，以适应各种应用需求和数据特性。理解各种数据库模型的结构、优缺点及适用场景，对于选择合适的数据库系统至关重要。本章将深入探讨主要的数据库模型，包括层次模型、网状模型、关系模型、面向对象模型以及文档型和键值型模型。\n数据库模型 结构描述 优点 缺点 典型应用 层次模型 以树状结构组织数据，每个记录（节点）有一个唯一的父节点，类似于文件系统的目录结构。 - 简单直观：结构易于理解和实现\n- 高效访问：适合一对多关系的数据访问速度快 - 灵活性差：不支持多对多关系，难以表示复杂数据关系\n- 数据冗余：容易导致数据重复，增加维护难度 IBM的IMS系统，用于电信和银行行业管理客户信息及交易记录。 网状模型 允许每个记录有多个父节点，通过指针（链接）形成网络结构，支持多对多关系。 - 更高灵活性：可表示复杂的多对多关系\n- 减少数据冗余：通过共享指针减少数据重复存储 - 复杂性高：模型设计和维护较为复杂\n- 操作困难：需要编程处理指针，增加开发难度 CODASYM和IDMS，应用于航空航天、制造和电信等需要复杂数据关系管理的领域。 关系模型 采用表格（关系）形式组织数据，每个表由行（记录）和列（字段）组成，表与表之间通过主键和外键建立关系。 - 高度灵活：支持复杂查询和多表关联\n- 数据独立性：逻辑数据独立于物理存储，便于维护和扩展\n- 标准化：SQL提供一致的操作接口 - 性能瓶颈：大规模数据和高并发访问可能导致性能问题\n- 设计复杂：规范化过程复杂，设计不当可能导致查询效率低下 Oracle、MySQL、PostgreSQL、Microsoft SQL Server，广泛应用于金融、电子商务、企业管理等各个领域。 面向对象模型 将数据表示为对象，包含属性（字段）和方法（操作），对象通过继承、封装和多态等机制建立关系，支持复杂数据类型和嵌套结构。 - 自然的数据建模：与面向对象编程语言无缝集成，便于开发复杂应用\n- 支持复杂数据类型：适用于处理嵌套和多态数据结构 - 学习曲线陡峭：需要理解面向对象概念和数据库设计，增加开发难度\n- 性能开销：封装和继承机制可能带来额外性能开销 db4o、ObjectDB，适用于CAD/CAM系统、工程设计和科学计算等需要复杂数据表示和操作的应用。 文档型模型 以文档为基本存储单元，通常使用JSON、BSON或XML格式，支持嵌套和复杂的数据结构，每个文档可拥有不同的字段和结构。 - 灵活模式设计：无需预定义模式，支持动态和多样化数据结构\n- 易于扩展：支持水平扩展，适合分布式部署\n- 高效数据访问：适用于内容管理和用户数据存储 - 事务支持有限：多数文档型数据库在事务管理方面不如关系数据库成熟\n- 数据一致性：分布式环境下保持数据一致性较复杂 MongoDB、CouchDB，广泛应用于内容管理系统、日志存储、实时分析和用户数据管理等场景。 键值型模型 以键-值对形式存储数据，键是唯一标识，值可以是任意数据类型（字符串、对象、二进制数据等）。 - 极高性能：结构简单，读写速度极快\n- 易于扩展：支持分布式架构，便于水平扩展\n- 灵活性：值可以是任意数据类型，适应不同存储需求 - 功能有限：缺乏复杂查询和事务支持，适用场景有限\n- 数据关联复杂：不支持多表关联，无法直接处理复杂数据关系 Redis、Riak，适用于缓存系统、会话管理、实时数据分析和排行榜等需要高性能读写的场景。 各数据库模型对比总结 特性 层次模型 网状模型 关系模型 面向对象模型 文档型模型 键值型模型 数据结构 树状结构 网络结构 表格（关系） 对象 文档（JSON/BSON/XML） 键-值对 适用关系 一对多 多对多 多种关系 复杂数据关系 灵活、动态的数据结构 简单、键值访问 扩展性 低 中 中 中 高 非常高 查询复杂度 低 中 高 高 中到高 低 事务支持 较弱 较弱 强 中到强 较弱 较弱 典型数据库系统 IBM IMS CODASYM, IDMS Oracle, MySQL, PostgreSQL db4o, ObjectDB MongoDB, CouchDB Redis, Riak 典型应用领域 电信、银行 航空航天、制造 金融、电子商务、企业管理 CAD/CAM、工程设计 内容管理、日志存储 缓存系统、会话管理 选择合适的数据库模型 选择合适的数据库模型需考虑以下因素：\n数据关系复杂度：关系模型适合复杂关系，层次和网状模型适合固定或多对多关系。 扩展需求：NoSQL模型如文档型和键值型适合需要高扩展性的应用。 查询需求：关系模型支持复杂查询，键值型适合简单、高速访问。 事务需求：关系模型提供强事务支持，NoSQL模型在事务管理上相对较弱。 开发环境：面向对象模型适合与面向对象编程语言集成的应用。 关系数据库管理系统（RDBMS） 关系数据库管理系统（Relational Database Management System，简称RDBMS）是基于关系模型的数据库管理系统。RDBMS通过结构化的表格（关系）来组织和管理数据，利用SQL（结构化查询语言）进行数据操作和查询。RDBMS以其高度的灵活性、数据独立性和强大的数据一致性保障，成为现代数据库系统的主流选择。\n基本概念 RDBMS的核心概念围绕关系模型展开，主要包括表、行、列、主键、外键、视图和索引等。\n术语 定义 表（Table） 数据以行和列的形式存储的二维结构，每个表代表一个实体或关系。 行（Row/Record） 表中的每一条记录，代表实体的一个实例。 列（Column/Field） 表中的一个属性，定义了数据的类型和含义。 主键（Primary Key） 唯一标识表中每一行的字段，确保记录的唯一性。 外键（Foreign Key） 用于建立与其他表关联的字段，维护数据之间的参照完整性。 视图（View） 虚拟表，通过查询一个或多个表生成，提供数据的不同视角。 索引（Index） 数据结构，用于加速数据的检索，提高查询性能。 主要RDBMS软件 市场上有多种RDBMS软件，每种软件在功能、性能和适用场景上有所不同。以下是一些主要的RDBMS软件及其特点：\n数据库系统 特点 适用场景 MySQL 开源、易用、性能优越，广泛用于Web应用。支持多种存储引擎（如InnoDB）。 网站开发、内容管理系统、电子商务平台 PostgreSQL 开源、功能强大，支持复杂查询、事务和扩展性。具有高度的标准兼容性。 数据分析、地理信息系统（GIS）、金融应用 Oracle Database 商业级数据库，功能全面，支持大规模数据和高并发。提供高级安全性和数据恢复功能。 企业级应用、金融系统、电信行业 Microsoft SQL Server 集成度高，与微软生态系统无缝衔接，提供丰富的管理工具和商业智能功能。 企业应用、商业智能、数据仓库 SQLite 轻量级、嵌入式数据库，无需独立服务器，适合移动和嵌入式应用。 移动应用、嵌入式系统、桌面应用 SQL语言 SQL（Structured Query Language）是用于管理和操作关系数据库的标准语言。SQL分为多个子语言，每个子语言负责不同的数据库操作。\n数据定义语言（DDL） DDL用于定义数据库的结构和模式，主要包括创建、修改和删除数据库对象的命令。\n命令 功能 示例 CREATE 创建数据库、表、视图等对象 CREATE TABLE Employees (ID INT PRIMARY KEY, Name VARCHAR(50)); ALTER 修改现有数据库对象 ALTER TABLE Employees ADD COLUMN Age INT; DROP 删除数据库对象 DROP TABLE Employees; TRUNCATE 清空表中的所有数据，但保留表结构 TRUNCATE TABLE Employees; 数据操作语言（DML） DML用于对数据库中的数据进行操作，包括插入、更新和删除。\n命令 功能 示例 INSERT 向表中插入新记录 INSERT INTO Employees (ID, Name) VALUES (1, 'Alice'); UPDATE 更新表中现有记录 UPDATE Employees SET Age = 30 WHERE ID = 1; DELETE 删除表中的记录 DELETE FROM Employees WHERE ID = 1; 数据控制语言（DCL） DCL用于控制对数据库的访问权限，确保数据的安全性。\n命令 功能 示例 GRANT 授予用户权限 GRANT SELECT, INSERT ON Employees TO user1; REVOKE 撤销用户权限 REVOKE INSERT ON Employees FROM user1; 数据查询语言（DQL） DQL主要用于查询数据库中的数据，最常用的命令是SELECT。\n命令 功能 示例 SELECT 从一个或多个表中检索数据 SELECT Name, Age FROM Employees WHERE Age \u0026gt; 25; RDBMS的优势与挑战 优势 数据一致性与完整性：通过事务管理和约束机制（如主键、外键、唯一约束等），确保数据的一致性和完整性。 灵活的查询能力：SQL提供了强大的查询功能，支持复杂的数据检索和操作。 数据独立性：逻辑数据与物理存储分离，便于数据库的维护和扩展。 安全性：细粒度的权限控制，确保数据的安全访问。 标准化：SQL作为标准语言，具有广泛的兼容性和可移植性。 挑战 性能瓶颈：在处理大规模数据和高并发访问时，RDBMS可能面临性能瓶颈，需通过优化索引、查询和硬件资源来提升性能。 复杂性：数据库设计和管理需要专业知识，规范化过程复杂，错误设计可能导致查询效率低下。 扩展性限制：传统RDBMS在水平扩展（即分布式扩展）方面存在一定困难，面对大数据和分布式应用时可能不够灵活。 典型RDBMS软件详解 MySQL 特点：\n开源免费，社区支持活跃。 多存储引擎（如InnoDB支持事务和外键）。 易于安装和使用，适合中小型应用。 应用案例：\nWordPress：作为全球最流行的内容管理系统，WordPress依赖MySQL存储和管理网站内容。 Facebook：早期阶段大量使用MySQL来管理用户数据和社交关系。 PostgreSQL 特点：\n开源且功能丰富，支持复杂查询和扩展。 强大的事务管理和数据完整性保障。 支持多种数据类型和索引方式，如JSON、GIS数据。 应用案例：\nReddit：使用PostgreSQL来管理大量用户生成内容和社交互动。 Disqus：全球最大的评论系统之一，依赖PostgreSQL处理高并发的评论数据。 Oracle Database 特点：\n商业级数据库，提供全面的功能和高可靠性。 支持大规模数据处理和高可用性配置（如Real Application Clusters）。 强大的安全性和数据恢复能力。 应用案例：\n银行系统：如摩根大通、汇丰银行使用Oracle数据库管理客户账户和交易数据。 航空公司：如美国航空使用Oracle数据库优化航班调度和客户服务。 Microsoft SQL Server 特点：\n与微软生态系统紧密集成，如Azure云服务。 提供丰富的商业智能和数据分析工具（如SQL Server Reporting Services）。 支持高级安全和合规性功能。 应用案例：\n企业资源规划（ERP）系统：许多企业使用SQL Server作为其ERP系统的后端数据库。 商业智能应用：通过集成的分析工具，帮助企业进行数据驱动的决策。 SQL语言的深入应用 联接（JOIN） 联接操作用于在多个表之间建立关联，从而检索相关的数据。\n类型 描述 示例 内联接（INNER JOIN） 仅返回两个表中匹配的记录。 SELECT Employees.Name, Departments.DeptName FROM Employees INNER JOIN Departments ON Employees.DeptID = Departments.ID; 左外联接（LEFT OUTER JOIN） 返回左表的所有记录，以及右表中匹配的记录。 SELECT Employees.Name, Departments.DeptName FROM Employees LEFT OUTER JOIN Departments ON Employees.DeptID = Departments.ID; 右外联接（RIGHT OUTER JOIN） 返回右表的所有记录，以及左表中匹配的记录。 SELECT Employees.Name, Departments.DeptName FROM Employees RIGHT OUTER JOIN Departments ON Employees.DeptID = Departments.ID; 全外联接（FULL OUTER JOIN） 返回两个表中所有记录，匹配的部分合并，不匹配的部分填充NULL。 SELECT Employees.Name, Departments.DeptName FROM Employees FULL OUTER JOIN Departments ON Employees.DeptID = Departments.ID; 子查询（Subquery） 子查询是嵌套在其他查询中的查询，用于分步骤获取数据。\n类型 描述 示例 标量子查询（Scalar Subquery） 返回单个值的子查询。 SELECT Name FROM Employees WHERE ID = (SELECT ManagerID FROM Departments WHERE DeptName = 'Sales'); 相关子查询（Correlated Subquery） 子查询依赖于外部查询的每一行数据。 SELECT e.Name FROM Employees e WHERE EXISTS (SELECT 1 FROM Departments d WHERE d.ManagerID = e.ID); 非相关子查询（Non-correlated Subquery） 子查询独立于外部查询，可以单独执行。 SELECT Name FROM Employees WHERE DeptID IN (SELECT ID FROM Departments WHERE Location = 'New York'); 聚合函数（Aggregate Functions） 聚合函数用于对一组值执行计算，返回单一的汇总结果。\n函数 描述 示例 COUNT() 计算行数。 SELECT COUNT(*) FROM Employees; SUM() 计算总和。 SELECT SUM(Salary) FROM Employees WHERE DeptID = 1; AVG() 计算平均值。 SELECT AVG(Age) FROM Employees; MAX() 找出最大值。 SELECT MAX(Salary) FROM Employees; MIN() 找出最小值。 SELECT MIN(Age) FROM Employees; 事务控制（Transaction Control） 事务是数据库操作的逻辑单元，确保数据的一致性和完整性。事务具有ACID特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。\n命令 功能 示例 BEGIN TRANSACTION 开始一个事务。 BEGIN TRANSACTION; COMMIT 提交事务，保存所有更改。 COMMIT; ROLLBACK 回滚事务，撤销所有更改。 ROLLBACK; SAVEPOINT 设置事务中的保存点，可以部分回滚。 SAVEPOINT SavePoint1; RELEASE SAVEPOINT 释放之前设置的保存点。 RELEASE SAVEPOINT SavePoint1; ROLLBACK TO SAVEPOINT 回滚到指定的保存点。 ROLLBACK TO SAVEPOINT SavePoint1; 查询优化 查询优化是提升数据库性能的重要手段，通过合理设计查询语句和数据库结构，减少查询时间和资源消耗。\n执行计划（Execution Plan） 执行计划是数据库查询优化器为执行查询而制定的步骤和路径。通过分析执行计划，可以了解查询的执行过程，识别性能瓶颈。\n示例：\n1 2 3 4 EXPLAIN SELECT Employees.Name, Departments.DeptName FROM Employees INNER JOIN Departments ON Employees.DeptID = Departments.ID WHERE Employees.Age \u0026gt; 30; 索引优化 索引是加速数据检索的重要工具。合理设计索引结构，可以显著提升查询性能。\n索引类型 描述 适用场景 单列索引（Single-Column Index） 基于单个列创建的索引。 经常在WHERE子句中使用的单个列。 复合索引（Composite Index） 基于多个列创建的索引，按列的顺序排列。 经常在WHERE子句中使用多个列组合。 唯一索引（Unique Index） 确保索引列中的值唯一。 主键和需要唯一约束的列。 全文索引（Full-Text Index） 支持全文搜索，适用于大文本字段。 内容管理系统、搜索引擎。 哈希索引（Hash Index） 基于哈希表实现的索引，适用于等值查询。 内存数据库和高性能缓存系统。 示例：\n1 2 CREATE INDEX idx_age ON Employees(Age); CREATE INDEX idx_deptid_name ON Employees(DeptID, Name); 查询重写（Query Rewrite） 通过重写查询语句，使其更加高效，减少不必要的计算和数据扫描。\n示例： 优化前：\n1 SELECT * FROM Employees WHERE Age \u0026gt; 30 AND Age \u0026gt; 25; 优化后：\n1 SELECT * FROM Employees WHERE Age \u0026gt; 30; RDBMS的安全性 RDBMS提供多层次的安全机制，确保数据的机密性、完整性和可用性。\n用户认证与授权 用户认证：验证用户身份，确保只有授权用户可以访问数据库系统。 授权：赋予用户特定的权限，如SELECT、INSERT、UPDATE、DELETE等操作权限。 示例：\n1 2 CREATE USER \u0026#39;user1\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; GRANT SELECT, INSERT ON Employees TO \u0026#39;user1\u0026#39;@\u0026#39;localhost\u0026#39;; 角色与权限管理 通过角色管理，将权限分配给角色，再将角色分配给用户，简化权限管理。\n示例：\n1 2 3 CREATE ROLE \u0026#39;read_write\u0026#39;; GRANT SELECT, INSERT, UPDATE, DELETE ON Employees TO \u0026#39;read_write\u0026#39;; GRANT \u0026#39;read_write\u0026#39; TO \u0026#39;user1\u0026#39;; 数据加密 静态数据加密：对存储在数据库中的数据进行加密，防止未经授权的访问。 传输加密：通过SSL/TLS加密数据库客户端与服务器之间的通信，保护数据传输过程中的安全。 示例：\n1 2 -- 开启SSL连接 ALTER USER \u0026#39;user1\u0026#39;@\u0026#39;localhost\u0026#39; REQUIRE SSL; 审计与监控 RDBMS通常提供审计功能，记录用户活动和数据操作，帮助检测和防范潜在的安全威胁。\n示例：\n1 2 -- 启用审计日志 AUDIT SELECT, INSERT, UPDATE, DELETE ON Employees BY user1; 数据库备份与恢复 备份与恢复是确保数据安全和业务连续性的关键措施。\n备份策略 完全备份（Full Backup）：备份整个数据库，恢复简单但耗时。 增量备份（Incremental Backup）：仅备份自上次备份以来发生变化的数据，节省存储空间和时间。 差异备份（Differential Backup）：备份自上次完全备份以来所有变化的数据。 恢复方法 恢复点：选择特定时间点或备份集进行恢复。 恢复时间目标（RTO）：系统恢复所需的最大允许时间。 恢复点目标（RPO）：允许的数据丢失量，通常以时间衡量。 示例：\n1 2 3 4 5 -- 完全备份 BACKUP DATABASE Employees TO DISK = \u0026#39;backup_employees.bak\u0026#39;; -- 恢复数据库 RESTORE DATABASE Employees FROM DISK = \u0026#39;backup_employees.bak\u0026#39;; 高可用性方案 主从复制（Master-Slave Replication）：将主数据库的更改复制到从数据库，实现读写分离和负载均衡。 集群与分片（Clustering and Sharding）：将数据库分布在多个节点上，提高系统的可用性和扩展性。 示例：\n1 2 3 -- 设置主从复制 CHANGE MASTER TO MASTER_HOST=\u0026#39;master_host\u0026#39;, MASTER_USER=\u0026#39;replica_user\u0026#39;, MASTER_PASSWORD=\u0026#39;password\u0026#39;; START SLAVE; RDBMS的性能优化 提升RDBMS性能的方法包括优化查询、合理设计索引、调整数据库参数和利用缓存机制。\n查询优化 通过编写高效的SQL查询，减少不必要的数据扫描和计算。\n示例： 避免使用SELECT *，仅查询需要的列：\n1 SELECT Name, Age FROM Employees WHERE Age \u0026gt; 30; 索引优化 合理设计和维护索引，提升数据检索速度。\n示例： 定期重建和优化索引：\n1 2 -- 重建索引 ALTER INDEX idx_age ON Employees REBUILD; 数据库参数调整 根据实际工作负载调整数据库配置参数，如缓冲池大小、连接数和缓存策略，优化数据库性能。\n示例： 调整MySQL的缓冲池大小：\n1 SET GLOBAL innodb_buffer_pool_size = 2G; 缓存机制 利用缓存机制，如查询缓存、应用层缓存和分布式缓存，减少数据库的直接访问，提高响应速度。\n示例： 启用MySQL查询缓存：\n1 2 SET GLOBAL query_cache_size = 262144; SET GLOBAL query_cache_type = 1; SQL与查询优化 SQL（Structured Query Language，结构化查询语言）是关系数据库中用于定义、操作和查询数据的核心工具。查询优化则是提升SQL语句执行效率、减少资源消耗的重要技术。本章将详细介绍SQL的基本语法、高级功能以及常见的查询优化技术，帮助构建高效的数据库应用。\n基本SQL语法 SQL分为四类主要操作语言：\n数据定义语言（DDL） 用于创建和管理数据库结构。\n示例： 1 2 3 4 5 6 7 8 CREATE TABLE Employees ( ID INT PRIMARY KEY, Name VARCHAR(50), Age INT, DeptID INT ); ALTER TABLE Employees ADD COLUMN Salary DECIMAL(10, 2); DROP TABLE Employees; 数据操作语言（DML） 用于对表中的数据进行增、删、改。\n示例： 1 2 3 INSERT INTO Employees (ID, Name, Age, DeptID) VALUES (1, \u0026#39;Alice\u0026#39;, 30, 101); UPDATE Employees SET Salary = 75000 WHERE ID = 1; DELETE FROM Employees WHERE Age \u0026lt; 25; 数据查询语言（DQL） 用于从表中检索数据。\n示例： 1 SELECT Name, Age FROM Employees WHERE DeptID = 101; 数据控制语言（DCL） 用于设置访问权限，确保数据安全。\n示例： 1 2 GRANT SELECT, INSERT ON Employees TO \u0026#39;user1\u0026#39;; REVOKE INSERT ON Employees FROM \u0026#39;user1\u0026#39;; 高级SQL功能 联接（JOIN） 联接用于在多个表之间关联数据。\n类型 描述 示例 内联接 返回两个表中匹配的记录 SELECT e.Name, d.DeptName FROM Employees e INNER JOIN Departments d ON e.DeptID = d.ID; 左外联接 返回左表所有记录，匹配的右表记录 SELECT e.Name, d.DeptName FROM Employees e LEFT JOIN Departments d ON e.DeptID = d.ID; 右外联接 返回右表所有记录，匹配的左表记录 SELECT e.Name, d.DeptName FROM Employees e RIGHT JOIN Departments d ON e.DeptID = d.ID; 全外联接 返回两个表的所有记录 SELECT e.Name, d.DeptName FROM Employees e FULL OUTER JOIN Departments d ON e.DeptID = d.ID; 子查询（Subquery） 子查询是嵌套在其他查询中的查询。\n标量子查询：返回单个值。 1 SELECT Name FROM Employees WHERE DeptID = (SELECT ID FROM Departments WHERE DeptName = \u0026#39;Sales\u0026#39;); 相关子查询：依赖外部查询。 1 2 3 SELECT e.Name FROM Employees e WHERE EXISTS ( SELECT 1 FROM Departments d WHERE d.ManagerID = e.ID ); 聚合函数（Aggregate Functions） 用于对一组数据进行汇总操作。\n常用函数： 1 2 3 SELECT COUNT(*) AS TotalEmployees FROM Employees; SELECT AVG(Salary) AS AvgSalary FROM Employees; SELECT MAX(Age) AS OldestEmployee FROM Employees; 事务控制（Transactions） 事务是一组逻辑操作单元，具有ACID特性。\n示例： 1 2 3 4 BEGIN TRANSACTION; UPDATE Employees SET Salary = Salary * 1.1 WHERE DeptID = 101; DELETE FROM Employees WHERE Age \u0026lt; 20; COMMIT; 查询优化技术 查询优化旨在通过调整SQL语句和数据库结构，提升查询效率。\n索引优化 索引是提高查询效率的重要工具。\n单列索引：适合WHERE条件中的单个字段。 1 CREATE INDEX idx_name ON Employees(Name); 复合索引：适合多条件查询。 1 CREATE INDEX idx_dept_age ON Employees(DeptID, Age); 注意事项： 避免在频繁更新的列上创建索引。 删除冗余或未使用的索引。 查询重写 通过优化SQL语句结构，减少资源消耗。\n避免重复计算： 优化前： 1 SELECT * FROM Employees WHERE Age \u0026gt; 30 AND Age \u0026gt; 25; 优化后： 1 SELECT * FROM Employees WHERE Age \u0026gt; 30; 避免SELECT *： 查询所需列而非所有列。 1 SELECT Name, Age FROM Employees WHERE Age \u0026gt; 30; 执行计划分析 通过分析执行计划，找出SQL性能瓶颈。\n示例： 1 EXPLAIN SELECT Name FROM Employees WHERE Age \u0026gt; 30; 分区表与分片 将数据分区或分片存储，提升查询效率。\n范围分区： 1 2 3 4 CREATE TABLE Employees_2023 PARTITION BY RANGE (HireDate) ( PARTITION p1 VALUES LESS THAN (\u0026#39;2023-01-01\u0026#39;), PARTITION p2 VALUES LESS THAN (\u0026#39;2023-12-31\u0026#39;) ); 分片应用： 将数据分布到多个数据库节点。 缓存机制 利用缓存减少数据库访问频率。\n查询缓存： 1 2 SET GLOBAL query_cache_size = 1048576; -- 设置缓存大小 SET GLOBAL query_cache_type = 1; -- 开启查询缓存 数据库参数调优 调整数据库系统的配置参数。\n缓冲池优化： 1 SET GLOBAL innodb_buffer_pool_size = 2G; 连接池设置： 1 SET GLOBAL max_connections = 500; 查询优化实例 以下是一个实际优化案例的完整流程：\n初始查询：\n1 2 3 SELECT * FROM Employees WHERE Age \u0026gt; 30 AND DeptID IN ( SELECT ID FROM Departments WHERE Location = \u0026#39;New York\u0026#39; ); 分析问题：\n查询包含嵌套子查询，执行成本较高。 使用SELECT *增加不必要的数据传输。 优化查询：\n改用联接替代子查询。 明确选择所需列。 1 2 3 4 SELECT e.Name, e.Age FROM Employees e INNER JOIN Departments d ON e.DeptID = d.ID WHERE e.Age \u0026gt; 30 AND d.Location = \u0026#39;New York\u0026#39;; 创建索引：\n1 2 CREATE INDEX idx_dept_location ON Departments(Location); CREATE INDEX idx_employees_age ON Employees(Age); SQL是数据库操作的核心工具，其高级功能（如联接、子查询、聚合函数和事务）极大地增强了数据管理的灵活性。通过索引优化、查询重写、执行计划分析等技术，可以显著提升查询效率和系统性能。在实际应用中，结合数据库结构设计、硬件资源配置和缓存机制等多种优化手段，能够构建性能卓越的数据库系统，为复杂业务需求提供可靠支持。\n数据库设计 数据库设计是构建高效、可靠的数据存储和管理系统的关键过程，其目标是以合理的方式组织和结构化数据，以满足特定的业务需求和性能要求。一个良好的数据库设计能够提高数据存取效率，确保数据的一致性和完整性，并为系统的扩展和维护奠定坚实的基础。本章将深入探讨数据库设计的各个阶段，包括需求分析、概念设计、逻辑设计、物理设计，以及反规范化和性能优化等内容。\n需求分析 需求分析是数据库设计的起点，旨在深入了解业务需求和数据需求，为后续的设计工作提供明确的指导方向。通过与业务人员、用户和其他利益相关者的沟通，收集和整理系统所需的功能和性能要求。\n功能需求 功能需求关注系统需要实现的具体业务功能和数据处理能力。例如，在一个电子商务平台中，功能需求可能包括用户注册与登录、商品浏览与搜索、购物车管理、订单处理、支付结算和售后服务等。明确这些功能需求有助于确定数据库需要存储和管理哪些数据，以及需要支持哪些类型的操作。\n非功能需求 非功能需求涉及系统的性能、安全性、可用性、可扩展性和易维护性等方面。例如，系统需要支持的并发用户数量、页面响应时间、数据的备份和恢复策略、安全认证机制以及未来的业务增长预期等。这些非功能需求将直接影响数据库的设计策略，如需要采用何种数据分区、索引优化和高可用性方案等。\n概念设计 概念设计阶段旨在通过抽象和建模，将现实世界的业务需求转化为数据模型，独立于具体的数据库管理系统。最常用的工具是实体-关系模型（ER模型），通过识别实体、属性和关系，建立数据的概念模型。\n实体-关系模型（ER模型） ER模型由实体（Entity）、属性（Attribute）和关系（Relationship）组成：\n实体：表示现实世界中可区分的对象或概念，如用户、商品、订单等。 属性：描述实体的特征或性质，如用户的姓名、邮箱，商品的价格、库存等。 关系：表示实体之间的关联，如用户下订单、订单包含商品等。 通过绘制ER图，可以直观地展示各实体及其之间的关系，有助于理解数据结构和业务逻辑。例如，在一个在线图书商城中，可能存在以下实体和关系：\n实体： 用户（User）：属性包括用户ID、姓名、邮箱、密码等。 商品（Product）：属性包括商品ID、名称、作者、价格、库存等。 订单（Order）：属性包括订单ID、订单日期、总金额、用户ID等。 关系： 用户与订单：一个用户可以有多个订单（1对多关系）。 订单与商品：一个订单可以包含多个商品，一个商品也可以出现在多个订单中（多对多关系）。 逻辑设计 逻辑设计阶段将概念模型转换为逻辑数据模型，通常是关系模型，以便在关系数据库管理系统中实现。在这一阶段，需要将实体和关系映射为数据库表，定义字段、数据类型、主键、外键和其他约束。\n关系模式设计 根据ER模型，将每个实体转换为一个关系（表），每个属性转换为表中的字段。对于多对多关系，需要创建关联表来表示。例如：\n用户表（User）： UserID（主键） UserName Email Password 商品表（Product）： ProductID（主键） Name Author Price Stock 订单表（Order）： OrderID（主键） OrderDate TotalAmount UserID（外键，关联User表） 订单明细表（OrderDetail）： OrderID（外键，关联Order表） ProductID（外键，关联Product表） Quantity UnitPrice 规范化 规范化是逻辑设计的重要步骤，旨在通过遵循一定的范式，消除数据冗余，避免数据更新异常。常见的范式包括：\n第一范式（1NF）：确保每个字段都是原子性的，不可再分割。 第二范式（2NF）：在1NF的基础上，消除非主属性对主键的部分函数依赖，使每个非主属性完全依赖于主键。 第三范式（3NF）：在2NF的基础上，消除非主属性之间的传递函数依赖，使每个非主属性直接依赖于主键。 通过规范化，可以提高数据的完整性和一致性，减少数据冗余。例如，将订单总金额存储在订单表中，而不在订单明细表中重复存储，以避免因数据冗余导致的更新异常。\n物理设计 物理设计阶段将逻辑数据模型转化为物理存储结构，涉及具体的数据库管理系统的实现细节和性能优化措施。\n存储结构 选择适当的数据类型和存储引擎，根据数据的性质和访问模式进行优化。例如，对于数值型数据，选择整数或浮点数类型；对于文本数据，选择VARCHAR或TEXT类型。还需要考虑字符集和编码方式，以支持多语言和特殊字符。\n索引设计 根据查询需求和数据访问频率，设计合适的索引以提高查询性能。常见的索引类型包括：\n主键索引：自动创建，用于唯一标识记录。 唯一索引：确保字段的值唯一，例如邮箱地址。 普通索引：在经常用于查询条件的字段上创建，如商品名称、作者等。 复合索引：在多个字段上创建的索引，适用于多条件组合查询。 需要权衡索引的数量和类型，以避免过多的索引增加写入操作的开销。\n分区与分库分表 对于大规模的数据，可以采用分区、分库或分表的策略，将数据分散存储，提高系统的可扩展性和性能。\n水平分区（Sharding）：根据某个规则（如用户ID的范围），将数据行分布到不同的物理节点上。 垂直分区：将表按照字段拆分，将经常访问的字段和不常访问的字段分开存储。 分区表：在同一数据库中，将表的数据按照某个字段的值（如日期）分区存储。 反规范化与性能优化 在实际应用中，为了满足性能要求，可能需要对高度规范化的数据库进行反规范化处理，适当引入数据冗余，以减少复杂的联接操作和提高查询效率。\n反规范化策略 增加冗余字段：在表中增加冗余的字段，存储关联表中的数据，减少联接。例如，在订单明细表中直接存储商品名称和价格。 预计算和缓存：对经常需要计算的统计数据，提前计算并存储在表中，如商品的总销量、平均评分等。 合并表：将频繁一起访问的表合并为一个表，减少联接操作的开销。 权衡与一致性维护 反规范化可能导致数据冗余和一致性问题，需要在性能和数据完整性之间进行权衡。为了维护数据一致性，可以采用以下措施：\n触发器：在数据修改时自动更新相关的冗余数据。 存储过程：使用存储过程封装数据操作逻辑，确保数据一致性。 应用程序逻辑：在应用层实现数据一致性的维护，确保在数据更新时同步修改相关数据。 数据库设计案例 以一个在线教育平台为例，综合运用上述设计方法，构建一个满足业务需求的数据库。\n需求分析 功能需求：用户注册与登录、课程浏览与搜索、课程购买、在线学习、课后测验、论坛交流等。 非功能需求：支持高并发用户访问、快速响应时间、安全的用户数据保护、支持多媒体内容的存储与访问。 概念设计 识别主要的实体和关系：\n实体： 用户（User）：属性包括用户ID、用户名、密码、邮箱、注册日期等。 课程（Course）：属性包括课程ID、标题、简介、价格、教师ID等。 教师（Teacher）：属性包括教师ID、姓名、资历、简介等。 订单（Order）：属性包括订单ID、用户ID、订单日期、总金额等。 学习记录（LearningRecord）：属性包括用户ID、课程ID、学习进度、最后访问时间等。 关系： 用户与订单：用户下订单购买课程。 课程与教师：一个课程由一位教师创建。 用户与课程：用户购买并学习课程。 用户与学习记录：记录用户在课程中的学习进度。 逻辑设计 将实体和关系转换为数据库表，定义字段和约束：\n用户表（User）： UserID（主键） UserName Password Email RegisterDate 课程表（Course）： CourseID（主键） Title Description Price TeacherID（外键，关联Teacher表） 教师表（Teacher）： TeacherID（主键） Name Qualification Bio 订单表（Order）： OrderID（主键） UserID（外键，关联User表） OrderDate TotalAmount 订单明细表（OrderDetail）： OrderID（外键，关联Order表） CourseID（外键，关联Course表） Price 学习记录表（LearningRecord）： UserID（外键，关联User表） CourseID（外键，关联Course表） Progress LastAccessDate 物理设计 数据类型选择：根据数据性质，选择适当的数据类型，如UserName使用VARCHAR(50)，Price使用DECIMAL(10,2)。 索引设计：在常用的查询字段上建立索引，如UserName、Email、CourseID、TeacherID等。 分区策略：对于大表（如学习记录表），可以根据用户ID或日期进行分区，提高查询性能。 反规范化与优化 冗余字段：在订单明细表中增加课程标题和教师姓名，减少联表查询的次数。 预计算数据：在课程表中增加字段，存储课程的购买人数、平均评分等统计信息，方便快速获取。 缓存机制：对于热门课程的信息，使用缓存技术（如Redis）存储，减少数据库访问压力。 数据库规范化 数据库规范化（Normalization）是一种系统化的设计方法，旨在通过消除冗余数据和解决数据异常来提升数据库的效率、完整性和可维护性。规范化的核心是分解数据表，确保每个表只表达单一的主题，同时利用范式（Normal Forms）作为指导原则。尽管这一过程看似复杂，但如果通过生动的例子、图表和形象化的类比，规范化的概念将变得清晰易懂。\n什么是规范化？ 想象你在管理一个书店的数据库。初始设计可能是这样的：\n订单ID 书名 作者 顾客姓名 顾客电话 订单日期 101 数据库原理 王强 李华 13888888888 2024-11-25 102 数据结构与算法 张三 张伟 13777777777 2024-11-24 101 计算机网络 李四 李华 13888888888 2024-11-25 这张表看似能存储所有信息，但问题也随之而来。例如，顾客信息被重复存储，每次有新订单都需要再次输入电话号码。如果某位顾客更改了电话号码，你需要在所有记录中逐一修改，否则数据将不一致。这种冗余不仅浪费存储空间，还导致了数据异常。\n规范化的目标就是将这样的表拆分为更小、更精确的表，以消除冗余和异常。例如，将顾客信息和订单信息分离后，数据库看起来像这样：\n顾客表（Customer）\n顾客ID 姓名 电话 1 李华 13888888888 2 张伟 13777777777 订单表（Order）\n订单ID 书名 作者 顾客ID 订单日期 101 数据库原理 王强 1 2024-11-25 102 数据结构与算法 张三 2 2024-11-24 103 计算机网络 李四 1 2024-11-25 这样，冗余消失了，顾客的信息只需要存储一次，更新起来也更方便。这就是规范化的魔力。\n规范化的基础：函数依赖 在理解规范化的过程中，“函数依赖”是一个关键概念。简单来说，如果某个字段的值依赖于另一个字段的值，就称其具有函数依赖关系。例如：\n顾客电话依赖于顾客姓名，因为每个顾客的电话是唯一的。 书名依赖于订单ID，因为每个订单记录了特定的书籍。 通过分析和优化这些依赖关系，可以识别出冗余和不必要的存储。\n我们可以用一幅简单的图形化表示来说明：\n1 2 订单ID → 顾客姓名 → 顾客电话 订单ID → 书名 → 作者 这表示“订单ID”决定了“顾客姓名”和“书名”，“顾客姓名”进一步决定了“顾客电话”，“书名”决定了“作者”。规范化的目标就是消除这种传递关系，使每个字段直接依赖主键，避免依赖链条。\n范式的分级 数据库规范化通常按照以下几个范式分级，每一级范式解决特定的问题。\n第一范式（1NF） 1NF的要求是所有字段必须是原子值，不能包含重复的字段或列表。例如：\n订单ID 书名 顾客姓名 101 数据库原理, 计算机网络 李华 这种设计违背了1NF的原则，因为“书名”包含了多个值。为符合1NF，需要将其拆分成多行：\n订单ID 书名 顾客姓名 101 数据库原理 李华 101 计算机网络 李华 第二范式（2NF） 2NF在1NF的基础上消除了部分函数依赖。例如，在订单表中，“顾客姓名”和“顾客电话”依赖于“订单ID”，但它们实际上属于顾客信息，与订单无关。这种依赖需要通过拆分表来解决：\n订单表：\n订单ID 书名 顾客ID 101 数据库原理 1 顾客表：\n顾客ID 姓名 电话 1 李华 13888888888 第三范式（3NF） 3NF要求消除传递函数依赖。例如，如果“订单表”中存储了“书籍价格”，而“书籍价格”依赖于“书名”，就会导致传递依赖。解决方案是将“书籍价格”提取到独立的“书籍表”中：\n书籍表：\n书名 价格 数据库原理 50.0 计算机网络 70.0 范式之间的平衡：规范化与反规范化 虽然规范化能够减少冗余和提升一致性，但过度规范化可能导致查询性能下降。比如，要查询某位顾客的订单详情，可能需要联接多个表，这会增加计算开销。因此，在实际应用中，需要根据业务需求在规范化与反规范化之间找到平衡。\n反规范化的常见手段包括：\n在订单表中直接存储“顾客姓名”，避免联表查询。 在“订单表”中预存订单总价，减少实时计算。 反规范化的设计需要权衡性能与数据一致性，可以借助触发器或程序逻辑来维护一致性。\n例子：开餐馆的账本 假设你经营了一家餐馆，记账时把每张餐桌点的菜写在一张纸上。最初的设计可能是这样的：\n桌号 菜品 价格 1 红烧肉, 糖醋鱼 120 2 青椒炒肉, 番茄炒蛋 80 后来你发现记录中信息太混乱，无法统计单个菜品的销量，也不知道每道菜的单价。这就像一个未规范化的数据库，数据堆叠在一起，难以维护。于是你改变策略，把每道菜单独记录：\n桌号 菜品 单价 数量 总价 1 红烧肉 60 1 60 1 糖醋鱼 60 1 60 通过这样的“规范化”，你可以轻松统计每道菜的销量，同时确保账目清晰。\n事务管理与并发控制 在数据库的世界里，“事务管理与并发控制”是一门涉及秩序与效率的艺术。事务（Transaction）是数据库操作的逻辑单元，并发控制则是协调多个事务同时执行的一种机制。良好的事务管理能够保证数据的一致性和完整性，而并发控制则在数据共享与冲突之间寻找平衡。\n事务的基本概念 从理论上说，事务是数据库操作的最小工作单位，具有ACID特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。这些特性确保了即使发生系统崩溃、硬件故障或多用户冲突，数据库仍然可以维持正常状态。\n原子性：事务是不可分割的整体，要么全部完成，要么完全回滚。例如，转账操作中，钱从一个账户中扣除，必须保证同时存入另一个账户。 一致性：事务前后，数据库必须从一个一致状态转移到另一个一致状态。例如，转账后，总金额保持不变。 隔离性：并发事务之间互不干扰，各自的中间状态对其他事务是不可见的。 持久性：一旦事务提交，其结果永久保存在数据库中，即使系统崩溃也不丢失。 用一个餐馆的比喻来理解：一名服务员接到的每张订单可以看作一个事务。服务员记录订单（原子性），确保账单金额总和与点菜数量一致（一致性）。每位服务员专注于自己顾客的点单，不会被其他服务员打扰（隔离性）。即便餐馆突然停电，记好的账单不会丢失（持久性）。\n并发问题与事务隔离级别 并发控制的意义在于，在多个用户同时访问数据库时，既能提高效率，又能避免数据不一致。然而，若没有良好的控制，并发问题将接踵而至：\n脏读：事务A读取了事务B未提交的数据，而事务B最终回滚，导致数据不一致。 不可重复读：事务A两次读取同一数据，发现其值被事务B修改了。 幻读：事务A两次查询某范围的数据，发现事务B插入了新的数据行。 为解决这些问题，SQL标准定义了四种事务隔离级别，从低到高依次为：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和序列化（Serializable）。隔离级别越高，并发性能越低，但数据一致性越强。\n读未提交允许脏读，性能最好。 读已提交确保事务只能读取已提交的数据。 可重复读防止不可重复读，但可能产生幻读。 序列化将所有事务排队，完全避免并发问题。 用一幅表格直观呈现隔离级别与并发问题的关系：\n隔离级别 脏读 不可重复读 幻读 读未提交（最低） 可能 可能 可能 读已提交 不可能 可能 可能 可重复读 不可能 不可能 可能 序列化（最高） 不可能 不可能 不可能 锁机制与并发控制 数据库通过锁机制实现并发控制。锁是一种资源控制手段，可以限制事务对数据的访问。主要有两种类型：\n共享锁（S锁）：允许多个事务同时读取数据，但不允许写入。 排他锁（X锁）：允许事务独占访问数据，其他事务既不能读取，也不能写入。 锁的粒度可以是表级锁、行级锁，甚至是字段锁。粒度越小，并发性能越高，但管理开销也越大。\n假设你和朋友在图书馆借书，图书管理员的操作就像数据库的锁机制。管理员允许你和朋友同时阅读同一本书（共享锁），但若有人想借走书（排他锁），其他人就不得再阅读。这种机制确保了书籍的完整性和使用效率。\n死锁与其解决 在并发环境中，事务之间可能形成一种互相等待的状态，称为死锁。用餐馆来比喻：两个顾客同时需要服务员A和B分别送水和点单，但服务员A正在忙碌点单，而B正在倒水，双方都在等待对方释放资源，这种场景就是死锁。\n数据库常用两种方法处理死锁：\n超时检测：若事务等待时间超过一定阈值，数据库强制回滚某一事务。 死锁检测：通过维护资源等待图，周期性检查循环依赖，并中断死锁事务。 例如，在MySQL中，死锁检测可以通过以下方式开启：\n1 SET innodb_deadlock_detect = ON; 日志与事务恢复 事务的持久性依赖于日志系统。数据库通过写入日志记录事务的操作步骤，确保即使发生崩溃，数据仍可恢复。主要有两种日志机制：\n重做日志（Redo Log）：记录已提交事务的修改，用于恢复丢失的数据。 撤销日志（Undo Log）：记录未提交事务的修改，用于回滚操作。 假设你正在超市购物，购物小票就是数据库的日志。如果你不小心摔碎了一盒鸡蛋，超市可以通过你的购物记录补给你一盒（重做日志）。如果你决定退还买错的商品，超市也会参考记录进行退货（撤销日志）。\n银行转账与晚餐账单 通过一个例子串联事务和并发控制的知识。想象你和朋友去餐馆吃饭，你们决定平均分摊账单。这可以看作一个事务，每人负责“转账”自己的部分。\n但假设系统允许“读未提交”的隔离级别，朋友查询了你还没提交的转账金额，提前转走了多余的钱，这种情况下系统数据就会不一致。于是，你要求提高到“读已提交”级别，这样朋友只能看到已完成的金额。但如果某个朋友插入了一个新的账单记录，你会发现账单金额突然多了，这就是“幻读”。\n为彻底解决问题，你选择了最高隔离级别“序列化”。现在，所有人必须排队转账，任何人都无法插队或提前修改数据。这虽然牺牲了效率，但保证了金额的准确性。\n数据库索引 在现代数据库管理系统中，索引（Index）是提升数据检索效率的关键技术之一。索引的作用类似于书籍的目录，通过为数据库表中的特定列创建结构化的查找机制，极大地减少了查询所需的时间和资源消耗。深入理解索引的原理、类型、设计与优化方法，不仅有助于提升数据库性能，还能在数据规模不断扩大的环境中保持系统的响应速度。\n索引的概念与作用 索引是一种数据结构，旨在快速定位数据库表中的特定记录。没有索引时，数据库在执行查询操作时需要进行全表扫描（Full Table Scan），逐行检查每条记录是否符合查询条件。这种方法在数据量庞大时效率极低，导致查询性能瓶颈。\n通过在表的一个或多个列上创建索引，数据库系统可以利用索引结构迅速定位目标记录，避免全表扫描。例如，在一个包含数百万条用户记录的表中，如果我们需要查找特定用户名的用户，索引能够显著缩短查询时间，从而提升整体系统性能。\n索引的基本原理 索引通常基于树结构（如B树）或哈希表构建，能够高效地进行数据的插入、删除和查找操作。以B树索引为例，其平衡的多级结构使得查找操作的时间复杂度保持在O(log n)级别，适用于大规模数据的高效检索。\n常见索引类型 数据库系统支持多种索引类型，每种类型在不同的应用场景下展现出其独特的优势。以下是几种常见的索引类型及其特点：\n索引类型 结构特点 适用场景 优缺点 B树索引 平衡树结构，支持有序数据存储 范围查询、排序操作 支持范围查询，适用广泛；维护成本适中 哈希索引 基于哈希表实现，键值直接映射 等值查询，如主键查找 查找速度极快；不支持范围查询 位图索引 使用位向量表示数据存在与否 低基数列，如性别、状态 高效处理复杂的多条件查询；占用较大存储 全文索引 针对文本内容进行索引，支持模糊匹配 文本搜索、内容管理系统 支持复杂文本搜索；占用较多存储，更新开销大 B树索引 B树索引是最常见的索引类型，广泛应用于各种数据库系统中。其多级平衡树结构确保了查找、插入和删除操作的高效性。B树索引特别适合处理有序数据和范围查询，例如按日期、价格区间查询记录。\n示例： 在一个销售记录表中，如果需要查询某一时间段内的所有销售记录，B树索引能够快速定位起始和结束日期之间的记录，避免全表扫描，显著提升查询效率。\n哈希索引 哈希索引基于哈希表实现，通过哈希函数将键值映射到固定位置，实现极快速的等值查询。然而，哈希索引不支持范围查询，因为哈希函数打破了数据的有序性。\n示例： 在用户登录系统中，验证用户名和密码的操作属于等值查询，此时使用哈希索引可以迅速定位用户记录，提高验证效率。\n位图索引 位图索引通过位向量表示数据的存在与否，特别适用于低基数列（即取值较少的列），如性别、状态等。位图索引能够高效地进行位操作，适合复杂的多条件查询。\n示例： 在一个员工表中，查询既是全职员工又在特定部门工作的记录，位图索引可以通过位操作迅速筛选出符合条件的记录，大幅提升查询性能。\n全文索引 全文索引用于在文本字段中进行内容搜索，支持复杂的文本匹配操作，如模糊搜索、词根匹配等。它常用于搜索引擎、内容管理系统等需要处理大量文本数据的应用场景。\n示例： 在博客平台中，用户搜索关键词时，全文索引能够快速查找到包含相关内容的文章，提升搜索体验。\n索引设计与优化 索引设计是数据库性能优化的重要环节，合理的索引设计不仅能提升查询速度，还能减少存储空间的占用。然而，索引的创建和维护也会带来一定的开销，因此需要在查询性能和维护成本之间找到平衡。\n设计原则 选择合适的列： 优先为经常出现在WHERE子句、JOIN条件或排序操作中的列创建索引。例如，在电商系统中，商品ID、用户ID和订单日期等字段通常是高频查询的目标，适合建立索引。\n避免过多索引： 每个索引都会占用存储空间，并在数据插入、更新、删除时带来额外的维护开销。应根据实际需求合理创建索引，避免不必要的索引堆积。\n复合索引： 对于多个列经常一起使用的查询，创建复合索引可以显著提升查询性能。例如，在一个订单表中，经常需要按用户ID和订单日期进行查询，复合索引（用户ID, 订单日期）可以提高这类查询的效率。\n优化策略 覆盖索引： 覆盖索引包含查询所需的所有列，使查询仅通过索引即可完成，避免回表操作（回表是指通过索引查找到记录位置后，再访问表数据）。例如，若查询只涉及用户名和邮箱，可以在这两个字段上创建复合索引，实现覆盖索引。\n索引选择性： 高选择性的列（即不同值较多的列）更适合创建索引，能够更有效地过滤数据。选择性低的列（如性别）创建索引的效果不明显，甚至可能降低性能。\n索引碎片整理： 随着数据的频繁插入、更新和删除，索引可能会产生碎片，影响查询性能。定期重建或重组索引，可以减少碎片，恢复索引的高效性。\n索引的维护 索引的维护包括定期监控、重建和优化，以确保其在数据变化后的性能表现。随着数据库中的数据不断插入、更新和删除，索引可能会出现碎片，影响查询性能。因此，定期维护索引是必不可少的。\n维护方法 监控索引使用情况： 通过数据库提供的监控工具（如MySQL的SHOW INDEX命令、PostgreSQL的pg_stat_user_indexes视图），分析哪些索引经常被使用，哪些索引很少使用。根据分析结果，决定是否需要调整或删除某些索引，以优化存储和性能。\n重建索引： 当索引碎片严重时，重建索引可以恢复其性能。不同数据库系统提供了不同的重建方式，如在线重建和离线重建。在线重建允许在不影响数据库正常运行的情况下进行，而离线重建则需要暂时停机。\n示例： 在一个电商平台，随着商品数量的增加和订单的频繁变化，原有的索引可能会出现碎片。通过定期重建索引，可以确保查询商品信息和订单状态时，依然保持高效的性能。\n调整索引结构： 根据查询需求的变化，调整索引的结构。例如，随着业务的发展，某些查询条件可能发生变化，此时需要添加新的索引或修改现有的复合索引，以适应新的查询模式。\n实践中的索引维护 在实际操作中，索引维护需要结合具体业务场景和数据库系统的特性。例如，针对高并发写操作的数据库，频繁的索引重建可能会影响系统的稳定性，此时需要选择合适的维护窗口或采用在线重建技术。此外，定期的索引审计也是确保数据库性能的重要步骤，能够及时发现并解决潜在的索引问题。\n索引的高级应用 除了基本的索引类型和设计原则，数据库系统还支持一些高级索引技术，以满足更复杂的查询需求和性能优化要求。\n聚集索引与非聚集索引 聚集索引（Clustered Index）： 聚集索引决定了表中数据的物理存储顺序。一个表只能有一个聚集索引，通常选择主键作为聚集索引。聚集索引的优势在于能够高效地进行范围查询和排序操作，但其维护成本较高，特别是在频繁插入和删除操作的情况下。\n非聚集索引（Non-Clustered Index）： 非聚集索引是独立于数据存储顺序的索引，表中可以存在多个非聚集索引。非聚集索引通过指针引用数据位置，适用于各种查询场景。相比聚集索引，非聚集索引的维护成本较低，但在某些查询操作中可能需要额外的查找步骤。\n索引分区 索引分区是一种将索引数据分割存储在不同物理区域的方法，旨在提高大规模数据集的管理和查询效率。通过分区，数据库系统可以并行处理查询请求，减少单个索引结构的负载。\n示例： 在一个跨地域的用户数据库中，可以按地理区域分区索引，使得查询特定区域的用户数据时，仅需访问相关分区，提升查询效率。\n自适应索引 自适应索引是一种智能索引技术，能够根据查询模式和数据分布自动调整索引结构。通过机器学习算法，自适应索引能够动态优化索引布局，进一步提升查询性能。\n示例： 在一个动态变化的社交网络数据库中，自适应索引能够根据用户行为的变化，自动调整好友关系索引，确保高效的查询和推荐。\n索引的挑战与权衡 尽管索引在提升查询性能方面具有显著优势，但其设计与维护过程中也面临诸多挑战，需要在不同因素之间进行权衡。\n存储空间： 索引需要占用额外的存储空间，特别是在为多个列创建复合索引或为大规模数据集创建全文索引时，存储需求会显著增加。因此，在索引设计时，需要权衡查询性能提升与存储成本之间的关系。\n维护开销： 每当表中的数据发生插入、更新或删除操作时，相关索引也需要相应地进行维护。这些操作会增加数据库的写入延迟，影响整体性能。因此，频繁变化的数据表应谨慎选择索引类型和数量。\n索引选择性： 低选择性的列（如性别、状态）不适合创建单列索引，因为其过滤效果有限，可能导致查询性能不升反降。在这种情况下，可以考虑通过复合索引或位图索引等高级技术，提升查询效率。\n查询优化器的选择： 数据库查询优化器需要根据索引结构和查询语句，选择最优的执行计划。然而，在复杂查询和多索引环境下，优化器可能难以做出最佳选择，导致性能未达预期。因此，合理的索引设计和统计信息的维护，对于优化器的决策至关重要。\n实例分析：电商平台的索引优化 以一个大型电商平台为例，用户数据、商品数据和订单数据量庞大，查询频繁。为了提升系统性能，需针对不同的数据表设计合适的索引策略。\n用户表（Users） 常用查询： 按用户名、邮箱、用户ID查找用户信息。 索引设计： 主键索引：用户ID（聚集索引）。 唯一索引：用户名、邮箱（确保唯一性并提升查找效率）。 非聚集索引：注册日期（支持按注册时间的范围查询）。 商品表（Products） 常用查询： 按商品ID、分类、价格区间查询商品信息。 索引设计： 主键索引：商品ID（聚集索引）。 非聚集索引：分类ID、价格（支持快速按分类或价格范围的查询）。 复合索引：分类ID + 价格（优化按分类和价格的联合查询）。 订单表（Orders） 常用查询： 按订单ID、用户ID、订单日期查询订单信息。 索引设计： 主键索引：订单ID（聚集索引）。 非聚集索引：用户ID、订单日期（支持按用户和时间范围的查询）。 复合索引：用户ID + 订单日期（优化按用户和时间的联合查询）。 订单明细表（OrderItems） 常用查询： 按订单ID、商品ID查询订单明细。 索引设计： 主键索引：订单明细ID（聚集索引）。 非聚集索引：订单ID、商品ID（支持按订单和商品的快速查询）。 通过上述索引设计，电商平台能够高效地处理各种查询需求，提升用户体验和系统响应速度。同时，定期监控和维护索引，确保其在数据变化中的性能表现，也是保持系统高效运行的重要保障。\n存储过程与触发器 在数据库管理系统中，**存储过程（Stored Procedures）和触发器（Triggers）**是两种强大的编程工具，它们能够增强数据库的功能性和自动化程度。存储过程允许将一系列SQL语句封装成一个可重复使用的程序模块，而触发器则是在特定事件发生时自动执行的代码。这两者不仅提升了数据库操作的效率和一致性，还在业务逻辑的实现和数据完整性的维护中发挥着重要作用。\n存储过程 定义与使用 存储过程是一组预编译的SQL语句，存储在数据库中，并以名称调用。它们可以接受参数，执行复杂的操作，并返回结果。存储过程的主要目的是封装业务逻辑，简化重复性的数据库操作，减少客户端与服务器之间的数据传输，提高系统的安全性和性能。\n示例： 假设我们需要频繁地为用户添加订单。通过创建一个存储过程，可以简化这一过程，确保每次添加订单时都执行相同的验证和插入操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 CREATE PROCEDURE AddOrder @UserID INT, @ProductID INT, @Quantity INT AS BEGIN -- 检查用户是否存在 IF EXISTS (SELECT 1 FROM Users WHERE UserID = @UserID) BEGIN -- 插入订单 INSERT INTO Orders (UserID, ProductID, Quantity, OrderDate) VALUES (@UserID, @ProductID, @Quantity, GETDATE()) PRINT \u0026#39;订单已成功添加。\u0026#39; END ELSE BEGIN PRINT \u0026#39;用户不存在，无法添加订单。\u0026#39; END END 通过调用 AddOrder 存储过程，开发人员无需每次都编写复杂的SQL语句，只需传递必要的参数即可完成操作。\n优势与应用场景 存储过程在数据库管理中具有多方面的优势：\n性能优化： 存储过程在首次执行时会被编译和优化，后续调用时可以复用执行计划，减少编译时间和资源消耗。\n安全性增强： 通过存储过程，可以限制用户对底层表的直接访问，只允许通过存储过程执行特定操作，从而提高数据安全性。\n代码复用与维护： 将业务逻辑集中在存储过程中，减少了代码的重复，提高了系统的可维护性和一致性。\n减少网络流量： 存储过程在服务器端执行，客户端只需发送调用请求，减少了客户端与服务器之间的数据传输量。\n应用场景：\n复杂业务逻辑的实现： 如订单处理、库存管理、用户认证等需要多步骤操作的业务场景。\n批量数据处理： 大规模数据的插入、更新和删除操作，通过存储过程可以高效地执行。\n报表生成与数据分析： 将复杂的查询和数据处理逻辑封装在存储过程中，简化报表生成过程。\n存储过程的最佳实践 为了充分发挥存储过程的优势，以下是一些最佳实践建议：\n参数化设计： 使用参数传递动态值，避免硬编码，提高存储过程的灵活性和重用性。\n错误处理： 在存储过程中加入错误处理机制（如 TRY...CATCH），确保在发生错误时能够妥善处理，避免数据不一致。\n性能优化： 定期分析和优化存储过程的执行计划，确保其高效执行。避免在存储过程中执行不必要的复杂计算或多余的查询。\n版本控制与文档化： 对存储过程进行版本控制，并撰写详细的文档，便于团队协作和维护。\n触发器 定义与用途 触发器是一种特殊类型的存储过程，当特定的数据库事件（如插入、更新、删除操作）发生时，自动执行的代码块。触发器主要用于实现数据完整性约束、自动化任务和审计日志记录等功能。\n示例： 在订单表中，每当有新订单插入时，自动更新相应的商品库存。\n1 2 3 4 5 6 7 8 9 10 CREATE TRIGGER trg_UpdateInventory ON Orders AFTER INSERT AS BEGIN UPDATE Products SET Stock = Stock - i.Quantity FROM Products p INNER JOIN inserted i ON p.ProductID = i.ProductID END 此触发器在订单插入后自动执行，确保库存数据的一致性。\n类型与用途 触发器根据触发事件和触发时间的不同，可以分为以下几类：\n触发器类型 描述 用途 DML触发器 在数据操作语言（DML）事件（INSERT、UPDATE、DELETE）发生时触发 数据验证、自动更新相关表、记录审计日志 DDL触发器 在数据定义语言（DDL）事件（CREATE、ALTER、DROP）发生时触发 监控数据库结构变化、强制执行规范 INSTEAD OF触发器 替代原始的DML操作执行 自定义复杂的DML操作逻辑、处理视图的更新 AFTER触发器 在DML操作完成后执行 保证操作完成后的数据一致性、触发后续动作 用途示例：\n数据完整性维护： 通过触发器确保在更新或删除操作时，相关表的数据保持一致。例如，删除用户时，自动删除其所有相关订单记录。\n自动化任务执行： 定时生成报表、发送通知邮件等，通过触发器在特定事件发生时自动执行相应任务。\n审计与日志记录： 记录数据变更历史，追踪谁在何时对数据进行了何种操作，增强系统的可追溯性和安全性。\n实现与注意事项 虽然触发器功能强大，但在设计和实现时需要注意以下几点：\n避免复杂逻辑： 触发器应尽量保持简单，避免在其中执行复杂的业务逻辑，以免影响数据库性能和可维护性。\n防止递归触发： 不当的触发器设计可能导致递归调用，造成无限循环和系统崩溃。需要谨慎设计触发器的触发条件和执行逻辑。\n性能影响： 触发器在每次触发事件发生时执行，如果触发器中包含耗时操作，可能会显著影响数据库的响应时间。因此，应优化触发器的代码，确保其高效执行。\n调试与测试： 由于触发器自动执行，调试和测试相对复杂。应编写详尽的测试用例，确保触发器在各种情况下都能正确执行。\n示例： 在用户表中，防止同一用户重复注册。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE TRIGGER trg_PreventDuplicateUser ON Users INSTEAD OF INSERT AS BEGIN IF EXISTS (SELECT 1 FROM Users WHERE Username IN (SELECT Username FROM inserted)) BEGIN RAISERROR(\u0026#39;用户名已存在，无法重复注册。\u0026#39;, 16, 1) END ELSE BEGIN INSERT INTO Users (Username, Password, Email) SELECT Username, Password, Email FROM inserted END END 此触发器在用户插入操作前检查用户名是否已存在，防止重复注册。\n用户定义函数 除了存储过程和触发器，数据库系统还支持用户定义函数（User-Defined Functions, UDFs），用于封装可重复使用的计算逻辑。与存储过程不同，函数通常返回一个值，并可在SQL语句中作为表达式使用。\n示例： 计算订单的总金额。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CREATE FUNCTION dbo.CalculateTotalAmount ( @OrderID INT ) RETURNS DECIMAL(10,2) AS BEGIN DECLARE @Total DECIMAL(10,2) SELECT @Total = SUM(Quantity * UnitPrice) FROM OrderItems WHERE OrderID = @OrderID RETURN @Total END 通过调用 dbo.CalculateTotalAmount 函数，可以轻松获取特定订单的总金额。\n类型与用途 用户定义函数根据返回值的不同，可分为以下几类：\n函数类型 描述 用途 标量函数 返回单一值 复杂计算、数据格式转换 表值函数 返回表类型的结果集 复杂查询逻辑封装、可重用的子查询 内联表值函数 类似视图，返回表类型的结果集，但更灵活 高效查询封装、动态数据筛选 用途示例：\n数据格式转换： 将日期格式转换为特定的字符串格式。\n复杂计算： 计算利润、税费等需要多步计算的业务指标。\n查询逻辑封装： 封装常用的查询逻辑，提高SQL语句的可读性和维护性。\n用户定义函数的最佳实践 为了有效利用用户定义函数，以下是一些最佳实践建议：\n简洁高效： 函数应保持简洁，避免复杂的逻辑和多余的计算，确保高效执行。\n参数化设计： 使用参数传递动态值，提高函数的通用性和灵活性。\n避免副作用： 函数应避免修改数据库中的数据，仅用于计算和返回结果，保持函数的纯粹性。\n文档化： 为函数编写详细的文档，说明其功能、参数和返回值，便于团队协作和维护。\n存储过程与触发器的对比与选择 尽管存储过程和触发器都用于封装数据库操作，但它们在使用场景和实现方式上存在显著差异。理解它们的特点有助于在实际应用中做出合理的选择。\n特性 存储过程 触发器 调用方式 显式调用 自动触发 执行时机 由应用程序或用户调用 数据库事件发生时自动执行 用途 封装业务逻辑、批量操作 数据完整性维护、自动化任务 控制权 调用者控制执行 数据库控制执行 调试难度 较易调试 较难调试，因其自动触发特性 性能影响 根据调用频率和复杂度影响 频繁触发时可能影响性能 选择建议：\n使用存储过程： 当需要封装复杂的业务逻辑、执行批量操作或减少客户端与服务器之间的通信时，存储过程是理想的选择。\n使用触发器： 当需要在特定数据操作时自动维护数据完整性、实现审计日志或执行自动化任务时，触发器更为适合。\n实例分析：银行系统中的存储过程与触发器 考虑一个银行系统，涉及账户管理、交易处理和审计记录等功能。通过合理使用存储过程和触发器，可以提升系统的效率和安全性。\n存储过程示例 存储过程：转账操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 CREATE PROCEDURE TransferFunds @FromAccountID INT, @ToAccountID INT, @Amount DECIMAL(10,2) AS BEGIN BEGIN TRANSACTION -- 检查账户余额 IF (SELECT Balance FROM Accounts WHERE AccountID = @FromAccountID) \u0026gt;= @Amount BEGIN -- 扣除转出账户余额 UPDATE Accounts SET Balance = Balance - @Amount WHERE AccountID = @FromAccountID -- 增加转入账户余额 UPDATE Accounts SET Balance = Balance + @Amount WHERE AccountID = @ToAccountID COMMIT TRANSACTION PRINT \u0026#39;转账成功。\u0026#39; END ELSE BEGIN ROLLBACK TRANSACTION PRINT \u0026#39;余额不足，转账失败。\u0026#39; END END 此存储过程封装了转账操作的逻辑，确保在扣除和增加账户余额时的一致性和安全性。\n触发器示例 触发器：记录交易日志\n1 2 3 4 5 6 7 8 9 CREATE TRIGGER trg_LogTransaction ON Transactions AFTER INSERT AS BEGIN INSERT INTO TransactionLogs (TransactionID, AccountID, Amount, TransactionDate) SELECT TransactionID, AccountID, Amount, GETDATE() FROM inserted END 此触发器在新交易记录插入后自动将交易信息记录到日志表中，便于后续的审计和监控。\n数据库安全 在信息化时代，数据库作为存储和管理关键数据的核心组件，其安全性直接关系到企业运营的稳定性和数据的完整性。数据库安全不仅涉及防止未经授权的访问和数据泄露，还包括保护数据免受各种威胁和攻击的侵害。深入理解数据库安全的各个方面，包括威胁类型、访问控制机制、加密技术以及审计与合规要求，是构建安全可靠数据库系统的基石。\n安全威胁与攻击类型 数据库系统面临多种安全威胁和攻击，这些威胁可能来自内部人员、外部黑客或恶意软件。了解这些威胁的本质，有助于制定有效的防护策略。\nSQL注入 **SQL注入（SQL Injection）**是一种常见且严重的攻击手段，攻击者通过在输入字段中插入恶意SQL代码，试图操控数据库执行未预期的操作。SQL注入不仅可能导致数据泄露，还可能破坏数据库的完整性，甚至获取系统的完全控制权。\n原理与示例：\n假设有一个简单的登录表单，用户输入用户名和密码后，应用程序生成如下SQL查询：\n1 SELECT * FROM Users WHERE Username = \u0026#39;输入的用户名\u0026#39; AND Password = \u0026#39;输入的密码\u0026#39;; 攻击者可以在用户名字段输入如下内容：\n1 \u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1 生成的SQL查询变为：\n1 SELECT * FROM Users WHERE Username = \u0026#39;\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#39; AND Password = \u0026#39;任何密码\u0026#39;; 由于 '1'='1' 始终为真，查询将返回所有用户记录，导致攻击者绕过身份验证。\n防护措施：\n使用预编译语句和参数化查询： 通过预定义查询结构，确保用户输入被视为数据而非代码。\n1 2 3 4 PreparedStatement pstmt = connection.prepareStatement(\u0026#34;SELECT * FROM Users WHERE Username = ? AND Password = ?\u0026#34;); pstmt.setString(1, username); pstmt.setString(2, password); ResultSet rs = pstmt.executeQuery(); 输入验证与过滤： 对用户输入进行严格的验证，拒绝非法字符和不符合预期格式的输入。\n最小权限原则： 限制数据库用户的权限，仅授予其完成必要操作所需的最低权限，减少潜在的攻击面。\n非授权访问 **非授权访问（Unauthorized Access）**指未经许可的用户或系统访问数据库中的敏感数据。这种访问可能导致数据泄露、篡改甚至删除，严重威胁企业的数据安全。\n常见途径：\n弱密码和凭证管理： 使用简单或默认密码，使攻击者更容易通过猜测或暴力破解获取访问权限。\n配置漏洞： 数据库配置不当，如开放不必要的端口、未禁用默认账户等，增加了被攻击的风险。\n内部威胁： 内部员工滥用权限，未经授权访问或操作敏感数据。\n防护措施：\n强密码策略： 强制使用复杂密码，定期更换密码，避免使用默认凭证。\n多因素认证（MFA）： 在用户登录时引入额外的认证因素，提高账户的安全性。\n最小权限原则： 仅授予用户完成其职责所需的最低权限，减少潜在的滥用风险。\n网络隔离与防火墙： 通过网络隔离和防火墙规则，限制对数据库服务器的访问，仅允许可信网络或IP地址访问。\n访问控制 访问控制是数据库安全的核心机制，旨在确保只有授权用户能够访问和操作数据库中的资源。有效的访问控制策略能够防止未经授权的访问和数据泄露，维护数据的机密性、完整性和可用性。\n用户认证与授权 **用户认证（Authentication）和用户授权（Authorization）**是访问控制的两大基本组成部分。\n用户认证： 确认用户的身份，确保其具备合法的访问权限。常见的认证方式包括：\n基于密码的认证： 用户通过输入用户名和密码进行认证。 多因素认证（MFA）： 除了密码外，用户还需提供其他认证因素，如短信验证码、指纹等。 单点登录（SSO）： 用户通过一次认证即可访问多个关联的系统和应用。 用户授权： 确定用户在认证后的权限，决定其可以访问哪些资源以及可以执行哪些操作。授权策略通常基于角色（Role-Based Access Control, RBAC）或属性（Attribute-Based Access Control, ABAC）。\n示例：\n在一个企业数据库中，普通员工可能只能访问和修改与自己相关的记录，而管理人员则拥有更高的权限，可以访问和管理整个部门的数据。\n实施方法：\n角色定义： 根据组织结构和职责，定义不同的角色，如管理员、开发人员、普通用户等。\n权限分配： 为每个角色分配相应的权限，如读取、写入、修改、删除等操作权限。\n权限验证： 在用户执行操作时，数据库系统检查其角色和权限，决定是否允许执行该操作。\n角色与权限管理 **角色与权限管理（Role and Permission Management）**通过将权限分配给角色，而不是直接分配给用户，简化了权限管理，提高了安全性和灵活性。\n优势：\n简化管理： 通过定义角色，集中管理权限，避免为每个用户单独分配权限，降低管理复杂性。\n提高安全性： 通过角色划分，确保用户只能访问其职责范围内的数据和功能，减少权限滥用的风险。\n灵活性与可扩展性： 角色可以根据组织变化进行调整，动态适应不同的业务需求。\n实施步骤：\n角色定义： 根据组织结构和职责，定义各类角色。例如，数据库管理员（DBA）、开发人员、业务分析师等。\n权限分配： 为每个角色分配相应的数据库权限，如SELECT、INSERT、UPDATE、DELETE等操作权限。\n用户分配： 将用户分配到相应的角色，使其继承该角色的权限。\n示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 -- 创建角色 CREATE ROLE db_admin; CREATE ROLE data_analyst; CREATE ROLE application_user; -- 分配权限 GRANT ALL PRIVILEGES ON DATABASE SalesDB TO db_admin; GRANT SELECT, INSERT, UPDATE ON TABLE Sales TO data_analyst; GRANT SELECT ON TABLE Products TO application_user; -- 将用户分配到角色 GRANT db_admin TO Alice; GRANT data_analyst TO Bob; GRANT application_user TO Charlie; 通过上述操作，Alice拥有SalesDB数据库的所有权限，Bob可以查询和修改Sales表的数据，而Charlie只能读取Products表的数据。\n加密技术 加密技术是保护数据库中敏感数据的重要手段，通过将数据转化为不可读的形式，即使数据被非法获取，也无法被轻易解读。加密技术主要分为数据加密和传输加密两大类。\n数据加密 **数据加密（Data Encryption）**旨在保护存储在数据库中的数据，使其在未经授权的情况下无法被读取或篡改。数据加密可以在多个层次进行，包括：\n静态数据加密（At-Rest Encryption）： 对存储在磁盘上的数据进行加密，防止物理介质被盗或非法访问时数据泄露。\n字段级加密（Column-Level Encryption）： 仅对数据库表中的特定敏感字段进行加密，如信用卡号码、社会保障号码等。\n透明数据加密（Transparent Data Encryption, TDE）： 通过数据库管理系统自动加密和解密数据，无需修改应用程序代码，简化了加密过程。\n示例：\n在银行系统中，用户的账户信息和交易记录属于高度敏感的数据，必须通过加密技术加以保护。\n实施方法：\n对称加密： 使用相同的密钥进行加密和解密，适用于大规模数据的快速加密。\n1 2 3 4 5 6 7 8 9 10 -- 创建加密密钥 CREATE MASTER KEY ENCRYPTION BY PASSWORD = \u0026#39;StrongPassword!\u0026#39;; -- 创建加密证书 CREATE CERTIFICATE MyCertificate WITH SUBJECT = \u0026#39;Data Encryption\u0026#39;; -- 加密列 ALTER TABLE Customers ADD EncryptedSSN VARBINARY(128) ENCRYPTED WITH (ENCRYPTION_TYPE = COLUMN_ENCRYPTION_KEY, ALGORITHM = \u0026#39;AEAD_AES_256_CBC_HMAC_SHA_256\u0026#39;, COLUMN_ENCRYPTION_KEY = MyColumnKey); 非对称加密： 使用一对公钥和私钥进行加密和解密，适用于需要安全密钥交换的场景。\n挑战与考虑：\n密钥管理： 安全地生成、存储和管理加密密钥是加密技术的关键，密钥泄露将导致加密数据的安全性丧失。\n性能开销： 加密和解密操作会增加系统的计算负担，可能影响数据库的性能，需要权衡加密强度与性能之间的关系。\n兼容性与集成： 确保加密技术与现有系统和应用程序的兼容性，避免在加密过程中引入新的问题。\n传输加密 **传输加密（Data in-Transit Encryption）**旨在保护数据在网络传输过程中的安全，防止数据在传输过程中被窃听、篡改或伪造。常见的传输加密技术包括：\n安全套接字层（Secure Sockets Layer, SSL）和传输层安全（Transport Layer Security, TLS）： 为客户端与数据库服务器之间的通信提供加密保护，确保数据在传输过程中的机密性和完整性。\n1 2 3 4 5 -- 配置SQL Server使用TLS EXEC sp_configure \u0026#39;show advanced options\u0026#39;, 1; RECONFIGURE; EXEC sp_configure \u0026#39;force encryption\u0026#39;, 1; RECONFIGURE; 虚拟专用网络（Virtual Private Network, VPN）： 通过加密隧道保护数据在公共网络上的传输，适用于远程访问和跨地域的数据传输。\n示例：\n在云数据库环境中，确保客户端与云数据库服务之间的通信通过TLS加密，防止数据在公共互联网中被拦截和窃取。\n实施方法：\n配置数据库服务器： 启用TLS或SSL，生成和安装服务器证书，确保所有传输的数据都经过加密。\n客户端配置： 配置数据库客户端使用加密连接，验证服务器证书，防止中间人攻击。\n挑战与考虑：\n证书管理： 需要定期更新和管理SSL/TLS证书，确保证书的有效性和可信度。\n性能影响： 加密和解密操作会增加传输延迟，尤其是在高吞吐量的环境中，需要优化加密参数以减少性能损耗。\n兼容性： 确保所有客户端和服务器支持所选的加密协议和算法，避免因协议不兼容导致连接失败。\n审计与合规 **审计与合规（Auditing and Compliance）**是数据库安全的重要组成部分，旨在通过监控和记录数据库活动，确保数据处理过程符合相关法规和标准，提升数据的透明度和可追溯性。\n审计机制 **审计机制（Auditing Mechanisms）**通过记录数据库操作的详细信息，帮助管理员监控用户行为、检测异常活动、调查安全事件和满足合规要求。常见的审计内容包括：\n用户活动审计： 记录用户的登录、登出、权限变更、查询和修改操作等。\n数据访问审计： 追踪对敏感数据的访问和操作，如读取、插入、更新和删除。\n系统事件审计： 记录数据库服务器的系统事件，如启动、关闭、配置变更和错误日志。\n实施方法：\n启用数据库审计功能： 大多数数据库管理系统（如SQL Server、Oracle、MySQL）提供内置的审计功能，可以配置审计策略并定义要记录的事件类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 -- SQL Server审计示例 CREATE SERVER AUDIT Audit_SalesDB TO FILE (FILEPATH = \u0026#39;C:\\Audits\\\u0026#39;, MAXSIZE = 100 MB, MAX_ROLLOVER_FILES = 10) WITH (ON_FAILURE = CONTINUE); CREATE SERVER AUDIT SPECIFICATION Audit_SalesDB_Spec FOR SERVER AUDIT Audit_SalesDB ADD (SCHEMA_OBJECT_CHANGE_GROUP), ADD (SUCCESSFUL_LOGIN_GROUP), ADD (FAILED_LOGIN_GROUP); ALTER SERVER AUDIT Audit_SalesDB WITH (STATE = ON); ALTER SERVER AUDIT SPECIFICATION Audit_SalesDB_Spec WITH (STATE = ON); 使用第三方审计工具： 部署专门的审计软件，提供更高级的审计分析和报告功能，如Splunk、IBM Guardium等。\n挑战与考虑：\n存储与管理： 审计日志可能会快速增长，需要有效的存储和管理策略，确保审计数据的完整性和可用性。\n隐私保护： 审计过程中可能涉及敏感数据，需遵守相关隐私法规，确保审计数据的保密性。\n实时监控与报警： 实现实时监控和自动报警，及时发现和响应潜在的安全事件。\n合规要求 数据库安全不仅是技术问题，还涉及遵循各种法律法规和行业标准。不同国家和行业有不同的合规要求，企业需要了解并遵守相关规定，以避免法律风险和经济损失。\n主要合规标准：\n通用数据保护条例（GDPR）： 欧盟制定的数据保护法规，要求企业保护欧盟公民的个人数据，规定了数据处理、存储和传输的严格标准。\n健康保险可携性与责任法案（HIPAA）： 美国的健康信息隐私法规，要求医疗机构保护患者的敏感健康信息。\n支付卡行业数据安全标准（PCI DSS）： 适用于处理信用卡信息的企业，规定了数据加密、访问控制和审计等安全要求。\n金融行业合规标准（如SOX、FINRA）： 针对金融机构的数据管理和审计要求，确保财务数据的准确性和安全性。\n实施策略：\n了解适用法规： 企业需明确其业务涉及的地理区域和行业，识别需要遵循的合规标准。\n制定合规策略： 根据法规要求，制定数据保护和安全策略，包括数据加密、访问控制、审计和报告等。\n定期审计与评估： 通过内部审计和第三方评估，检查合规性，发现并修复安全漏洞，确保持续符合合规要求。\n培训与意识提升： 对员工进行合规培训，提升其数据保护意识，确保遵守安全政策和操作规范。\n示例：\n一家跨国电商公司在欧盟设有分支机构，必须遵守GDPR规定。为此，公司实施了全面的数据加密策略，限制对个人数据的访问，定期进行安全审计，并建立了数据泄露应急响应机制，确保在发生数据泄露时能够快速响应和报告。\n前沿研究与发展 数据库安全领域不断发展，随着技术的进步和新型威胁的出现，研究者和从业者不断探索新的安全机制和防护策略。以下是一些当前前沿的研究方向和发展趋势：\n基于人工智能的安全防护 人工智能（AI）和机器学习（ML）在数据库安全中的应用正变得越来越普遍。通过分析大量的数据库活动数据，AI和ML算法能够识别出异常行为和潜在威胁，提升检测和响应的效率。\n应用示例：\n异常检测： 利用机器学习模型分析正常的数据库操作模式，实时检测与之不符的异常行为，如异常的查询频率、数据访问模式等，及时发现潜在的安全威胁。\n自动化威胁响应： 结合AI技术，自动化处理检测到的威胁，执行如阻断可疑连接、通知管理员等响应措施，减少人为干预和响应时间。\n挑战与前景：\n尽管基于AI的安全防护具有很大潜力，但也面临数据隐私保护、模型训练的准确性和可解释性等挑战。未来的发展将集中在提升模型的透明度和可解释性，以及优化算法以适应不断变化的威胁环境。\n同态加密与安全多方计算 **同态加密（Homomorphic Encryption）**允许在加密数据上直接执行计算操作，而无需解密数据，从而在保证数据隐私的前提下，实现数据的处理和分析。这一技术在保护敏感数据的同时，支持云计算和大数据分析等应用场景。\n应用示例：\n在医疗领域，医院可以将患者数据加密后存储在云端，研究人员可以在不解密数据的情况下，执行统计分析和数据挖掘，保护患者隐私。\n**安全多方计算（Secure Multi-Party Computation, SMPC）**则允许多个参与方在不泄露各自输入数据的情况下，共同完成计算任务，适用于需要数据共享但又需保护隐私的场景。\n前景与挑战：\n同态加密和SMPC的研究正在迅速发展，尽管目前的计算性能和算法效率仍有待提高，但随着技术的进步，预计将在更多实际应用中得到广泛采用。\n区块链与数据库安全 区块链技术以其去中心化、不可篡改和透明性的特点，为数据库安全提供了新的思路。通过将数据库操作记录在区块链上，可以实现数据的透明审计和防篡改保护。\n应用示例：\n在供应链管理中，利用区块链记录每一次的商品转移和交易操作，确保数据的透明和不可篡改，提升供应链的可追溯性和信任度。\n前景与挑战：\n区块链与传统数据库的结合仍处于探索阶段，主要挑战包括数据存储的可扩展性、隐私保护以及与现有系统的集成问题。然而，随着技术的成熟，区块链有望在提高数据库系统的安全性和透明度方面发挥重要作用。\n数据库备份与恢复 在信息系统中，数据库备份与恢复是确保数据持久性和系统可用性的关键环节。无论是由于硬件故障、软件错误、操作失误还是恶意攻击，数据的丢失或损坏都可能对企业造成重大损失。因此，制定科学的备份策略和高效的恢复方法，是每个数据库管理员（DBA）必须掌握的重要技能。本章将深入探讨数据库备份与恢复的原理、策略、方法及其在实际应用中的实施细节，并结合前沿研究成果，展望未来的发展趋势。\n备份策略 备份策略是指根据业务需求、数据重要性和恢复要求，制定的一系列备份计划和方法。一个完善的备份策略应综合考虑数据的变化频率、系统的可用性要求、存储资源的限制等因素，以实现数据的高效保护和快速恢复。\n完全备份 **完全备份（Full Backup）**是指对数据库中所有数据进行完整复制的一种备份方式。无论数据是否发生变化，完全备份都会将整个数据库的所有数据文件、日志文件等全部备份下来。\n优点：\n恢复速度快： 由于备份包含了所有数据，恢复时无需依赖其他备份文件，能够迅速恢复整个数据库。 操作简单： 完全备份的过程相对简单，不需要追踪数据的变化情况。 缺点：\n存储空间需求大： 随着数据库规模的增长，完全备份所需的存储空间也呈线性增长，成本较高。 备份时间长： 对于大型数据库，完全备份的时间可能较长，影响系统的可用性。 示例：\n假设一家电商公司每天需要对其销售数据库进行备份。每次完全备份会复制整个数据库的所有数据，包括历史订单、用户信息和产品库存等。这确保了在任何数据丢失或损坏的情况下，都能够通过最近的完全备份快速恢复系统。\n增量备份 **增量备份（Incremental Backup）**只备份自上一次备份以来发生变化的数据。这种方法可以显著减少备份所需的时间和存储空间。\n优点：\n节省存储空间： 由于仅备份变化的数据，增量备份所需的存储空间远小于完全备份。 备份速度快： 备份过程仅涉及变化的数据，时间开销较小。 缺点：\n恢复过程复杂： 恢复时需要先恢复最近的完全备份，然后依次应用所有增量备份，过程较为复杂且耗时。 依赖链条长： 如果某个增量备份文件损坏，可能导致后续所有增量备份无法恢复。 示例：\n继续上述电商公司的例子，假设每天进行一次完全备份，每小时进行一次增量备份。当发生数据丢失时，DBA需要首先恢复最近的完全备份，然后依次应用每个小时的增量备份，以确保数据的完整性和一致性。\n差异备份 **差异备份（Differential Backup）**备份自上一次完全备份以来发生变化的所有数据。与增量备份不同，差异备份每次备份的数据量会逐渐增加，直到下一个完全备份。\n优点：\n恢复过程简化： 恢复时只需恢复最近的完全备份和最新的差异备份，减少了恢复步骤和时间。 适中的存储需求： 相比于增量备份，差异备份在存储空间和备份时间上取得了平衡。 缺点：\n存储空间增长： 随着时间的推移，差异备份的数据量会增加，占用更多的存储空间。 备份时间较长： 尽管比完全备份快，但差异备份的备份时间仍然较长，尤其是在数据变化频繁的情况下。 示例：\n电商公司每周进行一次完全备份，每天进行一次差异备份。当需要恢复数据时，DBA只需恢复最近的完全备份和当天的差异备份，避免了多次增量备份的复杂性。\n恢复方法 数据库恢复是指在数据丢失或损坏后，利用备份数据和日志文件，将数据库恢复到特定状态的过程。恢复方法的选择取决于备份策略和业务需求。\n恢复点 **恢复点（Recovery Point）**指的是数据库可以恢复到的特定时间点或事务状态。根据业务需求，恢复点可以分为以下几类：\n时间点恢复（Point-in-Time Recovery, PITR）： 将数据库恢复到某个具体的时间点，适用于需要精确恢复到特定状态的场景。 事务级恢复： 将数据库恢复到特定的事务状态，确保所有事务的一致性和完整性。 数据块级恢复： 恢复特定的数据块，适用于局部数据损坏的情况。 示例：\n如果电商公司在凌晨2点发生了数据损坏事件，而最后一次备份是在凌晨1点30分，那么通过时间点恢复，可以将数据库恢复到凌晨1点30分，确保不包含损坏的数据。\n恢复时间目标（RTO）与恢复点目标（RPO） **恢复时间目标（Recovery Time Objective, RTO）和恢复点目标（Recovery Point Objective, RPO）**是衡量备份与恢复策略有效性的两个关键指标。\n指标 定义 目标 RTO 指定系统恢复正常运行所需的最长时间 系统需要在X小时内恢复 RPO 指定允许的数据丢失的最大时间范围 数据丢失不超过Y分钟 实施策略：\n评估业务需求： 根据业务连续性和数据重要性，确定合理的RTO和RPO。 制定备份计划： 根据RTO和RPO，选择合适的备份策略（完全备份、增量备份、差异备份）。 优化恢复流程： 确保恢复流程高效可靠，能够在规定的RTO内完成恢复操作。 示例：\n对于电商公司的订单处理系统，RTO设定为2小时，RPO设定为15分钟。这意味着在系统发生故障时，需要在2小时内恢复系统，且数据丢失不超过15分钟。为此，公司可能每天进行多次完全备份，每隔10分钟进行日志备份，以满足RPO要求。\n灾难恢复计划 **灾难恢复计划（Disaster Recovery Plan, DRP）**是指在发生重大灾害（如自然灾害、火灾、洪水、网络攻击等）时，确保关键数据库和应用系统能够迅速恢复运行的详细计划和措施。灾难恢复计划不仅包括备份与恢复策略，还涉及业务连续性、通信计划、资源分配等多个方面。\n关键组成部分 风险评估与影响分析： 识别潜在的灾难风险，评估其对业务和数据库的影响，确定优先级和应对策略。 备份与恢复策略： 制定全面的备份计划，确保数据的定期备份和可靠恢复。 恢复流程与步骤： 明确灾难发生后的恢复步骤和责任分工，确保团队协同高效。 通信计划： 确保在灾难发生时，相关人员能够迅速获取信息并采取行动，保持内部和外部的有效沟通。 测试与演练： 定期进行灾难恢复演练，验证恢复策略的有效性，发现并改进潜在的问题。 文档与更新： 记录详细的灾难恢复计划，并根据实际情况和技术发展进行定期更新。 实施方法 制定灾难恢复策略： 根据业务需求和风险评估，选择适合的备份类型、备份频率和恢复方法。 配置异地备份： 将备份数据存储在地理位置不同的地点，防止单一灾难事件导致数据全部丢失。 建立冗余系统： 部署高可用性架构，如主从复制、集群和分片，确保系统在部分组件故障时仍能正常运行。 定义恢复优先级： 根据业务重要性，确定不同系统和数据的恢复优先级，确保关键业务能够优先恢复。 培训与演练： 定期培训团队成员，确保他们熟悉灾难恢复流程，并通过演练提升应急响应能力。 示例：\n一家全球运营的电商公司制定了详细的灾难恢复计划，包括每天进行一次完全备份，每小时进行一次增量备份，并将备份数据同步到不同的地理区域。公司还建立了高可用性数据库集群，确保在某个数据中心发生故障时，其他数据中心能够接管业务，保证系统的持续运行。\n高可用性方案 高可用性方案旨在通过冗余和容错机制，确保数据库系统在硬件故障、软件错误或其他异常情况下仍能持续提供服务，最小化系统停机时间和业务中断。\n主从复制 **主从复制（Master-Slave Replication）**是一种常见的数据库复制技术，通过将主数据库（Master）上的数据实时复制到一个或多个从数据库（Slave），实现数据的冗余备份和负载均衡。\n工作原理：\n数据变更记录： 主数据库记录所有数据变更操作（如INSERT、UPDATE、DELETE）。 日志传输： 主数据库将变更日志传输到从数据库。 数据应用： 从数据库根据变更日志，应用相应的数据变更，保持与主数据库的一致性。 优点：\n数据冗余： 主从复制提供了数据备份，提升数据的可靠性和可用性。 负载均衡： 从数据库可以承担查询请求，减轻主数据库的负载，提升系统性能。 高可用性： 当主数据库发生故障时，从数据库可以迅速接管，确保业务的连续性。 缺点：\n数据延迟： 从数据库的数据更新存在一定的延迟，可能导致数据不一致。 复杂性增加： 需要额外的配置和管理，确保复制过程的稳定性和一致性。 单点故障风险： 如果主数据库宕机且没有及时切换，从数据库的更新可能不及时，影响系统的高可用性。 示例：\n一家大型社交媒体平台采用主从复制架构，主数据库负责处理写操作和数据变更，从数据库则处理大量的读操作。这不仅提高了系统的读性能，还在主数据库发生故障时，能够快速切换到从数据库，确保平台的持续运行。\n集群与分片 **集群（Clustering）和分片（Sharding）**是两种高级的数据库高可用性和扩展性方案。\n集群：\n**数据库集群（Database Clustering）**通过将多个数据库实例组成一个集群，共同管理和提供服务，提升系统的可靠性和性能。\n类型：\n主动-主动集群（Active-Active Cluster）： 集群中的所有节点同时处理请求，提高系统的处理能力和容错性。 主动-被动集群（Active-Passive Cluster）： 集群中的主节点处理请求，从节点处于待命状态，主节点故障时，从节点接管服务。 优点：\n高可用性： 集群架构通过冗余节点，确保系统在部分节点故障时仍能正常运行。 可扩展性： 可以通过增加集群节点，水平扩展系统的处理能力，适应业务增长。 缺点：\n复杂性高： 集群的配置和管理较为复杂，需要专业的技术支持。 一致性维护： 多节点间的数据一致性维护可能带来额外的开销和挑战。 示例：\n一家金融机构采用数据库集群架构，所有节点共同处理交易请求，确保在任何一个节点发生故障时，其他节点能够无缝接管，保障交易系统的高可用性和数据一致性。\n分片：\n**数据库分片（Database Sharding）**将数据库按一定规则（如按用户ID、地理区域等）划分为多个独立的分片，每个分片由不同的数据库实例管理，提升系统的扩展性和性能。\n优点：\n水平扩展： 通过分片，可以将数据分布到多个服务器，避免单一数据库实例的性能瓶颈。 并行处理： 分片后的数据库实例可以并行处理请求，提升系统的吞吐量和响应速度。 缺点：\n复杂性增加： 分片规则的设计和实施较为复杂，需要精确的分片策略和高效的分片管理工具。 跨分片操作： 某些操作需要跨分片访问数据，可能导致性能下降和事务一致性问题。 示例：\n一家全球在线游戏公司采用分片技术，将玩家数据按地理区域分片，每个区域的数据由独立的数据库实例管理。这样不仅提高了数据访问的速度，还能根据不同区域的用户负载，灵活地扩展和调整数据库资源。\n实验与案例研究 通过实际的实验和案例研究，可以更好地理解和掌握数据库备份与恢复的实际应用和优化方法。\n数据库搭建与配置 在实际环境中，数据库的搭建与配置是备份与恢复策略实施的基础。以下是一个简化的实验步骤：\n选择数据库管理系统（DBMS）： 例如，选择MySQL、PostgreSQL或Oracle等。 安装和配置数据库： 根据DBMS的文档，完成数据库的安装和初步配置。 创建示例数据库和表： 创建一个包含用户、订单和产品等表的示例数据库，用于后续的备份与恢复实验。 配置备份工具： 安装并配置适用于所选DBMS的备份工具，如mysqldump、pg_dump或Oracle RMAN。 示例：\n1 2 3 4 5 6 7 8 # 使用mysqldump进行完全备份 mysqldump -u root -p SalesDB \u0026gt; SalesDB_full_backup.sql # 使用mysqldump进行增量备份 mysqldump -u root -p --single-transaction --flush-logs --master-data=2 SalesDB \u0026gt; SalesDB_incremental_backup.sql # 使用pg_dump进行完全备份（PostgreSQL） pg_dump -U postgres -F c -b -v -f SalesDB_full_backup.backup SalesDB SQL查询实践 备份与恢复不仅涉及物理文件操作，还需要通过SQL语句进行数据的导出和导入。以下是一些常用的备份与恢复SQL命令：\n备份命令：\n1 2 3 4 5 6 -- SQL Server 完全备份 BACKUP DATABASE SalesDB TO DISK = \u0026#39;C:\\Backups\\SalesDB_full.bak\u0026#39; WITH FORMAT, MEDIANAME = \u0026#39;SalesDBBackup\u0026#39;, NAME = \u0026#39;Full Backup of SalesDB\u0026#39;; 恢复命令：\n1 2 3 4 5 -- SQL Server 恢复数据库 RESTORE DATABASE SalesDB FROM DISK = \u0026#39;C:\\Backups\\SalesDB_full.bak\u0026#39; WITH REPLACE, RECOVERY; 示例：\n假设在发生数据丢失后，需要通过完全备份恢复数据库：\n停止数据库服务： 确保在恢复过程中，数据库不接受任何新的操作。 执行恢复命令： 使用RESTORE DATABASE命令从备份文件中恢复数据。 验证恢复结果： 检查数据库的完整性和一致性，确保恢复成功。 数据库设计案例 通过分析实际的数据库设计案例，可以了解如何根据不同的业务需求，制定合适的备份与恢复策略。\n案例：金融机构的备份策略\n一家大型金融机构拥有庞大的交易数据库，数据的准确性和可用性至关重要。其备份策略包括：\n每天进行一次完全备份，保留最近的七个完全备份文件。 每小时进行一次增量备份，保留最近的24个增量备份文件。 每十分钟记录事务日志，确保可以实现细粒度的恢复点。 将备份数据存储在不同的地理位置，确保数据冗余。 定期进行灾难恢复演练，验证备份与恢复流程的有效性。 恢复流程：\n当系统发生故障时，DBA按照以下步骤进行恢复：\n停止故障数据库的操作。 恢复最近的完全备份。 应用所有增量备份，直至故障发生前的最后一个增量备份。 回放事务日志，确保数据的一致性和完整性。 启动数据库服务，验证恢复结果。 性能调优实验 备份与恢复操作对数据库性能有显著影响，尤其是在大规模数据环境中。通过性能调优实验，可以优化备份与恢复的效率，减少对生产系统的影响。\n实验步骤：\n创建大规模数据库： 构建包含数百万条记录的示例数据库，用于性能测试。 实施不同的备份策略： 比较完全备份、增量备份和差异备份的性能表现。 测量备份时间和存储空间： 记录每种备份策略的备份时间和所需存储空间。 优化备份参数： 调整备份工具的参数，如压缩级别、并行度等，提升备份效率。 分析恢复时间： 通过恢复实验，评估不同备份策略下的恢复时间和数据一致性。 示例结果：\n备份策略 备份时间 存储空间 恢复时间 完全备份 2小时 100GB 1.5小时 增量备份 15分钟 10GB 2小时 差异备份 30分钟 50GB 1.75小时 通过优化备份参数，完全备份的时间缩短至1.5小时，增量备份的时间缩短至10分钟，差异备份的时间缩短至25分钟，同时保证了数据的一致性和完整性。\n前沿研究与发展 数据库备份与恢复领域随着技术的进步和业务需求的变化，持续涌现出新的研究方向和发展趋势。以下是当前的一些前沿研究内容：\n云备份与恢复 **云备份与恢复（Cloud Backup and Recovery）**利用云计算平台的弹性和可扩展性，实现数据库备份和恢复的自动化和高效化。\n研究方向：\n多云备份策略： 在多个云服务提供商之间分布备份数据，提高数据冗余和可用性。 自动化恢复流程： 利用云服务的自动化工具，实现快速、无缝的数据库恢复。 成本优化： 研究在保证备份可靠性的前提下，如何优化备份存储和传输成本。 示例：\n利用AWS RDS的备份功能，自动进行数据库快照备份，并将备份数据存储在不同的区域，实现高可用性和灾难恢复。\n区块链技术在备份中的应用 区块链技术以其去中心化、不可篡改和透明性的特点，为数据库备份与恢复提供了新的思路。通过将备份数据记录在区块链上，可以实现数据的高可靠性和安全性。\n研究方向：\n分布式备份验证： 利用区块链的透明性，验证备份数据的完整性和一致性。 去中心化备份存储： 将备份数据分布存储在区块链网络中，提升数据的安全性和可用性。 智能合约自动化备份： 利用智能合约，自动执行备份任务和恢复流程，提升操作的自动化和可靠性。 示例：\n通过区块链网络记录每一次数据库备份操作的哈希值，确保备份数据未被篡改，同时利用智能合约自动触发备份任务，提升备份的可靠性和自动化程度。\n增强的备份压缩与去重技术 随着数据量的急剧增长，传统的备份方法面临着存储空间和传输效率的挑战。增强的备份压缩与去重技术通过智能算法，进一步提高备份数据的压缩率和存储效率。\n研究方向：\n智能压缩算法： 开发高效的压缩算法，适应不同类型的数据，提升压缩效果和解压速度。 数据去重技术： 通过识别和消除重复数据，减少备份数据的存储空间。 实时压缩与去重： 实现备份过程中的实时压缩和去重，提升备份效率和系统性能。 示例：\n利用先进的压缩算法和数据去重技术，对备份数据进行实时压缩和去重，显著减少存储空间需求，同时提升备份和恢复的速度。\n分布式数据库 在当今数据驱动的时代，随着企业业务的全球化和数据量的急剧增长，单一数据库系统往往难以满足高性能、可扩展性和高可用性的需求。**分布式数据库（Distributed Databases）**作为一种解决方案，通过将数据分布存储在多个物理位置，实现了卓越的性能和可靠性。本章将深入探讨分布式数据库的核心概念、数据分布策略、事务管理、CAP定理与权衡，以及分布式一致性协议等关键主题。同时，结合前沿研究，展望分布式数据库技术的未来发展趋势。\n分布式数据库概念 分布式数据库系统通过将数据分布在多个物理节点上，实现了数据的冗余存储和并行处理，从而提升了系统的性能、可扩展性和容错能力。这些节点可以位于同一数据中心，也可以跨越不同的地理区域，甚至不同的云平台。尽管数据分布在多个位置，分布式数据库系统对用户而言表现为一个统一的数据库，使得数据访问和管理更加简便。\n分布式数据库的关键特性 分布式数据库具备以下几个关键特性：\n透明性（Transparency）： 用户和应用程序无需关心数据的物理分布位置，系统提供统一的访问接口。 可扩展性（Scalability）： 通过增加节点，可以水平扩展系统的存储和计算能力，适应不断增长的数据量和访问需求。 高可用性（High Availability）： 通过数据冗余和故障转移机制，确保系统在部分节点故障时仍能正常运行。 容错性（Fault Tolerance）： 系统能够在部分组件失效的情况下，保持数据的完整性和系统的正常操作。 一致性（Consistency）： 确保所有节点上的数据在任何时刻都是一致的，满足特定的一致性模型。 数据分布策略 数据分布策略决定了数据在各个节点之间的分配方式，合理的数据分布策略能够优化查询性能、提高系统的可扩展性和可用性。主要的数据分布策略包括垂直分割、水平分割和混合分割。\n分布策略 描述 优点 缺点 垂直分割 将表的列按功能或使用频率分布到不同的节点上。 优化特定类型查询；增强数据安全性 查询需要跨节点时增加复杂性；可能导致数据冗余 水平分割 将表的行按某种规则（如用户ID范围）分布到不同的节点上。 提高可扩展性；减少单节点负载 跨分区查询复杂；数据可能不均匀分布 混合分割 结合垂直分割和水平分割，对表的部分列和行同时进行分割。 提供更高的灵活性和性能；减少冗余 设计和管理复杂；跨分区操作开销较大 垂直分割 垂直分割通过将表的不同列存储在不同的节点上，优化了特定类型的查询。例如，一个用户表可以将基础信息（如用户名、密码）存储在一个节点，将用户活动信息（如订单历史、购物车）存储在另一个节点。这种分割方式可以提升查询性能和数据安全性，但在需要同时访问多个分区的数据时，会增加查询的复杂性和延迟。\n示例：\n假设一个电商平台的用户表包含用户基本信息和交易信息。通过垂直分割，可以将基本信息存储在一个节点上，而将交易信息存储在另一个节点上，以优化不同类型的查询操作。\n水平分割 水平分割通过将表的行按特定规则分布到不同的节点上，实现了数据的负载均衡和可扩展性。例如，一个全球性的社交网络平台可以按地理区域将用户数据分布到不同的数据中心，每个数据中心处理其区域内的用户请求。这种策略能够显著提升系统的性能和可扩展性，但在跨区域查询时会增加复杂性和延迟。\n示例：\n在一个全球性社交媒体平台，用户数据可以按地理区域进行水平分割。亚洲用户数据存储在亚洲的数据中心，欧洲用户数据存储在欧洲的数据中心，以降低访问延迟并优化区域内的查询性能。\n混合分割 混合分割结合了垂直分割和水平分割的优势，通过同时对表的列和行进行分割，实现更灵活和高效的数据分布。例如，一个大型在线教育平台可以将课程信息和学生成绩先按科目进行垂直分割，再按地区进行水平分割。这种策略既优化了不同类型的数据访问，又提高了系统的可扩展性和性能。\n示例：\n在一个大型在线教育平台，课程信息和学生成绩可以先按科目进行垂直分割，再按地区进行水平分割。例如，数学课程数据存储在一个分区中，物理课程数据存储在另一个分区中；每个科目的数据又按不同的地区分布到多个节点上，以实现高效的数据访问和管理。\n分布式事务管理 在分布式数据库系统中，事务管理尤为重要，因为事务可能涉及多个节点的数据操作。有效的分布式事务管理能够确保数据的一致性和完整性，即使在节点故障或网络分区的情况下。常见的分布式事务管理协议包括两阶段提交（2PC）和三阶段提交（3PC）。\n两阶段提交（2PC） 两阶段提交协议（Two-Phase Commit, 2PC）是分布式事务管理中最常用的协议之一，旨在确保跨多个节点的事务操作要么全部成功，要么全部回滚，保持数据的一致性。\n工作原理：\n准备阶段（Prepare Phase）：\n协调者向所有参与者（即涉及的数据库节点）发送准备提交的请求。 每个参与者执行事务操作并锁定相关资源，向协调者回复是否准备就绪（Vote Yes）或是否需要回滚（Vote No）。 提交阶段（Commit Phase）：\n如果所有参与者都回复“准备就绪”，协调者向所有参与者发送提交的指令，参与者提交事务并释放资源。 如果有任何参与者回复“需要回滚”，协调者向所有参与者发送回滚的指令，参与者回滚事务并释放资源。 优点：\n一致性保证： 确保分布式事务在所有参与者之间保持一致，要么全部提交，要么全部回滚。 实现简单： 2PC协议较为简单，易于理解和实现。 缺点：\n阻塞问题： 在准备阶段，如果协调者或参与者发生故障，可能导致系统阻塞，无法继续处理事务。 性能开销大： 需要多次通信和锁定资源，增加了事务处理的延迟和系统的负载。 示例：\n在一个跨国银行系统中，客户从一个国家的分支机构转账到另一个国家的分支机构。转账操作涉及两个不同的数据库节点。通过2PC协议，系统确保在扣除发起账户余额和增加接收账户余额两个操作要么都成功，要么都失败，保持账户余额的一致性。\n三阶段提交（3PC） 三阶段提交协议（Three-Phase Commit, 3PC）是对2PC协议的改进，旨在解决2PC的阻塞问题，提高协议的可用性和可靠性。\n工作原理：\nCan Commit阶段：\n协调者向所有参与者发送“能否提交”的请求。 参与者回复是否准备好进行事务提交。 Pre-Commit阶段：\n如果所有参与者都同意提交，协调者向所有参与者发送“预提交”的指令。 参与者执行预提交操作，并回复协调者确认。 Do Commit阶段：\n协调者向所有参与者发送“正式提交”的指令，参与者正式提交事务并释放资源。 优点：\n非阻塞性： 3PC协议通过增加一个阶段，减少了在协调者或参与者发生故障时系统阻塞的概率。 更高的可靠性： 通过更精细的状态管理，提高了协议在复杂网络环境下的可靠性。 缺点：\n协议复杂性增加： 3PC比2PC更加复杂，涉及更多的通信步骤和状态管理。 性能开销更大： 额外的阶段和通信增加了事务处理的延迟和系统负载。 示例：\n在一个全球性的股票交易平台，交易操作涉及多个国家的数据库节点。通过3PC协议，系统能够在网络分\n区或节点故障时，继续推进事务的提交或回滚，避免系统阻塞，确保交易的一致性和可靠性。\nCAP定理与权衡 **CAP定理（CAP Theorem）**是分布式系统设计中的一个基本原则，由计算机科学家Eric Brewer提出。CAP定理指出，在任何分布式系统中，不可能同时满足以下三个属性：\n一致性（Consistency）： 所有节点在同一时间看到的数据是一致的。 可用性（Availability）： 每个请求都能在有限的时间内得到响应，无论结果如何。 分区容忍性（Partition Tolerance）： 系统在发生网络分区（即部分节点无法通信）时，仍能继续运行。 CAP定理强调，在面临网络分区时，系统只能选择保证一致性或保证可用性，而无法同时兼顾两者。这一理论对分布式数据库的设计和选择具有重要指导意义。\nCAP定理的应用与权衡 在实际设计分布式数据库时，理解并应用CAP定理能够帮助开发者根据业务需求和系统环境做出合理的权衡选择。\n系统类型 描述 示例 CP系统（Consistency + Partition Tolerance） 在网络分区时，优先保证数据一致性，可能会牺牲部分可用性。 HBase、MongoDB（在某些配置下） AP系统（Availability + Partition Tolerance） 在网络分区时，优先保证系统的可用性，可能会牺牲部分数据一致性。 Cassandra、DynamoDB CA系统（Consistency + Availability） 理论上的模型，要求在没有网络分区的理想环境下同时满足一致性和可用性。 实际中难以实现 应用示例：\n在设计一个全球性的电子商务平台时，开发团队需要在一致性和可用性之间做出权衡。假设平台需要在全球范围内实时更新库存信息，以防止超卖。在这种情况下，团队可能选择CP系统，优先保证库存数据的一致性，即使在某些区域发生网络分区时，部分服务可能暂时不可用。\n相反，如果平台更注重用户体验和系统的高可用性，即使在某些区域发生网络分区，系统仍然能够响应用户的查询和操作，团队可能选择AP系统，接受在某些情况下数据的一致性可能会有短暂的延迟。\n分布式一致性协议 为了在分布式系统中实现数据的一致性，研究者和工程师开发了多种一致性协议。这些协议旨在协调多个节点的数据操作，确保在分布式环境下保持数据的一致性和完整性。以下是两种广泛应用的一致性协议：Paxos和Raft。\nPaxos协议 Paxos协议由计算机科学家Leslie Lamport提出，是一种容错一致性算法，旨在在分布式系统中达成一致。Paxos被认为是实现分布式一致性的理论基础，广泛应用于各种分布式系统中。\n工作原理：\nPaxos协议通过选举一个提议者（Proposer），并通过接受者（Acceptor）和学习者（Learner）的协作，达成对一个提议的共识。其核心步骤包括：\n准备阶段（Prepare Phase）：\n提议者向所有接受者发送一个准备请求，带有一个唯一的提议编号。 接受者回复提议者，承诺不接受编号低于该提议编号的任何提议，并返回已接受的最高编号提议。 提议阶段（Propose Phase）：\n根据接受者的响应，提议者选择一个提议值（可能是之前最高编号提议的值）并发送给所有接受者。 接受阶段（Accept Phase）：\n接受者在未收到更高编号的提议请求的情况下，接受提议者发送的提议值，并通知学习者。 学习阶段（Learn Phase）：\n学习者接收到多数接受者的接受通知后，确定提议值为共识结果，并将其应用于系统。 优点：\n高容错性： 能够在少数节点故障或网络分区的情况下，仍然达成一致。 理论完善： Paxos有严格的数学证明，确保在所有可能的分布式环境中都能正确运行。 缺点：\n复杂性高： Paxos协议的实现和理解相对复杂，尤其是在实际系统中应用时，需要处理各种异常情况。 性能开销大： 多阶段的通信和协调增加了协议的延迟和系统的负载。 示例：\n在一个分布式键值存储系统中，Paxos协议用于协调多个副本节点的数据写入操作。每当有新的写请求到达时，提议者通过Paxos协议与接受者协商，确保所有副本节点最终都存储相同的数据值，保持数据的一致性。\nRaft协议 Raft协议是由Diego Ongaro和John Ousterhout提出的，旨在提供一种比Paxos更易于理解和实现的一致性协议。Raft通过明确的领导者选举和日志复制机制，实现分布式系统中的一致性。\n工作原理：\nRaft协议主要包括以下三个子协议：\n领导者选举（Leader Election）：\n系统中的节点通过选举过程选出一个领导者（Leader），其他节点作为跟随者（Follower）或候选者（Candidate）。 领导者负责管理日志复制和客户端请求的处理。 日志复制（Log Replication）：\n领导者接收来自客户端的日志条目，并将其复制到所有跟随者节点。 当日志条目被大多数节点确认后，领导者将其提交，并通知跟随者应用该条目。 安全性（Safety）：\n确保已提交的日志条目在所有节点上都是一致的。 防止旧的领导者覆盖新的日志条目，维护系统的一致性。 优点：\n易于理解和实现： Raft协议的设计更加直观，易于开发者理解和实现。 高效性： 通过明确的领导者角色和高效的日志复制机制，Raft协议在性能上表现良好。 强一致性： 确保所有节点在任何时间点的数据一致性，适用于需要严格一致性的应用场景。 缺点：\n单点领导者风险： 领导者节点的故障可能导致系统短暂的不可用，尽管Raft协议通过快速选举机制减轻了这一风险。 扩展性限制： 在极大规模的分布式系统中，领导者节点可能成为性能瓶颈。 示例：\n在一个分布式数据库系统中，Raft协议用于管理节点间的日志复制和事务提交。领导者节点负责接收客户端的写请求，将其日志条目复制到跟随者节点，并在大多数节点确认后提交事务，确保数据的一致性和可靠性。\n前沿研究与发展 随着分布式数据库技术的不断演进，研究者和工程师们在优化一致性协议、提高系统性能和扩展性等方面进行了大量的探索。以下是一些当前的前沿研究方向和发展趋势：\n弹性一致性协议 **弹性一致性协议（Elastic Consistency Protocols）**旨在根据系统的实时负载和网络状况，动态调整一致性级别和策略，以实现更高的性能和可用性。例如，在网络状况良好时，系统可以采用强一致性模型，而在网络分区或高负载情况下，系统则可以临时降低一致性要求，保持高可用性。\n研究方向：\n自适应一致性： 基于实时监控数据，自动调整一致性协议的参数和策略。 混合一致性模型： 结合强一致性和最终一致性，提供灵活的选择以满足不同业务需求。 动态资源分配： 根据负载情况，动态分配和调整系统资源，优化一致性协议的执行效率。 示例：\n一种弹性一致性协议在低负载和稳定网络环境下采用Raft协议，确保强一致性；在高负载或网络不稳定时，切换到最终一致性模型，提升系统的响应速度和可用性。\n分布式事务的优化 **分布式事务的优化（Distributed Transaction Optimization）**旨在减少分布式事务管理的延迟和开销，提高系统的吞吐量和响应速度。研究者们探索了多种优化技术，包括并行化事务处理、轻量级协议和预测性算法等。\n研究方向：\n并行化事务处理： 通过并行执行事务中的独立操作，减少事务的整体执行时间。 轻量级一致性协议： 开发更加高效的一致性协议，减少通信和协调的开销。 事务预测与预处理： 利用机器学习算法预测事务的执行路径和资源需求，提前进行优化和资源分配。 示例：\n一种分布式数据库系统通过将复杂事务拆分为多个独立的子事务，并行处理这些子事务，同时采用轻量级的Paxos变体，实现了显著的性能提升和响应速度优化。\n无中心化一致性协议 **无中心化一致性协议（Decentralized Consistency Protocols）**探索在没有明确领导者或协调者的情况下，实现分布式系统的一致性。无中心化协议通过去除单点故障，提升系统的容错性和扩展性。\n研究方向：\n去领导化的共识机制： 发展无需固定领导者的共识协议，确保系统的高度可用性和灵活性。 分布式哈希表（DHT）与共识： 结合DHT技术，实现分布式环境下的数据查找和一致性。 多领导者共识： 允许多个领导者并行操作，通过协调机制确保一致性和冲突解决。 示例：\n一种无中心化一致性协议通过在所有节点之间分布式地进行提议和投票，实现数据的一致性，无需依赖单一的领导者节点，提升了系统的容错性和扩展能力。\n前沿研究案例 为了更好地理解分布式数据库的设计与实现，以及不同策略和协议的性能表现，以下通过具体的实验和案例研究进行说明。\n实验与案例研究 分布式数据库搭建与配置 在实际环境中，搭建和配置分布式数据库系统是理解其工作原理和性能的基础。以下是一个简化的实验步骤：\n选择分布式数据库系统： 例如，选择Apache Cassandra、Google Spanner或CockroachDB等。 部署多节点集群： 在不同物理或虚拟服务器上部署多个数据库节点，组成一个分布式集群。 配置数据分布策略： 根据业务需求，选择合适的数据分布策略（如水平分割或垂直分割），并进行配置。 实施分布式事务管理： 配置和测试分布式事务管理协议，如2PC或Raft，确保事务的一致性和可靠性。 监控与性能测试： 使用监控工具（如Prometheus、Grafana）监控系统性能，进行负载测试和故障模拟，评估系统的表现和稳定性。 示例：\n使用Apache Cassandra搭建一个分布式数据库集群，包括三个数据中心，每个数据中心部署多个节点。配置水平分割策略，根据用户ID将数据分布到不同的数据中心，实现高可用性和低延迟的数据访问。\n分布式事务管理实践 通过具体的分布式事务管理实践，可以深入理解2PC和Raft协议的实际应用和性能表现。\n案例：跨地域订单处理系统\n一个全球性的电商平台需要处理跨地域的订单事务，涉及多个数据库节点的数据操作。通过实施2PC协议，确保在不同地区的数据中心之间的订单信息一致性和完整性。\n实验步骤：\n配置2PC协议： 在涉及的数据库节点上配置2PC协议，确保事务的协调和一致性。 执行跨地域事务： 模拟跨地区的订单创建和更新操作，观察事务的一致性和恢复能力。 故障模拟： 在事务执行过程中，模拟网络分区和节点故障，评估2PC协议在异常情况下的表现。 性能评估： 测量事务的延迟和系统的吞吐量，分析2PC协议对系统性能的影响。 结果分析：\n在正常情况下，2PC协议能够确保跨地域事务的一致性。然而，在网络分区发生时，系统可能出现事务阻塞，导致部分操作无法完成。通过优化网络配置和引入超时机制，减轻了事务阻塞的影响，但仍然需要权衡一致性和可用性之间的关系。\n分布式数据库设计案例 通过分析实际的分布式数据库设计案例，可以了解如何根据不同的业务需求，制定合理的数据分布策略和事务管理方案。\n案例：全球社交媒体平台的分布式数据库设计\n一个全球性的社交媒体平台需要支持数亿用户的高并发访问和实时数据更新。为满足高性能和高可用性的需求，设计了如下的分布式数据库架构：\n数据分布策略：\n水平分割： 根据用户ID将用户数据分布到不同的数据库节点，确保每个节点的负载均衡。 垂直分割： 将用户的基本信息和用户的动态信息存储在不同的数据库节点，优化不同类型的数据访问。 事务管理：\n采用Raft协议： 选举一个领导者节点，负责协调用户动态信息的写入和更新，确保数据的一致性。 异步复制： 实现数据的异步复制，提高系统的写入性能和响应速度。 高可用性设计：\n多数据中心部署： 在全球多个地理区域部署数据库节点，确保数据的本地化存储和快速访问。 故障转移机制： 配置自动故障转移和恢复机制，确保在某个数据中心发生故障时，其他数据中心能够无缝接管服务。 实施效果：\n通过上述设计，社交媒体平台实现了高并发访问的支持，用户在全球范围内均能获得快速的响应体验。同时，系统具备高度的容错能力和可扩展性，能够根据业务增长灵活地调整和扩展数据库资源。\n前沿研究与发展 随着分布式数据库技术的不断演进，研究者和工程师们在优化一致性协议、提高系统性能和扩展性等方面进行了大量的探索。以下是一些当前的前沿研究方向和发展趋势：\n基于人工智能的安全防护 人工智能（AI）和机器学习（ML）在数据库安全中的应用正变得越来越普遍。通过分析大量的数据库活动数据，AI和ML算法能够识别出异常行为和潜在威胁，提升检测和响应的效率。\n应用示例：\n异常检测： 利用机器学习模型分析正常的数据库操作模式，实时检测与之不符的异常行为，如异常的查询频率、数据访问模式等，及时发现潜在的安全威胁。 自动化威胁响应： 结合AI技术，自动化处理检测到的威胁，执行如阻断可疑连接、通知管理员等响应措施，减少人为干预和响应时间。 挑战与前景：\n尽管基于AI的安全防护具有很大潜力，但也面临数据隐私保护、模型训练的准确性和可解释性等挑战。未来的发展将集中在提升模型的透明度和可解释性，以及优化算法以适应不断变化的威胁环境。\n同态加密与安全多方计算 **同态加密（Homomorphic Encryption）**允许在加密数据上直接执行计算操作，而无需解密数据，从而在保证数据隐私的前提下，实现数据的处理和分析。这一技术在保护敏感数据的同时，支持云计算和大数据分析等应用场景。\n应用示例：\n在医疗领域，医院可以将患者数据加密后存储在云端，研究人员可以在不解密数据的情况下，执行统计分析和数据挖掘，保护患者隐私。\n**安全多方计算（Secure Multi-Party Computation, SMPC）**则允许多个参与方在不泄露各自输入数据的情况下，共同完成计算任务，适用于需要数据共享但又需保护隐私的场景。\n前景与挑战：\n同态加密和SMPC的研究正在迅速发展，尽管目前的计算性能和算法效率仍有待提高，但随着技术的进步，预计将在更多实际应用中得到广泛采用。\n区块链与数据库安全 区块链技术以其去中心化、不可篡改和透明性的特点，为数据库安全提供了新的思路。通过将数据库操作记录在区块链上，可以实现数据的透明审计和防篡改保护。\n应用示例：\n在供应链管理中，利用区块链记录每一次的商品转移和交易操作，确保数据的透明和不可篡改，提升供应链的可追溯性和信任度。\n前景与挑战：\n区块链与传统数据库的结合仍处于探索阶段，主要挑战包括数据存储的可扩展性、隐私保护以及与现有系统的集成问题。然而，随着技术的成熟，区块链有望在提高数据库系统的安全性和透明度方面发挥重要作用。\n增强的备份压缩与去重技术 随着数据量的急剧增长，传统的备份方法面临着存储空间和传输效率的挑战。增强的备份压缩与去重技术通过智能算法，进一步提高备份数据的压缩率和存储效率。\n研究方向：\n智能压缩算法： 开发高效的压缩算法，适应不同类型的数据，提升压缩效果和解压速度。 数据去重技术： 通过识别和消除重复数据，减少备份数据的存储空间。 实时压缩与去重： 实现备份过程中的实时压缩和去重，提升备份效率和系统性能。 示例：\n利用先进的压缩算法和数据去重技术，对备份数据进行实时压缩和去重，显著减少存储空间需求，同时提升备份和恢复的速度。\nNoSQL数据库 随着互联网应用的迅猛发展，传统的关系型数据库（Relational Database Management Systems, RDBMS）在处理大规模、多样化和高并发的数据需求时，逐渐显现出局限性。**NoSQL数据库（Not Only SQL Databases）**作为一种新兴的数据存储解决方案，因其高扩展性、灵活的数据模型和卓越的性能，成为现代应用系统中不可或缺的一部分。本章将深入探讨NoSQL数据库的基本概念、分类、数据模型、查询语言、应用场景及其与关系型数据库的对比。同时，结合前沿研究，展望NoSQL数据库技术的发展趋势。\nNoSQL数据库概述 NoSQL数据库是一类非关系型数据库，旨在解决关系型数据库在大数据时代面临的挑战。NoSQL数据库通常具有以下特点：\n高扩展性（Scalability）： 通过水平扩展（增加更多的服务器节点）来处理大规模数据和高并发请求。 灵活的数据模型（Flexible Data Models）： 支持多种数据模型，如键值存储、文档存储、列族存储和图数据库，适应不同类型的数据需求。 高性能（High Performance）： 优化读写操作，适用于实时数据处理和大规模数据分析。 分布式架构（Distributed Architecture）： 数据分布在多个节点上，具备高可用性和容错能力。 去模式化（Schema-less）： 不需要预定义固定的表结构，支持动态数据模式，便于快速迭代和开发。 NoSQL数据库的分类 NoSQL数据库根据其数据模型和存储方式的不同，主要分为以下四类：\n类型 描述 示例 键值存储 基于键值对的数据存储，适用于简单的数据查询和高速读写操作。 Redis、Riak、Amazon DynamoDB 文档存储 以文档（通常是JSON、BSON等格式）形式存储数据，支持复杂的数据结构和嵌套关系。 MongoDB、CouchDB、RavenDB 列族存储 以列簇（Column Families）为单位存储数据，适合处理大规模的分布式数据集和高吞吐量的读写操作。 Apache Cassandra、HBase、ScyllaDB 图数据库 以图（Graph）的形式表示和存储数据，适用于复杂的关系查询和图算法。 Neo4j、Amazon Neptune、ArangoDB 各类NoSQL数据库的详细解析 键值存储数据库 键值存储数据库是NoSQL数据库中最简单的一类，通过键（Key）和值（Value）的形式存储数据。每个键对应一个唯一的值，值可以是任何类型的数据，如字符串、JSON对象、二进制数据等。这种模型适用于缓存、会话存储和简单的数据查询。\n特点：\n高性能读写： 由于数据按键直接访问，读写操作非常快速。 简单的数据模型： 适合存储简单的数据结构，便于快速开发和部署。 高度可扩展： 通过增加更多的节点，可以轻松应对大规模数据和高并发请求。 示例：\nRedis是最著名的键值存储数据库之一，广泛用于缓存、消息队列和实时数据处理。通过简单的命令，如GET和SET，Redis能够高效地存取数据。\n1 2 3 4 5 # 在Redis中设置一个键值对 SET user:1000 \u0026#34;John Doe\u0026#34; # 获取键对应的值 GET user:1000 文档存储数据库 文档存储数据库以文档（通常是JSON、BSON等格式）为单位存储数据，支持复杂的数据结构和嵌套关系。这种模型适用于内容管理系统、电子商务平台和任何需要存储半结构化数据的应用。\n特点：\n灵活的数据模式： 不需要预定义固定的表结构，支持动态和复杂的数据结构。 丰富的查询能力： 提供强大的查询语言，支持基于字段、嵌套文档和数组的复杂查询。 高可扩展性： 通过分片（Sharding）和复制（Replication）机制，实现水平扩展和高可用性。 示例：\nMongoDB是最流行的文档存储数据库之一，广泛应用于Web应用、移动应用和大数据分析。通过MongoDB的查询语言，可以轻松地查询和操作嵌套文档。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;orders\u0026#34;: [ { \u0026#34;order_id\u0026#34;: 5001, \u0026#34;product\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;quantity\u0026#34;: 1 }, { \u0026#34;order_id\u0026#34;: 5002, \u0026#34;product\u0026#34;: \u0026#34;Mouse\u0026#34;, \u0026#34;quantity\u0026#34;: 2 } ] } 1 2 3 4 5 6 7 8 9 10 11 12 // 在MongoDB中插入一个文档 db.users.insertOne({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;orders\u0026#34;: [ {\u0026#34;order_id\u0026#34;: 5001, \u0026#34;product\u0026#34;: \u0026#34;Laptop\u0026#34;, \u0026#34;quantity\u0026#34;: 1}, {\u0026#34;order_id\u0026#34;: 5002, \u0026#34;product\u0026#34;: \u0026#34;Mouse\u0026#34;, \u0026#34;quantity\u0026#34;: 2} ] }); // 查询用户ID为1000的用户 db.users.find({ \u0026#34;user_id\u0026#34;: 1000 }) 列族存储数据库 列族存储数据库以列簇（Column Families）为单位存储数据，适合处理大规模的分布式数据集和高吞吐量的读写操作。这种模型特别适用于时间序列数据、日志存储和实时分析。\n特点：\n高写入吞吐量： 适合大规模的并发写入操作，常用于日志记录和事件存储。 列级压缩： 通过压缩相似列的数据，提高存储效率和查询性能。 强大的分布式特性： 支持数据的自动分片和复制，确保数据的高可用性和容错性。 示例：\nApache Cassandra是领先的列族存储数据库，广泛应用于电信、金融和社交媒体等需要高可用性和高性能的数据密集型应用。\n1 2 3 4 5 6 7 8 9 10 11 -- 在Cassandra中创建一个表 CREATE TABLE users ( user_id UUID PRIMARY KEY, name TEXT, email TEXT, created_at TIMESTAMP ); -- 插入数据 INSERT INTO users (user_id, name, email, created_at) VALUES (uuid(), \u0026#39;John Doe\u0026#39;, \u0026#39;john.doe@example.com\u0026#39;, toTimestamp(now())); 图数据库 图数据库以图（Graph）的形式表示和存储数据，适用于复杂的关系查询和图算法。这种模型特别适合社交网络、推荐系统、知识图谱和欺诈检测等应用场景。\n特点：\n高效的关系查询： 能够快速遍历和查询节点之间的复杂关系。 直观的数据表示： 以节点和边的形式直观地表示实体及其关系，易于理解和建模。 支持图算法： 内置丰富的图算法，如最短路径、社区发现和图遍历，适用于数据分析和挖掘。 示例：\nNeo4j是最著名的图数据库之一，广泛应用于社交网络分析、推荐系统和网络安全等领域。通过Cypher查询语言，可以轻松地查询和分析图数据。\n1 2 3 4 5 6 7 8 // 在Neo4j中创建节点和关系 CREATE (john:Person {name: \u0026#39;John Doe\u0026#39;}), (jane:Person {name: \u0026#39;Jane Smith\u0026#39;}), (john)-[:FRIENDS_WITH]-\u0026gt;(jane); // 查询John的朋友 MATCH (john:Person {name: \u0026#39;John Doe\u0026#39;})-[:FRIENDS_WITH]-\u0026gt;(friend) RETURN friend.name; NoSQL数据库与关系型数据库的对比 为了更清晰地理解NoSQL数据库与传统关系型数据库的区别，以下表格总结了两者在多个维度上的对比：\n维度 关系型数据库（RDBMS） NoSQL数据库 数据模型 基于表、行和列的严格模式 多种数据模型（键值、文档、列族、图） 扩展性 垂直扩展（增加更强大的服务器） 水平扩展（增加更多的服务器节点） 查询语言 使用结构化查询语言（SQL） 多种查询语言，部分支持类似SQL的查询语言 事务支持 强一致性，支持ACID事务 一致性模型多样，部分支持ACID事务，更多支持最终一致性 灵活性 需要预定义模式，变更复杂 模式灵活，适应动态和多样化的数据结构 性能 对复杂查询和事务优化较好 对大规模并发读写和特定查询优化较好 应用场景 金融系统、企业管理系统、ERP等 大数据分析、实时应用、社交网络、内容管理等 NoSQL数据库的应用场景 NoSQL数据库因其独特的优势，广泛应用于以下场景：\n大数据与实时分析： 需要处理海量数据并进行实时分析的应用，如日志分析、监控系统和物联网数据处理。 社交网络： 需要存储和查询复杂的用户关系和互动数据，适合使用图数据库。 内容管理系统： 需要灵活的数据模型和快速的内容检索，适合使用文档存储数据库。 电子商务平台： 需要高并发的订单处理和库存管理，适合使用键值存储或列族存储数据库。 移动应用： 需要快速的读写操作和灵活的数据结构，适合使用键值存储或文档存储数据库。 前沿研究与发展 随着技术的不断进步，NoSQL数据库领域也在不断演变，涌现出许多新的研究方向和发展趋势：\n多模型数据库 **多模型数据库（Multi-Model Databases）**支持多种数据模型（如文档、键值、图等）在同一数据库系统中共存，提供更大的灵活性和统一的数据管理。通过统一的查询语言和数据接口，开发者可以在一个平台上处理多种类型的数据，简化了系统架构和数据集成。\n研究方向：\n统一查询语言： 发展能够同时支持多种数据模型的查询语言，如ArangoDB的AQL。 优化存储引擎： 设计高效的存储引擎，能够在同一数据库中高效地存储和检索多种数据模型。 事务管理： 实现跨模型的一致性事务管理，确保不同数据模型之间的数据一致性和完整性。 示例：\nArangoDB是一个领先的多模型数据库，支持文档、图和键值数据模型。开发者可以在同一数据库中存储和查询不同类型的数据，满足复杂的应用需求。\n1 2 3 4 5 6 7 8 9 10 11 // 在ArangoDB中插入文档和图数据 db.users.insert({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }); db.users.insert({ \u0026#34;user_id\u0026#34;: 1001, \u0026#34;name\u0026#34;: \u0026#34;Jane Smith\u0026#34; }); db.relationships.insert({ \u0026#34;_from\u0026#34;: \u0026#34;users/1000\u0026#34;, \u0026#34;_to\u0026#34;: \u0026#34;users/1001\u0026#34;, \u0026#34;relation\u0026#34;: \u0026#34;friends\u0026#34; }); // 查询用户及其朋友 FOR user IN users FILTER user.name == \u0026#34;John Doe\u0026#34; FOR friend IN OUTBOUND user relationships RETURN friend.name 分布式一致性优化 **分布式一致性优化（Distributed Consistency Optimization）**旨在提升分布式数据库系统中一致性协议的性能和效率。通过改进协议算法、减少通信开销和优化资源管理，研究者们致力于在保持数据一致性的前提下，提升系统的吞吐量和响应速度。\n研究方向：\n轻量级一致性协议： 设计更加高效的协议，减少通信步骤和资源消耗。 混合一致性模型： 结合强一致性和最终一致性，提供灵活的选择以适应不同的业务需求。 容错与恢复机制： 提升协议在面对节点故障和网络分区时的容错能力和恢复效率。 示例：\nRaft协议的改进版本，如RAFTZ，旨在减少协议的通信开销和提升在高并发环境下的一致性性能。\n云原生NoSQL数据库 **云原生NoSQL数据库（Cloud-Native NoSQL Databases）**专为云计算环境设计，充分利用云平台的弹性、可扩展性和分布式特性。云原生数据库通常具备自动扩展、自动修复和按需分配资源等功能，适应动态变化的工作负载和业务需求。\n研究方向：\n自动扩展与弹性管理： 实现基于负载和资源利用率的自动扩展和收缩机制，优化资源分配和成本。 无服务器架构： 设计无服务器的数据库架构，简化部署和管理，提升系统的灵活性和可用性。 跨云兼容性： 实现跨多个云平台的数据同步和互操作，提升数据的可迁移性和系统的容错性。 示例：\nAmazon DynamoDB是一个云原生NoSQL数据库，提供高性能、无服务器的数据库服务。通过自动分片和复制机制，DynamoDB能够在全球范围内快速扩展，满足大规模应用的需求。\n1 2 3 4 5 6 # 使用AWS CLI创建DynamoDB表 aws dynamodb create-table \\ --table-name Users \\ --attribute-definitions AttributeName=UserID,AttributeType=S \\ --key-schema AttributeName=UserID,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 实验与案例研究 通过实际的实验和案例研究，可以更好地理解NoSQL数据库的设计与应用，以及不同类型数据库的性能表现和适用场景。\nNoSQL数据库搭建与配置 在实际环境中，搭建和配置NoSQL数据库系统是理解其工作原理和性能的基础。以下是一个简化的实验步骤，以MongoDB为例：\n安装MongoDB： 根据官方文档，在不同的服务器节点上安装MongoDB。 配置集群： 设置副本集（Replica Set）或分片集群（Sharded Cluster），确保数据的高可用性和可扩展性。 创建数据库和集合： 根据业务需求，创建相应的数据库和集合，定义索引和数据结构。 实施数据分布策略： 配置水平分片，选择合适的分片键，以实现数据的均衡分布和高效访问。 监控与性能测试： 使用监控工具（如MongoDB Atlas、Prometheus）监控系统性能，进行负载测试和故障模拟，评估系统的表现和稳定性。 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 启动MongoDB副本集 mongod --replSet rs0 --bind_ip localhost,\u0026lt;node1_ip\u0026gt;,\u0026lt;node2_ip\u0026gt;,\u0026lt;node3_ip\u0026gt; --port 27017 --dbpath /var/lib/mongodb --logpath /var/log/mongodb/mongod.log --fork # 初始化副本集 mongo --host \u0026lt;node1_ip\u0026gt;:27017 rs.initiate( { _id: \u0026#34;rs0\u0026#34;, members: [ { _id: 0, host: \u0026#34;\u0026lt;node1_ip\u0026gt;:27017\u0026#34; }, { _id: 1, host: \u0026#34;\u0026lt;node2_ip\u0026gt;:27017\u0026#34; }, { _id: 2, host: \u0026#34;\u0026lt;node3_ip\u0026gt;:27017\u0026#34; } ] } ) NoSQL数据库查询实践 NoSQL数据库的查询语言和操作方式与关系型数据库有所不同，理解其查询语法和优化方法对于高效使用NoSQL数据库至关重要。以下是MongoDB的一些常用查询操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // 插入文档 db.users.insertOne({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: new Date() }); // 查询文档 db.users.find({ \u0026#34;user_id\u0026#34;: 1000 }); // 更新文档 db.users.updateOne( { \u0026#34;user_id\u0026#34;: 1000 }, { $set: { \u0026#34;email\u0026#34;: \u0026#34;john.doe@newdomain.com\u0026#34; } } ); // 删除文档 db.users.deleteOne({ \u0026#34;user_id\u0026#34;: 1000 }); // 创建索引 db.users.createIndex({ \u0026#34;email\u0026#34;: 1 }); 数据库设计案例 通过分析实际的数据库设计案例，可以了解如何根据不同的业务需求，选择合适的NoSQL数据库类型和优化策略。\n案例：实时数据分析平台的NoSQL数据库设计\n一家科技公司需要构建一个实时数据分析平台，用于处理和分析来自数百万设备的实时数据流。为满足高吞吐量、低延迟和灵活的数据模型需求，团队选择了Apache Cassandra作为核心数据库。\n设计策略：\n数据分布策略：\n水平分割： 根据设备ID将数据分布到不同的Cassandra节点，确保负载均衡和高可扩展性。 列族设计： 为每种数据类型（如传感器数据、日志数据）创建独立的列族，优化数据的读写性能。 事务管理：\n轻量级事务： 利用Cassandra的轻量级事务（Lightweight Transactions）实现某些关键操作的一致性需求。 高可用性设计：\n多数据中心部署： 在不同地理区域部署Cassandra数据中心，确保数据的冗余和容错能力。 自动故障转移： 配置Cassandra的自动故障转移机制，确保在节点或数据中心故障时，系统能够快速恢复。 实施效果：\n通过上述设计，实时数据分析平台能够高效地处理数百万设备的实时数据流，确保系统的高可用性和数据的高一致性。Cassandra的高写入吞吐量和灵活的数据模型，使得平台能够快速响应业务需求和数据变化。\n前沿研究与发展 NoSQL数据库领域正在不断发展，研究者和工程师们在提升系统性能、扩展性和一致性等方面进行了大量探索。以下是一些当前的前沿研究方向和发展趋势：\n多模型数据库 **多模型数据库（Multi-Model Databases）**支持多种数据模型（如文档、键值、图等）在同一数据库系统中共存，提供更大的灵活性和统一的数据管理。通过统一的查询语言和数据接口，开发者可以在一个平台上处理多种类型的数据，简化了系统架构和数据集成。\n研究方向：\n统一查询语言： 发展能够同时支持多种数据模型的查询语言，如ArangoDB的AQL。 优化存储引擎： 设计高效的存储引擎，能够在同一数据库中高效地存储和检索多种数据模型。 事务管理： 实现跨模型的一致性事务管理，确保不同数据模型之间的数据一致性和完整性。 示例：\nArangoDB是一个领先的多模型数据库，支持文档、图和键值数据模型。开发者可以在同一数据库中存储和查询不同类型的数据，满足复杂的应用需求。\n1 2 3 4 5 6 7 8 9 10 11 // 在ArangoDB中插入文档和图数据 db.users.insert({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }); db.users.insert({ \u0026#34;user_id\u0026#34;: 1001, \u0026#34;name\u0026#34;: \u0026#34;Jane Smith\u0026#34; }); db.relationships.insert({ \u0026#34;_from\u0026#34;: \u0026#34;users/1000\u0026#34;, \u0026#34;_to\u0026#34;: \u0026#34;users/1001\u0026#34;, \u0026#34;relation\u0026#34;: \u0026#34;friends\u0026#34; }); // 查询用户及其朋友 FOR user IN users FILTER user.name == \u0026#34;John Doe\u0026#34; FOR friend IN OUTBOUND user relationships RETURN friend.name 分布式一致性优化 **分布式一致性优化（Distributed Consistency Optimization）**旨在提升分布式数据库系统中一致性协议的性能和效率。通过改进协议算法、减少通信开销和优化资源管理，研究者们致力于在保持数据一致性的前提下，提升系统的吞吐量和响应速度。\n研究方向：\n轻量级一致性协议： 设计更加高效的协议，减少通信步骤和资源消耗。 混合一致性模型： 结合强一致性和最终一致性，提供灵活的选择以适应不同的业务需求。 容错与恢复机制： 提升协议在面对节点故障和网络分区时的容错能力和恢复效率。 示例：\nRaft协议的改进版本，如RAFTZ，旨在减少协议的通信开销和提升在高并发环境下的一致性性能。\n云原生NoSQL数据库 **云原生NoSQL数据库（Cloud-Native NoSQL Databases）**专为云计算环境设计，充分利用云平台的弹性、可扩展性和分布式特性。云原生数据库通常具备自动扩展、自动修复和按需分配资源等功能，适应动态变化的工作负载和业务需求。\n研究方向：\n自动扩展与弹性管理： 实现基于负载和资源利用率的自动扩展和收缩机制，优化资源分配和成本。 无服务器架构： 设计无服务器的数据库架构，简化部署和管理，提升系统的灵活性和可用性。 跨云兼容性： 实现跨多个云平台的数据同步和互操作，提升数据的可迁移性和系统的容错性。 示例：\nAmazon DynamoDB是一个云原生NoSQL数据库，提供高性能、无服务器的数据库服务。通过自动分片和复制机制，DynamoDB能够在全球范围内快速扩展，满足大规模应用的需求。\n1 2 3 4 5 6 # 使用AWS CLI创建DynamoDB表 aws dynamodb create-table \\ --table-name Users \\ --attribute-definitions AttributeName=UserID,AttributeType=S \\ --key-schema AttributeName=UserID,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 实验与案例研究 通过实际的实验和案例研究，可以更好地理解NoSQL数据库的设计与应用，以及不同类型数据库的性能表现和适用场景。\nNoSQL数据库搭建与配置 在实际环境中，搭建和配置NoSQL数据库系统是理解其工作原理和性能的基础。以下是一个简化的实验步骤，以MongoDB为例：\n安装MongoDB： 根据官方文档，在不同的服务器节点上安装MongoDB。 配置集群： 设置副本集（Replica Set）或分片集群（Sharded Cluster），确保数据的高可用性和可扩展性。 创建数据库和集合： 根据业务需求，创建相应的数据库和集合，定义索引和数据结构。 实施数据分布策略： 配置水平分片，选择合适的分片键，以实现数据的均衡分布和高效访问。 监控与性能测试： 使用监控工具（如MongoDB Atlas、Prometheus）监控系统性能，进行负载测试和故障模拟，评估系统的表现和稳定性。 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 启动MongoDB副本集 mongod --replSet rs0 --bind_ip localhost,\u0026lt;node1_ip\u0026gt;,\u0026lt;node2_ip\u0026gt;,\u0026lt;node3_ip\u0026gt; --port 27017 --dbpath /var/lib/mongodb --logpath /var/log/mongodb/mongod.log --fork # 初始化副本集 mongo --host \u0026lt;node1_ip\u0026gt;:27017 rs.initiate( { _id: \u0026#34;rs0\u0026#34;, members: [ { _id: 0, host: \u0026#34;\u0026lt;node1_ip\u0026gt;:27017\u0026#34; }, { _id: 1, host: \u0026#34;\u0026lt;node2_ip\u0026gt;:27017\u0026#34; }, { _id: 2, host: \u0026#34;\u0026lt;node3_ip\u0026gt;:27017\u0026#34; } ] } ) NoSQL数据库查询实践 NoSQL数据库的查询语言和操作方式与关系型数据库有所不同，理解其查询语法和优化方法对于高效使用NoSQL数据库至关重要。以下是MongoDB的一些常用查询操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // 插入文档 db.users.insertOne({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: new Date() }); // 查询文档 db.users.find({ \u0026#34;user_id\u0026#34;: 1000 }); // 更新文档 db.users.updateOne( { \u0026#34;user_id\u0026#34;: 1000 }, { $set: { \u0026#34;email\u0026#34;: \u0026#34;john.doe@newdomain.com\u0026#34; } } ); // 删除文档 db.users.deleteOne({ \u0026#34;user_id\u0026#34;: 1000 }); // 创建索引 db.users.createIndex({ \u0026#34;email\u0026#34;: 1 }); 数据库设计案例 通过分析实际的数据库设计案例，可以了解如何根据不同的业务需求，选择合适的NoSQL数据库类型和优化策略。\n案例：实时数据分析平台的NoSQL数据库设计\n一家科技公司需要构建一个实时数据分析平台，用于处理和分析来自数百万设备的实时数据流。为满足高吞吐量、低延迟和灵活的数据模型需求，团队选择了Apache Cassandra作为核心数据库。\n设计策略：\n数据分布策略：\n水平分割： 根据设备ID将数据分布到不同的Cassandra节点，确保负载均衡和高可扩展性。 列族设计： 为每种数据类型（如传感器数据、日志数据）创建独立的列族，优化数据的读写性能。 事务管理：\n轻量级事务： 利用Cassandra的轻量级事务（Lightweight Transactions）实现某些关键操作的一致性需求。 高可用性设计：\n多数据中心部署： 在不同地理区域部署Cassandra数据中心，确保数据的冗余和容错能力。 自动故障转移： 配置Cassandra的自动故障转移机制，确保在节点或数据中心故障时，系统能够快速恢复。 实施效果：\n通过上述设计，实时数据分析平台能够高效地处理数百万设备的实时数据流，确保系统的高可用性和数据的高一致性。Cassandra的高写入吞吐量和灵活的数据模型，使得平台能够快速响应业务需求和数据变化。\n前沿研究与发展 随着NoSQL数据库技术的不断进步，研究者和工程师们在提升系统性能、扩展性和一致性等方面进行了大量探索。以下是一些当前的前沿研究方向和发展趋势：\n弹性一致性协议 **弹性一致性协议（Elastic Consistency Protocols）**旨在根据系统的实时负载和网络状况，动态调整一致性级别和策略，以实现更高的性能和可用性。例如，在网络状况良好时，系统可以采用强一致性模型，而在网络分区或高负载情况下，系统则可以临时降低一致性要求，保持高可用性。\n研究方向：\n自适应一致性： 基于实时监控数据，自动调整一致性协议的参数和策略。 混合一致性模型： 结合强一致性和最终一致性，提供灵活的选择以满足不同业务需求。 动态资源分配： 根据负载情况，动态分配和调整系统资源，优化一致性协议的执行效率。 示例：\n一种弹性一致性协议在低负载和稳定网络环境下采用Raft协议，确保强一致性；在高负载或网络不稳定时，切换到最终一致性模型，提升系统的响应速度和可用性。\n多模型数据库 **多模型数据库（Multi-Model Databases）**支持多种数据模型（如文档、键值、图等）在同一数据库系统中共存，提供更大的灵活性和统一的数据管理。通过统一的查询语言和数据接口，开发者可以在一个平台上处理多种类型的数据，简化了系统架构和数据集成。\n研究方向：\n统一查询语言： 发展能够同时支持多种数据模型的查询语言，如ArangoDB的AQL。 优化存储引擎： 设计高效的存储引擎，能够在同一数据库中高效地存储和检索多种数据模型。 事务管理： 实现跨模型的一致性事务管理，确保不同数据模型之间的数据一致性和完整性。 示例：\nArangoDB是一个领先的多模型数据库，支持文档、图和键值数据模型。开发者可以在同一数据库中存储和查询不同类型的数据，满足复杂的应用需求。\n1 2 3 4 5 6 7 8 9 10 11 // 在ArangoDB中插入文档和图数据 db.users.insert({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }); db.users.insert({ \u0026#34;user_id\u0026#34;: 1001, \u0026#34;name\u0026#34;: \u0026#34;Jane Smith\u0026#34; }); db.relationships.insert({ \u0026#34;_from\u0026#34;: \u0026#34;users/1000\u0026#34;, \u0026#34;_to\u0026#34;: \u0026#34;users/1001\u0026#34;, \u0026#34;relation\u0026#34;: \u0026#34;friends\u0026#34; }); // 查询用户及其朋友 FOR user IN users FILTER user.name == \u0026#34;John Doe\u0026#34; FOR friend IN OUTBOUND user relationships RETURN friend.name 分布式一致性优化 **分布式一致性优化（Distributed Consistency Optimization）**旨在提升分布式数据库系统中一致性协议的性能和效率。通过改进协议算法、减少通信开销和优化资源管理，研究者们致力于在保持数据一致性的前提下，提升系统的吞吐量和响应速度。\n研究方向：\n轻量级一致性协议： 设计更加高效的协议，减少通信步骤和资源消耗。 混合一致性模型： 结合强一致性和最终一致性，提供灵活的选择以适应不同的业务需求。 容错与恢复机制： 提升协议在面对节点故障和网络分区时的容错能力和恢复效率。 示例：\nRaft协议的改进版本，如RAFTZ，旨在减少协议的通信开销和提升在高并发环境下的一致性性能。\n云原生NoSQL数据库 **云原生NoSQL数据库（Cloud-Native NoSQL Databases）**专为云计算环境设计，充分利用云平台的弹性、可扩展性和分布式特性。云原生数据库通常具备自动扩展、自动修复和按需分配资源等功能，适应动态变化的工作负载和业务需求。\n研究方向：\n自动扩展与弹性管理： 实现基于负载和资源利用率的自动扩展和收缩机制，优化资源分配和成本。 无服务器架构： 设计无服务器的数据库架构，简化部署和管理，提升系统的灵活性和可用性。 跨云兼容性： 实现跨多个云平台的数据同步和互操作，提升数据的可迁移性和系统的容错性。 示例：\nAmazon DynamoDB是一个云原生NoSQL数据库，提供高性能、无服务器的数据库服务。通过自动分片和复制机制，DynamoDB能够在全球范围内快速扩展，满足大规模应用的需求。\n1 2 3 4 5 6 # 使用AWS CLI创建DynamoDB表 aws dynamodb create-table \\ --table-name Users \\ --attribute-definitions AttributeName=UserID,AttributeType=S \\ --key-schema AttributeName=UserID,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 实验与案例研究 通过实际的实验和案例研究，可以更好地理解NoSQL数据库的设计与应用，以及不同类型数据库的性能表现和适用场景。\nNoSQL数据库搭建与配置 在实际环境中，搭建和配置NoSQL数据库系统是理解其工作原理和性能的基础。以下是一个简化的实验步骤，以MongoDB为例：\n安装MongoDB： 根据官方文档，在不同的服务器节点上安装MongoDB。 配置集群： 设置副本集（Replica Set）或分片集群（Sharded Cluster），确保数据的高可用性和可扩展性。 创建数据库和集合： 根据业务需求，创建相应的数据库和集合，定义索引和数据结构。 实施数据分布策略： 配置水平分片，选择合适的分片键，以实现数据的均衡分布和高效访问。 监控与性能测试： 使用监控工具（如MongoDB Atlas、Prometheus）监控系统性能，进行负载测试和故障模拟，评估系统的表现和稳定性。 示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 启动MongoDB副本集 mongod --replSet rs0 --bind_ip localhost,\u0026lt;node1_ip\u0026gt;,\u0026lt;node2_ip\u0026gt;,\u0026lt;node3_ip\u0026gt; --port 27017 --dbpath /var/lib/mongodb --logpath /var/log/mongodb/mongod.log --fork # 初始化副本集 mongo --host \u0026lt;node1_ip\u0026gt;:27017 rs.initiate( { _id: \u0026#34;rs0\u0026#34;, members: [ { _id: 0, host: \u0026#34;\u0026lt;node1_ip\u0026gt;:27017\u0026#34; }, { _id: 1, host: \u0026#34;\u0026lt;node2_ip\u0026gt;:27017\u0026#34; }, { _id: 2, host: \u0026#34;\u0026lt;node3_ip\u0026gt;:27017\u0026#34; } ] } ) NoSQL数据库查询实践 NoSQL数据库的查询语言和操作方式与关系型数据库有所不同，理解其查询语法和优化方法对于高效使用NoSQL数据库至关重要。以下是MongoDB的一些常用查询操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // 插入文档 db.users.insertOne({ \u0026#34;user_id\u0026#34;: 1000, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;created_at\u0026#34;: new Date() }); // 查询文档 db.users.find({ \u0026#34;user_id\u0026#34;: 1000 }); // 更新文档 db.users.updateOne( { \u0026#34;user_id\u0026#34;: 1000 }, { $set: { \u0026#34;email\u0026#34;: \u0026#34;john.doe@newdomain.com\u0026#34; } } ); // 删除文档 db.users.deleteOne({ \u0026#34;user_id\u0026#34;: 1000 }); // 创建索引 db.users.createIndex({ \u0026#34;email\u0026#34;: 1 }); 数据库设计案例 通过分析实际的数据库设计案例，可以了解如何根据不同的业务需求，选择合适的NoSQL数据库类型和优化策略。\n案例：实时数据分析平台的NoSQL数据库设计\n一家科技公司需要构建一个实时数据分析平台，用于处理和分析来自数百万设备的实时数据流。为满足高吞吐量、低延迟和灵活的数据模型需求，团队选择了Apache Cassandra作为核心数据库。\n设计策略：\n数据分布策略：\n水平分割： 根据设备ID将数据分布到不同的Cassandra节点，确保负载均衡和高可扩展性。 列族设计： 为每种数据类型（如传感器数据、日志数据）创建独立的列族，优化数据的读写性能。 事务管理：\n轻量级事务： 利用Cassandra的轻量级事务（Lightweight Transactions）实现某些关键操作的一致性需求。 高可用性设计：\n多数据中心部署： 在不同地理区域部署Cassandra数据中心，确保数据的冗余和容错能力。 自动故障转移： 配置Cassandra的自动故障转移机制，确保在节点或数据中心故障时，系统能够快速恢复。 实施效果：\n通过上述设计，实时数据分析平台能够高效地处理数百万设备的实时数据流，确保系统的高可用性和数据的高一致性。Cassandra的高写入吞吐量和灵活的数据模型，使得平台能够快速响应业务需求和数据变化。\n前沿研究与发展 随着NoSQL数据库技术的不断进步，研究者和工程师们在提升系统性能、扩展性和一致性等方面进行了大量探索。以下是一些当前的前沿研究方向和发展趋势：\n自动化数据分布策略 **自动化数据分布策略（Automated Data Distribution Strategies）**旨在通过智能算法，根据数据访问模式和负载情况，自动调整数据的分布方式，优化系统性能和资源利用率。\n研究方向：\n智能分片算法： 开发能够自动识别和调整分片键的算法，优化数据分布和访问效率。 自适应负载均衡： 实现基于实时监控数据的动态负载均衡，确保系统在不同负载条件下的稳定性和高性能。 预测性数据分布： 利用机器学习模型预测数据访问模式，提前调整数据分布策略，提升系统的响应速度和效率。 示例：\n一种自动化分片算法通过分析历史数据访问日志，动态调整分片键和分片范围，确保热点数据均匀分布在各个节点上，避免单点过载，提高系统的整体性能。\n无服务器架构的NoSQL数据库 **无服务器架构（Serverless Architecture）**为NoSQL数据库提供了更高的灵活性和可扩展性，开发者无需关注底层基础设施的管理，专注于业务逻辑的实现。无服务器NoSQL数据库通过自动扩展和按需计费模式，优化了资源利用率和成本效益。\n研究方向：\n自动资源管理： 实现基于工作负载的自动扩展和资源分配，提升系统的灵活性和可用性。 高效的函数调用： 优化无服务器架构中的函数调用机制，减少延迟和响应时间。 安全性与隔离： 加强无服务器数据库的安全性和多租户隔离，确保数据的机密性和完整性。 示例：\nGoogle Cloud Firestore是一个无服务器的NoSQL文档数据库，提供高性能、自动扩展和实时同步功能。开发者可以通过简单的API接口，实现复杂的数据操作和实时应用的开发。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 在Firestore中添加文档 firebase.firestore().collection(\u0026#39;users\u0026#39;).add({ user_id: 1000, name: \u0026#39;John Doe\u0026#39;, email: \u0026#39;john.doe@example.com\u0026#39;, created_at: firebase.firestore.FieldValue.serverTimestamp() }); // 查询文档 firebase.firestore().collection(\u0026#39;users\u0026#39;).where(\u0026#39;user_id\u0026#39;, \u0026#39;==\u0026#39;, 1000).get() .then(querySnapshot =\u0026gt; { querySnapshot.forEach(doc =\u0026gt; { console.log(doc.id, \u0026#39; =\u0026gt; \u0026#39;, doc.data()); }); }); 分布式图数据库的优化 **分布式图数据库（Distributed Graph Databases）**专注于处理大规模的图数据，优化图的存储、查询和遍历性能。随着社交网络、知识图谱和推荐系统等应用的兴起，分布式图数据库的需求不断增长，研究者们致力于提升其性能和扩展性。\n研究方向：\n高效的图遍历算法： 开发能够在分布式环境下高效执行的图遍历和分析算法。 分布式图存储优化： 优化图数据的分布和存储方式，减少跨节点的数据访问和通信开销。 并行图处理： 实现图数据的并行处理和分析，提升大规模图数据的处理效率。 示例：\nAmazon Neptune是一个分布式图数据库，支持高效的图遍历和查询，适用于社交网络分析、知识图谱构建和推荐系统等应用。通过内置的图查询语言（如Gremlin和SPARQL），开发者可以轻松地进行复杂的图数据分析。\n1 2 // 使用Gremlin查询用户及其朋友 g.V().has(\u0026#39;name\u0026#39;, \u0026#39;John Doe\u0026#39;).out(\u0026#39;friends\u0026#39;).values(\u0026#39;name\u0026#39;) ","date":"2024-11-24T00:00:00Z","image":"https://echudet.github.io/p/intro-database/bg_hu_b6316529ba51dc15.jpg","permalink":"https://echudet.github.io/p/intro-database/","title":"Intro Database"},{"content":"在复习中挣扎抽出时间，写了两三周终于写完了，感觉难度上和mit6s081没差很远，日后有空整理完资料再发上来。\nTODO\n","date":"2024-11-21T15:26:00+08:00","permalink":"https://echudet.github.io/p/pa/","title":"Pa"},{"content":" 本人计网入门依靠的是科大的计网视频和《计算机网络-自顶向下方法》一书，课后选择了头歌的计网实训 （宣称是以《计算机网络-自顶向下方法》为蓝本的简单测试）和CS144的Lab 作为练习巩固知识。此路线可供参考。\n头歌的实训部分题目存在时间差异性，答案只能找讨论区。坑比较多，但是里面实训内容还是有意思的，不过难度上不难，基本照做就过了。\nCS144使用WSL似乎有坑，也可能只是我的是20.04版本导致的。在lab4和lab7整合的时候会出问题，要改WSL的网络配置。\n网络模型与架构 计算机网络的设计与实现依赖于一系列标准化的模型与架构，这些模型为不同层次的网络功能提供了结构化的框架，确保各层之间能够有效协作。两种最为广泛采用的网络模型是OSI七层模型和TCP/IP模型。理解这些模型不仅有助于掌握网络的基本原理，也为实际网络的设计、故障排除和优化提供了理论基础。\nOSI七层模型 开放系统互联（OSI）模型由国际标准化组织（ISO）在1984年提出，旨在标准化不同系统间的通信过程。OSI模型将网络通信过程划分为七个独立但相互关联的层次，每一层负责特定的功能，从而实现复杂网络系统的模块化设计。\n物理层：负责数据的物理传输，包括电缆、光纤等传输介质的选择及信号的编码和解码。其主要任务是将数据比特流通过物理介质传输到另一端。 数据链路层：确保数据在物理层上的可靠传输，处理帧的建立、错误检测与纠正，以及介质访问控制。常见的协议有以太网（Ethernet）和点对点协议（PPP）。 网络层：负责数据包的路由选择和转发，确保数据能够从源地址传输到目标地址。核心协议包括IP（Internet Protocol）和路由协议如OSPF、BGP。 传输层：提供端到端的通信服务，确保数据的完整性和可靠性。主要协议有传输控制协议（TCP）和用户数据报协议（UDP）。 会话层：管理用户会话，包括会话的建立、维护和终止，确保不同应用之间的通信能够有序进行。 表示层：负责数据的格式转换、加密与解密，确保不同系统之间能够正确理解和处理数据。例如，数据压缩和加密技术都在此层实现。 应用层：为用户提供网络服务和应用接口，包括HTTP、FTP、SMTP等协议，支持各种网络应用如浏览器、邮件客户端等。 OSI参考模型：\n1~4层 数据流层 第一层：物理层(Physical)，二进制传输；关键词：信号、介质。 第二层：数据链路层(Data Link)，介质访问；关键词：帧、媒介访问控制；提 供数据的可靠传输，与物理寻址、网络拓扑、网络接入、错误通知、帧的顺序传 送和流控制有关。 第三层：网络层(Network)，寻址和最优路径选择关键词：路径选择、路由、寻 3 址；在路由时提供连接和路径选择。 第四层：传输层(Transport)，终端对终端的连接；关键词：可靠性、流控制、 错误校正。 5~7层 应用层 第五层：会话层(Session)，内部宿主连接；关键词：会话；管理表现层实体间 的数据交换。 第六层：表现层(Presentation)，数据显示和加密；关键词：通用格式；与数据 结构和数据传输语法间的协商有关。 第七层：应用层(Application)，用户界面；关键词：浏览器；向用户程序提供 网络服务。 TCP/IP模型 TCP/IP模型是因特网的基础协议栈，由美国国防部在20世纪70年代开发，用于实现不同网络之间的互联和通信。与OSI模型相比，TCP/IP模型更加简洁，通常被划分为四个层次，每一层涵盖多个OSI层的功能。\n网络接口层：对应OSI模型的物理层和数据链路层，负责数据的实际传输，包括硬件设备的驱动和网络接口的管理。常见技术有以太网、Wi-Fi等。 互联网层：对应OSI的网络层，主要负责数据包的路由选择和转发，核心协议是IP（IPv4和IPv6）。此外，ICMP（Internet Control Message Protocol）和ARP（Address Resolution Protocol）也在此层工作。 传输层：与OSI的传输层功能相似，提供端到端的通信服务。主要协议包括TCP和UDP，分别用于需要可靠传输的应用和对速度要求更高但可以容忍一定丢包的应用。 应用层：涵盖OSI的会话层、表示层和应用层，提供各种网络应用服务。常见协议包括HTTP、FTP、SMTP、DNS等，支持多种网络应用和服务。 TCP/IP模型：\n网络接口层: 主要是指物理层次的一些接口,比如电缆等． 网络层: 从任何互联网上的网络发送源包，不依靠路径和网络让它们到达目的地 最好路径选择和包交换发生在这层 传输层: 处理服务质量事务，如可靠性、流控制和误差校正 应用层: 处理高等级协议、辅助事务、加密和会话控制 OSI模型与TCP/IP模型的比较 尽管OSI模型和TCP/IP模型在层次划分和具体实现上存在差异，但两者都旨在提供一个结构化的框架，以促进不同系统间的互操作性。OSI模型更加理论化和细致，提供了七个明确的层次，适合作为教学和理解网络通信原理的工具。而TCP/IP模型则更加实用和简洁，直接对应于因特网的实际实现，广泛应用于实际网络的设计与部署。\n在实际应用中，TCP/IP模型由于其简洁性和广泛的支持，成为互联网的主流协议栈。然而，OSI模型的层次划分仍然在网络教育和某些专业领域中具有重要参考价值。所以本文是以OSI模型的视角来引入的。\n物理层I 物理层是计算机网络模型中的最底层，负责数据在物理媒介上的实际传输。它定义了硬件设备、传输介质、信号形式以及数据编码方式等关键要素，确保比特流能够从发送端可靠地传输到接收端。理解物理层的实现细节对于设计高效、稳定的网络至关重要。物理层的主要职责包括：\n比特传输：将数据分解为比特，并通过物理媒介传输这些比特。 信号编码：将数字信号转换为适合传输媒介的形式，如电信号、光信号或无线电波。 传输速率：定义比特传输的速度，即数据传输速率，通常以比特每秒（bps）表示。 物理拓扑：确定网络中设备的物理连接方式，如星型、总线型或环型拓扑。 接口标准：规定设备之间的物理连接接口，如电缆类型、连接器形状等。 传输介质的具体实现 物理层支持多种传输介质，每种介质都有其独特的特性和适用场景。\n双绞线\n双绞线是最常用的有线传输介质之一，广泛应用于局域网（LAN）中。它由两根绝缘铜线紧密绞合而成，以减少电磁干扰和串扰。\n类别： Cat5e：支持高达100 Mbps的传输速率，适用于标准以太网。 Cat6：支持高达1 Gbps甚至10 Gbps的传输速率，适用于需要更高带宽的网络。 Cat6a和Cat7：进一步提高传输速率和抗干扰能力，适用于高性能网络环境。 实现细节： 屏蔽与非屏蔽：屏蔽双绞线（STP）在每对线外增加屏蔽层，提供更好的抗干扰能力，适用于高噪声环境；非屏蔽双绞线（UTP）成本较低，适用于大多数家庭和办公环境。 连接器：使用RJ45连接器，通过插头将双绞线连接到网络设备的端口。 同轴电缆\n同轴电缆由一根中心导体、绝缘层、编织屏蔽层和外护套组成，具有较高的带宽和抗干扰能力。\n应用： 早期用于以太网（10BASE-2和10BASE-5）。 现主要用于有线电视和互联网接入。 实现细节： 阻抗匹配：典型的同轴电缆阻抗为50Ω或75Ω，需与网络设备匹配以避免信号反射。 连接方式：使用BNC连接器，通过压接或螺纹固定确保可靠连接。 光纤\n光纤利用光信号进行数据传输，提供极高的带宽和传输距离，几乎不受电磁干扰影响。\n类型： 单模光纤（Single-Mode Fiber, SMF）：适用于长距离传输，核心直径约为8-10微米，使用激光作为光源。 多模光纤（Multi-Mode Fiber, MMF）：适用于短距离传输，核心直径约为50-62.5微米，使用LED作为光源。 实现细节： 连接器：常用的光纤连接器包括ST、SC、LC等，每种连接器具有不同的插入机制和兼容性。 光源与检测器：使用激光器或LED作为光源，光电二极管或雪崩光电二极管（APD）作为检测器，将光信号转换为电信号。 波分复用（WDM）：通过在不同波长上传输多路光信号，提高光纤的带宽利用率。 无线传输\n无线传输通过电磁波在空气中传递数据，提供灵活的连接方式，适用于无法布线或移动性要求高的场景。\n技术： Wi-Fi：基于IEEE 802.11标准，广泛用于家庭、办公和公共场所的无线局域网。 蓝牙：适用于短距离设备间通信，如无线耳机、键盘等。 5G/6G：第五代和第六代移动通信技术，提供高速、大容量和低延迟的广域无线连接。 实现细节： 频段与信道：Wi-Fi常用的频段包括2.4 GHz和5 GHz，每个频段分为多个信道，以减少干扰。 调制技术：使用正交频分复用（OFDM）、相位调制（如QAM）等技术，提高频谱利用率和抗干扰能力。 天线设计：包括单天线、MIMO（多输入多输出）等技术，通过多天线提高传输速率和覆盖范围。 信号与编码 物理层负责将数据比特转换为适合传输介质的信号形式，并通过适当的编码技术提高传输效率和可靠性。\n模拟信号与数字信号\n模拟信号：连续变化的信号，适用于长距离传输，如无线电广播。 数字信号：离散的比特流，适用于计算机网络，具有更高的抗干扰能力和可靠性。 编码方法\n编码技术在物理层中至关重要，影响数据传输的效率和可靠性。常见的编码方法包括：\n曼彻斯特编码（Manchester Encoding）： 原理：每个比特周期内信号中间有一次电平翻转，0和1通过不同的电平变化表示。 优点：自同步，易于时钟恢复，抗噪性能好。 应用：以太网（10BASE-T）。 非归零编码（Non-Return to Zero, NRZ）： 原理：使用高电平表示1，低电平表示0，信号在比特周期内保持不变。 优点：简单，带宽效率高。 缺点：长时间相同电平可能导致时钟恢复困难，易受直流偏移影响。 差分曼彻斯特编码（Differential Manchester Encoding）： 原理：信号在每个比特周期的中间有一次电平翻转，开始时是否翻转表示比特值。 优点：增强了抗干扰能力，减少了直流分量。 应用：令牌环网络（Token Ring）。 4B/5B编码： 原理：将4位数据编码为5位码，以确保信号中有足够的电平变化。 优点：提高了传输的可靠性，便于时钟恢复。 应用：光纤通道（Fiber Channel）。 传输速率与带宽 传输速率和带宽是衡量物理层性能的关键指标。\n传输速率：指数据传输的速度，通常以比特每秒（bps）表示。现代网络的传输速率从几十kbps到数百Gbps不等。 带宽：指传输介质在特定频率范围内能够传输的最大数据量，通常以Hz表示。带宽越大，理论上支持的传输速率越高。 影响因素\n介质类型：光纤的带宽远高于铜缆，适合高速长距离传输。 信号衰减与失真：长距离传输中信号衰减和失真会限制有效传输距离和速率，需要中继器或放大器来补偿。 干扰与噪声：电磁干扰（EMI）、串扰等因素会影响信号质量，需采用屏蔽技术和差分信号传输来减少影响。 调制与编码技术：先进的调制和编码技术能够提高信号的抗干扰能力和频谱利用率，从而提升传输速率。 物理拓扑结构的实现 物理拓扑描述了网络设备在物理空间中的连接方式，不同的拓扑结构适用于不同的应用场景和需求。\n星型拓扑\n在星型拓扑中，所有网络设备通过独立的连接线缆连接到一个中央节点（如集线器、交换机或路由器）。这种拓扑结构具有以下特点：\n优点： 易于管理和扩展，添加或移除设备不会影响其他设备。 中央节点的集中管理简化了网络配置和故障排除。 单个设备故障不会影响整个网络，只会中断该设备的连接。 缺点： 中央节点是单点故障，一旦中央节点失效，整个网络将瘫痪。 需要更多的电缆，增加了布线成本。 实现细节： 中央节点选择：现代网络通常使用交换机作为中央节点，提供更高的性能和智能化管理。 布线规范：遵循标准布线规范（如TIA/EIA-568），确保信号的可靠传输和兼容性。 环型拓扑\n环型拓扑中，所有网络设备通过环形结构连接，每个设备仅与两个邻近设备直接通信。数据在环中按照单一方向传输。\n优点： 每个设备有固定的传输路径，避免数据冲突，提高传输效率。 网络结构简单，适合中小规模网络。 缺点： 任意一个设备或连接的故障都会中断整个网络。 扩展和维护较为复杂，需重新配置环路。 实现细节： 令牌环协议：使用令牌控制访问，确保数据传输的有序性和无冲突性。 故障检测与恢复：采用冗余环路和自动切换技术（如FDDI）提高网络的可靠性。 总线型拓扑\n总线型拓扑将所有网络设备连接到同一根主干线缆（总线），数据在总线上双向传输。\n优点： 布线简单，成本低廉，适用于小规模网络。 易于部署和扩展，只需在总线上添加设备即可。 缺点： 总线成为共享带宽的瓶颈，数据传输效率低下。 总线上的任何故障（如断裂）都会影响整个网络。 不适合高密度或高流量的网络环境。 实现细节： 终端设备：在总线的两端安装终端电阻，防止信号反射。 信号传输：使用逻辑信号传输和碰撞检测（如CSMA/CD）来管理数据访问。 网状拓扑\n网状拓扑中，每个网络设备与多个其他设备直接连接，形成多条冗余路径。分为全网状和部分网状两种形式。\n优点： 高度冗余和容错能力，单个连接或设备故障不会影响整个网络的运行。 提供多个数据传输路径，提高网络的可靠性和性能。 缺点： 布线复杂，成本高昂，特别是在大规模网络中。 管理和维护较为困难，需要先进的路由协议和网络管理工具。 实现细节： 路由协议：使用动态路由协议（如OSPF、BGP）来管理数据在网状网络中的传输路径。 网络设备：采用高性能路由器和交换机，支持多路径路由和负载均衡。 除此之外，还有一些扩补结构未提及，如双环拓补、树型拓补、完全拓补、蜂窝拓补等。\n接口标准与设备 物理层的实现依赖于各种接口标准和设备，这些标准确保不同厂商和类型的设备能够兼容和互操作。\n网络接口卡（NIC）\n网络接口卡是计算机与网络之间的物理接口，负责数据的发送和接收。\n类型： 有线NIC：支持以太网，通常配备RJ45端口，用于连接双绞线或同轴电缆。 无线NIC：支持Wi-Fi或其他无线标准，通过天线进行无线通信。 实现细节： 驱动程序：操作系统需要相应的驱动程序来控制NIC的功能和性能。 MAC地址：每个NIC拥有唯一的媒体访问控制（MAC）地址，用于网络层的数据帧识别。 交换机与路由器\n交换机和路由器在物理层与数据链路层之间起到桥梁作用，负责数据的转发和路由选择。\n交换机： 功能：基于MAC地址转发数据帧，实现局域网内部的高效通信。 实现细节：高端交换机支持VLAN（虚拟局域网）、QoS（服务质量）等高级功能，以优化网络性能和管理。 路由器： 功能：基于IP地址进行数据包的路由选择，实现不同网络之间的通信。 实现细节：路由器内置路由表，使用动态路由协议（如OSPF、BGP）自动更新路由信息，支持NAT（网络地址转换）和防火墙功能，增强网络安全性。 电气特性与信号完整性 确保信号在传输过程中的完整性和稳定性是物理层的重要任务，涉及电气特性和信号完整性的管理。\n阻抗匹配\n阻抗匹配是指传输线路和设备的阻抗相等，避免信号反射和失真。实现细节：\n使用相同阻抗的传输介质和连接器，如75Ω同轴电缆用于视频传输，50Ω用于数据通信。 在连接点安装阻抗匹配器或终端电阻，确保信号在传输介质上的连续性。 信号衰减与放大\n信号在传输过程中会逐渐衰减，影响接收端的信号质量。实现细节：\n中继器：物理层设备，用于重新生成和放大信号，延长传输距离。 放大器：在长距离传输中使用光纤放大器（如EDFA）或电信号放大器，确保信号强度足够。 信号完整性\n信号完整性指的是信号在传输过程中保持其原有形态，避免失真和误码。实现细节：\n差分信号传输：通过传输两条反向信号线，减少电磁干扰，提高信号的抗噪能力。 抖动控制：使用时钟恢复和抖动滤波技术，减少信号的时序误差，确保数据准确接收。 屏蔽与接地：采用屏蔽电缆和良好的接地设计，防止外部电磁干扰影响信号质量。 传输介质的选择与应用 根据具体应用场景和需求，选择合适的传输介质对网络性能和可靠性具有重要影响。\n家庭网络： 常用双绞线（Cat5e或Cat6），经济实惠，布线方便。 无线传输（Wi-Fi）提供灵活的设备连接，但受距离和干扰限制。 企业网络： 光纤用于骨干网络和长距离连接，提供高带宽和低延迟。 双绞线用于办公区域的接入层，支持高速以太网连接。 无线接入点（AP）覆盖办公区，满足移动设备的连接需求。 数据中心： 使用高速光纤（如InfiniBand、10G/40G/100G Ethernet）连接服务器和存储设备，确保高吞吐量和低延迟。 双绞线用于设备间的短距离连接，提供灵活的布线选项。 广域网络（WAN）： 光纤广泛应用于城域网和骨干网，支持高速数据传输和大规模用户连接。 无线传输（如微波、卫星通信）用于远程和难以布线的区域，提供覆盖广泛的连接解决方案。 物理层技术不断发展，以满足日益增长的网络需求和性能要求。\n高速光纤技术：随着数据需求的激增，光纤传输速率不断提升，从10G到100G甚至更高，支持大规模数据中心和云计算的高速连接。 新型编码与调制技术：采用更高效的编码和调制方法，如正交振幅调制（QAM）和相位调制，提高信号的传输效率和抗干扰能力。 集成光电子技术：将光电子器件集成到单一芯片上，减少设备体积和功耗，提升传输速度和可靠性。 无线技术的进化：发展更高频段（如毫米波、太赫兹波）和更先进的天线技术（如MIMO、Massive MIMO），提升无线传输的带宽和覆盖范围。 能源效率与环保：优化物理层设备的能耗，采用低功耗设计和绿色材料，推动网络技术的可持续发展。 物理层作为计算机网络的基础，负责将数据从抽象的比特流转化为实际的物理信号，并通过各种传输介质传递到目标设备。\n数据链路层II 数据链路层是计算机网络模型中的第二层，位于物理层之上，负责在相邻网络节点之间可靠地传输数据帧。它不仅确保数据的无误传输，还管理媒体访问控制、流量控制以及错误检测与纠正等关键功能。数据链路层的有效运作对于整个网络的稳定性和性能至关重要，特别是在多节点环境中，它起到了桥梁和协调者的作用。\n数据链路层的核心功能 数据链路层的主要职责包括将网络层传输下来的数据包封装成帧（Frame），并在物理层上传输这些帧。同时，它还负责帧的接收和解封装，将其传递给网络层。为了确保数据的准确性和高效性，数据链路层实现了多种关键功能：\n帧的封装与解封装：数据链路层将来自网络层的数据包封装成帧，添加必要的头部和尾部信息，以便在物理介质上传输。接收端的数据链路层则负责将接收到的帧进行解封装，提取出原始的数据包。 媒体访问控制（MAC）：在共享传输介质（如以太网）中，数据链路层负责管理多个设备对同一介质的访问，避免数据冲突和碰撞。常见的MAC协议包括载波监听多路访问/碰撞检测（CSMA/CD）和载波监听多路访问/碰撞避免（CSMA/CA）。 错误检测与纠正：由于物理传输过程中的噪声和干扰可能导致数据错误，数据链路层通过添加校验码（如循环冗余检验CRC）来检测传输错误，并采取相应的纠正措施，如重传机制，确保数据的完整性。 流量控制：为了防止发送端过快地发送数据，导致接收端处理不过来，数据链路层实现了流量控制机制，协调发送和接收的速率，保持数据传输的平稳和高效。 帧排序与同步：在数据传输过程中，帧可能会出现乱序或丢失的情况，数据链路层负责对接收到的帧进行排序和同步，确保数据的有序和完整传递。 媒体访问控制（MAC） 在多节点共享传输介质的环境中，如何有效地管理设备对介质的访问成为数据链路层的重要任务。媒体访问控制（MAC）协议通过协调设备的发送行为，减少数据冲突和碰撞，提高网络的整体性能。\n载波监听多路访问/碰撞检测（CSMA/CD） 是以太网中常用的MAC协议。当一个设备准备发送数据时，它首先监听传输介质，确保没有其他设备正在发送。如果介质空闲，设备便开始传输数据。然而，在数据传输过程中，可能会有多个设备同时发送，导致信号碰撞。CSMA/CD协议要求设备在检测到碰撞后立即停止发送，并等待一个随机的时间后重新尝试发送，这样可以有效地减少碰撞的频率和影响。\n载波监听多路访问/碰撞避免（CSMA/CA） 则主要应用于无线网络，如Wi-Fi。由于无线环境中设备无法直接检测到碰撞，CSMA/CA通过在发送数据前先发送请求发送（RTS）和清除发送（CTS）信号，来避免多个设备同时发送数据，从而减少碰撞的发生。\n数据帧结构 数据链路层的数据单元称为帧（Frame），其结构通常包括头部、载荷和尾部三个部分，每一部分承担不同的功能：\n头部（Header）：包含控制信息，如源MAC地址、目的MAC地址、帧类型等。这些信息用于识别帧的发送者和接收者，以及帧所承载的数据类型。 载荷（Payload）：承载实际的数据内容，通常是来自网络层的数据包。载荷的大小根据具体协议和网络需求而有所不同。 尾部（Trailer）：主要用于错误检测，通常包含循环冗余检验（CRC）码。接收端通过计算接收到的数据帧的CRC码，与尾部的CRC码进行比对，以检测数据传输中的错误。 错误检测与纠正 在数据传输过程中，噪声和干扰可能导致数据错误，数据链路层通过多种方法检测和纠正这些错误，确保数据的准确性和完整性。\n循环冗余检验（CRC） 是一种常用的错误检测技术，通过对帧中的数据进行多项式除法运算，生成一个校验码并附加在帧的尾部。接收端使用相同的算法计算接收到的数据的CRC码，并与尾部的CRC码进行比较。如果两者不一致，说明数据在传输过程中出现了错误，接收端可以请求重传该帧。\n奇偶校验 是另一种较为简单的错误检测方法，通过在数据帧中添加一个比特来表示数据中1的个数是奇数还是偶数。虽然实现简单，但只能检测出单个比特错误，且无法纠正错误，因此在现代网络中应用较少。\n在某些高可靠性需求的网络中，数据链路层还可能实现错误纠正机制，如前向纠错（FEC），通过冗余数据来自动纠正一定范围内的错误，而无需重传。这在实时性要求高的应用中尤为重要，如视频会议和在线游戏。\n数据链路层协议 数据链路层的协议定义了帧的格式、传输方式以及错误处理机制等，确保不同设备之间能够有效地通信。以下是几种常见的数据链路层协议：\n以太网（Ethernet） 是最广泛使用的局域网（LAN）技术，支持多种传输介质，如双绞线、同轴电缆和光纤。以太网帧的结构包括前导码、目的MAC地址、源MAC地址、类型字段、载荷和CRC校验码。以太网支持全双工和半双工通信，并通过交换机技术大幅提高网络的性能和可靠性。\n点对点协议（PPP） 主要用于串行连接，如拨号上网和点对点广域网（WAN）连接。PPP提供了多种功能，包括链路建立、认证、加密和压缩等。PPP帧结构简单，包含标志字段、地址字段、控制字段、协议字段、载荷和FCS（帧校验序列）。\n高级数据链路控制（HDLC） 是一种面向比特的同步数据链路层协议，广泛应用于广域网（WAN）中。HDLC支持点对点和点对多点的通信模式，提供帧定界、流量控制和错误检测等功能。HDLC帧结构包括起始标志、地址字段、控制字段、信息字段和帧校验序列（FCS）。\n链路层安全 数据链路层在网络安全中也扮演着重要角色，通过提供链路级别的安全机制，保护数据在传输过程中的机密性和完整性。\n链路层加密 技术通过在发送端对数据帧进行加密，确保即使数据在传输过程中被截获，未经授权的用户也无法解读数据内容。这在无线网络中尤为重要，防止数据被窃听和篡改。常见的链路层加密协议包括WPA2（用于Wi-Fi）和MACsec（用于以太网）。\n链路层认证 通过验证设备的身份，确保只有经过授权的设备能够加入网络，防止非法设备接入。以太网中的IEEE 802.1X协议是一种常用的链路层认证机制，通过基于端口的网络访问控制（PNAC）实现设备的身份验证和授权。\n流量控制与拥塞管理 数据链路层不仅需要管理设备对传输介质的访问，还需处理数据流量的控制，以防止网络拥塞和数据丢失。\n流量控制 机制通过协调发送端和接收端的数据传输速率，确保接收端不会因为处理不过来而导致数据丢失。例如，滑动窗口协议允许发送端根据接收端的缓冲能力调整发送数据的数量，从而实现动态的流量控制。\n拥塞管理 则主要针对整个网络的流量负载，通过监测和控制数据流量，避免网络因过载而导致性能下降。数据链路层通过优先级队列、带宽管理和流量整形等技术，动态调整数据传输策略，确保关键应用的优先级和网络资源的合理分配。\n数据链路层的实现细节 实现数据链路层功能需要依赖硬件设备和软件协议的协同工作。以下是一些关键的实现细节：\n网络接口卡（NIC） 是数据链路层的核心硬件设备，负责将数据从计算机转换为适合传输介质的信号，并接收来自介质的信号并转换回数据。现代NIC支持多种功能，如硬件级的CRC校验、帧过滤和流量控制，提升数据链路层的性能和效率。\n交换机和桥接设备 在数据链路层起到了连接不同网络段的作用。交换机通过学习和维护MAC地址表，实现高效的数据帧转发，避免广播风暴和数据冲突。桥接设备则用于连接不同的局域网，分隔广播域，提升网络的可扩展性和性能。\n协议栈实现 数据链路层协议的实现通常依赖于操作系统中的协议栈，提供API（应用程序接口）供上层应用使用。协议栈需要高效地处理帧的封装、发送、接收和错误处理等任务，同时与物理层和网络层紧密协作，确保数据的顺利传输。\n性能优化与挑战 数据链路层在实际应用中面临诸多挑战和性能优化的需求。\n延迟与吞吐量 数据链路层需要在保证可靠性的同时，尽量减少数据传输的延迟，提升网络的吞吐量。采用高效的帧处理算法、优化流量控制机制和提升硬件设备的处理能力，是提升数据链路层性能的关键。\n抗干扰能力 尤其是在无线网络环境中，数据链路层需要应对复杂的干扰和信号衰减问题。通过采用先进的调制技术、多天线技术（如MIMO）和智能干扰管理策略，可以显著提升链路的稳定性和抗干扰能力。\n安全性 数据链路层的安全性直接影响整个网络的安全性。随着网络攻击手段的不断升级，数据链路层需要不断更新和强化安全机制，防止数据窃听、篡改和非法访问。\n兼容性与互操作性 随着不同厂商和技术的多样化，数据链路层协议需要保持高度的兼容性和互操作性，确保不同设备和网络能够无缝协同工作。这要求协议设计具备良好的扩展性和灵活性，以适应不断变化的网络需求和技术发展。\n网络层III 网络层是计算机网络模型中的第三层，位于数据链路层和传输层之间，负责在不同网络之间进行数据包的传输和路由选择。它的主要职责是确保数据能够从源设备顺利传输到目标设备，无论它们位于何处。网络层通过逻辑地址（如IP地址）对数据包进行寻址和路由，管理网络间的通信，并处理数据包的分段和重组。理解网络层的工作原理和相关协议对于设计高效、可扩展和可靠的网络至关重要。\n网络层的核心功能 网络层的核心功能包括：\n逻辑寻址与地址分配：网络层为每个设备分配唯一的逻辑地址（如IP地址），确保数据包能够准确地找到目标设备。 路由选择与转发：确定数据包从源到目标的最佳路径，并将数据包在各个网络节点之间转发。 数据包分段与重组：将大数据包分割成适合传输的小数据包，并在目标设备处重新组合。 拥塞控制与流量管理：监控网络流量，避免网络拥堵，提高整体网络性能。 错误处理与诊断：检测和报告传输过程中的错误，帮助网络管理员进行故障排除。 主要协议及其原理 IP协议（Internet Protocol）\nIP协议是网络层的核心协议，负责在不同网络之间传输数据包。它有两个主要版本：IPv4和IPv6。\nIPv4（第四版互联网协议） 是当前最广泛使用的IP协议版本，使用32位地址空间，支持约43亿个唯一地址。IPv4数据包结构包括：\n版本号：标识协议版本（IPv4）。 头部长度：指定IP头部的长度。 服务类型：定义数据包的优先级和服务质量。 总长度：数据包的总长度（头部加载荷）。 标识、标志、片偏移：用于数据包分段和重组。 生存时间（TTL）：限制数据包在网络中的寿命，防止无限循环。 协议：指示数据包的上层协议（如TCP、UDP）。 源地址与目标地址：标识数据包的发送者和接收者。 头部校验和：用于检测头部中的错误。 选项：可选字段，用于特定功能。 IPv6（第六版互联网协议） 作为IPv4的继任者，采用128位地址空间，提供了几乎无限的地址数量，并引入了许多改进，如简化的头部结构、内置的安全功能（IPsec）和更有效的路由支持。IPv6的数据包结构包括：\n版本号：标识协议版本（IPv6）。 流量类别：类似IPv4的服务类型，定义优先级和服务质量。 流标签：用于标识特定的数据流。 有效载荷长度：数据包的载荷部分长度。 下一个头部：指示下层协议（如TCP、UDP）。 跳数限制：类似IPv4的TTL，用于限制数据包的生命周期。 源地址与目标地址：128位的IPv6地址，标识发送者和接收者。 扩展头部：用于额外功能，如安全和路由。 IP协议的路由选择 是基于目标地址的层次化结构。路由器使用路由表，根据最长前缀匹配原则选择最合适的路由路径，将数据包转发到下一跳，直至到达目标网络。\nICMP协议（Internet Control Message Protocol）\nICMP是网络层的一个辅助协议，主要用于传输控制消息和错误报告。它不用于数据的实际传输，而是用于反馈和诊断网络问题。常见的ICMP消息类型包括：\n回显请求与回显应答（Ping）：用于测试目标主机的连通性和响应时间。 目的不可达：报告数据包无法到达目标网络或主机的原因，如网络不可达、主机不可达、协议不可达等。 时间超过：当数据包的TTL值降至零时，发送此消息，防止数据包在网络中无限循环。 重定向：通知源主机使用更合适的路由路径发送数据包。 ICMP通过在IP头部中指定协议类型字段实现与IP协议的集成，确保控制消息能够正确地传输和处理。\nARP与RARP协议（Address Resolution Protocol \u0026amp; Reverse Address Resolution Protocol）\nARP（地址解析协议） 用于将网络层的IP地址转换为链路层的MAC地址。在一个局域网内，发送端需要知道目标设备的MAC地址才能正确地发送数据帧。ARP工作流程如下：\nARP请求：发送端广播一个ARP请求，询问谁拥有目标IP地址。 ARP响应：拥有该IP地址的设备回复其MAC地址。 缓存记录：发送端将IP-MAC映射关系缓存，以便后续通信快速访问。 RARP（逆地址解析协议） 的功能与ARP相反，用于将MAC地址转换为IP地址。它主要用于磁盘启动的无盘工作站，帮助设备在启动时获取其IP地址。\n路由协议（Routing Protocols）\n路由协议在网络层中起着关键作用，负责动态地维护路由表，确保数据包能够找到最佳路径。主要的路由协议包括：\nRIP（Routing Information Protocol） 是一种基于距离向量的内部网关协议，使用跳数作为度量标准，最大跳数限制为15跳。RIP的优点是简单易用，但由于其跳数限制和收敛速度较慢，适用于小型或中型网络。\nOSPF（Open Shortest Path First） 是一种基于链路状态的内部网关协议，使用Dijkstra算法计算最短路径。OSPF支持分区域路由，提高了大规模网络的可扩展性和管理效率。它能够快速收敛，适用于复杂和动态变化的网络环境。\nBGP（Border Gateway Protocol） 是一种基于路径向量的外部网关协议，用于在不同自治系统（AS）之间交换路由信息。BGP是互联网的核心协议，支持政策路由和多路径选择，能够处理大规模和多样化的网络结构。\nIP地址分配与子网划分 IP地址的分配和子网划分是网络层设计中的重要方面，影响网络的可扩展性、安全性和效率。\nIP地址分配 通常由区域性互联网注册管理机构（RIR）负责，按需分配给各个互联网服务提供商（ISP）和大型组织。IP地址可以通过静态分配或动态分配（如DHCP）进行管理。\n子网划分（Subnetting） 通过将一个大型网络分割为多个较小的子网，提高了地址空间的利用率和网络的管理性。子网划分使用子网掩码（Subnet Mask）来确定网络地址和主机地址的范围。例如，一个典型的IPv4地址使用32位，子网掩码如255.255.255.0表示前24位为网络地址，后8位为主机地址。\nCIDR（Classless Inter-Domain Routing） 是一种更灵活的IP地址分配和路由选择方法，通过可变长度子网掩码（VLSM）实现更高效的地址空间利用和路由表的缩减。CIDR采用“/”符号表示网络前缀长度，如192.168.1.0/24，表示前24位为网络地址。\n网络层安全 网络层的安全性至关重要，涉及保护数据包的完整性、保密性和可用性。\nIPsec（Internet Protocol Security） 是一种在网络层提供安全通信的协议套件，通过加密和认证保护IP数据包。IPsec支持两种主要模式：\n传输模式：仅加密数据部分，适用于点对点通信。 隧道模式：加密整个IP包，适用于虚拟专用网（VPN）连接。 防火墙（Firewall） 在网络层进行数据包过滤，基于IP地址、端口号和协议类型控制数据流动，防止未授权访问和网络攻击。\n路由器安全 通过配置访问控制列表（ACL）和使用安全路由协议，防止恶意路由信息的传播和网络设备的攻击。\n数据包分段与重组 当数据包超过传输介质的最大传输单元（MTU）时，网络层需要将其分段为更小的片段，以适应传输要求。分段过程涉及：\n分段：源设备根据目标网络的MTU，将数据包拆分为多个片段，每个片段包含部分数据和必要的分段信息（如分段偏移量）。 传输：各个片段独立传输，通过不同路径到达目标设备。 重组：目标设备根据分段偏移量和标识信息，将片段重新组合成完整的数据包，确保数据的完整性。 分段与重组提高了网络的灵活性和适应性，但也增加了处理开销和延迟。因此，合理配置MTU和优化分段策略是网络性能优化的重要方面。\n拥塞控制与流量管理 网络拥塞是指数据流量超过网络处理能力，导致延迟增加和数据包丢失。网络层通过多种机制进行拥塞控制和流量管理，以确保网络的稳定性和高效性。\n拥塞避免算法 如随机早期检测（RED）和加权随机早期检测（WRED），通过监控网络队列长度，提前丢弃部分数据包，防止拥塞的进一步恶化。\n流量整形（Traffic Shaping） 通过限制数据传输速率，平滑流量峰值，减少网络负载波动，提升整体网络性能。\n负载均衡 技术通过分配数据包到多个路由路径，避免单一路径的过载，提高网络的吞吐量和可靠性。\n路由选择算法 网络层的路由选择算法决定了数据包在网络中的传输路径，影响网络的效率和可靠性。\n距离向量算法（如RIP） 基于跳数或其他简单度量标准，通过邻居间的信息交换，逐步确定最佳路径。其优点是实现简单，但存在收敛速度慢和路由环路的问题。\n链路状态算法（如OSPF） 每个路由器维护整个网络的拓扑图，通过分发链路状态信息，使用Dijkstra算法计算最短路径。链路状态算法具有更快的收敛速度和更高的灵活性，适用于大型和复杂的网络。\n路径向量算法（如BGP） 通过维护自治系统（AS）之间的路径信息，实现跨域路由选择。路径向量算法支持策略路由和路径过滤，适用于互联网的全球路由需求。\n动态路由与静态路由 静态路由 由网络管理员手动配置，适用于小型或稳定的网络环境。静态路由的优点是简单、开销低，但缺乏灵活性和自动适应性，不适用于动态变化的网络环境。\n动态路由 通过路由协议自动发现和维护路由信息，适应网络拓扑的变化。动态路由具有高度的灵活性和可扩展性，但需要更多的网络资源和协议管理。\n多路径路由与负载均衡 在复杂的网络环境中，多路径路由技术允许数据包通过多个路径传输，提高网络的冗余性和可靠性。通过负载均衡策略，数据流量可以均匀分布到不同的路径，避免单一路径的过载，提升整体网络性能和稳定性。\nECMP（Equal-Cost Multi-Path） 是一种常用的多路径路由策略，允许多个具有相同成本的路径同时使用，通过哈希算法将数据包分配到不同的路径，实现负载均衡。\n网络层的发展方向 随着互联网的不断发展和技术的进步，网络层也在不断演化，以满足更高的性能、安全性和可扩展性需求。\n软件定义网络（SDN） 将路由决策从传统的网络设备转移到集中式控制器，提供更灵活和可编程的网络管理方式，简化网络配置和优化路由选择。\n网络功能虚拟化（NFV） 通过将网络功能（如路由、负载均衡、防火墙等）虚拟化，实现网络功能的动态部署和扩展，提升网络的灵活性和资源利用率。\nIPv6的全面部署 随着IPv4地址枯竭，IPv6的广泛应用将成为未来网络发展的主流，提供更大的地址空间和更高效的路由机制，支持物联网（IoT）和移动互联网的快速增长。\n量子路由与量子网络 研究量子通信和量子路由技术，探索在未来量子计算和量子通信时代，网络层的新功能和新协议，提升网络的安全性和传输效率。\n传输层IV 传输层是计算机网络模型中的第四层，位于网络层之上，负责为应用层提供端到端的通信服务。它的主要任务是确保数据在源主机与目标主机之间可靠、高效地传输。传输层不仅处理数据的传输，还涉及数据的分段与重组、流量控制、错误检测与恢复，以及拥塞控制等关键功能。理解传输层的工作原理和其核心协议，对于设计和维护稳定、可靠的网络通信至关重要。\n传输层的核心功能 传输层的核心职责是为应用层提供透明的数据传输服务，无论底层网络的拓扑结构如何复杂。它通过逻辑上的通信端口，将不同应用程序的数据流进行区分和管理，确保数据能够正确、有序地到达目标应用。传输层实现了端到端的通信，负责数据的分段与重组，使得大块数据可以通过网络有效传输。同时，传输层还管理数据流量，防止接收端因处理不过来而导致数据丢失，并通过错误检测与恢复机制，确保数据传输的可靠性。此外，传输层还实施拥塞控制，防止网络过载，保障整体网络的性能和稳定性。\n传输协议的类型与特点 传输层主要有两种核心协议：传输控制协议（TCP）和用户数据报协议（UDP）。这两种协议在设计目标、功能实现和应用场景上各有不同，分别适用于不同类型的网络通信需求。\n传输控制协议（TCP） 是一种面向连接的协议，提供可靠的、按序的、无差错的数据传输服务。TCP通过建立连接（三次握手过程）确保通信双方准备好进行数据交换。在数据传输过程中，TCP使用序列号和确认应答机制，确保每个数据包都被正确接收。如果发送端未收到某个数据包的确认应答，会自动重传该数据包。此外，TCP还实现了流量控制，通过窗口机制动态调整数据传输速率，防止接收端因处理能力不足而导致的数据丢失。拥塞控制方面，TCP采用慢启动、拥塞避免、快速重传和快速恢复等算法，动态调整传输速率，适应网络负载变化，避免网络拥堵。由于这些机制，TCP适用于需要高可靠性和数据完整性的应用，如网页浏览、文件传输、电子邮件等。\n用户数据报协议（UDP） 则是一种无连接的协议，提供简单、低开销的传输服务。与TCP不同，UDP不建立连接，也不保证数据的可靠传输和顺序。它通过数据报的方式进行传输，每个UDP数据报独立处理，既不进行错误恢复，也不实现流量控制和拥塞控制。这使得UDP在传输效率上具有优势，适用于对实时性要求高、可以容忍一定数据丢失的应用，如视频直播、在线游戏、语音通信等。由于UDP的简单性，它的开销较低，可以支持更高的数据传输速率和更少的延迟。\n连接管理与数据传输机制 在TCP中，连接管理是通过建立和终止连接来实现的。建立连接需要三次握手过程：首先，客户端发送一个带有SYN（同步）标志的数据包到服务器，表示希望建立连接；服务器回应一个带有SYN和ACK（确认）标志的数据包，表示接受连接请求；最后，客户端发送一个带有ACK标志的数据包，完成连接的建立。这一过程确保了双方都准备好进行数据传输，并同步了初始序列号。\n数据传输过程中，TCP将应用层数据分段，每个段都带有序列号和确认号，以确保数据的有序和完整。接收端通过确认应答机制，告知发送端哪些数据已经成功接收。如果发送端未收到某个数据包的确认，应答机制会触发重传机制，重新发送丢失的数据包。这种机制确保了数据传输的可靠性。\n终止连接则需要四次挥手过程：一方发送一个带有FIN（结束）标志的数据包，表示其数据传输已经完成；接收方回应一个带有ACK标志的数据包，确认接收到终止请求；接收方随后发送一个带有FIN标志的数据包，表示其数据传输也已完成；最后，发送方回应一个带有ACK标志的数据包，完成连接的终止。这一过程确保了双方都能有序地关闭连接，避免数据丢失或重复。\n流量控制与拥塞控制 流量控制 旨在协调发送端和接收端的数据传输速率，防止接收端因处理能力不足而导致的数据丢失。TCP通过窗口机制实现流量控制，发送端维护一个发送窗口，限制在未确认数据前可以发送的数据量。接收端通过调整窗口大小，通知发送端其缓冲能力，动态调节数据传输速率，确保接收端能够及时处理数据，避免数据溢出和丢失。\n拥塞控制 则是为了防止网络过载，确保网络的稳定性和高效性。当网络中的数据流量超过网络的承载能力时，会导致拥塞，增加数据包丢失和延迟。TCP通过一系列算法（如慢启动、拥塞避免、快速重传和快速恢复）动态调整数据传输速率，以适应网络的当前负载状况。慢启动算法在连接初期逐步增加发送窗口大小，快速重传和快速恢复则在检测到丢包后迅速减少发送速率，防止拥塞的进一步恶化。这些机制使得TCP能够在不同的网络条件下自适应地优化数据传输，保持网络的稳定和高效。\n可靠性与数据完整性 TCP通过多种机制确保数据传输的可靠性和完整性。首先，TCP使用序列号和确认应答机制，确保每个数据包都被正确接收和按序排列。每个数据包都带有一个唯一的序列号，接收端通过确认应答（ACK）告知发送端已成功接收的数据包序列号。如果发送端未收到确认应答，会自动重传丢失的数据包。\n此外，TCP在传输过程中实现了错误检测，通过校验和（Checksum）机制对数据包进行完整性检查，确保数据在传输过程中未被篡改或损坏。如果校验和不匹配，接收端会丢弃该数据包，并触发重传机制。\nTCP还通过流量控制和拥塞控制机制，避免接收端和网络过载，进一步保障了数据传输的可靠性。通过这些机制，TCP能够提供一个可靠的、端到端的数据传输通道，确保数据从源端到目标端的准确无误传输。\n传输层安全 传输层在网络安全中扮演着重要角色，通过提供安全的数据传输机制，保护数据的机密性、完整性和可用性。TCP本身不具备加密功能，但常与安全协议（如TLS/SSL）结合使用，提供数据加密和身份验证，防止数据被窃听和篡改。\nTLS（传输层安全协议） 是一种广泛使用的传输层安全协议，位于应用层和传输层之间。它通过加密数据流、验证服务器和客户端的身份，确保数据在传输过程中的安全性。TLS使用对称加密、非对称加密和数字签名等技术，提供强大的数据保护能力。常见的应用场景包括HTTPS（安全的HTTP）、电子邮件传输（如SMTP over TLS）和安全的文件传输协议（如FTPS）。\nDTLS（Datagram Transport Layer Security） 是TLS的变体，专门用于无连接的UDP协议。它继承了TLS的安全特性，提供数据加密和身份验证，同时适应UDP的特性，如数据报的无序和重传。DTLS广泛应用于实时通信和物联网（IoT）等需要低延迟和高安全性的场景。\n通过这些安全机制，传输层能够有效地保护数据在传输过程中的安全性，防止各种网络攻击，如中间人攻击、数据窃听和数据篡改，确保网络通信的保密性和完整性。\n传输层的未来发展 随着互联网和通信技术的不断发展，传输层也在不断演进，以满足更高的性能、安全性和灵活性的需求。以下是传输层未来发展的几个主要方向：\n高效传输协议：为应对日益增长的数据传输需求，研究人员和工程师不断开发更高效的传输协议，如QUIC（Quick UDP Internet Connections）。QUIC基于UDP协议，集成了TCP的可靠性和TLS的安全性，显著减少连接建立和传输延迟，提升网页加载速度和实时应用性能。QUIC的设计还支持多路复用，减少了TCP的队头阻塞问题，提高了网络资源的利用效率。\n多路径传输：随着多网络接口设备的普及，如智能手机和物联网设备，传输层协议需要支持多路径传输技术。多路径传输允许数据包通过多个网络路径同时传输，提高数据传输的可靠性和带宽利用率。例如，Multipath TCP（MPTCP）扩展了传统TCP协议，支持在同一连接中使用多个网络接口（如Wi-Fi和蜂窝网络），实现更高的传输速度和更强的网络连接稳定性。\n低延迟和高吞吐量：为了满足实时应用和大规模数据传输的需求，传输层协议需要优化延迟和吞吐量。通过采用先进的拥塞控制算法、减少握手次数和优化数据包处理，传输层可以显著降低数据传输延迟，提高整体网络性能。这对于在线游戏、视频会议、虚拟现实（VR）和增强现实（AR）等对实时性要求极高的应用尤为重要。\n安全性增强：随着网络攻击手段的不断进化，传输层的安全机制也需要不断加强。新一代安全协议将更加注重数据的隐私保护和抗量子计算攻击能力。研究人员正在探索基于量子密钥分发（QKD）和量子抗攻击加密算法的新型传输层安全协议，以应对未来量子计算带来的安全挑战。\n网络功能虚拟化与软件定义网络：随着网络功能虚拟化（NFV）和软件定义网络（SDN）的兴起，传输层协议的实现和管理将更加灵活和可编程。通过将传输层功能虚拟化，网络管理员可以动态调整传输策略，优化网络资源分配，提升网络的整体效率和可扩展性。SDN的集中式控制机制还允许更细粒度地管理传输层的行为，实现更加智能和自适应的网络管理。\n会话层V 会话层是计算机网络模型中的第五层，位于传输层之上，表示层之下。它的主要职责是管理用户会话，确保不同应用程序之间的通信能够有序进行。会话层通过建立、维护和终止会话连接，为应用层提供稳定的通信环境，并支持数据交换的同步与恢复。理解会话层的功能和实现机制，对于构建可靠、协调的网络通信系统具有重要意义。\n会话层的核心功能 会话层的核心功能主要包括会话管理、同步控制以及对话控制。这些功能共同确保了在复杂网络环境中，多个应用程序能够高效、无冲突地进行数据交换。\n会话管理\n会话管理是会话层的基础功能，负责建立、维护和终止会话连接。会话的建立通常涉及双方的协商，确定通信的参数和协议，确保双方都准备好进行数据交换。一旦会话建立，双方可以在会话期间进行数据传输，并在需要时重新协商会话参数。\n建立会话时，会话层会进行一系列的握手操作，类似于传输层的三次握手，但更为复杂和灵活。会话管理确保了会话的可靠性和持续性，即使在网络中断或设备重启的情况下，也能够恢复会话，继续数据传输。\n同步控制\n在数据传输过程中，特别是对于大规模数据传输或长时间运行的会话，会话层提供同步点（Checkpoint）的功能。这些同步点允许系统在发生故障时，从最近的同步点恢复数据传输，而无需从头开始。这大大提高了数据传输的效率和可靠性，减少了因中断带来的数据丢失和重复传输。\n同步控制通过在数据流中插入控制信息，实现数据传输的同步与协调。这样，即使在数据传输过程中出现中断，系统也能够通过同步点迅速定位并恢复传输，确保数据的一致性和完整性。\n对话控制\n对话控制管理通信双方的对话方式，决定数据传输的方向和顺序。它支持全双工和半双工两种通信模式：\n全双工（Full-Duplex）：允许通信双方同时发送和接收数据，实现双向同时传输。这种模式适用于需要高效率、实时通信的应用，如视频会议和在线游戏。 半双工（Half-Duplex）：通信双方不能同时发送和接收数据，只能单向传输。这种模式适用于资源受限或通信需求较低的环境，如对讲机通信。 对话控制确保了通信双方在数据传输过程中的协调与有序，避免了数据冲突和传输混乱，提高了通信的整体效率和可靠性。\n会话层协议与实现 会话层通过一系列协议和机制，实现会话管理、同步控制和对话控制。以下是几种常见的会话层协议及其工作原理：\nNetBIOS（Network Basic Input/Output System）\nNetBIOS是早期用于局域网通信的会话层协议，主要用于Windows网络环境中。它提供了会话管理、名称解析和数据传输等功能，使得不同计算机之间能够建立会话，进行文件共享和打印服务。NetBIOS在应用层和会话层之间提供了一个中间接口，简化了应用程序的网络通信过程。\nNetBIOS通过调用API，实现会话的建立、数据的发送与接收、会话的终止等功能。尽管NetBIOS在现代网络中逐渐被更先进的协议所取代，但它在网络通信的发展历史中仍具有重要的地位。\nRPC（Remote Procedure Call）\nRPC是一种分布式计算协议，允许应用程序在不同计算机上调用远程服务，就像调用本地函数一样。RPC通过会话层协议管理远程调用的会话，确保调用过程的可靠性和一致性。\nRPC的工作流程包括客户端发起远程调用请求，服务器接收请求并执行相应的服务，然后将结果返回给客户端。会话层在这一过程中负责会话的建立、数据的传输和会话的终止，确保远程调用的顺利进行。\nPPTP（Point-to-Point Tunneling Protocol）\nPPTP是一种用于建立虚拟专用网（VPN）的会话层协议，允许用户通过公共网络安全地访问私有网络资源。PPTP通过会话层协议建立加密的隧道，保护数据在传输过程中的安全性和机密性。\nPPTP的会话管理功能包括隧道的建立、加密参数的协商和隧道的终止。它通过在会话层实现加密和认证，确保VPN连接的安全性，防止数据被窃听和篡改。\n同步与恢复机制 会话层通过同步机制，确保在长时间的数据传输过程中能够有效应对网络中断或系统故障。同步点的设置允许系统在发生故障时，从最近的同步点恢复数据传输，避免了整个会话的中断和数据的重复传输。\n恢复机制依赖于同步点的信息，重新建立会话连接后，系统能够快速定位并恢复数据传输，确保会话的连续性和数据的一致性。这种机制在大规模分布式系统和需要长时间运行的应用中尤为重要，显著提升了系统的鲁棒性和用户体验。\n会话层与其他层的协作 会话层在OSI模型中处于中间层次，负责协调传输层和表示层之间的通信。它通过调用传输层的服务，建立和管理会话连接，同时向表示层提供稳定的通信接口，确保数据格式和编码的一致性。\n会话层与传输层的紧密协作，确保了端到端的数据传输的可靠性和效率。与表示层的协作则确保了不同系统之间的数据兼容性和可理解性，使得不同应用程序能够无缝地进行数据交换和通信。\n会话层的挑战 随着网络应用的不断发展和复杂性增加，会话层也面临着诸多挑战：\n多会话管理：现代网络应用往往需要同时管理多个会话，如浏览器中的多个标签页、视频会议中的多路数据流等。如何高效管理和调度多个会话，确保每个会话的可靠性和实时性，是会话层面临的重要挑战。\n会话恢复与迁移：在移动网络和分布式系统中，会话的恢复与迁移变得尤为关键。例如，用户在移动设备上切换网络连接时，如何无缝地迁移会话，避免数据丢失和传输中断，是会话层需要解决的问题。\n安全性与隐私保护：随着网络攻击手段的不断升级，会话层需要加强安全机制，防止会话劫持、数据篡改和未经授权的访问。通过引入更强大的加密和认证机制，会话层能够更好地保护数据的安全性和用户的隐私。\n自动化与智能化：随着人工智能和机器学习技术的发展，会话层也有望引入智能化的会话管理和优化策略。通过自动化的会话控制和动态优化，会话层能够更高效地应对网络变化和用户需求，提升整体网络的性能和用户体验。\n表示层VI 表示层是计算机网络模型中的第六层，位于会话层之上，应用层之下。它的主要职责是确保不同系统之间的数据能够被正确理解和处理。表示层通过数据格式转换、加密与解密、压缩与解压缩等功能，实现数据在不同计算机系统之间的无缝传输。理解表示层的工作原理和实现机制，对于构建跨平台、跨应用的网络通信系统至关重要。\n表示层的核心功能 表示层的核心功能包括数据格式转换、数据加密与解密、数据压缩与解压缩，以及数据的语法和语义协商。这些功能确保了数据在传输过程中能够保持其原有的结构和意义，无论发送端和接收端的系统架构、操作系统或应用程序如何不同。\n数据格式转换\n不同的计算机系统可能使用不同的数据表示方式，如字符编码、数值表示等。表示层负责将发送端的数据格式转换为接收端能够理解的格式，确保数据在不同系统之间的兼容性。例如，字符编码转换（如从ASCII到Unicode）使得不同语言和符号的文本能够正确显示和处理。\n数据加密与解密\n为了保护数据的机密性和完整性，表示层常常实现数据的加密与解密。加密将明文数据转换为密文，防止未经授权的访问和窃取；解密则将密文还原为明文，确保数据在传输过程中不被篡改或泄露。常见的加密算法包括对称加密（如AES、DES）和非对称加密（如RSA、ECC）。\n数据压缩与解压缩\n数据压缩通过减少数据的冗余，提高传输效率，降低带宽占用。表示层在发送端对数据进行压缩，在接收端进行解压缩，确保数据在传输过程中占用更少的空间，同时保持数据的完整性和可用性。常见的压缩算法包括ZIP、GZIP、JPEG（用于图像）和MP3（用于音频）。\n语法和语义协商\n表示层还负责管理数据的语法和语义，确保发送端和接收端对数据的解释一致。这包括定义数据的结构、类型和意义，使得不同系统之间能够准确理解和处理传输的数据。例如，XML和JSON是常用的数据交换格式，表示层通过解析和生成这些格式的数据，实现跨平台的数据共享。\n表示层协议与标准 表示层通过一系列协议和标准，实现其核心功能，确保不同系统之间的数据互操作性和一致性。以下是几种常见的表示层协议和标准：\nMIME（Multipurpose Internet Mail Extensions）\nMIME扩展了电子邮件协议，支持多种数据类型的传输，如文本、图像、音频和视频。MIME定义了数据的格式和编码方式，使得不同类型的数据能够在电子邮件中正确传输和显示。通过MIME，电子邮件客户端能够识别和处理附件，支持丰富的媒体内容。\nSSL/TLS（Secure Sockets Layer / Transport Layer Security）\n虽然SSL/TLS主要被视为传输层安全协议，但其在表示层也起到了关键作用，尤其是在数据加密和解密方面。SSL/TLS通过在传输层和表示层之间提供加密功能，确保数据在传输过程中的机密性和完整性。它广泛应用于HTTPS（安全的HTTP）、FTPS（安全的FTP）和其他需要安全传输的协议中。\nASN.1（Abstract Syntax Notation One）\nASN.1是一种描述数据结构的标准，用于定义不同系统之间的数据交换格式。它通过抽象语法定义语言（ASDL）描述数据的结构和类型，使得不同系统能够生成和解析统一格式的数据。ASN.1广泛应用于电信、网络管理和安全协议中，如SNMP（Simple Network Management Protocol）和X.509证书。\nXDR（External Data Representation）\nXDR是一种用于跨平台数据交换的标准，定义了数据的编码和解码方式。它确保了不同系统之间的数据表示方式一致，使得数据在不同平台间传输时不会出现格式错误。XDR广泛应用于分布式系统和网络协议中，如NFS（Network File System）和RPC（Remote Procedure Call）。\n表示层的数据编码与转换 表示层通过多种编码和转换技术，实现数据在不同系统之间的兼容性和互操作性。这些技术包括字符编码、数据序列化、格式转换等。\n字符编码\n字符编码定义了字符与数字之间的映射关系，使得文本数据能够在不同系统间正确传输和显示。常见的字符编码包括：\nASCII（American Standard Code for Information Interchange）：使用7位二进制数表示128个字符，适用于英文字符集。 Unicode：使用多种编码方式（如UTF-8、UTF-16）表示全球范围内的字符，支持多语言和符号。 ISO 8859：一系列标准，扩展了ASCII，支持多种语言和字符集。 字符编码转换是表示层的重要功能，确保发送端和接收端使用相同的编码标准，避免字符显示错误或数据丢失。\n数据序列化与反序列化\n数据序列化是将复杂的数据结构（如对象、树、图）转换为线性字节流，以便在网络中传输或存储。反序列化则是将接收到的字节流还原为原始的数据结构。表示层通过序列化和反序列化，实现数据的跨平台传输和共享。\n常见的数据序列化格式包括：\nJSON（JavaScript Object Notation）：轻量级、易于人类阅读和编写，广泛用于Web应用和APIs。 XML（eXtensible Markup Language）：可扩展的标记语言，适用于复杂的数据结构和配置文件。 Protocol Buffers：由Google开发的高效、语言中立的数据序列化格式，适用于大规模分布式系统。 格式转换\n表示层还负责不同数据格式之间的转换，确保数据在传输过程中保持其结构和意义。例如，将图像数据从BMP格式转换为JPEG格式，或将音频数据从WAV格式转换为MP3格式。格式转换不仅提高了数据传输的效率，还支持不同系统和应用对数据格式的需求。\n表示层的安全性 表示层在网络安全中扮演着重要角色，通过提供数据加密和认证机制，保护数据在传输过程中的机密性和完整性。\n数据加密\n表示层通过数据加密技术，防止未经授权的用户访问和窃取传输中的数据。常见的加密算法包括对称加密算法（如AES、DES）和非对称加密算法（如RSA、ECC）。数据加密确保了即使数据在传输过程中被截获，攻击者也无法解读其内容。\n数据完整性与认证\n表示层通过数据完整性校验和认证机制，确保数据在传输过程中未被篡改或伪造。校验和算法（如MD5、SHA）用于验证数据的完整性，防止数据被恶意修改。认证机制（如数字签名）用于验证数据的来源，确保数据来自可信的发送方。\n安全协议\n表示层与传输层的安全协议（如SSL/TLS）紧密结合，提供全面的数据保护。通过加密、认证和完整性校验，安全协议确保了数据在传输过程中的机密性、完整性和可靠性，防止各种网络攻击，如中间人攻击、数据窃听和篡改。\n表示层与其他层的协作 表示层在OSI模型中处于中间层次，负责协调会话层和应用层之间的数据交换。它通过调用会话层的服务，建立和管理数据传输会话，同时向应用层提供统一的数据接口，确保不同应用程序能够无缝地进行通信。\n表示层与会话层的紧密协作，确保了会话的建立、维护和终止，同时通过数据格式转换和编码，保证了数据在传输过程中的一致性和可理解性。与应用层的协作则体现在向应用程序提供标准化的数据接口，使得不同应用能够方便地进行数据交换和通信，无需关心底层的具体实现细节。\n表示层的挑战与未来发展 随着网络应用的不断发展和多样化，表示层也面临着诸多挑战和发展机遇：\n数据多样性与复杂性：现代网络应用支持多种数据类型和复杂的数据结构，如多媒体内容、实时数据流和大规模分布式数据。表示层需要不断更新和优化其数据格式转换和编码技术，以适应这些多样化和复杂的数据需求。\n性能与效率：数据格式转换、加密与解密、压缩与解压缩等表示层功能可能会带来额外的计算开销，影响数据传输的性能。未来表示层的发展需要在提供丰富功能的同时，优化性能和效率，采用更高效的算法和硬件加速技术，确保数据传输的快速和低延迟。\n安全性增强：随着网络攻击手段的不断进化，表示层的安全机制也需要不断强化。未来，表示层需要采用更先进的加密算法和认证机制，增强数据传输的安全性，防止新型的网络威胁和攻击。同时，表示层还需要与其他安全层次（如传输层和应用层）协同工作，提供多层次的安全防护。\n跨平台与互操作性：随着多种操作系统、设备和应用程序的普及，表示层需要确保不同平台和系统之间的数据互操作性。通过制定和遵循统一的数据格式和编码标准，表示层能够实现跨平台的数据交换和通信，支持多样化的网络应用需求。\n自动化与智能化：借助人工智能和机器学习技术，表示层有望实现更加智能化的数据处理和优化。例如，智能化的数据格式转换和压缩算法能够根据数据类型和传输需求自动选择最佳方案，提高数据传输的效率和质量。此外，智能化的安全机制能够实时监测和应对网络威胁，提升整体网络的安全性。\n表示层的实际应用案例 为了更好地理解表示层的作用和实现，以下是几个实际应用案例，展示了表示层在不同场景中的应用和重要性：\nWeb浏览器与服务器通信\n当用户通过Web浏览器访问一个网站时，表示层负责将浏览器生成的HTML、CSS、JavaScript等数据转换为标准的网络传输格式，并在传输过程中进行加密（如通过HTTPS）。服务器接收到数据后，表示层负责解密和解析这些数据，确保浏览器能够正确地渲染和显示网页内容。\n电子邮件传输\n电子邮件客户端和服务器之间的数据交换也依赖于表示层的功能。邮件内容可能包含文本、图像、附件等多种数据类型，表示层通过MIME协议将这些不同类型的数据进行编码和封装，确保邮件在传输过程中保持其原有的格式和内容。接收端的表示层则负责解码和解析邮件内容，确保用户能够正确地查看和处理邮件。\n视频流媒体传输\n在视频流媒体应用中，表示层负责对视频数据进行压缩和编码（如使用H.264、H.265编码），减少数据量，提高传输效率。同时，通过加密技术保护视频内容的版权和隐私，防止未经授权的访问和复制。接收端的表示层则负责解码和解压缩视频数据，确保用户能够流畅地观看高清视频内容。\n跨平台文件共享\n在跨平台文件共享应用中，不同操作系统之间可能使用不同的文件格式和编码标准。表示层通过数据格式转换和编码，确保文件在不同平台间的兼容性和可读性。例如，文本文件在不同操作系统间传输时，表示层会处理字符编码的转换，确保文本内容在接收端正确显示。\n表示层的技术工具与库 为了实现表示层的各种功能，开发者和网络工程师常常依赖于一系列技术工具和软件库，这些工具和库简化了数据格式转换、加密、压缩等操作，提升了开发效率和系统性能。\n数据格式转换工具\nFFmpeg：一个开源的多媒体处理工具，支持音视频数据的转换、压缩和流媒体传输。FFmpeg广泛应用于视频流媒体应用、视频编辑和转换任务中。 libxml2：一个用于解析和处理XML文档的开源库，支持多种编程语言，广泛应用于Web服务、配置文件和数据交换等场景。 加密与解密库\nOpenSSL：一个强大的开源加密库，支持多种加密算法和安全协议，如SSL/TLS。OpenSSL广泛应用于Web服务器、安全通信和数据加密等领域。 libsodium：一个现代化的加密库，提供简单易用的加密函数，适用于需要高安全性和高性能的应用程序。 数据压缩与解压缩库\nzlib：一个广泛使用的开源压缩库，支持DEFLATE算法，适用于文件压缩、网络传输和数据存储等场景。 LZ4：一个高性能的压缩算法库，提供极低的压缩和解压缩延迟，适用于需要实时数据压缩的应用，如日志处理和内存数据存储。 数据序列化库\nProtocol Buffers：由Google开发的高效数据序列化库，支持多种编程语言，适用于分布式系统和网络通信。 JSON Libraries：如Jackson（Java）、jsoncpp（C++）、json.org（多语言支持），用于JSON数据的解析和生成，广泛应用于Web应用和APIs。 表示层的最佳实践 为了确保表示层的高效性、可靠性和安全性，以下是一些最佳实践：\n标准化数据格式\n采用标准化的数据格式（如JSON、XML、Protocol Buffers）进行数据交换，确保不同系统和应用之间的兼容性和互操作性。标准化格式简化了数据解析和生成过程，减少了格式转换错误。\n实现高效的数据编码\n选择适当的数据编码和压缩算法，根据应用需求平衡数据传输效率和资源消耗。对于实时应用，优先选择低延迟、高效能的编码方式；对于大规模数据传输，优先选择高压缩比的编码方式，减少带宽占用。\n加强数据安全性\n在表示层实施强大的数据加密和认证机制，确保数据在传输过程中的机密性和完整性。使用最新的加密算法和安全协议，防止数据被窃听、篡改和伪造。\n优化同步与恢复机制\n合理设置同步点和恢复策略，确保在网络中断或系统故障时能够快速恢复数据传输，减少数据丢失和重复传输。采用自动化的同步与恢复机制，提高系统的鲁棒性和可靠性。\n实施数据验证与校验\n在表示层实现数据验证和校验机制，确保接收端能够正确解析和处理数据。使用校验和、哈希算法等技术，验证数据的完整性和一致性，防止数据错误和篡改。\n监控与日志记录\n在表示层实施监控和日志记录，实时监测数据传输过程中的性能和安全状态。通过分析日志数据，及时发现和解决数据传输中的问题，优化系统性能和安全性。\n表示层的未来发展趋势 随着网络技术和应用需求的不断演进，表示层也在持续发展，以应对更高的性能、安全性和灵活性要求。以下是表示层未来发展的几个主要趋势：\n更高效的数据编码与压缩技术\n随着数据量的激增，表示层需要采用更高效的数据编码和压缩技术，以提高数据传输效率，减少带宽占用。例如，新的压缩算法能够在保持数据质量的同时，提供更高的压缩比和更低的压缩延迟，满足大规模数据传输和实时应用的需求。\n增强的安全机制\n未来，表示层将引入更强大的加密和认证机制，提升数据传输的安全性。新一代加密算法将提供更高的安全性和抗量子计算攻击能力，防止数据在传输过程中被窃取和篡改。此外，表示层还将加强对数据完整性和来源的验证，确保数据的可靠性和可信度。\n跨平台与多语言支持\n随着多平台、多语言的应用环境日益普及，表示层需要提供更好的跨平台和多语言支持。通过标准化的数据格式和灵活的序列化机制，表示层能够实现不同系统和编程语言之间的无缝数据交换，支持多样化的网络应用需求。\n智能化与自动化数据处理\n借助人工智能和机器学习技术，表示层将实现更智能化和自动化的数据处理。智能化的数据格式转换和编码优化能够根据数据类型和传输需求自动选择最佳方案，提高数据传输的效率和质量。同时，自动化的数据校验和错误检测机制能够实时监测和修复数据传输中的问题，提升系统的可靠性和用户体验。\n集成多功能与灵活扩展\n未来，表示层将集成更多功能，如数据分发、内容分发网络（CDN）支持、实时数据处理等，以满足多样化的应用需求。通过模块化和灵活的协议设计，表示层能够轻松扩展和适应新兴的网络技术和应用场景，保持其在网络通信中的核心地位。\n应用层VII 应用层是计算机网络模型中的第七层，也是最接近用户的一层。它负责为用户提供各种网络服务和应用接口，使得用户能够直接利用网络进行通信、数据交换和资源共享。应用层定义了各种应用程序与网络之间的接口和协议，确保不同应用能够在网络环境中有效协同工作。理解应用层的功能、协议和实现机制，对于开发和管理各种网络应用具有重要意义。\n应用层的核心功能 应用层的核心功能在于为用户提供直接的网络服务和应用支持。它通过定义和实现各种协议，使得不同类型的应用程序能够在网络上进行通信和数据交换。应用层不仅处理用户的请求和响应，还负责数据的格式化、加密和传输控制，确保数据在不同系统和应用之间的兼容性和安全性。\n服务提供与用户接口\n应用层充当了用户与网络之间的桥梁，直接与用户的应用程序交互。它为各种应用提供了标准化的接口，使得开发者可以利用这些接口轻松地实现网络功能，而无需关心底层的通信细节。常见的应用层服务包括文件传输、电子邮件、网页浏览、即时通信等。\n数据表示与处理\n尽管表示层负责数据的格式转换和编码，但应用层在某些高级应用中也涉及数据的处理和展示。例如，网页浏览器在接收来自服务器的HTML、CSS和JavaScript数据后，会在应用层进行解析和渲染，呈现给用户可视化的网页内容。\n常见的应用层协议 应用层协议定义了不同应用程序之间的数据交换规则和格式，确保数据在传输过程中的一致性和可理解性。以下是几种常见的应用层协议及其详细原理：\nHTTP/HTTPS（HyperText Transfer Protocol / Secure）\nHTTP 是用于在Web浏览器和Web服务器之间传输超文本的协议。它基于请求-响应模型，浏览器（客户端）向服务器发送请求，服务器返回响应。HTTP协议的主要特点包括无状态性、灵活的消息格式和支持多种数据类型。\n请求方法：如GET（请求资源）、POST（提交数据）、PUT（更新资源）、DELETE（删除资源）等。 状态码：如200（成功）、404（未找到）、500（服务器错误）等，用于指示请求的处理结果。 报文结构：包括请求行、请求头、空行和请求体（对于有数据的请求）。 HTTPS 是在HTTP基础上添加了SSL/TLS加密层，提供安全的数据传输。它通过加密通信内容、防止数据篡改和身份验证，确保数据在传输过程中的机密性和完整性。HTTPS广泛应用于需要保护敏感信息的场景，如在线购物、银行交易和个人数据传输。\nFTP/SFTP（File Transfer Protocol / Secure File Transfer Protocol）\nFTP 是一种用于在客户端和服务器之间传输文件的协议，支持文件的上传、下载、删除和目录管理。FTP使用两个连接：控制连接（用于发送命令）和数据连接（用于传输文件数据）。它支持匿名访问和用户认证，适用于大规模文件传输和共享。\n命令集：如USER（用户名）、PASS（密码）、LIST（列出目录）、RETR（下载文件）、STOR（上传文件）等。 传输模式：主动模式和被动模式，用于处理不同的网络环境和防火墙设置。 SFTP 是基于SSH（Secure Shell）的安全文件传输协议，提供加密的文件传输和更高的安全性。与传统FTP相比，SFTP在单一连接上进行控制和数据传输，简化了防火墙配置，广泛应用于需要高安全性文件传输的场景。\nSMTP/POP3/IMAP（Simple Mail Transfer Protocol / Post Office Protocol 3 / Internet Message Access Protocol）\n这些协议共同构成了电子邮件系统的基础，负责邮件的发送、接收和管理。\nSMTP 主要用于发送邮件，负责将邮件从发送方的邮件服务器传输到接收方的邮件服务器。SMTP基于文本命令和响应，支持邮件的中继和转发。 POP3 主要用于从邮件服务器下载邮件到本地客户端，用户可以在本地设备上查看和管理邮件。POP3通常会从服务器上删除已下载的邮件，适用于单一设备访问邮件的场景。 IMAP 提供更灵活的邮件访问和管理功能，支持在服务器上保留邮件副本，允许用户在多个设备上同步查看和操作邮件。IMAP适用于需要跨设备访问邮件的用户，如手机、平板和电脑。 DNS（Domain Name System）\nDNS 是用于将域名解析为IP地址的协议，充当互联网的“电话簿”。当用户在浏览器中输入一个域名（如www.example.com）时，DNS服务器将其转换为对应的IP地址（如192.0.2.1），使得网络设备能够定位和连接到目标服务器。\n解析过程：包括递归查询和迭代查询，逐步查询根域名服务器、顶级域名服务器和权威域名服务器，最终获取目标域名的IP地址。 记录类型：如A记录（IPv4地址）、AAAA记录（IPv6地址）、CNAME记录（别名）、MX记录（邮件交换）等，定义了不同类型的域名解析信息。 DHCP（Dynamic Host Configuration Protocol）\nDHCP 是用于自动分配IP地址和其他网络配置参数（如子网掩码、默认网关、DNS服务器地址）的协议。通过DHCP，网络管理员可以简化IP地址管理，避免地址冲突和手动配置错误。\n工作流程：包括发现（DHCPDISCOVER）、提供（DHCPOFFER）、请求（DHCPREQUEST）和确认（DHCPACK）四个阶段，确保客户端能够自动获取有效的网络配置。 租约机制：DHCP服务器为客户端分配IP地址的时间段，租约到期后，客户端需要重新请求IP地址，保证IP地址的有效利用。 Telnet/SSH（Telecommunication Network / Secure Shell）\n这些协议用于远程登录和管理计算机系统。\nTelnet 是一种基于文本的远程登录协议，允许用户通过网络连接到远程主机，执行命令和管理系统。由于Telnet传输的数据未加密，容易受到监听和中间人攻击，因此在现代网络中逐渐被更安全的协议所取代。 SSH 是Telnet的安全替代方案，通过加密通信内容，提供安全的远程登录和命令执行功能。SSH支持身份验证、数据加密和数据完整性校验，广泛应用于远程服务器管理、文件传输和隧道建立等场景。 SNMP（Simple Network Management Protocol）\nSNMP 是用于网络设备管理和监控的协议，允许网络管理员监控网络性能、发现设备故障和配置设备参数。\n组件：包括管理站（Manager）、代理（Agent）和管理信息库（MIB）。 操作：如GET（获取设备信息）、SET（配置设备参数）、TRAP（设备主动发送警报）等，支持实时监控和远程管理。 应用场景：广泛应用于企业网络、数据中心和大型互联网基础设施，帮助管理员实现自动化的网络管理和故障排除。 应用层服务与应用 应用层不仅定义了各种协议，还支持了众多具体的网络应用和服务，这些应用涵盖了人们日常生活和工作中的各个方面。\nWeb浏览与内容传输\nWeb浏览器（如Chrome、Firefox、Safari）通过HTTP/HTTPS协议与Web服务器通信，访问和呈现网页内容。Web应用程序（如在线购物、社交网络、云服务）依赖于应用层协议，实现数据的动态加载和交互功能。内容分发网络（CDN）通过优化内容传输路径，提升了全球范围内的网页加载速度和用户体验。\n文件传输与共享\n文件传输协议（FTP/SFTP）支持文件的上传、下载和共享，广泛应用于网站管理、软件分发和数据备份等场景。现代云存储服务（如Dropbox、Google Drive、OneDrive）通过应用层协议提供便捷的文件同步和共享功能，支持跨设备访问和协同工作。\n电子邮件通信\n电子邮件系统通过SMTP/POP3/IMAP协议，实现邮件的发送、接收和管理。电子邮件客户端（如Outlook、Thunderbird）和Web邮件服务（如Gmail、Yahoo Mail）为用户提供便捷的邮件管理和通信功能，支持附件传输、多账户管理和邮件分类等高级功能。\n实时通信与多媒体应用\n即时通信工具（如微信、WhatsApp、Slack）通过应用层协议实现实时消息传输、语音通话和视频会议等功能。多媒体应用（如视频流媒体服务、在线游戏、虚拟现实）依赖于高效的应用层协议和传输机制，确保数据的实时性和流畅性，提高用户体验。\n网络管理与监控\nSNMP协议支持网络设备的监控和管理，通过应用层的管理工具（如Nagios、Zabbix、PRTG）实现网络性能监控、故障检测和资源管理。网络管理系统通过收集和分析SNMP数据，帮助管理员优化网络配置、提升网络效率和保障网络安全。\n云计算与虚拟化服务\n云计算平台（如Amazon Web Services、Microsoft Azure、Google Cloud）通过应用层协议提供各种云服务，如计算、存储、数据库和人工智能服务。虚拟化技术通过应用层接口，实现资源的动态分配和管理，提高了数据中心的资源利用率和灵活性。\n应用层安全 应用层在网络安全中扮演着重要角色，通过提供数据加密、身份验证和访问控制等机制，保护网络通信的安全性和完整性。\n数据加密与传输安全\n应用层协议（如HTTPS、SFTP、TLS）通过加密技术，确保数据在传输过程中的机密性和完整性。HTTPS通过SSL/TLS协议加密HTTP通信，防止数据被窃听和篡改，广泛应用于Web浏览和在线交易。SFTP通过SSH协议加密文件传输，保障数据传输的安全性。\n身份验证与授权\n应用层实现了多种身份验证机制，确保只有经过授权的用户能够访问网络服务和资源。常见的身份验证方法包括基于用户名和密码的认证、双因素认证（2FA）、OAuth授权框架等。授权机制通过访问控制列表（ACL）和权限管理，限制用户对特定资源和功能的访问权限，提升系统的安全性。\n防护机制与安全策略\n应用层安全防护机制包括防火墙、入侵检测系统（IDS）、入侵防御系统（IPS）和应用层网关等。这些机制通过监控和过滤网络流量，检测和阻止恶意攻击，如跨站脚本攻击（XSS）、SQL注入攻击和拒绝服务攻击（DoS/DDoS）。同时，应用层安全策略通过定期更新和漏洞修补，防止已知的安全威胁和攻击。\n应用层的未来发展趋势 随着互联网技术的不断进步和应用需求的日益多样化，应用层也在不断演化，以适应更高的性能、安全性和灵活性的要求。以下是应用层未来发展的几个主要趋势：\n新兴应用协议与标准\n随着物联网（IoT）、虚拟现实（VR）、增强现实（AR）和边缘计算等新兴技术的发展，应用层需要制定和采用新的协议和标准，支持这些应用的特定需求。例如，MQTT（Message Queuing Telemetry Transport）是一种轻量级的消息传输协议，专为物联网设备设计，具有低带宽消耗和高可靠性。\n更高效的数据传输与处理\n为了应对大规模数据传输和实时应用的需求，应用层协议将不断优化数据传输效率和处理能力。采用先进的数据压缩技术、优化协议头部和消息格式，以及利用多线程和异步处理机制，可以显著提升数据传输速度和系统吞吐量，满足高性能应用的需求。\n增强的安全性与隐私保护\n随着网络威胁的不断演化，应用层的安全机制将进一步强化。引入更强大的加密算法、采用多因素认证、实施细粒度的访问控制以及利用区块链技术实现数据的不可篡改性，都是提升应用层安全性的有效手段。同时，隐私保护机制将确保用户数据的机密性和合法使用，遵守数据保护法规（如GDPR、CCPA）以保障用户隐私权。\n智能化与自动化服务\n人工智能（AI）和机器学习（ML）技术的融入，将使应用层服务更加智能化和自动化。例如，智能推荐系统通过分析用户行为和偏好，提供个性化的内容和服务；自动化的安全防护系统能够实时检测和响应网络威胁，提升网络的安全性和稳定性。此外，智能化的网络管理工具将实现更高效的资源分配和故障诊断，优化网络性能和用户体验。\n跨平台与多设备支持\n随着移动设备、可穿戴设备和多种操作系统的普及，应用层需要提供更好的跨平台和多设备支持。通过采用响应式设计、统一的API和兼容性协议，应用层能够实现数据和服务的无缝同步和协同工作，提升用户在不同设备上的体验和便利性。\n云原生与微服务架构\n云原生和微服务架构的兴起，推动了应用层协议和服务的模块化、可扩展和弹性设计。通过将应用拆分为多个独立的微服务，应用层能够实现更高的灵活性和可维护性，支持大规模分布式系统和动态资源管理。容器化技术和编排工具（如Docker、Kubernetes）进一步提升了应用层服务的部署和管理效率，促进了应用层的快速迭代和创新。\n应用层的实际应用案例 为了理解应用层的作用和实现，以下是几个实际应用案例，展示了应用层在不同场景中的应用和重要性：\n电子商务平台\n电子商务平台（如亚马逊、淘宝）依赖于应用层协议实现商品展示、用户购物车管理、支付处理和订单跟踪等功能。通过HTTP/HTTPS协议，用户能够浏览商品、下订单和完成支付过程。同时，应用层协议确保了用户数据的安全传输和交易的可靠性，提升了用户体验和平台的运营效率。\n在线教育系统\n在线教育平台（如Coursera、edX）通过应用层协议提供视频课程、实时讲座、在线测试和学习管理等功能。通过HTTPS协议，确保教育内容和用户数据的安全传输。应用层还支持多媒体数据的高效传输和实时互动，促进了远程教育和在线学习的发展。\n社交网络服务\n社交网络平台（如Facebook、Twitter）依赖于应用层协议实现用户注册、消息发布、好友管理和内容分享等功能。通过HTTP/HTTPS和WebSocket协议，实现实时的消息传递和动态内容更新，增强了用户互动和社交体验。应用层还支持多媒体内容的上传和展示，丰富了平台的功能和吸引力。\n云存储与云计算服务\n云存储服务（如Dropbox、Google Drive）和云计算平台（如AWS、Azure）通过应用层协议提供数据存储、计算资源和各种云服务。应用层协议确保了数据的安全存储和高效访问，支持多用户和跨设备的数据同步与共享。云计算服务通过应用层接口，提供弹性计算、数据分析和人工智能等高级功能，满足企业和开发者的多样化需求。\n应用层的技术工具与库 为了实现应用层的各种功能，开发者和网络工程师常常依赖于一系列技术工具和软件库，这些工具和库简化了协议实现、数据处理和安全管理等操作，提升了开发效率和系统性能。\nWeb开发框架\nDjango（Python）：一个高效的Web开发框架，提供强大的数据库管理、用户认证和模板系统，适用于快速开发和部署Web应用。 Ruby on Rails（Ruby）：一个以约定优于配置为理念的Web框架，支持快速开发和敏捷编程，广泛应用于初创企业和快速迭代的项目。 Express.js（Node.js）：一个灵活的Web应用框架，基于JavaScript，适用于构建高性能和可扩展的网络应用。 API开发与管理工具\nPostman：一个功能强大的API开发和测试工具，支持API请求的构建、发送、调试和文档生成，广泛应用于API开发和测试流程中。 Swagger/OpenAPI：一个用于设计、构建和文档化RESTful API的框架，提供自动化文档生成和API模拟功能，提升API开发的效率和可维护性。 安全库与工具\nOWASP ZAP（Zed Attack Proxy）：一个开源的Web应用安全测试工具，提供自动化的漏洞扫描和手动测试功能，帮助开发者发现和修复安全漏洞。 JWT（JSON Web Tokens）库：用于实现基于令牌的认证和授权机制，支持跨平台和跨语言的安全认证，广泛应用于现代Web应用和API服务中。 数据序列化与通信库\nProtocol Buffers（Protobuf）：由Google开发的高效数据序列化库，支持多种编程语言，适用于高性能分布式系统和网络通信。 gRPC：一个高性能、开源的远程过程调用（RPC）框架，基于HTTP/2和Protocol Buffers，实现了高效的服务间通信，广泛应用于微服务架构和分布式系统。 应用层的最佳实践 采用标准化的应用层协议和数据格式（如HTTP/HTTPS、JSON、XML、Protocol Buffers），确保不同系统和应用之间的兼容性和互操作性。标准化协议简化了数据交换和通信过程，减少了开发和集成的复杂性。\n在应用层实施多层次的安全机制，包括数据加密、身份验证、访问控制和输入验证等，确保数据传输和应用访问的安全性。使用最新的安全协议和加密算法，防止数据泄露和未经授权的访问。\n通过合理设计数据结构、采用高效的数据序列化和压缩算法，优化数据的传输和处理效率。减少不必要的数据传输和冗余，提升应用性能和用户体验。\n设计应用层架构时，考虑高可用性和容错性，确保应用在面对网络故障、服务器崩溃和其他异常情况下能够快速恢复和继续运行。采用负载均衡、冗余部署和自动故障转移等技术，提升应用的稳定性和可靠性。\n在应用层实施全面的监控和日志管理，实时监测应用的性能、健康状态和安全事件。通过分析日志数据，及时发现和解决问题，优化应用性能和用户体验。使用集中式日志管理工具和监控系统，实现对大规模分布式应用的有效监控和管理。\n采用持续集成与持续部署（CI/CD）实践，自动化应用的构建、测试和部署流程，提升开发效率和代码质量。通过自动化测试和持续监控，确保应用在快速迭代过程中保持高可靠性和稳定性。\n应用层概括 应用层作为计算机网络模型中的最高层，承担着为用户提供各种网络服务和应用接口的关键职责。通过定义和实现多种协议和标准，应用层确保了不同应用程序之间的数据交换和通信的兼容性与可靠性。常见的应用层协议如HTTP/HTTPS、FTP/SFTP、SMTP/POP3/IMAP、DNS、DHCP、Telnet/SSH和SNMP等，支持了广泛的网络应用和服务，涵盖了从网页浏览、文件传输、电子邮件到网络管理等各个方面。\n这里先告一段落，本篇主要是说明了OSI七层一个大体的架构，而在intro net（2）中将会进一步介绍网络安全、无线移动网络、网络管理等内容。\n","date":"2024-11-21T00:00:00Z","image":"https://echudet.github.io/p/intro-net/bg_hu_5328d7e1a75902f8.png","permalink":"https://echudet.github.io/p/intro-net/","title":"Intro Net"},{"content":"网络安全 网络安全领域涵盖了广泛的攻击类型和防护机制，旨在保护网络基础设施、数据和用户免受各种威胁。深入了解这些攻击的原理以及相应的防护措施，是构建安全网络环境的关键。以下内容将详细介绍几种常见的网络攻击类型，分析其工作原理，并探讨针对这些攻击的防护机制及相关协议的详细原理。\n网络攻击类型 恶意软件攻击（Malware Attacks）\n恶意软件是一类旨在破坏、窃取或非法获取信息的恶意程序，包括病毒、蠕虫、特洛伊木马、勒索软件和间谍软件等。\n病毒通过附着在合法程序或文件上，一旦用户执行该程序，病毒便会传播到其他文件或系统，可能导致数据损坏或系统崩溃。 蠕虫是一种自我复制的恶意软件，能够通过网络自动传播，无需附着在其他程序上。蠕虫常利用网络漏洞进行传播，可能导致网络拥堵和服务中断。 特洛伊木马伪装成合法软件，诱骗用户下载安装。一旦执行，特洛伊木马便会为攻击者提供对受感染系统的远程控制权限，窃取敏感信息或执行其他恶意操作。 勒索软件通过加密用户的数据，要求支付赎金以恢复访问权限。这类攻击对个人和企业都构成严重威胁，可能导致重要数据的永久丢失。 间谍软件秘密监控用户的活动，收集敏感信息，如键盘记录、屏幕截图和浏览历史，供攻击者利用或出售。 网络钓鱼（Phishing）\n网络钓鱼是一种社会工程攻击，攻击者通过伪装成可信赖的实体（如银行、社交平台或电子商务网站），诱骗用户提供敏感信息（如用户名、密码、信用卡信息）。\n钓鱼邮件通常包含伪装成合法机构的信件，诱导用户点击恶意链接或下载附件。一旦用户点击链接，可能被引导到伪造的网站，输入的敏感信息将被攻击者窃取。\n仿冒网站设计与真实网站极为相似，用户在不知情的情况下输入的登录凭证或支付信息会直接被攻击者获取。\n短信钓鱼（Smishing） 和 语音钓鱼（Vishing）分别通过短信和电话进行诱骗，获取用户的个人信息和账户凭证。\n拒绝服务攻击（DoS/DDoS）\n拒绝服务攻击旨在通过大量流量或资源消耗，使目标服务器、网络或服务无法正常运行。分布式拒绝服务攻击（DDoS）利用多个受感染的设备（如僵尸网络）同时发起攻击，增加了防御的难度。\nSYN Flood攻击通过发送大量半开连接请求，消耗服务器的资源，使其无法处理合法请求。\nUDP Flood攻击通过发送大量无连接的UDP数据包，耗尽目标服务器的带宽和处理能力，导致服务中断。\nHTTP Flood攻击通过模拟大量合法用户的HTTP请求，消耗目标服务器的计算资源，导致网站响应缓慢或崩溃。\n中间人攻击（Man-in-the-Middle, MITM）\n中间人攻击指攻击者在通信双方之间拦截、修改或篡改数据传输，获取敏感信息或操纵通信内容。攻击者通常通过网络嗅探、伪造无线接入点或DNS欺骗等手段实现。\n无线网络中的MITM攻击，如通过创建伪造的Wi-Fi热点，诱骗用户连接，拦截和监控用户的网络通信。\nHTTPS劫持通过伪造SSL/TLS证书，诱骗用户接受不受信任的证书，进而拦截加密通信内容。\nSQL注入（SQL Injection）\nSQL注入是一种代码注入攻击，攻击者通过在应用程序的输入字段插入恶意SQL代码，绕过认证、访问或篡改数据库中的数据。常见于Web应用程序，利用输入验证不严的漏洞进行攻击。\n攻击流程通常包括：\n攻击者在登录表单或搜索框等输入字段中输入恶意SQL代码。 应用程序未对输入进行充分过滤或转义，直接将输入嵌入到SQL查询中。 数据库执行恶意查询，导致数据泄露、篡改或删除。 网络安全防护机制及协议原理 为了应对上述各种网络安全威胁，网络安全领域发展出了一系列防护机制和技术。以下将详细介绍几种主要的防护措施及其相关协议的原理。\n防火墙（Firewall）\n防火墙是网络安全的第一道防线，通过控制进出网络的数据包，阻止未经授权的访问和攻击。防火墙可以基于规则过滤流量，支持包过滤、防火墙代理、状态检测和深度包检测等多种模式。\n包过滤防火墙根据预定义的规则检查每个数据包的源地址、目的地址、端口号和协议类型，决定是否允许或拒绝该数据包通过。它工作在网络层和传输层，具有较低的延迟和高效的处理能力。\n状态检测防火墙（Stateful Firewall）不仅检查数据包的头部信息，还跟踪每个连接的状态。通过维护连接表，状态检测防火墙能够识别合法的会话，阻止非法或异常的流量，提高安全性。\n深度包检测防火墙（Deep Packet Inspection, DPI）对数据包的内容进行详细检查，识别和阻止特定类型的应用层攻击和恶意内容。DPI防火墙能够检测并阻止复杂的攻击，如SQL注入、跨站脚本（XSS）和恶意软件传播。\n入侵检测系统与入侵防御系统（IDS/IPS）\n入侵检测系统（IDS）和入侵防御系统（IPS）通过监控网络流量和系统活动，检测和响应潜在的安全威胁。\nIDS通过分析网络流量和系统日志，识别异常行为和已知的攻击模式，生成警报供网络管理员处理。IDS分为基于签名的检测和基于异常的检测：\n基于签名的IDS（Signature-Based IDS）通过匹配预定义的攻击签名，检测已知的攻击。 基于异常的IDS（Anomaly-Based IDS）通过建立正常行为的基线，检测与基线显著偏离的异常行为。 IPS在IDS的基础上，具备主动防御功能，能够在检测到威胁时自动采取措施，如丢弃恶意流量、封禁攻击源IP地址或重置连接。IPS能够实时阻止攻击，减少对网络和系统的影响。\n加密技术\n加密技术通过将数据转换为不可读的密文，保护数据的机密性和完整性。加密技术广泛应用于数据传输、存储和身份验证等领域，确保数据在传输过程中的安全性。\n对称加密使用相同的密钥进行加密和解密，具有高效的加密和解密速度，适用于大规模数据传输。常见的对称加密算法包括高级加密标准（AES）和数据加密标准（DES）。\n非对称加密使用一对密钥（公钥和私钥）进行加密和解密，提供更高的安全性和灵活性。公钥用于加密数据，私钥用于解密数据。常见的非对称加密算法包括RSA和椭圆曲线加密算法（ECC）。\n混合加密结合了对称加密和非对称加密的优点，通过非对称加密传输对称加密密钥，确保数据传输的安全性和高效性。这种方法广泛应用于SSL/TLS协议中，实现安全的互联网通信。\n虚拟专用网络（VPN）\nVPN通过在公共网络上建立加密的隧道，实现远程用户与企业内部网络的安全连接。VPN技术保护数据在传输过程中的机密性和完整性，防止数据被窃听和篡改。\nVPN协议包括：\nIPsec（Internet Protocol Security）：一种在网络层提供加密和认证的协议套件，支持点对点通信和虚拟专用网络。IPsec通过建立安全关联（Security Association, SA），使用加密和认证算法，保护IP数据包的传输安全。 SSL/TLS VPN：基于SSL/TLS协议，工作在应用层，为用户提供通过Web浏览器的安全访问。SSL/TLS VPN适用于远程办公和访问云服务，具有更好的兼容性和易用性。 OpenVPN：一个开源的VPN协议，基于OpenSSL库，支持多种加密算法和认证机制。OpenVPN提供高度可配置的安全连接，适用于各种网络环境和应用场景。 访问控制与身份验证\n访问控制和身份验证机制通过限制用户对网络资源的访问权限，确保只有经过授权的用户才能访问特定的数据和服务。\n访问控制基于预定义的策略和规则，决定用户对资源的访问权限。常见的访问控制模型包括：\n基于角色的访问控制（RBAC）：通过定义用户角色，赋予不同角色相应的访问权限，简化权限管理。 基于属性的访问控制（ABAC）：通过用户和资源的属性，动态决定访问权限，实现更细粒度的控制。 身份验证通过验证用户的身份，确保用户的合法性。常见的身份验证方法包括：\n基于密码的认证：使用用户名和密码组合进行身份验证，广泛应用于各种网络服务。 多因素认证（MFA）：结合多个身份验证因素（如密码、短信验证码、生物识别），提升认证的安全性。 基于证书的认证：使用数字证书和公钥基础设施（PKI），实现更强大的身份验证和数据加密。 安全信息与事件管理（SIEM）\nSIEM系统通过集中收集、分析和关联网络安全事件和日志数据，提供实时的安全监控和威胁检测。\nSIEM的主要功能包括：\n日志收集与归档：集中收集来自不同设备和应用程序的日志数据，统一存储和管理。 事件关联与分析：通过分析和关联不同来源的日志数据，识别潜在的安全威胁和攻击模式。 实时监控与警报：实时监控网络活动和安全事件，及时生成警报供网络管理员处理。 合规性报告：生成符合行业标准和法规要求的安全报告，支持审计和合规检查。 常见的SIEM工具包括Splunk、ELK Stack（Elasticsearch, Logstash, Kibana）和IBM QRadar，这些工具提供强大的数据分析和可视化功能，帮助组织提升安全态势感知和响应能力。\n各种攻击的详细原理与防护机制 SQL注入（SQL Injection）\n攻击原理： SQL注入攻击利用Web应用程序对用户输入的过滤不严，将恶意SQL代码嵌入到查询中，诱使数据库执行未授权的操作。攻击者可以通过注入命令，绕过认证、读取敏感数据、修改或删除数据库内容，甚至执行远程命令。\n防护机制：\n输入验证与过滤：对所有用户输入进行严格的验证和过滤，确保输入内容符合预期格式，防止恶意代码注入。 使用参数化查询：通过使用预编译的SQL语句和参数化查询，避免将用户输入直接嵌入到SQL查询中。例如，使用Prepared Statements（预编译语句）可以有效防止SQL注入攻击。 最小权限原则：为数据库用户分配最小必要的权限，限制其对数据库的访问和操作范围，减少潜在的攻击影响。 Web应用防火墙（WAF）：部署WAF，监控和过滤SQL注入攻击的流量，阻止恶意请求到达后端数据库。 跨站脚本攻击（Cross-Site Scripting, XSS）\n攻击原理： XSS攻击通过向Web页面注入恶意脚本代码，诱导用户浏览器执行这些脚本。攻击者可以窃取用户的会话Cookie、劫持用户账户、篡改网页内容，甚至传播恶意软件。\n防护机制：\n输入输出编码：对用户输入和输出进行适当的编码，防止恶意脚本代码被浏览器解释执行。常见的编码方法包括HTML实体编码、JavaScript编码和URL编码。 内容安全策略（Content Security Policy, CSP）：通过配置CSP，限制网页中允许执行的脚本源和资源类型，减少XSS攻击的风险。 使用安全的JavaScript库：采用经过安全审计和验证的JavaScript库，避免使用容易受到XSS攻击的脚本函数和方法。 HTTPOnly和Secure标志：为Cookie设置HTTPOnly和Secure标志，防止通过JavaScript访问和传输敏感Cookie数据。 跨站请求伪造（Cross-Site Request Forgery, CSRF）\n攻击原理： CSRF攻击诱导已认证的用户在不知情的情况下执行未经授权的操作，如更改账户信息、提交表单或进行交易。攻击者通过嵌入恶意请求的链接或表单，利用用户的浏览器在受信任的网站上执行操作。\n防护机制：\nCSRF令牌：在每个用户会话中生成唯一的CSRF令牌，并在表单提交或请求中验证该令牌的有效性，确保请求来自合法用户。 双重提交Cookie：将CSRF令牌存储在Cookie中，并通过请求参数或头部发送，服务器验证两者是否匹配，防止恶意请求的伪造。 SameSite Cookie属性：通过设置SameSite属性，限制Cookie在跨站请求中的发送，减少CSRF攻击的可能性。 验证Referer和Origin头部：检查请求的Referer和Origin头部，确保请求来自受信任的来源，阻止跨站请求。 分布式拒绝服务攻击（DDoS）\n攻击原理： DDoS攻击通过大量分布在不同地理位置的受感染设备（如僵尸网络）同时向目标服务器、网络或服务发送海量流量，耗尽其资源，使其无法正常响应合法用户的请求。\n防护机制：\n流量过滤与速率限制：通过路由器、防火墙和DDoS防护设备，过滤异常流量并限制每个IP地址的请求速率，防止恶意流量淹没网络资源。 分布式防御系统：采用全球分布的防御网络，通过分布式服务器和CDN，将流量分散到多个节点，减轻单点负载压力。 行为分析与流量清洗：通过监控和分析流量行为，识别并清洗恶意流量，确保合法流量的正常传输。 弹性扩展：利用云计算和自动扩展技术，根据流量需求动态增加或减少资源，提升网络的抗DDoS能力。 关键安全协议的原理与实现 SSL/TLS（Secure Sockets Layer / Transport Layer Security）\n原理： SSL/TLS协议在应用层和传输层之间工作，通过加密通信内容、验证服务器和客户端的身份，确保数据在传输过程中的机密性和完整性。TLS是SSL的继任者，提供更强大的加密和安全性。\n工作流程：\n握手阶段： 客户端Hello：客户端发送支持的TLS版本、加密算法和随机数。 服务器Hello：服务器选择TLS版本、加密算法，发送其数字证书和随机数。 证书验证：客户端验证服务器证书的合法性，确保服务器的身份。 密钥交换：客户端生成预主密钥，通过服务器的公钥加密发送给服务器，双方通过预主密钥生成对称加密密钥。 握手完成：双方确认密钥生成，开始加密通信。 数据传输阶段： 使用对称加密密钥对数据进行加密和解密，确保数据的机密性和完整性。 通过消息认证码（MAC）和哈希函数，验证数据的完整性，防止数据被篡改。 握手终止阶段： 双方通过发送“close_notify”消息，安全地终止TLS连接，确保未完成的数据传输被正确处理。 防护机制：\n加密：通过对称加密和非对称加密，保护数据的机密性。 认证：通过数字证书和公钥基础设施（PKI），验证服务器和客户端的身份，防止中间人攻击。 数据完整性：通过哈希函数和消息认证码，确保数据在传输过程中未被篡改。 IPsec（Internet Protocol Security）\n原理： IPsec是一套在网络层提供安全通信的协议套件，通过加密和认证保护IP数据包。IPsec可用于建立虚拟专用网（VPN），确保数据在公共网络中的安全传输。\n工作流程：\n认证头（AH）：提供数据包的源认证和数据完整性，但不提供加密。 封装安全负载（ESP）：提供数据包的加密和源认证，确保数据的机密性和完整性。 安全关联（SA）：IPsec通过安全关联定义加密和认证的参数，包括使用的协议（AH或ESP）、加密算法和密钥信息。 密钥管理：通过IKE（Internet Key Exchange）协议，协商和建立SA，管理密钥的生成和分发。 防护机制：\n数据加密：通过ESP协议，对数据进行加密，保护数据的机密性。 身份验证：通过AH和ESP协议，对数据包进行认证，防止数据包被篡改和伪造。 数据完整性：通过哈希函数和消息认证码，确保数据包的完整性，防止数据被修改。 OAuth（Open Authorization）\n原理： OAuth是一种开放标准，用于授权第三方应用访问用户在服务提供商上的资源，而无需暴露用户的凭证（如用户名和密码）。OAuth通过令牌（Token）机制，实现安全的授权流程。\n工作流程：\n授权请求：用户请求第三方应用访问其资源，第三方应用向服务提供商请求授权。 用户授权：服务提供商向用户展示授权页面，用户同意授权后，服务提供商生成授权码（Authorization Code）。 令牌请求：第三方应用使用授权码向服务提供商请求访问令牌（Access Token）。 令牌颁发：服务提供商验证授权码后，颁发访问令牌给第三方应用。 资源访问：第三方应用使用访问令牌向服务提供商请求访问用户资源，服务提供商验证令牌后，允许访问。 防护机制：\n令牌机制：通过使用短期有效的访问令牌，减少凭证泄露的风险。 作用域限制：定义令牌的访问权限，限制第三方应用对用户资源的访问范围，增强安全性。 刷新令牌：通过使用刷新令牌（Refresh Token），允许第三方应用在访问令牌过期后获取新的访问令牌，避免频繁的用户授权。 高级防护技术与未来趋势 零信任架构（Zero Trust Architecture）\n原理： 零信任架构是一种新的安全理念，强调“永不信任，始终验证”。在零信任模型中，所有用户、设备和网络流量均需经过严格验证和授权，无论其来源于内部还是外部。零信任架构通过微分段、最小权限原则和持续监控，实现全面的安全防护。\n关键组件：\n身份验证与授权：通过多因素认证（MFA）、角色基于访问控制（RBAC）等方法，确保只有经过授权的用户和设备能够访问特定资源。 微分段：将网络划分为多个小段，通过细粒度的访问控制策略，限制不同段之间的通信，减少攻击面和横向移动的可能性。 持续监控与分析：通过实时监控网络活动和用户行为，及时发现和响应异常行为和潜在威胁，提升整体安全态势感知能力。 防护机制：\n最小权限原则：为每个用户和设备分配最小必要的权限，限制其访问范围和操作能力，减少潜在的安全风险。 多因素认证：结合多个身份验证因素（如密码、短信验证码、生物识别），提高身份验证的安全性，防止凭证泄露和被盗用。 动态访问控制：根据上下文和风险评估，动态调整访问权限和策略，适应不同的安全需求和环境变化。 人工智能与机器学习在网络安全中的应用\n原理： 人工智能（AI）和机器学习（ML）技术在网络安全中的应用，旨在通过自动化威胁检测、行为分析和响应机制，提升安全防护的效率和准确性。AI/ML模型通过分析海量数据，识别复杂的攻击模式和异常行为，帮助网络管理员实时检测和应对安全威胁。\n应用场景：\n威胁检测与响应：通过训练AI/ML模型，识别网络流量中的异常模式和恶意活动，实现自动化的威胁检测和响应。 行为分析：分析用户和设备的行为模式，识别潜在的内部威胁和异常行为，防止数据泄露和滥用。 安全自动化：自动化执行安全任务，如补丁管理、漏洞扫描和安全策略更新，减少人工干预和操作错误。 防护机制：\n异常检测：通过机器学习算法，建立正常行为的基线，自动识别与基线显著偏离的异常行为，及时发现潜在的安全威胁。 预测分析：利用AI技术预测未来可能的攻击趋势和威胁，提前部署防护措施，提升网络的主动防御能力。 自动化响应：结合AI/ML模型，自动执行安全事件的响应和修复操作，减少响应时间和人为错误，提高安全防护的效率。 区块链技术在网络安全中的应用\n原理： 区块链技术通过去中心化和不可篡改的分布式账本，为网络安全提供了新的解决方案。区块链在身份验证、数据完整性验证和分布式防篡改存储等方面具有独特优势，能够防止数据被篡改和伪造，提升网络交易和通信的安全性。\n应用场景：\n身份管理：利用区块链的不可篡改性，实现去中心化的身份验证和管理，防止身份盗用和伪造。 数据完整性验证：通过区块链记录数据的哈希值，确保数据在传输和存储过程中的完整性，防止数据被篡改。 智能合约：利用区块链上的智能合约，自动执行预定义的安全协议和规则，确保交易和协议的透明性和可靠性。 防护机制：\n去中心化身份验证：通过区块链实现分布式身份验证，避免单点故障和中心化的身份管理漏洞，提升身份验证的安全性。 不可篡改的数据记录：利用区块链的链式结构和加密算法，确保数据记录的不可篡改性，防止数据篡改和伪造。 透明的交易和协议执行：通过智能合约，确保交易和协议的透明执行，减少人为干预和操作错误，提高系统的可信度和安全性。 高级防护技术与未来趋势 零信任架构的深入应用\n零信任架构不仅是一种安全理念，更是一套完整的安全策略和技术实现。随着企业网络环境的日益复杂和边缘计算的兴起，零信任架构将成为未来网络安全的重要方向。通过结合微分段、多因素认证、动态访问控制和持续监控，零信任架构能够实现全面的安全防护，提升网络的抗攻击能力和灵活性。\n量子安全与后量子密码学\n量子计算的发展对现有的加密算法构成了严重威胁，尤其是RSA和ECC等非对称加密算法将被量子计算机轻易破解。为应对这一挑战，研究人员正在积极开发后量子密码学（Post-Quantum Cryptography）算法，这些算法基于数学难题，能够抵抗量子计算攻击，确保未来的数据安全。\n后量子密码学算法包括基于格理论、多变量多项式、哈希函数和码理论等领域的加密算法。这些算法的设计目标是保持高效性和安全性，同时能够在现有的网络基础设施中实现无缝迁移和部署。\n区块链与分布式账本技术的广泛应用\n区块链技术不仅在加密货币领域取得了成功，还在网络安全、供应链管理、物联网和智能合约等多个领域展现出广泛的应用前景。通过去中心化和不可篡改的特性，区块链技术能够解决传统集中式系统存在的安全问题，如数据篡改、单点故障和信任缺失等。\n区块链在网络安全中的应用包括去中心化身份管理、数据完整性验证、分布式防篡改存储和智能合约安全执行等。未来，随着区块链技术的不断成熟和标准化，其在网络安全领域的应用将更加广泛和深入，推动网络安全防护技术的创新与发展。\n智能化与自动化的安全防护\n人工智能（AI）和机器学习（ML）技术的融合，为网络安全带来了新的机遇。通过自动化威胁检测、行为分析和响应机制，AI/ML能够提升网络安全的效率和准确性，减少人为干预和操作错误。\n智能化安全防护包括基于AI/ML的入侵检测系统、自动化威胁响应、智能防火墙和行为分析等。这些技术通过实时分析海量数据，识别复杂的攻击模式和异常行为，提供及时的威胁响应和防护措施，提升整体网络安全态势感知能力。\n增强的隐私保护与合规性\n随着数据隐私法规的日益严格，如《通用数据保护条例》（GDPR）和《加州消费者隐私法案》（CCPA），网络安全需要更加注重隐私保护和合规性。隐私增强技术（Privacy-Enhancing Technologies, PETs）通过数据匿名化、差分隐私和同态加密等方法，确保用户数据在传输和存储过程中的隐私性和机密性。\n隐私保护技术包括：\n差分隐私：通过添加噪声，保护数据集中的个体隐私，确保统计分析的准确性同时防止个人数据泄露。 同态加密：允许在加密数据上直接进行计算操作，确保数据在处理过程中的安全性和机密性。 数据匿名化：通过去标识化和混淆技术，保护个人身份信息，防止数据关联和反向工程。 云原生与微服务架构的安全性\n随着云计算和微服务架构的广泛应用，网络安全需要适应分布式和动态变化的网络环境。云原生安全和微服务安全通过集成安全措施到开发和部署流程中，提升网络安全的灵活性和可扩展性。\n云原生安全包括容器安全、服务网格安全和无服务器架构安全。通过使用安全的容器镜像、实现服务间的加密通信和动态安全策略，云原生安全能够有效保护分布式应用和服务的安全性。\n微服务安全通过细粒度的访问控制、服务间的认证和授权，以及安全的服务通信协议，确保微服务架构中的每个服务和组件都具备强大的安全防护能力，防止潜在的安全漏洞和攻击。\n总结 网络安全作为计算机网络的重要组成部分，涵盖了广泛的攻击类型和防护机制。通过深入了解各种攻击的原理和实现机制，以及相应的防护措施和安全协议，组织能够有效应对网络安全威胁，保护网络基础设施、数据和用户的安全。随着技术的不断进步和网络环境的日益复杂，网络安全领域也在不断演化，涌现出新的技术和理念，如零信任架构、后量子密码学、区块链安全、智能化防护和隐私增强技术等，进一步提升网络安全的水平和应对能力。\n理解和掌握网络安全的基本原理、主要威胁及其防护机制，对于构建安全、可靠的网络环境至关重要。通过实施多层次的防护措施、制定全面的安全策略、采用先进的安全技术和工具，组织能够有效提升网络防护能力，保障网络环境的安全性、可靠性和可用性。\n无线与移动网络 无线与移动网络是现代通信技术的核心组成部分，它们通过各种无线技术实现设备与网络的无缝连接，为用户提供灵活便捷的通信服务。以下内容将详细介绍无线与移动网络的基础架构、关键技术、协议原理、挑战与解决方案，以及未来发展方向。\n无线与移动网络基础架构 无线网络架构\n无线网络是通过无线电波传输数据的通信网络。典型的无线网络架构包括以下组成部分：\n接入点（Access Point, AP）：为终端设备提供无线连接，通过有线网络连接到核心网络。 终端设备：如智能手机、平板电脑、笔记本电脑等，使用无线网卡连接到接入点。 核心网络：负责数据的路由与转发，连接到互联网或专用网络。 移动网络架构\n移动网络是支持用户设备在网络覆盖范围内自由移动的无线通信网络。主要架构包括：\n无线接入网络（Radio Access Network, RAN）： 由基站（Base Station, BS）和移动终端组成。 提供无线电覆盖，支持移动终端的接入与通信。 核心网络（Core Network, CN）： 移动交换中心（Mobile Switching Center, MSC）：负责语音和数据通信的路由。 本地位置寄存器（Home Location Register, HLR）：存储用户的永久信息。 访客位置寄存器（Visitor Location Register, VLR）：存储用户的临时位置信息。 后端服务网络： 连接到互联网、电话网和其他数据网络，提供外部服务访问。 无线与移动网络的关键技术 无线电技术\n无线与移动网络的核心在于无线电技术的应用，其基本原理包括：\n频分多址（Frequency Division Multiple Access, FDMA）：通过划分频率带宽，实现多个用户同时通信。 时分多址（Time Division Multiple Access, TDMA）：通过划分时间片，实现用户时序共享信道。 码分多址（Code Division Multiple Access, CDMA）：通过扩频码区分用户，实现同时通信。 正交频分复用（Orthogonal Frequency Division Multiplexing, OFDM）：通过多载波传输数据，提高频谱效率。 移动通信技术代际演进\n1G（第一代移动通信）： 使用模拟信号，支持语音通信，速率低。 代表技术：AMPS（Advanced Mobile Phone System）。 2G（第二代移动通信）： 使用数字信号，提高语音质量。 支持短消息服务（SMS）。 代表技术：GSM（Global System for Mobile Communications）。 3G（第三代移动通信）： 支持多媒体服务，如视频通话和移动互联网。 代表技术：UMTS（Universal Mobile Telecommunications System）。 4G（第四代移动通信）： 提高数据传输速率，支持高清视频和流媒体服务。 代表技术：LTE（Long Term Evolution）。 5G（第五代移动通信）： 支持超高速数据传输、低延迟和大规模连接。 应用于智能城市、无人驾驶、物联网等场景。 天线技术\n全向天线：发射和接收信号范围广，但功率集中度低。 定向天线：信号集中在特定方向，适用于远距离通信。 MIMO（多输入多输出）：通过多天线技术提升传输速率和信号可靠性。 无线网络协议\n无线网络的通信依赖于一系列协议实现，其核心包括：\nIEEE 802.11（Wi-Fi）： 提供无线局域网（WLAN）连接。 支持多种频段（2.4GHz、5GHz）和标准（a/b/g/n/ac/ax）。 IEEE 802.16（WiMAX）： 提供无线城域网（WMAN）连接。 支持长距离高带宽通信。 蓝牙（Bluetooth）： 提供短距离设备通信。 应用于音频传输、物联网设备连接等。 无线与移动网络的关键协议原理 LTE协议原理\nLTE（长期演进）协议是4G网络的核心技术，旨在提供高速、低延迟的移动通信服务。\n关键技术：\nOFDM：通过多载波并行传输数据，提高频谱效率。 MIMO：利用多天线技术提升数据传输速率和可靠性。 调制技术：采用QPSK、16-QAM和64-QAM，根据信道质量动态调整调制方式。 5G NR（新无线电）协议\n5G NR是第五代移动通信的无线接口协议，支持高效频谱利用和超低延迟通信。\n关键特性：\n毫米波技术：利用高频段频谱实现超高速通信。 动态频谱共享：支持多种频段和技术共存。 切片技术：将网络分为多个虚拟切片，满足不同应用的需求。 无线局域网协议\nIEEE 802.11ax（Wi-Fi 6）：\n提高多用户环境下的性能。 支持OFDMA（正交频分多址）和BSS Coloring技术。 无线与移动网络的挑战与解决方案 频谱资源匮乏\n挑战：频谱资源有限，随着用户数量增加和数据需求增长，频谱资源日益紧张。\n解决方案：\n频谱共享：通过动态频谱访问和认知无线电技术提高频谱利用率。\n毫米波频段：利用未开发的高频段频谱缓解频谱压力。\n移动性管理\n挑战：用户在网络覆盖范围内的移动可能导致服务中断和切换延迟。\n解决方案：\n切换优化：通过快速切换技术和切换预测算法降低延迟。\n多连接技术：允许设备同时连接多个基站，确保无缝切换。\n网络安全\n挑战：无线网络易受到窃听、中间人攻击和拒绝服务攻击等威胁。\n解决方案：\n加密与认证：使用WPA3、EAP（扩展认证协议）等增强安全性。\n零信任架构：对所有设备和用户进行严格认证和授权。\n无线与移动网络的未来发展方向 6G网络\n支持太赫兹通信，实现10Gbps以上的数据速率。\n应用于全息通信、脑机接口和增强现实等领域。\n人工智能与无线网络\n优化无线资源分配，提高频谱利用率。\n实现智能化的网络故障诊断与修复。\n边缘计算\n将计算能力从云端迁移到网络边缘，降低延迟。\n提供更快的响应速度和本地数据处理能力。\n网络管理与监控 网络管理与监控不仅涉及网络设备的配置和维护，还包括性能优化、安全保障和故障排除等多个方面。\n网络管理的核心职能 网络管理主要包括配置管理、性能管理、故障管理、计费管理和安全管理等核心职能。这些职能相互关联，共同保障网络的正常运行。\n配置管理涉及对网络设备（如路由器、交换机、防火墙等）的配置和配置变更的管理。通过统一的配置策略和工具，网络管理员能够高效地部署新设备、更新现有设备的配置，并确保配置的一致性和准确性。\n性能管理旨在监控和优化网络的运行效率，包括带宽利用率、延迟、吞吐量和错误率等关键性能指标。通过持续的性能监控，网络管理员能够识别潜在的性能瓶颈，采取措施优化网络资源分配，提升整体网络性能。\n故障管理负责检测、诊断和恢复网络中的故障。通过实时的故障监控和自动化的故障检测机制，能够迅速定位问题根源，减少网络中断时间，确保业务的连续性和可靠性。\n计费管理涉及对网络资源使用情况的跟踪和计量，常用于服务提供商的计费和资源分配。通过准确的计费管理，能够实现资源的公平分配和高效利用，提升服务质量和用户满意度。\n安全管理涵盖对网络安全策略的实施和维护，包括访问控制、入侵检测、防火墙配置和安全事件响应等。通过全面的安全管理，能够防止未经授权的访问和恶意攻击，保护网络和数据的安全性。\n网络监控的技术与工具 网络监控是网络管理的重要组成部分，涉及对网络设备和流量的实时监控与分析。有效的网络监控能够提供关键的可视化数据，帮助网络管理员及时发现和解决问题，优化网络性能。\n主动监控通过定期发送探测包（如Ping、SNMP请求）到网络设备，检测设备的可达性和响应时间。这种方法能够及时发现设备的离线或响应迟缓问题，帮助管理员迅速采取措施。\n被动监控则通过捕获和分析实际的网络流量，实时监控数据包的流动和网络活动。这种方法能够深入了解网络流量的模式和趋势，识别异常流量和潜在的安全威胁。\n网络监控工具种类繁多，涵盖了从基础的网络设备监控到高级的流量分析和安全监控。例如，Nagios是一款广泛使用的开源监控工具，支持对网络设备和服务的全面监控，提供实时警报和报告功能。Zabbix则提供了更强大的数据收集和可视化能力，适用于大规模网络环境。SolarWinds和Paessler PRTG等商业监控工具则提供了更丰富的功能和支持，适用于企业级网络管理需求。\n网络管理协议的原理与应用 网络管理协议在实现网络设备配置、监控和控制方面发挥着关键作用。最常见的网络管理协议包括SNMP（简单网络管理协议）、NetConf（网络配置协议）和RESTful API等。\nSNMP是一种广泛应用于网络管理的协议，定义了网络设备与管理系统之间的通信方式。通过SNMP，网络管理员能够查询设备状态、监控性能指标和配置设备参数。SNMP使用MIB（管理信息库）来定义可管理对象，通过Agent和Manager的架构，实现对设备的远程管理。尽管SNMP在网络管理中具有重要地位，但其安全性较低，容易受到攻击，因此在现代网络中通常结合其他安全措施使用。\nNetConf是一个基于XML的网络配置协议，旨在提供更灵活和安全的网络设备配置和管理方式。与SNMP相比，NetConf支持更复杂的配置操作和事务处理，适用于现代化的网络设备和自动化管理系统。NetConf还支持通过SSH进行安全通信，提升了协议的安全性和可靠性。\nRESTful API通过HTTP协议提供了基于Web的网络管理接口，允许网络设备和管理系统之间进行标准化的通信。RESTful API因其简单、易用和可扩展性，被广泛应用于现代网络管理和自动化系统中。通过RESTful API，开发者可以利用标准的Web技术，实现对网络设备的自动化配置和监控，提升网络管理的效率和灵活性。\n网络安全在网络管理中的重要性 网络管理不仅关注网络的性能和可靠性，还需要全面考虑网络的安全性。随着网络攻击手段的不断进化，网络管理中的安全防护措施显得尤为重要。\n访问控制是网络管理中的基础安全措施，通过限制对网络设备和管理系统的访问权限，防止未经授权的用户和设备进行恶意操作。采用强密码策略、多因素认证和细粒度的权限管理，能够有效提升网络管理的安全性。\n入侵检测与防御通过部署IDS/IPS系统，监控网络流量和设备活动，及时发现和响应潜在的安全威胁。结合网络监控工具，能够实现对网络安全事件的实时监控和自动化响应，减少人为干预和响应时间。\n加密通信确保网络管理协议和数据传输的安全性，防止数据在传输过程中被窃听和篡改。通过使用TLS/SSL等加密协议，保护SNMP、NetConf和RESTful API等网络管理协议的通信安全，提升整体网络管理的安全性。\n安全审计与合规性通过定期的安全审计和合规检查，评估网络管理系统的安全性和合规性。遵循行业标准和法规要求，如ISO/IEC 27001、GDPR等，确保网络管理实践符合最佳安全和隐私保护标准。\n网络管理与监控的最佳实践 为了实现高效、可靠和安全的网络管理与监控，组织应遵循以下最佳实践：\n制定全面的网络管理策略，明确网络管理的目标、职责和流程，确保所有网络管理活动有序进行。策略应涵盖配置管理、性能监控、故障响应和安全防护等各个方面，确保网络管理的全面性和一致性。\n采用自动化工具和流程，提升网络管理的效率和准确性。通过使用自动化配置工具、脚本和集成的网络管理平台，减少手动操作和配置错误，提高网络管理的响应速度和灵活性。\n实施持续的性能监控与优化，通过实时监控关键性能指标，识别和解决网络性能瓶颈。利用性能分析工具，进行容量规划和资源优化，确保网络能够满足当前和未来的业务需求。\n加强网络安全防护，通过多层次的安全措施，保护网络设备和管理系统免受各种安全威胁。结合访问控制、入侵检测、加密通信和安全审计，构建全面的网络安全防护体系，保障网络的安全性和可靠性。\n进行定期的网络审计与评估，通过定期检查和评估网络管理实践，发现并修复潜在的问题和漏洞。利用审计报告和评估结果，优化网络管理策略和流程，提升整体网络管理的质量和效果。\n培训和提升网络管理员的技能，确保网络管理员具备最新的网络管理和安全防护知识。通过定期的培训和认证，提升团队的专业能力和应对复杂网络环境的能力，确保网络管理工作的高效性和安全性。\n网络管理与监控的挑战与解决方案 在实际应用中，网络管理与监控面临着诸多挑战，如网络规模的不断扩大、设备类型的多样化、实时性要求的提高以及安全威胁的复杂性增加等。以下是几种主要的挑战及其解决方案：\n1. 网络规模与复杂性：随着企业网络的规模扩大和设备种类增多，网络管理和监控的复杂性也随之增加。为应对这一挑战，组织应采用集中化的网络管理平台，实现对所有网络设备和服务的统一管理和监控。同时，通过自动化工具和脚本，简化配置和维护过程，减少人为操作和错误。\n2. 实时性要求：现代网络应用对实时性要求较高，网络管理和监控系统需要能够实时处理和分析大量数据，及时发现和响应网络问题。为满足这一需求，组织应采用高性能的监控工具，优化数据收集和处理流程，确保监控系统的实时性和响应速度。同时，利用分布式架构和边缘计算，提升数据处理的效率和速度。\n3. 安全威胁的不断演化：网络安全威胁\n网络管理与监控的未来发展趋势 随着技术的不断进步和网络环境的日益复杂，网络管理与监控领域也在不断演化，涌现出新的技术和方法。以下是网络管理与监控的几个未来发展趋势：\n1. 人工智能与机器学习的深度融合：AI和ML技术将在网络管理与监控中发挥越来越重要的作用。通过利用这些技术，网络管理系统能够自动化复杂的任务，如故障预测、性能优化和安全威胁检测。AI/ML模型能够分析海量的网络数据，识别出潜在的异常模式和攻击行为，实现智能化的网络管理和主动防护。\n2. 软件定义网络（SDN）与网络功能虚拟化（NFV）：SDN和NFV技术将进一步改变传统的网络管理模式。通过将网络控制层与数据层分离，SDN提供了更高的灵活性和可编程性，使得网络管理员能够动态调整网络配置和策略，快速响应业务需求。NFV则通过虚拟化网络功能，减少对专用硬件的依赖，提高网络资源的利用效率和部署速度。\n3. 云原生网络管理：随着云计算的普及，云原生网络管理将成为主流。云原生网络管理利用容器化、微服务和自动化工具，实现对云环境中动态和分布式网络资源的高效管理。通过与云服务提供商的集成，云原生网络管理能够实现跨多云和混合云环境的统一监控和管理，提升网络管理的灵活性和扩展性。\n4. 边缘计算与边缘监控：边缘计算将计算能力从中心云迁移到网络边缘，减少延迟并提升数据处理效率。为了支持这一趋势，边缘监控技术将被广泛应用，用于实时监控和管理边缘设备和应用。边缘监控能够提供更低的延迟和更高的实时性，适应物联网、智能城市和无人驾驶等新兴应用场景的需求。\n5. 增强的安全与隐私保护：随着网络攻击手段的不断演化，网络管理与监控系统需要具备更强大的安全防护能力。未来，网络管理系统将集成更先进的安全技术，如量子安全加密、行为分析和零信任架构，确保网络资源和数据的安全性。此外，隐私保护将成为网络管理的重要组成部分，确保用户数据在监控和管理过程中的隐私性和合规性。\n6. 可视化与用户体验优化：未来的网络管理与监控工具将更加注重可视化和用户体验，提供更直观和易用的界面，帮助网络管理员快速理解和操作复杂的网络环境。通过先进的数据可视化技术和交互式仪表板，网络管理系统能够提供清晰的网络状态和性能信息，提升管理效率和决策能力。\n7. 自动化与自愈网络：自动化将成为网络管理与监控的核心，通过自动化工具和脚本，实现网络配置、故障检测与恢复、性能优化等任务的自动化。自愈网络技术将进一步提升网络的自主管理能力，能够在检测到故障或异常时，自动采取修复措施，减少人工干预和系统停机时间，确保网络的高可用性和稳定性。\n总结 网络管理与监控作为计算机网络的重要组成部分，负责确保网络的高效、可靠和安全运行。通过全面的配置管理、性能管理、故障管理、计费管理和安全管理，网络管理与监控为企业和组织提供了强大的网络支持。随着网络规模的扩大和技术的不断进步，网络管理与监控面临着新的挑战和机遇。\n未来，人工智能与机器学习的深度融合、软件定义网络与网络功能虚拟化、云原生网络管理、边缘计算与边缘监控、增强的安全与隐私保护、可视化与用户体验优化以及自动化与自愈网络等趋势，将推动网络管理与监控技术的持续发展和创新。这些趋势不仅提升了网络管理的效率和灵活性，还增强了网络的安全性和可靠性，满足了现代网络环境对高性能和高可用性的需求。\n理解和掌握网络管理与监控的基本原理、关键技术和最佳实践，对于构建和维护稳定、高效、安全的网络环境具有重要意义。通过采用先进的网络管理与监控工具和技术，组织能够更好地应对网络挑战，保障业务的连续性和数据的安全性，提升整体网络运营的效率和质量。\n现代网络技术详解 现代网络技术在过去几十年中经历了飞速的发展，极大地改变了人们的通信方式、信息获取和业务运营模式。从基础设施的升级到先进的协议和创新的网络架构，现代网络技术不断推动着信息社会的进步。本文将深入探讨几种关键的现代网络技术，包括软件定义网络（SDN）、网络功能虚拟化（NFV）、云计算与边缘计算、物联网（IoT）网络、5G与即将到来的6G网络、人工智能（AI）在网络中的应用以及安全性增强技术。通过详细解析这些技术的原理、实现方式及其在实际应用中的作用，帮助读者全面理解现代网络技术的前沿动态和发展趋势。\n软件定义网络（SDN） 软件定义网络（Software-Defined Networking, SDN） 是一种新兴的网络架构理念，旨在通过将网络的控制层与数据层分离，实现网络的集中化管理和动态配置。传统网络设备（如路由器和交换机）在数据转发的同时承担网络控制功能，这种紧耦合的设计限制了网络的灵活性和可扩展性。SDN通过引入一个集中化的控制器，统一管理整个网络，使网络管理员能够通过编程方式控制网络行为，简化网络管理和优化资源分配。\nSDN的核心组成 包括：\n控制层（Control Plane）：由SDN控制器组成，负责网络的整体管理和控制策略的制定。控制器通过开放接口与应用层交互，实现网络的动态配置和优化。 数据层（Data Plane）：由转发设备（如SDN交换机）组成，负责根据控制层的指令进行数据包的转发和处理。数据层设备只执行简单的转发操作，不参与复杂的控制逻辑。 应用层（Application Plane）：包含各种网络应用和服务，如负载均衡、安全策略、流量分析等，通过开放接口与控制层通信，实现网络功能的定制化和自动化。 SDN的主要优势 包括：\n集中化管理：通过集中控制器统一管理网络设备，简化配置和维护，提升管理效率。 灵活性与可编程性：网络管理员可以通过编程方式动态调整网络配置，快速响应业务需求和变化。 自动化与优化：利用控制层的全局视图，实现流量优化、负载均衡和资源分配的自动化，提高网络性能和资源利用率。 简化网络设计：通过分离控制层和数据层，降低网络设计和部署的复杂性，提升网络的可扩展性和可维护性。 实际应用： SDN广泛应用于数据中心、企业网络和运营商网络中。数据中心利用SDN实现动态资源调配和高效流量管理，企业网络通过SDN提升网络的灵活性和安全性，运营商网络则通过SDN优化网络性能和服务交付。\n网络功能虚拟化（NFV） 网络功能虚拟化（Network Function Virtualization, NFV） 是一种网络架构概念，旨在通过将网络功能从专用硬件设备中抽象出来，转移到通用的虚拟化平台上运行，实现网络服务的灵活部署和动态管理。与SDN相辅相成，NFV通过虚拟化技术提高网络的灵活性和可扩展性，降低网络建设和运营成本。\nNFV的核心组件 包括：\n虚拟网络功能（Virtual Network Function, VNF）：实现具体网络功能的软件实例，如虚拟路由器、防火墙、负载均衡器等，运行在虚拟化平台上。 虚拟化基础设施管理（Virtualized Infrastructure Manager, VIM）：负责管理和调度虚拟化资源，如计算、存储和网络资源，确保VNF的高效运行。 NFV管理与编排（NFV Management and Orchestration, MANO）：负责VNF的生命周期管理，包括部署、配置、扩展和迁移，确保网络服务的动态适应和优化。 NFV的主要优势 包括：\n灵活性与可扩展性：通过虚拟化技术，网络功能可以根据需求动态部署和扩展，快速适应业务变化和流量波动。 成本效益：减少对专用硬件设备的依赖，降低资本支出（CAPEX）和运营支出（OPEX），提升资源利用率。 快速部署：通过自动化的管理和编排，实现网络功能的快速部署和配置，缩短服务交付时间。 创新与定制化：支持新型网络服务的快速开发和部署，满足不同业务场景和客户需求，实现网络服务的高度定制化。 实际应用： NFV广泛应用于电信运营商的数据中心和网络边缘，支持虚拟化的核心网络功能和边缘计算服务。同时，企业网络也通过NFV实现灵活的网络服务部署和优化，提升网络的敏捷性和效率。\n云计算与边缘计算 云计算（Cloud Computing） 和 边缘计算（Edge Computing） 是现代网络技术的重要组成部分，分别代表了集中化与分布式计算的两种不同趋势。两者在数据处理和服务交付方面具有互补性，共同推动了网络应用的发展。\n云计算的核心概念 包括：\n按需自助服务：用户可以根据需求随时获取计算资源，如存储、计算能力和应用服务。 广泛的网络访问：通过互联网，用户可以从任何地点访问云服务。 资源池化：云服务提供商通过虚拟化技术将计算资源集中管理和分配，提高资源利用率。 快速弹性：云服务能够根据用户需求快速扩展或缩减资源，支持业务的动态变化。 计量服务：通过按使用量计费，用户只需为实际使用的资源支付费用。 云计算的主要服务模型 包括：\n基础设施即服务（Infrastructure as a Service, IaaS）：提供虚拟化的计算、存储和网络资源，用户可以在此基础上构建和管理自己的应用。 平台即服务（Platform as a Service, PaaS）：提供开发和部署应用的平台，简化应用的开发、测试和部署流程。 软件即服务（Software as a Service, SaaS）：通过互联网提供完整的应用软件，用户无需安装和维护本地应用，直接通过浏览器或客户端访问。 边缘计算的核心概念： 边缘计算通过将数据处理和计算能力移至网络边缘，靠近数据源和用户，减少数据传输延迟，提高实时性和响应速度。边缘计算适用于对实时性要求高、数据量大且需要低延迟处理的应用场景，如物联网（IoT）、智能制造、无人驾驶和增强现实（AR）等。\n云计算与边缘计算的互补性： 云计算擅长处理大规模数据存储和复杂计算任务，提供强大的计算和分析能力；而边缘计算则擅长处理实时性要求高的数据，减少延迟和带宽消耗。两者结合，可以实现更高效、更灵活的网络服务架构，满足不同应用场景的需求。\n实际应用： 云计算广泛应用于企业的数据存储、应用托管和大数据分析等领域，支持各种在线服务和应用。边缘计算则应用于智能城市、工业自动化、远程医疗和智能交通等场景，通过在本地处理数据，提高系统的实时性和可靠性。\n物联网（IoT）网络 物联网（Internet of Things, IoT） 是指通过各种传感器、设备和网络，将物理世界中的物体连接到互联网，实现数据的采集、传输和分析。IoT网络的目标是实现智能化的环境感知、自动化控制和数据驱动的决策支持，广泛应用于智能家居、工业互联网、智慧城市、健康医疗和农业等领域。\nIoT网络的关键组成部分 包括：\n感知层：由各种传感器和设备组成，负责采集环境和物体的数据，如温度、湿度、位置、运动等。 网络层：负责数据的传输和通信，包括无线通信技术（如Wi-Fi、蓝牙、Zigbee、LoRaWAN）和有线通信技术（如以太网、光纤）。 应用层：实现数据的处理、分析和应用，支持智能化的控制和决策，如智能家居控制系统、工业监控系统和健康管理系统。 IoT网络的关键技术：\n无线通信技术：包括低功耗广域网（LPWAN）、蜂窝通信（如NB-IoT、LTE-M）、短距离通信（如蓝牙、Zigbee）和Wi-Fi等，满足不同应用场景的通信需求。 数据处理与分析：通过边缘计算和云计算，对 数据进行实时处理和深度分析，支持智能决策和自动化控制。\n安全性与隐私保护：确保IoT设备和数据传输的安全性，防止数据泄露、设备被篡改和未经授权的访问。采用加密技术、身份验证和访问控制等措施，保护IoT网络的安全。 IoT网络的挑战与解决方案：\n设备管理与可扩展性：随着IoT设备数量的爆炸性增长，管理和维护这些设备变得越来越复杂。解决方案包括自动化设备管理平台、远程固件更新和统一的设备配置管理。 能效与续航：许多IoT设备依赖电池供电，能效和续航时间成为关键问题。采用低功耗通信技术、优化数据传输和能源采集技术（如太阳能）可以延长设备的续航时间。 互操作性与标准化：不同厂商和应用场景下的IoT设备可能使用不同的通信协议和数据格式，导致互操作性问题。通过采用统一的标准和协议（如MQTT、CoAP、OneM2M），提高设备的兼容性和互操作性。 实际应用： IoT技术广泛应用于智能家居，通过智能传感器和设备实现家电的自动化控制和能源管理；在工业互联网中，IoT设备用于监控生产线的运行状态和设备健康状况，提高生产效率和降低维护成本；在智慧城市中，IoT技术用于交通管理、环境监测和公共安全，提升城市的智能化水平和居民的生活质量。\n5G与即将到来的6G网络 第五代移动通信技术（5G） 是当前最先进的移动网络标准，旨在提供更高的数据传输速率、更低的延迟和更大的连接容量。5G不仅提升了移动互联网的用户体验，还为新兴应用如增强现实（AR）、虚拟现实（VR）、无人驾驶和物联网（IoT）提供了强大的支持。\n5G网络的关键特性 包括：\n超高速数据传输：支持最高达20Gbps的峰值数据速率，满足高清视频、云游戏和大数据传输的需求。 极低延迟：实现1ms级的端到端延迟，支持实时性要求高的应用，如远程医疗和自动驾驶。 大规模连接：支持每平方公里百万级设备的连接，满足物联网和智能城市的需求。 网络切片：通过虚拟化技术，将物理网络划分为多个独立的虚拟网络，满足不同应用场景的需求，实现定制化的网络服务。 增强的可靠性与安全性：提供更高的网络可靠性和安全性，确保关键应用的稳定运行和数据安全。 即将到来的第六代移动通信技术（6G） 正在全球范围内积极研发，预计将在2030年前后商用。6G旨在进一步提升网络性能，支持更广泛的应用场景和更高的技术需求。\n6G网络的预期特性 包括：\n更高的频谱效率与速度：支持每秒数百Gbps的数据传输速率，进一步提升网络容量和速度。 极低的延迟：实现亚毫秒级的延迟，支持更加复杂和实时的应用，如全息通信和智能制造。 智能化网络管理：引入人工智能和机器学习技术，实现网络的自适应优化和智能化管理，提升网络的效率和灵活性。 集成多种通信技术：融合卫星通信、量子通信和光通信等多种先进技术，提升网络的覆盖范围和传输能力。 增强的安全性与隐私保护：采用先进的加密和认证技术，确保网络的安全性和用户数据的隐私性，防范新型的网络威胁和攻击。 实际应用： 5G技术在智能制造中实现了生产线的自动化和智能化，提高了生产效率和产品质量；在自动驾驶领域，5G网络支持车辆之间和车辆与基础设施之间的实时通信，提升了驾驶安全性和交通管理效率；在远程医疗中，5G实现了高清视频传输和实时的远程手术支持，提升了医疗服务的可及性和质量。\n人工智能（AI）在网络中的应用 人工智能（AI） 和 机器学习（ML） 技术在网络管理与安全中的应用，极大地提升了网络的智能化和自动化水平。AI/ML通过分析海量网络数据，识别复杂的模式和异常行为，实现智能化的网络优化、安全防护和故障诊断。\nAI在网络管理中的应用 包括：\n自动化配置与优化：利用AI算法自动调整网络配置，优化流量分配和资源利用，提升网络性能和效率。 预测性维护：通过分析设备运行数据和历史故障记录，预测潜在的设备故障和性能下降，提前采取维护措施，减少网络中断和故障影响。 智能流量管理：通过实时分析网络流量模式，动态调整带宽分配和路由策略，优化网络资源的使用，确保高优先级应用的性能。 AI在网络安全中的应用 包括：\n威胁检测与响应：利用AI/ML模型实时监控网络流量和用户行为，识别潜在的安全威胁和异常活动，自动触发响应措施，防止攻击的进一步扩展。 行为分析与异常检测：通过分析用户和设备的行为模式，识别异常行为和内部威胁，提升对复杂攻击的识别能力。 安全自动化：结合AI技术，实现安全事件的自动化响应和修复，减少人工干预和响应时间，提升安全防护的效率和准确性。 AI技术的关键优势 在于其强大的数据分析和模式识别能力，能够处理和分析海量的网络数据，识别复杂的攻击模式和异常行为，实现智能化的网络管理和安全防护。随着AI技术的不断进步，其在网络中的应用将更加广泛和深入，推动网络技术的创新与发展。\n安全性增强技术 在现代网络环境中，随着网络攻击手段的不断演进，传统的安全防护措施已难以满足日益复杂的安全需求。因此，安全性增强技术应运而生，旨在通过更先进的技术和方法，提升网络的整体安全性和抗攻击能力。\n区块链技术 在网络安全中的应用，通过其去中心化和不可篡改的特性，提供了一种全新的安全解决方案。区块链在身份验证、数据完整性验证和分布式防篡改存储等方面具有独特优势。通过区块链技术，可以实现去中心化的身份管理，防止身份盗用和伪造；利用区块链的链式结构和加密算法，确保数据记录的不可篡改性，防止数据篡改和伪造；通过智能合约，实现透明和自动化的安全协议执行，提升系统的可信度和安全性。\n量子加密技术 是另一项前沿的安全性增强技术，旨在应对量子计算对传统加密算法的威胁。量子加密技术利用量子力学的原理，提供绝对安全的加密通信，如量子密钥分发（Quantum Key Distribution, QKD）。QKD通过量子态的不可克隆性，确保密钥在传输过程中的安全性，防止被窃听和窃取。随着量子计算技术的不断发展，量子加密技术将成为未来网络安全的重要保障，确保数据在量子计算时代的安全传输和存储。\n同态加密 是一种允许在加密数据上直接进行计算操作的加密技术，确保数据在处理过程中的安全性和机密性。通过同态加密，数据可以在不解密的情况下进行分析和处理，防止数据在云计算和边缘计算环境中的泄露和篡改。这一技术在隐私保护和数据安全领域具有广泛的应用前景，支持安全的数据共享和协作。\n人工智能驱动的安全防护 通过结合AI/ML技术，实现更智能化和自动化的安全防护机制。AI驱动的入侵检测系统、智能防火墙和自动化安全响应系统，能够实时识别和响应复杂的安全威胁，提升网络的防护能力和响应效率。通过不断学习和优化，AI技术能够适应新的攻击手段和安全威胁，提供持续的安全保障。\n网络管理与监控的未来发展趋势 随着网络技术的不断进步和应用需求的日益多样化，网络管理与监控领域也在不断演化，涌现出新的技术和理念。以下是网络管理与监控的几个未来发展趋势：\n1. 人工智能与机器学习的深度融合：AI和ML技术将在网络管理与监控中发挥越来越重要的作用。通过利用这些技术，网络管理系统能够自动化复杂的任务，如故障预测、性能优化和安全威胁检测。AI/ML模型能够分析海量的网络数据，识别出潜在的异常模式和攻击行为，实现智能化的网络管理和主动防护。\n2. 软件定义网络（SDN）与网络功能虚拟化（NFV）的进一步发展：SDN和NFV将继续推动网络架构的创新，实现更高的灵活性和可编程性。随着5G和边缘计算的普及，SDN和NFV将在动态资源分配、网络切片和服务定制化方面发挥更大的作用，提升网络的适应性和弹性。\n3. 云原生网络管理：随着云计算的普及，云原生网络管理将成为主流。云原生网络管理利用容器化、微服务和自动化工具，实现对云环境中动态和分布式网络资源的高效管理。通过与云服务提供商的集成，云原生网络管理能够实现跨多云和混合云环境的统一监控和管理，提升网络管理的灵活性和扩展性。\n4. 边缘计算与边缘监控：边缘计算将计算能力从中心云迁移到网络边缘，减少延迟并提升数据处理效率。为了支持这一趋势，边缘监控技术将被广泛应用，用于实时监控和管理边缘设备和应用。边缘监控能够提供更低的延迟和更高的实时性，适应物联网、智能城市和无人驾驶等新兴应用场景的需求。\n5. 增强的安全与隐私保护：随着网络攻击手段的不断演化，网络管理与监控系统需要具备更强大的安全防护能力。未来，网络管理系统将集成更先进的安全技术，如量子安全加密、行为分析和零信任架构，确保网络资源和数据的安全性。同时，隐私保护将成为网络管理的重要组成部分，确保用户数据在监控和管理过程中的隐私性和合规性。\n6. 可视化与用户体验优化：未来的网络管理与监控工具将更加注重可视化和用户体验，提供更直观和易用的界面，帮助网络管理员快速理解和操作复杂的网络环境。通过先进的数据可视化技术和交互式仪表板，网络管理系统能够提供清晰的网络状态和性能信息，提升管理效率和决策能力。\n7. 自动化与自愈网络：自动化将成为网络管理与监控的核心，通过自动化工具和脚本，实现网络配置、故障检测与恢复、性能优化等任务的自动化。自愈网络技术将进一步提升网络的自主管理能力，能够在检测到故障或异常时，自动采取修复措施，减少人工干预和系统停机时间，确保网络的高可用性和稳定性。\n总结 现代网络技术通过软件定义网络、网络功能虚拟化、云计算与边缘计算、物联网、5G/6G网络、人工智能以及先进的安全性增强技术，极大地提升了网络的灵活性、效率和安全性。这些技术不仅满足了当前多样化的网络应用需求，也为未来的网络创新和发展奠定了坚实的基础。\n随着技术的不断演进，网络管理与监控将继续向智能化、自动化和高度集成化方向发展，结合人工智能和机器学习技术，实现更高效、更安全和更灵活的网络管理。同时，随着云原生架构和边缘计算的普及，网络管理与监控将更加注重分布式和动态环境下的高效管理和监控，确保网络能够适应不断变化的业务需求和技术挑战。\n理解和掌握这些现代网络技术的原理和应用，对于构建高性能、可靠和安全的网络环境具有重要意义。通过采用先进的网络技术和管理方法，组织能够更好地应对网络的复杂性和多样性，提升网络运营的效率和质量，支持业务的持续增长和创新。\n案例：构建现代企业网络基础设施 背景介绍 公司简介 ImagineTech 是一家全球化的中型企业，专注于软件开发和云服务，拥有多个分支机构遍布不同国家和地区。随着业务的快速扩展，公司需要构建一个高效、可靠且安全的网络基础设施，以支持其日益增长的运营需求和多样化的业务应用。\n网络架构设计 基础网络架构 ImagineTech 的网络架构基于现代网络技术，采用了软件定义网络（SDN）和网络功能虚拟化（NFV）来实现灵活的网络管理和资源优化。公司的核心网络由集中化的SDN控制器管理，负责整个网络的配置、监控和优化。通过NFV，传统的硬件网络功能（如路由器、防火墙和负载均衡器）被虚拟化，运行在通用服务器上，提高了网络的可扩展性和成本效益。\n无线与移动网络 为了支持分布在全球各地的分支机构和移动办公需求，ImagineTech 部署了先进的5G无线网络和企业级Wi-Fi 6解决方案。5G网络提供了高带宽、低延迟的连接，支持高清视频会议、远程协作和实时数据传输；Wi-Fi 6则确保了办公环境中的高速无线接入，满足了大量设备同时连接的需求。\n应用层的实现与管理 关键应用与协议 ImagineTech 依赖于多种应用层协议来支持其业务运营。公司内部的Web应用使用HTTP/HTTPS协议进行数据传输，确保了敏感信息的加密和安全传输。电子邮件服务通过SMTP、POP3和IMAP协议实现，确保了邮件的可靠传递和管理。为了支持远程文件访问和协作，FTP和SFTP协议被广泛应用于文件传输和存储管理中。\n云服务与边缘计算 公司将核心应用和数据迁移至云平台，利用IaaS和PaaS服务实现高效的资源管理和弹性扩展。与此同时，为了满足实时性要求高的应用，如实时监控和即时通信，ImagineTech 部署了边缘计算节点，靠近数据源进行本地处理，减少了数据传输延迟，提升了用户体验。\n网络安全的全面保障 安全策略与防护措施 为了保护企业网络免受各种威胁，ImagineTech 实施了多层次的安全策略，涵盖机密性、完整性和可用性（CIA三原则）。公司部署了先进的防火墙系统，通过深度包检测（DPI）技术过滤恶意流量，阻止未经授权的访问。入侵检测系统（IDS）和入侵防御系统（IPS）实时监控网络活动，及时发现和响应潜在的安全威胁。\n加密技术与VPN 所有敏感数据在传输过程中均采用TLS/SSL协议进行加密，确保数据的机密性和完整性。为支持远程办公和跨区域的安全连接，公司部署了基于IPsec和OpenVPN的虚拟专用网络（VPN），为员工提供了安全的远程访问通道，防止数据在公共网络中被窃听和篡改。\n身份验证与访问控制 ImagineTech 实施了严格的身份验证和访问控制机制，采用多因素认证（MFA）确保只有经过授权的用户才能访问关键系统和数据。基于角色的访问控制（RBAC）模型被应用于权限管理，确保用户只能访问其职责范围内的资源，减少内部安全风险。\n安全信息与事件管理（SIEM） 公司引入了Splunk作为其SIEM解决方案，集中收集和分析来自网络设备、服务器和应用的日志数据。通过实时的事件关联和行为分析，SIEM系统能够快速识别异常活动和潜在的安全威胁，辅助安全团队进行及时响应和修复。\n网络管理与监控的高效实施 集中化网络管理 利用SDN控制器，ImagineTech 实现了网络设备的集中化管理和自动化配置。网络管理员可以通过统一的控制台，实时监控网络状态，动态调整网络策略，优化资源分配，确保网络的高效运行。\n性能监控与优化 公司采用了Zabbix作为其网络性能监控工具，实时监控关键性能指标（KPI）如带宽利用率、延迟和丢包率。通过持续的数据分析和报告，网络团队能够识别性能瓶颈，进行容量规划和资源优化，提升整体网络性能和用户体验。\n故障管理与恢复 ImagineTech 配备了高效的故障管理系统，结合自动化工具和脚本，实现了故障的快速检测和自动化恢复。当网络设备出现故障时，系统能够自动触发恢复流程，如重新路由流量、重启设备或切换到备用路径，减少了网络中断时间，保障了业务的连续性。\n安全审计与合规性 公司定期进行网络安全审计和合规性检查，确保其网络管理实践符合行业标准和法律法规。通过定期的漏洞扫描和风险评估，ImagineTech 能够及时发现和修复安全漏洞，提升网络的整体安全性和合规性。\n现代网络技术的集成与应用 人工智能与机器学习 为了进一步提升网络管理和安全防护能力，ImagineTech 集成了AI/ML技术。通过机器学习算法，网络管理系统能够自动分析海量网络数据，识别复杂的流量模式和潜在的安全威胁，实现智能化的网络优化和自动化的安全响应。例如，AI驱动的入侵检测系统能够实时检测并阻止新型的网络攻击，提升网络的主动防护能力。\n物联网（IoT）集成 公司在智能办公和智慧制造中广泛应用了物联网技术，通过部署各类传感器和智能设备，实现了设备的远程监控和自动化控制。通过统一的IoT平台，网络管理团队能够实时监控设备状态，优化能源使用，提升生产效率。同时，采用边缘计算技术，对关键数据在本地进行处理，减少了数据传输延迟，提升了系统的实时性和可靠性。\n区块链技术 为增强数据的完整性和透明性，ImagineTech 在关键应用中引入了区块链技术。通过区块链的不可篡改性，确保了关键数据的真实性和一致性，防止数据被恶意篡改。智能合约的应用，实现了自动化的业务流程和协议执行，提升了系统的可信度和效率。\n持续改进与未来展望 持续优化与创新 ImagineTech 持续投资于网络技术的研究与开发，积极探索新兴技术如6G网络、量子安全加密和增强现实（AR）在网络中的应用。通过与技术供应商和研究机构的合作，公司不断优化和升级其网络基础设施，保持在技术前沿，满足不断变化的业务需求和市场竞争。\n应对未来挑战 面对日益复杂的网络威胁和不断扩展的网络规模，ImagineTech 通过引入零信任架构、后量子密码学和自动化安全防护等先进技术，提升网络的安全性和可扩展性。同时，借助云原生网络管理和边缘计算技术，确保网络能够灵活适应未来的应用场景和技术发展，保持高效、可靠和安全的运行状态。\n结论 通过构建基于SDN和NFV的现代网络架构，部署先进的无线与移动网络技术，实施全面的网络安全策略，利用AI和物联网技术，ImagineTech 成功打造了一个高效、可靠且安全的网络基础设施。这一综合性的网络解决方案不仅满足了当前业务的需求，还为未来的技术创新和业务扩展奠定了坚实的基础。\n这一案例展示了现代网络技术在企业中的实际应用和集成方法，强调了网络管理与监控、安全防护以及新兴技术的结合对于构建稳定、高效和安全的网络环境的重要性。通过深入理解和应用这些知识，企业能够有效应对网络挑战，提升运营效率，保障数据安全，实现业务的持续增长和成功。\n案例：构建全球性的医疗健康网络基础设施 背景介绍 公司简介 GlobalHealth Solutions（GHS）是一家国际性的医疗健康服务提供商，专注于提供远程医疗、电子健康记录（EHR）、移动健康应用和智能医疗设备。GHS在全球拥有多个医疗中心、远程诊所和研发实验室，服务范围涵盖了数百万患者。随着业务的扩展和技术的进步，GHS需要构建一个高度集成、可靠且安全的网络基础设施，以支持其多样化的医疗服务和全球运营。\n网络架构设计 基础网络架构 GHS采用了现代网络架构，结合软件定义网络（SDN）和网络功能虚拟化（NFV），实现了网络的集中化管理和灵活配置。核心网络由一个集中的SDN控制器管理，负责整个网络的配置、监控和优化。通过NFV，传统的网络功能如虚拟路由器、防火墙和负载均衡器被虚拟化，运行在通用服务器上，提高了网络的可扩展性和成本效益。\n无线与移动网络 为了支持全球各地的医疗中心和远程诊所，GHS部署了先进的5G无线网络和企业级Wi-Fi 6解决方案。5G网络提供了高带宽、低延迟的连接，支持远程手术、实时视频诊断和大规模物联网设备的接入；Wi-Fi 6则确保了医疗设施内部的高速无线接入，满足了大量智能医疗设备和移动终端的同时连接需求。\n应用层的实现与管理 关键应用与协议 GHS依赖于多种应用层协议来支持其医疗服务和运营。电子健康记录系统（EHR）通过HTTPS协议进行安全的数据传输，确保患者信息的机密性和完整性。远程医疗应用使用WebRTC和RTMP协议，实现高清视频会议和实时数据传输，确保医生与患者之间的高质量互动。移动健康应用通过RESTful API与后端服务器通信，支持数据同步和实时监控。\n云服务与边缘计算 GHS将核心医疗应用和数据存储迁移至云平台，利用IaaS和PaaS服务实现高效的资源管理和弹性扩展。与此同时，为了满足实时性要求高的医疗监控和诊断，GHS在各地医疗中心部署了边缘计算节点，靠近数据源进行本地处理，减少了数据传输延迟，提升了医疗服务的响应速度和可靠性。\n网络安全的全面保障 安全策略与防护措施 GHS实施了多层次的安全策略，涵盖了机密性、完整性和可用性（CIA三原则）。公司部署了高级防火墙系统，利用深度包检测（DPI）技术过滤恶意流量，阻止未经授权的访问。入侵检测系统（IDS）和入侵防御系统（IPS）实时监控网络活动，及时发现和响应潜在的安全威胁。\n加密技术与VPN 所有敏感医疗数据在传输过程中均采用TLS/SSL协议进行加密，确保数据的机密性和完整性。为了支持远程医疗和全球运营，GHS部署了基于IPsec和OpenVPN的虚拟专用网络（VPN），为医疗人员和远程诊所提供安全的远程访问通道，防止数据在公共网络中被窃听和篡改。\n身份验证与访问控制 GHS实施了严格的身份验证和访问控制机制，采用多因素认证（MFA）确保只有经过授权的医疗人员才能访问关键系统和患者数据。基于角色的访问控制（RBAC）模型被应用于权限管理，确保用户只能访问其职责范围内的资源，减少内部安全风险。\n安全信息与事件管理（SIEM） 公司引入了Splunk作为其SIEM解决方案，集中收集和分析来自网络设备、服务器和应用的日志数据。通过实时的事件关联和行为分析，SIEM系统能够快速识别异常活动和潜在的安全威胁，辅助安全团队进行及时响应和修复。\n网络管理与监控的高效实施 集中化网络管理 利用SDN控制器，GHS实现了网络设备的集中化管理和自动化配置。网络管理员可以通过统一的控制台，实时监控网络状态，动态调整网络策略，优化资源分配，确保网络的高效运行。\n性能监控与优化 公司采用了Zabbix作为其网络性能监控工具，实时监控关键性能指标（KPI）如带宽利用率、延迟和丢包率。通过持续的数据分析和报告，网络团队能够识别性能瓶颈，进行容量规划和资源优化，提升整体网络性能和用户体验。\n故障管理与恢复 GHS配备了高效的故障管理系统，结合自动化工具和脚本，实现了故障的快速检测和自动化恢复。当网络设备出现故障时，系统能够自动触发恢复流程，如重新路由流量、重启设备或切换到备用路径，减少了网络中断时间，保障了业务的连续性。\n安全审计与合规性 公司定期进行网络安全审计和合规性检查，确保其网络管理实践符合医疗行业的严格标准和法律法规，如HIPAA和GDPR。通过定期的漏洞扫描和风险评估，GHS能够及时发现和修复安全漏洞，提升网络的整体安全性和合规性。\n现代网络技术的集成与应用 人工智能与机器学习 为进一步提升网络管理和安全防护能力，GHS集成了AI/ML技术。通过机器学习算法，网络管理系统能够自动分析海量网络数据，识别复杂的流量模式和潜在的安全威胁，实现智能化的网络优化和自动化的安全响应。例如，AI驱动的入侵检测系统能够实时检测并阻止新型的网络攻击，提升网络的主动防护能力。\n物联网（IoT）集成 GHS在智能医疗设备和远程监控系统中广泛应用了物联网技术，通过部署各类传感器和智能设备，实现了设备的远程监控和自动化控制。通过统一的IoT平台，网络管理团队能够实时监控设备状态，优化能源使用，提升医疗服务的效率和质量。同时，采用边缘计算技术，对关键数据在本地进行处理，减少了数据传输延迟，提升了系统的实时性和可靠性。\n区块链技术 为增强数据的完整性和透明性，GHS在关键应用中引入了区块链技术。通过区块链的不可篡改性，确保了患者健康记录的真实性和一致性，防止数据被恶意篡改。智能合约的应用，实现了自动化的医疗流程和协议执行，提升了系统的可信度和效率。例如，区块链技术用于药品供应链管理，确保药品的来源和流通的透明性，防止假冒伪劣药品的流入。\n持续改进与未来展望 持续优化与创新 GHS持续投资于网络技术的研究与开发，积极探索新兴技术如6G网络、量子安全加密和增强现实（AR）在网络中的应用。通过与技术供应商和研究机构的合作，GHS不断优化和升级其网络基础设施，保持在技术前沿，满足不断变化的医疗需求和市场竞争。\n应对未来挑战 面对日益复杂的网络威胁和不断扩展的网络规模，GHS通过引入零信任架构、后量子密码学和自动化安全防护等先进技术，提升网络的安全性和可扩展性。同时，借助云原生架构和边缘计算技术，确保网络能够灵活适应未来的应用场景和技术发展，保持高效、可靠和安全的运行状态。\n结论 通过构建基于SDN和NFV的现代网络架构，部署先进的无线与移动网络技术，实施全面的网络安全策略，利用AI和物联网技术，以及引入区块链等前沿技术，GlobalHealth Solutions成功打造了一个高度集成、可靠且安全的全球性医疗健康网络基础设施。这一综合性的网络解决方案不仅满足了当前复杂多样的医疗服务需求，还为未来的技术创新和业务扩展奠定了坚实的基础。\n这一案例展示了现代网络技术在医疗行业中的实际应用和集成方法，强调了网络管理与监控、安全防护以及新兴技术的结合对于构建稳定、高效和安全的网络环境的重要性。通过深入理解和应用这些网络技术和管理方法，医疗机构能够有效应对网络挑战，提升运营效率，保障患者数据的安全性和隐私性，实现医疗服务的持续优化和创新。\n","date":"2024-11-21T00:00:00Z","image":"https://echudet.github.io/p/intro-net2/bg_hu_41256fe880f021ba.jpg","permalink":"https://echudet.github.io/p/intro-net2/","title":"Intro Net（2）"},{"content":"网络协议整理 物理层（Physical Layer） 物理层负责数据的实际传输，通过物理媒介（如电缆、光纤、无线电波）将比特流从一台设备传输到另一台设备。关键协议和技术包括：\nEthernet（以太网）：定义了局域网（LAN）中数据包的传输方式，包括电缆类型、信号电平和传输速率，广泛用于办公室、家庭和数据中心的有线网络连接。 IEEE 802.11（Wi-Fi）：无线局域网（WLAN）标准，支持无线数据传输，广泛应用于家庭、办公和公共场所的无线网络。 DSL（数字用户线）：通过电话线传输高速互联网数据，适用于家庭和小型企业的宽带接入。 数据链路层（Data Link Layer） 数据链路层负责节点之间的数据帧传输，确保数据在物理层的可靠传输，并处理帧的差错检测与纠正。关键协议和技术包括：\nEthernet（以太网）：除了物理层功能，Ethernet在数据链路层定义了帧格式和介质访问控制（MAC）机制。 Wi-Fi（IEEE 802.11）：在数据链路层定义了无线设备的接入和数据传输机制，管理无线网络中的设备连接。 PPP（点对点协议）：用于通过串行链路（如电话线、光纤）进行直接通信，支持数据链路层的封装和认证。 MAC（媒体访问控制）协议：如CSMA/CD（载波监听多路访问/碰撞检测），用于管理共享介质上的设备访问。 网络层（Network Layer） 网络层负责数据包在不同网络之间的路由选择和转发，确保数据从源头到目的地的传输。关键协议和技术包括：\nIP（互联网协议）：定义了数据包的格式和寻址方式，负责数据包的路由和转发，支持IPv4和IPv6。 ICMP（互联网控制消息协议）：用于在网络设备之间传递控制消息和错误报告，如“目标不可达”。 IPsec（互联网协议安全）：为IP通信提供加密和认证，确保数据的机密性、完整性和真实性，常用于建立虚拟专用网（VPN）。 ARP（地址解析协议）：将IP地址解析为物理MAC地址，支持局域网内的数据传输。 RIP（路由信息协议）和 OSPF（开放最短路径优先）：用于动态路由选择和网络路径优化。 传输层（Transport Layer） 传输层负责端到端的数据传输，提供可靠的通信服务，管理数据流量控制和错误检测。关键协议和技术包括：\nTCP（传输控制协议）：提供面向连接、可靠的数据传输，确保数据包按序到达，广泛应用于Web浏览（HTTP/HTTPS）、电子邮件（SMTP）和文件传输（FTP/SFTP）。 UDP（用户数据报协议）：提供无连接、不可靠的数据传输，适用于对实时性要求高但容忍数据丢失的应用，如视频会议、在线游戏和DNS查询。 SCTP（流控制传输协议）：结合TCP和UDP的特点，支持多流和多宿主，适用于电信信令和高可靠性应用。 会话层（Session Layer） 会话层负责建立、管理和终止应用程序之间的会话，提供同步和恢复机制。关键协议和技术包括：\nNetConf（网络配置协议）：基于XML的协议，用于网络设备的配置和管理，支持会话层的控制和事务处理。 SSH（安全外壳协议）：提供安全的远程登录和会话管理，确保会话数据的加密和完整性，广泛用于远程服务器管理和安全文件传输。 RPC（远程过程调用）：允许程序在不同计算机上执行代码和函数，支持分布式应用的通信和协调。 表示层（Presentation Layer） 表示层负责数据的语法和语义转换，确保不同系统之间的数据能够互操作。关键协议和技术包括：\nTLS/SSL（传输层安全协议/安全套接层）：在传输层之上提供加密和认证，确保数据传输的安全性和完整性，广泛应用于HTTPS、电子邮件和VPN连接。 Protocol Buffers（Protobuf）：由Google开发的高效数据序列化库，用于定义和序列化结构化数据，支持多种编程语言，提升数据传输效率。 gRPC：基于HTTP/2和Protocol Buffers的高性能远程过程调用（RPC）框架，实现高效的服务间通信，广泛应用于微服务架构和分布式系统。 应用层（Application Layer） 应用层提供用户直接交互的网络服务和应用，涵盖了各种高层协议和应用程序接口。关键协议和技术包括：\nHTTP/HTTPS（超文本传输协议/安全超文本传输协议）：用于Web浏览器和服务器之间的数据传输，支持网页内容的请求和响应，HTTPS在HTTP基础上加入了SSL/TLS加密层。 FTP/SFTP（文件传输协议/安全文件传输协议）：用于在客户端和服务器之间传输文件，支持文件的上传、下载和管理，SFTP基于SSH提供加密的文件传输。 SMTP/POP3/IMAP（简单邮件传输协议/邮局协议3/互联网消息访问协议）：构成电子邮件系统的基础，SMTP用于发送邮件，POP3和IMAP用于接收和管理邮件，IMAP支持多设备同步。 DNS（域名系统）：将域名解析为IP地址，支持互联网域名的管理和查询，关键于网站访问和电子邮件路由。 DHCP（动态主机配置协议）：自动分配IP地址和网络配置参数，简化网络设备的配置和管理，防止地址冲突。 Telnet：提供基于文本的远程登录服务，允许用户通过网络连接到远程主机执行命令，因安全性较低，逐渐被SSH取代。 SNMP（简单网络管理协议）：用于网络设备的管理和监控，支持设备状态查询、性能监控和配置管理，通过Manager和Agent架构实现远程管理。 RESTful API：基于HTTP协议的应用程序接口，支持标准化的网络通信和数据交换，广泛应用于Web服务、云应用和微服务架构，实现服务间的互操作性和数据共享。 OAuth（开放授权）：用于授权第三方应用访问用户在服务提供商上的资源，无需暴露用户的凭证，广泛应用于社交媒体和云服务的集成。 JWT（JSON Web Tokens）：基于JSON的开放标准，用于在网络应用环境间安全地传输声明，支持认证和授权机制，常用于基于令牌的认证和单点登录（SSO）。 WebRTC（网页实时通信）：支持浏览器之间的实时音视频通信和数据传输，提供低延迟的点对点通信能力，应用于视频会议、在线教育和远程医疗。 RTMP（实时消息协议）：用于音视频数据的实时传输和流媒体传输，支持低延迟的数据传输，广泛应用于直播流媒体服务和在线视频平台。 MQTT（消息队列遥测传输）：轻量级的消息传输协议，专为物联网设备设计，支持发布-订阅模式和低带宽通信，适用于实时数据传输和远程监控系统。 CoAP（受限应用协议）：专为受限设备和低功耗网络设计的Web传输协议，基于UDP，支持RESTful架构，适用于物联网设备的数据交互和管理。 OneM2M：全球统一的物联网标准，支持不同物联网平台和设备之间的互操作性，通过统一的API和协议实现数据交换和设备管理，促进物联网生态系统的协同发展。 安全协议 除了上述各层级的安全协议，还有一些关键的安全协议在现代网络中扮演重要角色：\nS/MIME（安全/多用途互联网邮件扩展）：用于电子邮件的加密和签名，确保邮件内容的机密性和发送者的身份验证，广泛应用于企业和敏感通信场景。 Kerberos：一种基于对称密钥密码学的网络身份验证协议，提供强大的身份验证和安全的通信环境，常用于企业内部网络和分布式系统中。 SASL（简单认证和安全层）：一种用于扩展认证机制的协议，支持多种认证方法（如PLAIN、CRAM-MD5、GSSAPI），增强了应用层协议（如SMTP、IMAP、LDAP）的安全性。 无线与移动网络协议 在无线与移动网络领域，除了IEEE 802.11（Wi-Fi）、IEEE 802.16（WiMAX）和蓝牙协议外，还有其他关键协议：\nLTE（长期演进）：4G移动通信技术标准，支持高速数据传输和低延迟通信，广泛应用于移动互联网和多媒体服务。 5G NR（新无线电）：第五代移动通信技术标准，支持超高速数据传输、极低延迟和大规模设备连接，适用于智能城市、无人驾驶和物联网等前沿应用。 NFC（近场通信）：短距离无线通信技术，支持设备间的快速数据交换和支付功能，广泛应用于移动支付和智能卡片系统。 网络管理协议 在网络管理与监控方面，除了SNMP、NetConf和RESTful API外，还有其他重要协议和技术：\nSyslog：一种标准化的日志记录协议，用于网络设备和应用程序生成和传输日志消息，支持集中式日志管理和故障诊断。 RMON（远程监控）：扩展SNMP功能的协议，提供更详细的网络流量和性能监控，帮助网络管理员深入分析网络状态和流量模式。 sFlow：一种流量采样协议，用于实时监控和分析大规模网络环境中的数据流，支持高效的流量管理和性能优化。 现代网络技术协议 现代网络技术不断演进，新的协议和标准不断涌现，以满足复杂和多样化的网络需求：\nSDN协议（如OpenFlow）：用于软件定义网络的控制层与数据层之间的通信，支持集中化的网络管理和动态配置，实现网络的高度可编程性。 NFV MANO（管理与编排）协议：用于网络功能虚拟化的管理与编排，支持虚拟网络功能（VNF）的生命周期管理和资源调度，提升网络的灵活性和自动化水平。 gRPC：基于HTTP/2和Protocol Buffers的高性能RPC框架，支持多语言开发，广泛应用于微服务架构和分布式系统，提升服务间通信效率。 MQTT-SN（MQTT for Sensor Networks）：MQTT的扩展版本，专为低功耗和受限设备设计，适用于无线传感器网络和物联网应用。 ","date":"2024-11-21T00:00:00Z","image":"https://echudet.github.io/p/intro-net3/bg_hu_f44a474f3f43d6.png","permalink":"https://echudet.github.io/p/intro-net3/","title":"Intro Net（3）"},{"content":" 点这里看隐藏内容！ 前段时间看到了rcore /ucore 的项目，打算忙完这段时间拿来试试手，刚好近日又参加了一个rust的数据结构比赛，故借此机会学习rust语法，以巩固基础。\n从Rust代码入门 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 use std::io::{self, Write}; use std::thread; use std::sync::{Arc, Mutex}; // 定义一个宏用于日志输出 macro_rules! log { ($($arg:tt)*) =\u0026gt; ({ println!(\u0026#34;[LOG]: {}\u0026#34;, format!($($arg)*)); }) } // 定义任务状态的枚举 #[derive(Debug)] enum Status { Pending, Completed, } // 定义一个泛型任务结构体 struct Task\u0026lt;T\u0026gt; { id: usize, description: T, status: Status, } impl\u0026lt;T\u0026gt; Task\u0026lt;T\u0026gt; { fn new(id: usize, description: T) -\u0026gt; Self { Task { id, description, status: Status::Pending, } } fn complete(\u0026amp;mut self) { self.status = Status::Completed; log!(\u0026#34;Task {} marked as completed.\u0026#34;, self.id); } } // 定义命令的枚举 enum Command { Add(String), List, Complete(usize), Exit, } impl Command { // 解析用户输入为命令 fn parse(input: \u0026amp;str) -\u0026gt; Option\u0026lt;Self\u0026gt; { let parts: Vec\u0026lt;\u0026amp;str\u0026gt; = input.trim().splitn(2, \u0026#39; \u0026#39;).collect(); match parts[0].to_lowercase().as_str() { \u0026#34;add\u0026#34; =\u0026gt; { if parts.len() \u0026lt; 2 { None } else { Some(Command::Add(parts[1].to_string())) } }, \u0026#34;list\u0026#34; =\u0026gt; Some(Command::List), \u0026#34;complete\u0026#34; =\u0026gt; { if parts.len() \u0026lt; 2 { None } else { parts[1].parse::\u0026lt;usize\u0026gt;().ok().map(Command::Complete) } }, \u0026#34;exit\u0026#34; =\u0026gt; Some(Command::Exit), _ =\u0026gt; None, } } } // 定义待办事项应用的主要结构体 struct TodoApp { tasks: Arc\u0026lt;Mutex\u0026lt;Vec\u0026lt;Task\u0026lt;String\u0026gt;\u0026gt;\u0026gt;\u0026gt;, next_id: usize, } impl TodoApp { fn new() -\u0026gt; Self { TodoApp { tasks: Arc::new(Mutex::new(Vec::new())), next_id: 1, } } fn run(\u0026amp;mut self) { loop { print!(\u0026#34;\u0026gt; \u0026#34;); io::stdout().flush().unwrap(); let mut input = String::new(); if io::stdin().read_line(\u0026amp;mut input).is_err() { println!(\u0026#34;Failed to read line\u0026#34;); continue; } match Command::parse(\u0026amp;input) { Some(Command::Add(desc)) =\u0026gt; { let task = Task::new(self.next_id, desc); self.tasks.lock().unwrap().push(task); log!(\u0026#34;Added task {}\u0026#34;, self.next_id); self.next_id += 1; }, Some(Command::List) =\u0026gt; { let tasks = self.tasks.lock().unwrap(); for task in tasks.iter() { println!( \u0026#34;{}: [{}] {}\u0026#34;, task.id, match task.status { Status::Pending =\u0026gt; \u0026#34;Pending\u0026#34;, Status::Completed =\u0026gt; \u0026#34;Completed\u0026#34;, }, task.description ); } }, Some(Command::Complete(id)) =\u0026gt; { let tasks = self.tasks.lock().unwrap(); if let Some(task) = tasks.iter_mut().find(|t| t.id == id) { task.complete(); } else { println!(\u0026#34;Task with ID {} not found.\u0026#34;, id); } }, Some(Command::Exit) =\u0026gt; { println!(\u0026#34;Exiting...\u0026#34;); break; }, None =\u0026gt; { println!(\u0026#34;Invalid command.\u0026#34;); }, } } } } fn main() { let mut app = TodoApp::new(); // 使用线程来运行应用，这里仅为示例展示并发 let app_handle = thread::spawn(move || { app.run(); }); // 等待线程完成 app_handle.join().unwrap(); } 代码详解 宏（Macros） 1 2 3 4 5 macro_rules! log { ($($arg:tt)*) =\u0026gt; ({ println!(\u0026#34;[LOG]: {}\u0026#34;, format!($($arg)*)); }) } 功能：定义了一个简单的日志宏，用于在控制台输出带有 [LOG]: 前缀的日志信息。 使用：通过 log!(\u0026quot;Message {}\u0026quot;, value); 调用。 枚举与模式匹配（Enums \u0026amp; Pattern Matching） 1 2 3 4 5 #[derive(Debug)] enum Status { Pending, Completed, } 定义了任务的两种状态：Pending 和 Completed。 1 2 3 4 5 6 enum Command { Add(String), List, Complete(usize), Exit, } 定义了用户可以输入的四种命令。 1 2 3 4 5 impl Command { fn parse(input: \u0026amp;str) -\u0026gt; Option\u0026lt;Self\u0026gt; { // 解析逻辑 } } 功能：将用户输入的字符串解析为 Command 枚举。 在 TodoApp::run 方法中，通过 match 语句对解析后的命令进行处理：\n1 2 3 4 5 6 7 match Command::parse(\u0026amp;input) { Some(Command::Add(desc)) =\u0026gt; { /* 添加任务 */ }, Some(Command::List) =\u0026gt; { /* 列出任务 */ }, Some(Command::Complete(id)) =\u0026gt; { /* 完成任务 */ }, Some(Command::Exit) =\u0026gt; { /* 退出应用 */ }, None =\u0026gt; { /* 无效命令 */ }, } 所有权与借用（Ownership \u0026amp; Borrowing） Task 结构体拥有任务描述的所有权。 在 TodoApp 中，使用 Arc\u0026lt;Mutex\u0026lt;Vec\u0026lt;Task\u0026lt;String\u0026gt;\u0026gt;\u0026gt;\u0026gt; 来安全地在多个线程间共享任务列表。 泛型（Generics） 1 2 3 4 5 struct Task\u0026lt;T\u0026gt; { id: usize, description: T, status: Status, } Task 结构体使用泛型 T 来使任务描述可以是任何类型，这里使用 String。 生命周期（Lifetimes） 在这个示例中，生命周期的显式标注并不明显，但在更复杂的应用中，生命周期用于确保引用的有效性。\n错误处理（Error Handling） 1 2 3 fn parse(input: \u0026amp;str) -\u0026gt; Option\u0026lt;Self\u0026gt; { // 解析失败时返回 None } 使用 Option 类型来处理可能的解析失败。 并发（Concurrency） 1 let tasks = Arc::new(Mutex::new(Vec::new())); 使用 Arc（原子引用计数）和 Mutex（互斥锁）来安全地在多个线程间共享和修改任务列表。 1 2 3 let app_handle = thread::spawn(move || { app.run(); }); 使用 thread::spawn 创建一个新线程来运行应用逻辑。 模块系统（Module System） 在这个简单的示例中，所有代码都在同一个文件中。对于更大的项目，代码通常会组织在多个模块和文件中，通过 mod 和 use 关键字进行管理。\n运行示例 以下是如何使用该待办事项应用的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; add Buy groceries [LOG]: Added task 1 \u0026gt; add Finish Rust project [LOG]: Added task 2 \u0026gt; list 1: [Pending] Buy groceries 2: [Pending] Finish Rust project \u0026gt; complete 1 [LOG]: Task 1 marked as completed. \u0026gt; list 1: [Completed] Buy groceries 2: [Pending] Finish Rust project \u0026gt; exit Exiting... 关键概念总结 所有权与借用：通过 Arc\u0026lt;Mutex\u0026lt;\u0026gt;\u0026gt; 共享任务列表，确保线程安全。 泛型：Task\u0026lt;T\u0026gt; 结构体的描述字段使用泛型，使其更具灵活性。 枚举与模式匹配：使用 enum 定义命令和任务状态，通过 match 语句处理不同的命令。 宏：自定义 log! 宏简化日志输出。 并发：使用线程来运行应用逻辑，展示基本的并发编程模式。 错误处理：通过 Option 类型处理命令解析中的潜在失败。 宏 Rust 的宏（Macros）是其强大元编程能力的重要组成部分，允许开发者在编译时生成代码，从而实现代码的复用和简化复杂的模式。理解宏的语法和使用方法对于深入掌握 Rust 编程非常关键。\n宏的基本概念 在 Rust 中，宏是一种在编译时展开的代码生成工具。它们与函数不同，函数在运行时执行，而宏在编译阶段进行代码替换。Rust 提供了两种主要类型的宏：\n声明宏（Declarative Macros）：也称为 macro_rules! 宏，通过模式匹配进行代码生成。 过程宏（Procedural Macros）：包括自定义派生宏（Derive Macros）、属性宏（Attribute Macros）和函数宏（Function-like Macros），它们通过编写函数来处理输入的代码并生成新的代码。 声明宏（macro_rules!） 声明宏是 Rust 最早和最常用的宏类型，使用 macro_rules! 关键字定义。它们通过匹配特定的模式来展开代码，允许在多个地方重用相似的代码模式。\n1 2 3 4 5 6 macro_rules! 宏名称 { (模式) =\u0026gt; { 展开的代码 }; // 可以定义多个模式匹配 } 模式：类似于函数的参数，定义了宏接受的输入形式。 展开的代码：根据匹配到的模式生成的代码，可以包含变量绑定、重复模式等。 以下是一个简单的 say_hello! 宏，它在调用时打印“Hello!”：\n1 2 3 4 5 6 7 8 9 macro_rules! say_hello { () =\u0026gt; { println!(\u0026#34;Hello!\u0026#34;); }; } fn main() { say_hello!(); } 更复杂的宏可以接受参数，例如一个 create_function! 宏，用于生成带有指定名称的函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 macro_rules! create_function { ($func_name:ident) =\u0026gt; { fn $func_name() { println!(\u0026#34;Function {:?} is called\u0026#34;, stringify!($func_name)); } }; } create_function!(foo); create_function!(bar); fn main() { foo(); bar(); } 常用的宏片段\n标识符（ident）：匹配变量名、函数名等标识符。 表达式（expr）：匹配任意表达式。 类型（ty）：匹配类型。 模式（pat）：匹配模式，用于 match 语句等。 路径（path）：匹配路径，如模块路径。 元组重复（$(...),*）：用于匹配重复的模式，例如多个参数。 宏可以定义重复的输入模式，使用 $(...),* 或 $(...),+ 来表示零个或多个、一个或多个重复。例如，定义一个 vec! 宏，接受多个元素并生成一个向量：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 macro_rules! vec_custom { ( $( $x:expr ),* ) =\u0026gt; { { let mut temp_vec = Vec::new(); $( temp_vec.push($x); )* temp_vec } }; } fn main() { let my_vec = vec_custom!(1, 2, 3, 4); println!(\u0026#34;{:?}\u0026#34;, my_vec); // 输出: [1, 2, 3, 4] } 宏的作用域与导出 宏在定义的作用域内有效，默认情况下，宏的名字不会被导出到外部模块。如果需要在多个模块中使用宏，可以通过 #[macro_export] 属性将宏导出：\n1 2 3 4 5 6 #[macro_export] macro_rules! exported_macro { () =\u0026gt; { println!(\u0026#34;This macro is exported!\u0026#34;); }; } 然后，在其他模块中即可使用 exported_macro!()。\n宏的调试与错误处理 由于宏在编译时展开，调试宏可能会比较复杂。Rust 提供了一些工具和技巧来帮助调试宏，例如使用 println! 在宏中输出调试信息，或者利用编译器的错误信息来定位问题。\n此外，宏的错误信息有时可能不够直观，因此编写宏时应尽量保持模式简单，必要时分解成多个小宏，提高可读性和可维护性。\n过程宏（Procedural Macros） 过程宏是一种更强大的宏类型，允许开发者以函数的形式处理输入的代码并生成新的代码。过程宏主要分为以下几类：\n自定义派生宏（Derive Macros）：用于自动为结构体或枚举生成实现特定特征的代码。例如，常用的 #[derive(Debug)]。 属性宏（Attribute Macros）：允许开发者为项添加自定义属性，并基于这些属性生成代码。 函数宏（Function-like Macros）：类似于声明宏，但语法上更像函数调用，允许更复杂的代码生成逻辑。 示例：自定义派生宏\n创建一个简单的自定义派生宏 HelloMacro，为结构体生成一个 hello 方法：\n首先，在一个独立的 crate（库）中定义宏：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 在 hello_macro_derive crate 中 use proc_macro::TokenStream; use quote::quote; use syn; #[proc_macro_derive(HelloMacro)] pub fn hello_macro_derive(input: TokenStream) -\u0026gt; TokenStream { // 解析输入的语法树 let ast = syn::parse(input).unwrap(); // 生成代码 impl_hello_macro(\u0026amp;ast) } fn impl_hello_macro(ast: \u0026amp;syn::DeriveInput) -\u0026gt; TokenStream { let name = \u0026amp;ast.ident; let gen = quote! { impl HelloMacro for #name { fn hello() { println!(\u0026#34;Hello, Macro! My name is {}\u0026#34;, stringify!(#name)); } } }; gen.into() } 然后，在使用宏的项目中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 在 main crate 中 use hello_macro_derive::HelloMacro; trait HelloMacro { fn hello(); } #[derive(HelloMacro)] struct Pancakes; fn main() { Pancakes::hello(); // 输出: Hello, Macro! My name is Pancakes } 宏的最佳实践 保持宏简单：复杂的宏难以阅读和维护，尽量保持宏逻辑简单，必要时分解成多个小宏。 使用明确的模式：清晰地定义宏的输入模式，避免模糊或过于宽泛的匹配，减少意外的代码生成。 提供良好的文档：为宏编写详细的文档和示例，帮助使用者理解其用法和限制。 避免过度使用：虽然宏强大，但过度使用可能导致代码难以理解。优先考虑使用函数、泛型和其他语言特性来实现功能，只有在必要时使用宏。 Rust 的宏系统提供了强大的代码生成和复用能力，通过声明宏和过程宏，开发者可以在编译时动态生成代码，简化复杂的模式和逻辑。理解宏的语法和使用方法需要一定的实践，但掌握宏将大大提升 Rust 编程的灵活性和效率。\n枚举与模式匹配 枚举（Enum）和模式匹配（Pattern Matching）是 Rust 中非常强大且核心的语法特性，能够帮助开发者编写简洁且高效的代码。下面将详细介绍这两个概念的语法及其应用。\n枚举（Enum） 枚举类型允许你定义一个类型，该类型可以是几个不同变体中的一个。与其他语言中的枚举不同，Rust 的枚举可以携带数据，这使得它们在表达复杂数据结构时非常灵活。\n定义枚举：在 Rust 中，使用 enum 关键字来定义枚举。每个变体可以是一个简单的标识符，也可以包含与之关联的数据。\n1 2 3 4 5 6 enum Message { Quit, Move { x: i32, y: i32 }, Write(String), ChangeColor(i32, i32, i32), } 在上述示例中，Message 枚举有四个变体：\nQuit：一个简单的标识符，不携带任何数据。 Move：一个带有命名字段的变体，包含 x 和 y 两个 i32 类型的值。 Write：一个携带 String 类型数据的变体。 ChangeColor：一个携带三个 i32 类型数据的元组变体。 使用枚举：可以通过匹配不同的变体来使用枚举类型。下面是一个示例，展示如何创建和使用 Message 枚举：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 fn main() { let msg1 = Message::Quit; let msg2 = Message::Move { x: 10, y: 20 }; let msg3 = Message::Write(String::from(\u0026#34;Hello, Rust!\u0026#34;)); let msg4 = Message::ChangeColor(255, 0, 0); process_message(msg1); process_message(msg2); process_message(msg3); process_message(msg4); } fn process_message(msg: Message) { match msg { Message::Quit =\u0026gt; println!(\u0026#34;Quit message received.\u0026#34;), Message::Move { x, y } =\u0026gt; println!(\u0026#34;Move to coordinates: ({}, {})\u0026#34;, x, y), Message::Write(text) =\u0026gt; println!(\u0026#34;Write message: {}\u0026#34;, text), Message::ChangeColor(r, g, b) =\u0026gt; println!(\u0026#34;Change color to RGB({}, {}, {})\u0026#34;, r, g, b), } } 在这个例子中，我们创建了四个不同的 Message 实例，并通过 process_message 函数处理它们。每个变体在 match 语句中都有对应的处理逻辑。\n模式匹配（Pattern Matching） 模式匹配是 Rust 中的一种强大工具，允许你根据数据的结构和内容进行分支处理。最常见的模式匹配工具是 match 表达式，但其他地方如 if let 和 while let 也广泛使用模式匹配。\nmatch 表达式\nmatch 表达式允许你对一个值进行多种模式的匹配，并执行相应的代码块。每个模式都需要覆盖所有可能的情况，确保代码的完整性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 enum Coin { Penny, Nickel, Dime, Quarter, } fn value_in_cents(coin: Coin) -\u0026gt; u8 { match coin { Coin::Penny =\u0026gt; 1, Coin::Nickel =\u0026gt; 5, Coin::Dime =\u0026gt; 10, Coin::Quarter =\u0026gt; 25, } } fn main() { let coin = Coin::Dime; println!(\u0026#34;The value of the coin is {} cents.\u0026#34;, value_in_cents(coin)); } 在这个示例中，match 表达式根据传入的 Coin 枚举变体返回相应的面值。\n解构枚举和结构体\n模式匹配不仅可以用于枚举，还可以用于解构结构体、元组和其他复杂的数据结构。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct Point { x: i32, y: i32, } enum Shape { Circle { radius: f64 }, Rectangle { width: f64, height: f64 }, } fn describe_shape(shape: Shape) { match shape { Shape::Circle { radius } =\u0026gt; println!(\u0026#34;Circle with radius {}\u0026#34;, radius), Shape::Rectangle { width, height } =\u0026gt; println!(\u0026#34;Rectangle with width {} and height {}\u0026#34;, width, height), } } fn main() { let shape1 = Shape::Circle { radius: 5.0 }; let shape2 = Shape::Rectangle { width: 10.0, height: 20.0 }; describe_shape(shape1); describe_shape(shape2); } 在这个例子中，match 表达式用于解构 Shape 枚举中的数据，并打印出具体的信息。\n使用 if let 简化匹配\n有时候，只需要匹配某一个特定的模式，而不关心其他情况。这时可以使用 if let 来简化代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 enum Option\u0026lt;T\u0026gt; { Some(T), None, } fn main() { let some_number = Option::Some(42); if let Option::Some(value) = some_number { println!(\u0026#34;The number is {}\u0026#34;, value); } else { println!(\u0026#34;No number found.\u0026#34;); } } if let 语句在模式匹配中非常有用，尤其是在处理 Option 和 Result 类型时，可以使代码更加简洁。\n通配符 _ 和守卫 if\n在模式匹配中，通配符 _ 用于匹配任何值，而守卫 if 则允许在模式匹配时添加额外的条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fn describe_number(x: i32) { match x { 0 =\u0026gt; println!(\u0026#34;Zero\u0026#34;), 1..=10 =\u0026gt; println!(\u0026#34;Between one and ten\u0026#34;), _ if x \u0026gt; 10 =\u0026gt; println!(\u0026#34;Greater than ten\u0026#34;), _ =\u0026gt; println!(\u0026#34;Something else\u0026#34;), } } fn main() { describe_number(5); // 输出: Between one and ten describe_number(15); // 输出: Greater than ten describe_number(-3); // 输出: Something else } 在这个示例中，match 表达式通过守卫 if x \u0026gt; 10 来区分大于十和其他情况。\n模式匹配的高级用法 解构引用和智能指针\n在 Rust 中，模式匹配还可以用于解构引用和智能指针，如 Box、Rc 和 Arc。\n1 2 3 4 5 6 7 8 9 10 11 12 13 enum List { Cons(i32, Box\u0026lt;List\u0026gt;), Nil, } fn main() { let list = List::Cons(1, Box::new(List::Cons(2, Box::new(List::Nil)))); match list { List::Cons(head, ref tail) =\u0026gt; println!(\u0026#34;Head is {}\u0026#34;, head), List::Nil =\u0026gt; println!(\u0026#34;Empty list\u0026#34;), } } 在这个例子中，Box\u0026lt;List\u0026gt; 被解构为 head 和 tail，其中 tail 是对剩余列表的引用。\n嵌套模式\nRust 允许在模式匹配中进行嵌套匹配，这对于处理复杂的数据结构非常有用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 enum Message { Text(String), Binary(Vec\u0026lt;u8\u0026gt;), Quit, } fn process_message(msg: Message) { match msg { Message::Text(ref text) if text.contains(\u0026#34;rust\u0026#34;) =\u0026gt; println!(\u0026#34;Text containing \u0026#39;rust\u0026#39;: {}\u0026#34;, text), Message::Text(text) =\u0026gt; println!(\u0026#34;Text: {}\u0026#34;, text), Message::Binary(data) =\u0026gt; println!(\u0026#34;Binary data of length {}\u0026#34;, data.len()), Message::Quit =\u0026gt; println!(\u0026#34;Quit message\u0026#34;), } } fn main() { let msg1 = Message::Text(String::from(\u0026#34;I love Rust!\u0026#34;)); let msg2 = Message::Binary(vec![1, 2, 3]); let msg3 = Message::Quit; process_message(msg1); process_message(msg2); process_message(msg3); } 在这个示例中，Message::Text 的匹配不仅解构了文本内容，还使用了守卫条件来检查文本是否包含特定字符串。\n实战示例：错误处理中的枚举与模式匹配 Rust 的标准库广泛使用枚举和模式匹配，特别是在错误处理方面。Result 和 Option 类型就是典型的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 use std::fs::File; use std::io::{self, Read}; fn read_username_from_file() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut f = File::open(\u0026#34;username.txt\u0026#34;)?; let mut s = String::new(); f.read_to_string(\u0026amp;mut s)?; Ok(s) } fn main() { match read_username_from_file() { Ok(username) =\u0026gt; println!(\u0026#34;Username: {}\u0026#34;, username), Err(e) =\u0026gt; println!(\u0026#34;Error reading file: {}\u0026#34;, e), } } 在这个例子中，read_username_from_file 函数返回一个 Result\u0026lt;String, io::Error\u0026gt;，表示读取文件内容可能成功也可能失败。main 函数通过 match 表达式来处理不同的结果。\n枚举和模式匹配是 Rust 中处理多变数据和控制流程的关键工具。枚举允许你定义具有不同变体的数据类型，模式匹配则提供了一种强大且灵活的方式来解构和处理这些数据。\n所有权与借用 所有权与借用（Ownership and Borrowing）是 Rust 语言中最核心且最具特色的概念之一，它们共同构成了 Rust 的内存安全性基础。\n所有权（Ownership） 在 Rust 中，所有权系统负责管理内存，确保内存安全而无需垃圾回收器。每一个值在 Rust 中都有一个所有者（owner），并且在任何时刻，一个值只能有一个所有者。所有权规则如下：\n每个值都有一个所有者。 每个值在同一时间只能有一个所有者。 当所有者超出作用域时，值会被自动丢弃（drop），从而释放内存。 示例：所有权的转移（Move）\n1 2 3 4 5 6 7 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); // s1 取得 String 的所有权 let s2 = s1; // 所有权从 s1 转移到 s2 // println!(\u0026#34;{}\u0026#34;, s1); // 这行代码将导致编译错误，因为 s1 不再拥有 String 的所有权 println!(\u0026#34;{}\u0026#34;, s2); // 正常输出 \u0026#34;hello\u0026#34; } 在这个例子中，s1 是 String 的所有者。当我们将 s1 赋值给 s2 时，所有权从 s1 转移到了 s2，因此 s1 不再有效。如果尝试使用 s1，编译器会报错，防止悬垂引用（dangling references）。\n示例：克隆（Clone）\n如果希望在转移所有权的同时保留原有所有权，可以使用 clone 方法进行深拷贝：\n1 2 3 4 5 6 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let s2 = s1.clone(); // 深拷贝，s1 和 s2 都拥有独立的 String println!(\u0026#34;s1 = {}, s2 = {}\u0026#34;, s1, s2); // 正常输出 \u0026#34;hello\u0026#34; 两次 } 然而，clone 方法会产生额外的开销，因为它需要在堆上分配新的内存并复制数据。因此，只有在确实需要时才应使用它。\n借用（Borrowing）与引用（References） 借用允许你在不获取所有权的情况下使用值。通过引用，可以在不转移所有权的情况下访问数据。引用分为不可变引用和可变引用。\n不可变引用（Immutable References）\n不可变引用允许你多次读取数据，但不能修改数据。一个值可以同时拥有多个不可变引用。\n1 2 3 4 5 6 7 fn main() { let s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;s; // 第一个不可变引用 let r2 = \u0026amp;s; // 第二个不可变引用 println!(\u0026#34;{} and {}\u0026#34;, r1, r2); // 正常输出 \u0026#34;hello and hello\u0026#34; } 在这个例子中，r1 和 r2 都是对 s 的不可变引用，允许同时存在多个不可变引用。\n可变引用（Mutable References）\n可变引用允许你修改数据，但在同一时间只能有一个可变引用。这一规则防止了数据竞争（data races）的发生。\n1 2 3 4 5 6 7 8 fn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;mut s; // 第一个可变引用 // let r2 = \u0026amp;mut s; // 这行代码将导致编译错误，因为同时只能有一个可变引用 r1.push_str(\u0026#34;, world\u0026#34;); println!(\u0026#34;{}\u0026#34;, r1); // 正常输出 \u0026#34;hello, world\u0026#34; } 尝试创建多个可变引用会导致编译错误，确保了数据的一致性和安全性。\n借用的工作原理 借用通过引用来实现，引用本身不拥有数据的所有权。引用的类型分为：\n\u0026amp;T：不可变引用，允许读取数据。 \u0026amp;mut T：可变引用，允许修改数据。 示例：函数中的借用\n1 2 3 4 5 6 7 8 9 10 fn main() { let s = String::from(\u0026#34;hello\u0026#34;); let len = calculate_length(\u0026amp;s); // 传递不可变引用 println!(\u0026#34;The length of \u0026#39;{}\u0026#39; is {}.\u0026#34;, s, len); } fn calculate_length(s: \u0026amp;String) -\u0026gt; usize { s.len() } 在这个例子中，calculate_length 函数借用了 s 的引用，而不是获取其所有权。这样，s 在函数调用后仍然有效，可以继续使用。\n示例：可变引用的使用\n1 2 3 4 5 6 7 8 9 fn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); change(\u0026amp;mut s); // 传递可变引用 println!(\u0026#34;{}\u0026#34;, s); // 正常输出 \u0026#34;hello, world\u0026#34; } fn change(s: \u0026amp;mut String) { s.push_str(\u0026#34;, world\u0026#34;); } 通过可变引用，change 函数能够修改 s 的内容，而无需获取其所有权。\n生命周期（Lifetimes） 生命周期是 Rust 编译器用来确保引用的有效性，防止悬垂引用的机制。每个引用都有一个生命周期，表示引用在内存中的有效范围。生命周期的标注通常通过撇号（'a）来表示。\n生命周期的基本规则\nRust 编译器通过生命周期推断来自动管理引用的生命周期，但在某些复杂情况下，需要显式标注生命周期。\n1 2 3 4 5 6 7 8 fn main() { let r; // 1 { let x = 5; r = \u0026amp;x; // 2 } // println!(\u0026#34;r: {}\u0026#34;, r); // 3，这将导致编译错误，因为 x 已经被释放 } 在这个例子中，r 的生命周期延长到了外部作用域，但 x 在内部作用域结束后被释放，因此 r 会成为悬垂引用。Rust 编译器会在编译时捕捉到这种错误。\n示例：函数中的生命周期标注\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fn main() { let string1 = String::from(\u0026#34;abcd\u0026#34;); let string2 = \u0026#34;xyz\u0026#34;; let result = longest(string1.as_str(), string2); println!(\u0026#34;The longest string is {}\u0026#34;, result); } fn longest\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;str) -\u0026gt; \u0026amp;\u0026#39;a str { if x.len() \u0026gt; y.len() { x } else { y } } 在这个例子中，函数 longest 返回一个引用，该引用的生命周期至少与 x 的生命周期一样长。生命周期标注 'a 明确了返回值的引用不会比 x 的引用更短，从而避免了悬垂引用的问题。\n所有权与借用的组合使用 所有权与借用经常在一起使用，确保数据的所有权管理和安全的引用访问。\n示例：所有权转移与借用的结合\n1 2 3 4 5 6 7 8 9 10 fn main() { let s1 = String::from(\u0026#34;hello\u0026#34;); let len = calculate_length(\u0026amp;s1); // 借用 s1 的引用 println!(\u0026#34;The length of \u0026#39;{}\u0026#39; is {}.\u0026#34;, s1, len); // s1 依然有效 } fn calculate_length(s: \u0026amp;String) -\u0026gt; usize { s.len() } 在这个例子中，s1 的所有权没有转移，而是通过不可变引用被借用给 calculate_length 函数。这样，s1 在函数调用后仍然有效，可以继续使用。\n可变借用与不可变借用的限制 Rust 的借用规则确保了在同一时间内，数据的访问是安全的。具体来说：\n如果一个值有一个或多个不可变引用，不能同时有可变引用。 如果一个值有一个可变引用，不能有其他的不可变或可变引用。 示例：借用规则的应用\n1 2 3 4 5 6 7 8 9 10 fn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); let r1 = \u0026amp;s; // 不可变引用 let r2 = \u0026amp;s; // 另一个不可变引用 println!(\u0026#34;{} and {}\u0026#34;, r1, r2); // 可以同时使用多个不可变引用 // let r3 = \u0026amp;mut s; // 编译错误：不可变引用存在时，不能创建可变引用 // println!(\u0026#34;{}\u0026#34;, r3); } 尝试在不可变引用存在的情况下创建可变引用，会导致编译错误。这一规则防止了数据竞争和不一致性。\n使用作用域来管理引用 Rust 的所有权和借用规则通过作用域来管理引用的生命周期。作用域是代码块 {} 内的范围，引用在其作用域结束时自动失效。\n示例：作用域中的借用\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn main() { let mut s = String::from(\u0026#34;hello\u0026#34;); { let r1 = \u0026amp;s; // 不可变引用 let r2 = \u0026amp;s; // 另一个不可变引用 println!(\u0026#34;{} and {}\u0026#34;, r1, r2); } // r1 和 r2 在此结束 let r3 = \u0026amp;mut s; // 现在可以创建可变引用 r3.push_str(\u0026#34;, world\u0026#34;); println!(\u0026#34;{}\u0026#34;, r3); } 在内部作用域结束后，r1 和 r2 的引用也随之结束，这时可以安全地创建 r3 作为 s 的可变引用。\n静态生命周期 有时候，引用的生命周期需要延续整个程序的运行周期，这时可以使用 'static 生命周期标注。'static 生命周期适用于存储在程序二进制中的字符串字面量等。\n1 2 3 4 fn main() { let s: \u0026amp;\u0026#39;static str = \u0026#34;I have a static lifetime.\u0026#34;; println!(\u0026#34;{}\u0026#34;, s); } 在这个例子中，字符串字面量 \u0026quot;I have a static lifetime.\u0026quot; 的生命周期是 'static，因为它存储在程序的二进制中，直到程序结束。\n所有权与借用的最佳实践 尽量使用不可变引用：在不需要修改数据时，优先使用不可变引用，这有助于提高代码的安全性和并发性。 避免不必要的所有权转移：通过借用而不是转移所有权，可以减少内存开销和提高性能。 明确生命周期：在复杂的函数和数据结构中，合理地使用生命周期标注，确保引用的有效性和安全性。 理解借用规则：熟练掌握 Rust 的借用规则，避免常见的所有权和借用错误，如悬垂引用和数据竞争。 实战示例：函数返回引用 在某些情况下，函数需要返回引用，这时必须明确生命周期标注，确保返回的引用在调用者使用时仍然有效。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fn main() { let string1 = String::from(\u0026#34;abcd\u0026#34;); let string2 = String::from(\u0026#34;xyz\u0026#34;); let result = longest(\u0026amp;string1, \u0026amp;string2); println!(\u0026#34;The longest string is {}\u0026#34;, result); } fn longest\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;a str) -\u0026gt; \u0026amp;\u0026#39;a str { if x.len() \u0026gt; y.len() { x } else { y } } 在这个例子中，函数 longest 接受两个字符串切片的引用，并返回其中较长的那个。生命周期标注 'a 表明返回的引用的生命周期与输入引用的生命周期一致，确保返回的引用在使用时是有效的。\n使用结构体与生命周期 当结构体包含引用时，也需要使用生命周期标注，确保结构体中的引用在结构体本身存在期间有效。\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct ImportantExcerpt\u0026lt;\u0026#39;a\u0026gt; { part: \u0026amp;\u0026#39;a str, } fn main() { let novel = String::from(\u0026#34;Call me Ishmael. Some years ago...\u0026#34;); let first_sentence = novel.split(\u0026#39;.\u0026#39;).next().expect(\u0026#34;Could not find a \u0026#39;.\u0026#39;\u0026#34;); let excerpt = ImportantExcerpt { part: first_sentence, }; println!(\u0026#34;Excerpt: {}\u0026#34;, excerpt.part); } 在这个例子中，ImportantExcerpt 结构体包含一个对字符串切片的引用。生命周期标注 'a 确保 part 字段的引用在 ImportantExcerpt 实例存在期间是有效的。\n所有权与借用在并发中的应用 Rust 的所有权与借用系统不仅保证了单线程中的内存安全，也在并发编程中发挥重要作用。通过 Arc（原子引用计数）和 Mutex（互斥锁），可以在多线程环境中安全地共享和修改数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 use std::sync::{Arc, Mutex}; use std::thread; fn main() { let counter = Arc::new(Mutex::new(0)); // 使用 Arc 和 Mutex 共享数据 let mut handles = vec![]; for _ in 0..10 { let counter = Arc::clone(\u0026amp;counter); // 克隆 Arc，增加引用计数 let handle = thread::spawn(move || { let mut num = counter.lock().unwrap(); // 获取锁 *num += 1; }); handles.push(handle); } for handle in handles { handle.join().unwrap(); // 等待所有线程完成 } println!(\u0026#34;Result: {}\u0026#34;, *counter.lock().unwrap()); } 在这个例子中，counter 被多个线程共享和修改。通过 Arc 实现引用计数，允许多个所有者共享数据；通过 Mutex 确保同时只有一个线程能够修改数据，从而保证线程安全。\n所有权与借用是 Rust 内存安全性的基石，通过严格的所有权规则和借用机制，Rust 在编译时确保了内存访问的安全性，避免了悬垂引用、数据竞争等常见的内存错误。\n生命周期 生命周期（Lifetimes）是 Rust 中一个关键但较为抽象的概念，它用于确保引用的有效性，防止悬垂引用（dangling references）和数据竞争（data races）。\n生命周期的基本概念 在 Rust 中，每个引用都有一个生命周期，生命周期描述了引用在内存中的有效范围。生命周期确保在引用有效期间，被引用的数据不会被销毁，从而避免悬垂引用。Rust 编译器通过生命周期检查来验证代码的安全性，确保引用的使用不会导致内存安全问题。\n生命周期注解的语法 生命周期注解使用撇号（'）加一个小写字母来表示，例如 'a。生命周期注解本身并不影响运行时行为，它们只是帮助编译器理解引用之间的关系。\n函数中的生命周期注解\n当函数的参数和返回值中包含引用时，编译器需要知道这些引用的生命周期关系。通过生命周期注解，可以明确指定不同引用之间的依赖关系。\n示例 1：简单的生命周期注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fn main() { let string1 = String::from(\u0026#34;abcd\u0026#34;); let string2 = String::from(\u0026#34;xyz\u0026#34;); let result = longest(\u0026amp;string1, \u0026amp;string2); println!(\u0026#34;The longest string is {}\u0026#34;, result); } fn longest\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;a str) -\u0026gt; \u0026amp;\u0026#39;a str { if x.len() \u0026gt; y.len() { x } else { y } } 在这个例子中，函数 longest 接受两个字符串切片的引用，并返回其中较长的那个引用。生命周期注解 'a 表示返回的引用 \u0026amp;'a str 至少与参数 x 和 y 中生命周期较短的一个一样长。这确保了返回的引用在调用者使用时仍然有效。\n示例 2：多个生命周期参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 fn main() { let string1 = String::from(\u0026#34;hello\u0026#34;); let string2 = String::from(\u0026#34;world\u0026#34;); let result = longest_with_an_announcement(\u0026amp;string1, \u0026amp;string2, \u0026#34;比较完成\u0026#34;); println!(\u0026#34;The longest string is {}\u0026#34;, result); } fn longest_with_an_announcement\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt;( x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;b str, ann: \u0026amp;str, ) -\u0026gt; \u0026amp;\u0026#39;a str { println!(\u0026#34;Announcement: {}\u0026#34;, ann); if x.len() \u0026gt; y.len() { x } else { y } } 在这个示例中，函数 longest_with_an_announcement 接受三个参数，其中前两个是具有不同生命周期 'a 和 'b 的引用，第三个参数 ann 是一个具有更短生命周期的不可变引用。返回值的生命周期被指定为 'a，意味着返回的引用与 x 的生命周期相关联。这种明确的生命周期注解帮助编译器理解不同引用之间的关系，确保引用的有效性。\n生命周期在结构体中的应用 生命周期注解不仅在函数中使用，也可以用于结构体中。当结构体包含引用时，必须为这些引用指定生命周期，以确保结构体的实例在引用有效期间存在。\n示例 3：结构体中的生命周期注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct ImportantExcerpt\u0026lt;\u0026#39;a\u0026gt; { part: \u0026amp;\u0026#39;a str, } fn main() { let novel = String::from(\u0026#34;Call me Ishmael. Some years ago...\u0026#34;); let first_sentence = novel.split(\u0026#39;.\u0026#39;).next().expect(\u0026#34;Could not find a \u0026#39;.\u0026#39;\u0026#34;); let excerpt = ImportantExcerpt { part: first_sentence, }; println!(\u0026#34;Excerpt: {}\u0026#34;, excerpt.part); } 在这个例子中，结构体 ImportantExcerpt 包含一个对字符串切片的引用 part。生命周期注解 'a 表示 part 的引用必须在 ImportantExcerpt 实例存在期间有效。这样，编译器可以确保 excerpt 的生命周期不超过 novel 的生命周期，避免引用无效。\n生命周期省略规则（Lifetime Elision） 在许多情况下，Rust 编译器可以通过生命周期省略规则（Lifetime Elision Rules）自动推断引用的生命周期，无需显式标注。这些规则主要适用于简单的函数签名。\n生命周期省略规则：\n每个引用参数都有一个独立的生命周期参数。 如果只有一个输入生命周期参数，那个生命周期被赋给所有输出生命周期参数。 如果有多个输入生命周期参数，但其中一个是 \u0026amp;self 或 \u0026amp;mut self，那么 self 的生命周期被赋给所有输出生命周期参数。 示例 4：生命周期省略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn main() { let string1 = String::from(\u0026#34;abcd\u0026#34;); let string2 = String::from(\u0026#34;xyz\u0026#34;); let result = longest(\u0026amp;string1, \u0026amp;string2); println!(\u0026#34;The longest string is {}\u0026#34;, result); } // 生命周期省略后的函数签名 fn longest(x: \u0026amp;str, y: \u0026amp;str) -\u0026gt; \u0026amp;str { if x.len() \u0026gt; y.len() { x } else { y } } 在这个示例中，函数 longest 的签名省略了生命周期注解。根据生命周期省略规则，编译器自动推断返回值的生命周期与输入引用的生命周期一致。因此，这个函数与前面带有显式生命周期注解的 longest 函数功能相同。\n生命周期与泛型 生命周期注解可以与泛型参数一起使用，进一步增强代码的灵活性和安全性。通过为泛型类型参数添加生命周期参数，可以确保泛型类型中包含的引用的有效性。\n示例 5：泛型类型中的生命周期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct Book\u0026lt;\u0026#39;a, T\u0026gt; { title: \u0026amp;\u0026#39;a str, content: T, } fn main() { let title = String::from(\u0026#34;Rust Programming\u0026#34;); let content = vec![1, 2, 3]; let book = Book { title: \u0026amp;title, content, }; println!(\u0026#34;Book Title: {}\u0026#34;, book.title); } 在这个例子中，结构体 Book 包含一个对字符串切片的引用 title 和一个泛型类型 T 的字段 content。生命周期注解 'a 确保 title 的引用在 Book 实例存在期间有效。这样，编译器可以验证 book 的生命周期与 title 的生命周期一致，避免引用无效。\n生命周期与方法 生命周期注解在方法定义中也起着重要作用，尤其是当方法涉及引用的返回值时。通过在方法签名中指定生命周期，可以确保返回的引用在调用者使用时是有效的。\n示例 6：方法中的生命周期注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 struct Rectangle\u0026lt;\u0026#39;a\u0026gt; { width: \u0026amp;\u0026#39;a u32, height: \u0026amp;\u0026#39;a u32, } impl\u0026lt;\u0026#39;a\u0026gt; Rectangle\u0026lt;\u0026#39;a\u0026gt; { fn area(\u0026amp;self) -\u0026gt; u32 { *self.width * *self.height } fn compare_height(\u0026amp;self, other: \u0026amp;Rectangle) -\u0026gt; bool { *self.height \u0026gt; *other.height } fn largest_dimension\u0026lt;\u0026#39;b\u0026gt;(\u0026amp;self, other: \u0026amp;\u0026#39;b Rectangle\u0026lt;\u0026#39;a\u0026gt;) -\u0026gt; \u0026amp;\u0026#39;a u32 { if self.width \u0026gt; other.width { self.width } else { other.width } } } fn main() { let width1 = 30; let height1 = 50; let width2 = 40; let height2 = 60; let rect1 = Rectangle { width: \u0026amp;width1, height: \u0026amp;height1, }; let rect2 = Rectangle { width: \u0026amp;width2, height: \u0026amp;height2, }; println!(\u0026#34;The area of rect1 is {}.\u0026#34;, rect1.area()); println!( \u0026#34;rect1 is taller than rect2: {}.\u0026#34;, rect1.compare_height(\u0026amp;rect2) ); let largest_width = rect1.largest_dimension(\u0026amp;rect2); println!(\u0026#34;The largest width is {}.\u0026#34;, largest_width); } 在这个示例中，结构体 Rectangle 包含对 width 和 height 的引用，具有生命周期 'a。方法 largest_dimension 使用了一个额外的生命周期 'b，用于标注 other 参数的引用。返回值的生命周期 'a 表示返回的引用与 self 的生命周期一致，确保返回的引用在调用者使用时是有效的。\n生命周期与闭包 生命周期注解在闭包中也可能涉及，尤其是当闭包捕获引用并返回它们时。编译器会根据闭包的上下文自动推断生命周期，但在复杂情况下，可能需要显式标注生命周期。\n示例 7：闭包中的生命周期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fn main() { let string = String::from(\u0026#34;Hello, Rust!\u0026#34;); let closure = |s: \u0026amp;str| -\u0026gt; \u0026amp;str { if s.len() \u0026gt; 5 { \u0026amp;s[0..5] } else { s } }; let result = closure(\u0026amp;string); println!(\u0026#34;Closure result: {}\u0026#34;, result); } 在这个例子中，闭包 closure 接受一个字符串切片的引用并返回其中一部分。编译器根据上下文自动推断生命周期，无需显式标注。\n高级生命周期用法 生命周期的使用不仅限于简单的函数和结构体，它们还可以用于更复杂的场景，如泛型函数、多生命周期参数、关联生命周期等。通过合理地使用生命周期注解，可以实现更灵活且安全的代码设计。\n示例 8：关联生命周期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct Pair\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt; { first: \u0026amp;\u0026#39;a str, second: \u0026amp;\u0026#39;b str, } fn main() { let first = String::from(\u0026#34;first\u0026#34;); let second = String::from(\u0026#34;second\u0026#34;); let pair = Pair { first: \u0026amp;first, second: \u0026amp;second, }; println!(\u0026#34;Pair: ({}, {})\u0026#34;, pair.first, pair.second); } 在这个示例中，结构体 Pair 包含两个不同生命周期 'a 和 'b 的引用。这允许 Pair 同时包含来自不同来源的引用，并确保它们在各自的生命周期内有效。\n生命周期与泛型约束 在泛型编程中，生命周期注解可以与泛型类型参数一起使用，进一步增强代码的灵活性和安全性。通过在生命周期注解中指定泛型类型参数的生命周期，可以确保泛型类型中包含的引用的有效性。\n示例 9：泛型函数中的生命周期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fn main() { let string1 = String::from(\u0026#34;abcd\u0026#34;); let string2 = String::from(\u0026#34;xyz\u0026#34;); let result = longest_with_char(\u0026amp;string1, \u0026amp;string2, \u0026#39;c\u0026#39;); println!(\u0026#34;The longest string containing \u0026#39;c\u0026#39; is {}\u0026#34;, result); } fn longest_with_char\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;a str, c: char) -\u0026gt; \u0026amp;\u0026#39;a str { if x.contains(c) { x } else { y } } 在这个例子中，函数 longest_with_char 接受两个字符串切片的引用和一个字符，并返回包含字符的较长字符串的引用。生命周期注解 'a 确保返回的引用与输入引用的生命周期一致，避免引用无效。\n生命周期与多态性 生命周期注解还可以与泛型和多态性结合使用，允许函数和结构体在不同的上下文中安全地操作引用。通过灵活地指定生命周期参数，可以实现高度可复用和安全的代码设计。\n示例 10：多态生命周期\n1 2 3 4 5 6 7 8 9 struct Holder\u0026lt;\u0026#39;a, T\u0026gt; { reference: \u0026amp;\u0026#39;a T, } fn main() { let value = 42; let holder = Holder { reference: \u0026amp;value }; println!(\u0026#34;Holder holds: {}\u0026#34;, holder.reference); } 在这个示例中，结构体 Holder 包含一个对泛型类型 T 的引用，具有生命周期 'a。这种设计允许 Holder 结构体在不同的上下文中持有不同类型的数据引用，同时确保引用在有效期间内安全使用。\n生命周期是 Rust 中确保引用安全的核心机制，通过生命周期注解，编译器能够在编译时检查引用的有效性，防止悬垂引用和数据竞争。理解生命周期的语法和功能，包括如何在函数、结构体和泛型中使用生命周期注解，是掌握 Rust 内存管理和编写安全高效代码的关键。\n通过上述示例，可以看到生命周期注解如何帮助编译器理解引用之间的关系，确保数据在引用有效期间不会被销毁或修改。尽管生命周期的概念较为抽象，但通过实践和不断的练习，可以逐步掌握其使用方法，充分发挥 Rust 的内存安全和性能优势。\n在实际编程中，建议多编写包含生命周期注解的函数和结构体，结合 Rust 的编译器提示和错误信息，不断优化代码的生命周期管理。此外，参考 Rust 官方文档和社区资源，如《The Rust Programming Language》（《Rust 程序设计语言》），可以进一步加深对生命周期的理解和应用。\n错误处理 错误处理是编程中不可或缺的一部分，Rust 通过其独特的错误处理机制，旨在在编译时捕捉潜在的问题，从而确保程序的可靠性和稳定性。Rust 提供了两种主要的错误处理类型：可恢复错误（Recoverable Errors） 和 不可恢复错误（Unrecoverable Errors）。\n可恢复错误与不可恢复错误 在 Rust 中，错误处理分为两类：\n可恢复错误（Recoverable Errors）：这些错误通常是由于外部因素引起的，如文件未找到、网络请求失败等。这类错误可以通过适当的处理机制进行恢复或提供替代方案。Rust 使用 Result 枚举来表示可恢复错误。\n不可恢复错误（Unrecoverable Errors）：这些错误通常是程序内部的逻辑错误，如数组越界访问、空指针引用等。这类错误通常无法通过程序恢复，因此 Rust 使用 panic! 宏来处理不可恢复错误。\n使用 Result 处理可恢复错误 Result 是一个泛型枚举，定义如下：\n1 2 3 4 enum Result\u0026lt;T, E\u0026gt; { Ok(T), Err(E), } Ok(T)：表示操作成功，并包含成功时的值。 Err(E)：表示操作失败，并包含错误信息。 示例：文件读取\n以下示例展示了如何使用 Result 类型处理文件读取中的可恢复错误。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 use std::fs::File; use std::io::{self, Read}; fn read_username_from_file() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut file = File::open(\u0026#34;username.txt\u0026#34;)?; // 尝试打开文件 let mut username = String::new(); file.read_to_string(\u0026amp;mut username)?; // 尝试读取文件内容 Ok(username) // 返回读取到的用户名 } fn main() { match read_username_from_file() { Ok(username) =\u0026gt; println!(\u0026#34;Username: {}\u0026#34;, username), Err(e) =\u0026gt; println!(\u0026#34;Error reading file: {}\u0026#34;, e), } } 在这个例子中：\nFile::open 和 file.read_to_string 返回 Result 类型。 ? 操作符用于简化错误处理。如果操作成功，返回 Ok 中的值；如果失败，立即返回 Err，并将错误传递给调用者。 在 main 函数中，通过 match 表达式处理 Result，根据成功或失败进行相应的处理。 使用 unwrap 和 expect\n有时候，为了简化代码，可以使用 unwrap 或 expect 方法直接获取 Result 中的值。如果是 Err，unwrap 会调用 panic!，而 expect 允许自定义错误信息。\n1 2 3 4 5 6 7 8 fn main() { let username = read_username_from_file().unwrap(); // 如果出错，程序会 panic println!(\u0026#34;Username: {}\u0026#34;, username); // 或者使用 expect 提供自定义错误信息 let username = read_username_from_file().expect(\u0026#34;Failed to read username from file\u0026#34;); println!(\u0026#34;Username: {}\u0026#34;, username); } 尽管 unwrap 和 expect 使代码更简洁，但它们在出错时会导致程序崩溃，因此仅在确定不会出错的情况下使用。\n错误传播与 ? 操作符 在处理多个可能出错的操作时，逐一检查每个 Result 会使代码冗长。Rust 提供了 ? 操作符来简化错误传播。\n1 2 3 4 5 fn read_username_from_file_shorter() -\u0026gt; Result\u0026lt;String, io::Error\u0026gt; { let mut username = String::new(); File::open(\u0026#34;username.txt\u0026#34;)?.read_to_string(\u0026amp;mut username)?; Ok(username) } 这里，? 操作符在遇到 Err 时会立即返回错误，否则会解包 Ok 中的值。这样，可以避免嵌套的 match 表达式，使代码更加简洁。\n使用 Option 处理可能不存在的值 虽然 Option 更常用于处理可能不存在的值，但它在错误处理中的某些场景也非常有用。Option 是一个泛型枚举，定义如下：\n1 2 3 4 enum Option\u0026lt;T\u0026gt; { Some(T), None, } Some(T)：表示存在某个值。 None：表示不存在值。 示例：查找元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fn find_index(vec: \u0026amp;Vec\u0026lt;i32\u0026gt;, target: i32) -\u0026gt; Option\u0026lt;usize\u0026gt; { for (index, \u0026amp;value) in vec.iter().enumerate() { if value == target { return Some(index); } } None } fn main() { let numbers = vec![10, 20, 30, 40]; match find_index(\u0026amp;numbers, 30) { Some(index) =\u0026gt; println!(\u0026#34;Found at index: {}\u0026#34;, index), None =\u0026gt; println!(\u0026#34;Not found\u0026#34;), } } 在这个例子中，find_index 函数返回 Option\u0026lt;usize\u0026gt;，表示目标值是否存在以及其索引位置。\n自定义错误类型 虽然标准库提供了许多错误类型，复杂的应用程序可能需要定义自己的错误类型以更精确地描述错误。自定义错误类型通常通过实现 std::fmt::Display 和 std::error::Error trait 来实现。\n示例：自定义错误类型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 use std::fmt; use std::error::Error; // 定义自定义错误类型 #[derive(Debug)] struct MyError { details: String, } impl MyError { fn new(msg: \u0026amp;str) -\u0026gt; MyError { MyError{details: msg.to_string()} } } // 实现 Display trait impl fmt::Display for MyError { fn fmt(\u0026amp;self, f: \u0026amp;mut fmt::Formatter) -\u0026gt; fmt::Result { write!(f, \u0026#34;{}\u0026#34;, self.details) } } // 实现 Error trait impl Error for MyError { fn description(\u0026amp;self) -\u0026gt; \u0026amp;str { \u0026amp;self.details } } // 使用自定义错误类型 fn might_fail(flag: bool) -\u0026gt; Result\u0026lt;(), MyError\u0026gt; { if flag { Ok(()) } else { Err(MyError::new(\u0026#34;Something went wrong\u0026#34;)) } } fn main() { match might_fail(false) { Ok(_) =\u0026gt; println!(\u0026#34;Success\u0026#34;), Err(e) =\u0026gt; println!(\u0026#34;Error: {}\u0026#34;, e), } } 在这个例子中，MyError 类型通过实现 Display 和 Error trait，使其能够与 Rust 的错误处理生态系统无缝集成。\n错误链与上下文 在复杂的应用程序中，错误可能会在多个层级传播，单纯的错误类型信息可能不够详细。Rust 提供了一些方法来增加错误的上下文信息，使调试更为方便。\n使用 map_err 增加上下文\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 use std::fs::File; use std::io::{self, Read}; #[derive(Debug)] enum MyError { Io(io::Error), Parse(std::num::ParseIntError), } impl From\u0026lt;io::Error\u0026gt; for MyError { fn from(error: io::Error) -\u0026gt; Self { MyError::Io(error) } } impl From\u0026lt;std::num::ParseIntError\u0026gt; for MyError { fn from(error: std::num::ParseIntError) -\u0026gt; Self { MyError::Parse(error) } } fn read_number_from_file() -\u0026gt; Result\u0026lt;i32, MyError\u0026gt; { let mut file = File::open(\u0026#34;number.txt\u0026#34;).map_err(|e| { println!(\u0026#34;Failed to open file: {}\u0026#34;, e); e })?; let mut contents = String::new(); file.read_to_string(\u0026amp;mut contents)?; let number: i32 = contents.trim().parse()?; Ok(number) } fn main() { match read_number_from_file() { Ok(n) =\u0026gt; println!(\u0026#34;Number: {}\u0026#34;, n), Err(e) =\u0026gt; println!(\u0026#34;Error: {:?}\u0026#34;, e), } } 在这个例子中，通过实现 From trait，将不同的错误类型转换为自定义的 MyError 类型，并使用 map_err 方法添加了更多的错误上下文信息。\npanic! 处理不可恢复错误 当遇到无法恢复的错误时，Rust 提供了 panic! 宏来终止程序。panic! 会打印错误信息，并在调试模式下打印堆栈跟踪。\n示例：使用 panic!\n1 2 3 4 fn main() { let v = vec![1, 2, 3]; println!(\u0026#34;{}\u0026#34;, v[99]); // 这将导致 panic，因为索引超出范围 } 运行上述代码时，程序会因为尝试访问超出范围的索引而崩溃，并输出类似如下的错误信息：\n1 2 thread \u0026#39;main\u0026#39; panicked at \u0026#39;index out of bounds: the len is 3 but the index is 99\u0026#39;, src/main.rs:3:20 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace 使用 panic! 的注意事项\n虽然 panic! 提供了一种简单的方式来处理严重错误，但在实际应用中应尽量避免频繁使用，因为它会导致程序崩溃。对于可预见的错误，优先使用 Result 类型进行处理。\npanic! 与 std::panic::catch_unwind 在某些场景下，可能需要捕获 panic!，以防止整个程序崩溃。Rust 提供了 std::panic::catch_unwind 来实现这一点，但需要注意，这通常只用于特定场景，如隔离危险代码。\n示例：捕获 panic!\n1 2 3 4 5 6 7 8 9 10 11 12 13 use std::panic; fn main() { let result = panic::catch_unwind(|| { println!(\u0026#34;About to panic!\u0026#34;); panic!(\u0026#34;This is a panic\u0026#34;); }); match result { Ok(_) =\u0026gt; println!(\u0026#34;No panic occurred\u0026#34;), Err(_) =\u0026gt; println!(\u0026#34;Panic occurred, but caught!\u0026#34;), } } 在这个例子中，catch_unwind 捕获了闭包中的 panic!，使得程序在发生 panic! 后仍能继续执行。\n使用第三方错误处理库 虽然 Rust 标准库提供了强大的错误处理机制，但在复杂项目中，使用第三方库可以进一步简化错误处理和增强功能。常用的错误处理库包括 anyhow 和 thiserror。\n示例：使用 anyhow 简化错误处理\nanyhow 提供了一个通用的错误类型 anyhow::Error，允许将不同类型的错误无缝地组合在一起。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 use anyhow::{Result, Context}; use std::fs::File; use std::io::Read; fn read_username_from_file() -\u0026gt; Result\u0026lt;String\u0026gt; { let mut file = File::open(\u0026#34;username.txt\u0026#34;) .with_context(|| \u0026#34;Failed to open username.txt\u0026#34;)?; let mut username = String::new(); file.read_to_string(\u0026amp;mut username) .with_context(|| \u0026#34;Failed to read username from file\u0026#34;)?; Ok(username) } fn main() -\u0026gt; Result\u0026lt;()\u0026gt; { let username = read_username_from_file()?; println!(\u0026#34;Username: {}\u0026#34;, username); Ok(()) } 在这个例子中，anyhow::Result 被用来简化错误处理，通过 with_context 方法添加了额外的错误上下文信息，使得错误信息更为详尽。\n示例：使用 thiserror 定义自定义错误\nthiserror 提供了一个方便的方式来定义和实现自定义错误类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 use thiserror::Error; use std::fs::File; use std::io::{self, Read}; #[derive(Error, Debug)] enum MyError { #[error(\u0026#34;IO error occurred: {0}\u0026#34;)] Io(#[from] io::Error), #[error(\u0026#34;Parse error occurred: {0}\u0026#34;)] Parse(#[from] std::num::ParseIntError), } fn read_number_from_file() -\u0026gt; Result\u0026lt;i32, MyError\u0026gt; { let mut file = File::open(\u0026#34;number.txt\u0026#34;)?; let mut contents = String::new(); file.read_to_string(\u0026amp;mut contents)?; let number: i32 = contents.trim().parse()?; Ok(number) } fn main() -\u0026gt; Result\u0026lt;(), MyError\u0026gt; { let number = read_number_from_file()?; println!(\u0026#34;Number: {}\u0026#34;, number); Ok(()) } 在这个例子中，thiserror 通过 #[derive(Error)] 自动为 MyError 枚举实现了 std::error::Error trait，并且通过 #[from] 属性自动生成了从其他错误类型到 MyError 的转换。\n错误处理最佳实践 优先使用 Result 类型处理可恢复错误：对于可能发生的错误，应优先使用 Result 类型进行显式的错误处理，而不是依赖于 panic!。\n利用 ? 操作符简化错误传播：在函数链中，使用 ? 操作符可以简化错误处理逻辑，使代码更为简洁和易读。\n提供上下文信息：在错误传播过程中，尽可能提供有意义的上下文信息，有助于调试和理解错误发生的原因。\n定义自定义错误类型：对于复杂项目，定义自己的错误类型可以更准确地描述错误，并与 Rust 的错误处理生态系统集成。\n避免不必要的 unwrap 和 expect：除非在明确知道不会出错的情况下，否则应避免使用 unwrap 和 expect，以防止程序因意外错误而崩溃。\n使用第三方错误处理库：在复杂的错误处理场景中，考虑使用 anyhow 和 thiserror 等第三方库，以简化错误处理流程并增强功能。\nRust 的错误处理机制通过 Result 和 Option 类型，以及 panic! 宏，提供了灵活而强大的工具来管理程序中的各种错误。Result 类型适用于可恢复错误，允许开发者显式地处理和传播错误信息；而 panic! 适用于不可恢复错误，迅速终止程序以避免潜在的内存安全问题。通过 ? 操作符和第三方库如 anyhow 和 thiserror，Rust 提供了简洁且高效的错误处理方式，使得编写健壮的 Rust 程序变得更加容易。\n","date":"2024-11-21T00:00:00Z","image":"https://echudet.github.io/p/rust%E9%9A%8F%E8%AE%B0/bg_hu_8d863d10e7926602.png","permalink":"https://echudet.github.io/p/rust%E9%9A%8F%E8%AE%B0/","title":"Rust随记"},{"content":" 为什么要学习Verilog 因为本人在学校的五个verilog的数逻实验都是借助了魔法海螺(ai)来完成的，临近考试，诚惶诚恐，故补此知识点。（并且之后的龙芯杯也需要使用）\n龙芯杯鸽了。最后是在小学期写CPU的时候真正学习了。\nVerliog知识点 Verilog 数据类型 基本数据类型\nwire reg 定义与用途 wire 用于声明连接不同模块或逻辑单元的信号，代表物理电路中的导线。 适用于组合逻辑和连续赋值（assign 语句）。 reg 用于在过程块（always 或 initial 块）中存储和保持值，类似于变量。 常用于时序逻辑（如寄存器）和组合逻辑中的状态存储。 赋值方式 只能通过连续赋值（assign）或模块实例化中的端口连接来驱动。 只能在过程块中通过阻塞赋值（=）或非阻塞赋值（\u0026lt;=）来驱动。 特性 不存储值，每当驱动信号发生变化时，wire 的值会立即更新。 可以被多个驱动源同时驱动（需要小心处理以避免竞争）。 能够存储值，保持其状态直到被新赋值。 在时序逻辑中用于描述寄存器和状态。 常见易错点\n错误使用赋值方式\n错误：在过程块外对 reg 进行赋值，或在连续赋值中对 reg 进行驱动。\n1 2 3 4 5 6 7 8 reg a; assign a = b \u0026amp; c; // 错误：不能使用 assign 语句驱动 reg 类型 //正确做法 reg a; always @(*) begin a = b \u0026amp; c; // 使用过程块内的赋值 end 错误：在过程块内对 wire 进行赋值。\n1 2 3 4 5 6 7 8 wire a; always @(*) begin a = b \u0026amp; c; // 错误：不能在过程块内赋值 wire 类型 end //正确做法 wire a; assign a = b \u0026amp; c; // 使用 assign 语句 混淆 wire 和 reg 的用途\n错误：将 reg 用于需要被多个模块驱动的信号，或者将 wire 用于需要存储状态的信号。\n1 2 3 4 5 6 7 8 9 10 11 // 错误示例：将需要存储状态的信号声明为 wire wire [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // 错误：wire 类型不能在过程块中赋值 end // 正确做法 reg [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // 使用 reg 类型 end 多重驱动信号\n错误：多个驱动源同时驱动同一个 wire 信号，可能导致竞态条件（race condition）或短路。\n1 2 3 wire a; assign a = b; assign a = c; // 错误：多个驱动源 解决方法：确保每个 wire 信号只有一个驱动源，或使用三态缓冲区控制信号驱动。\n忘记初始化 reg\n错误：在仿真中忘记初始化 reg，可能导致不确定的初始值。\n1 2 3 4 reg [3:0] count; always @(posedge clk) begin count \u0026lt;= count + 1; // count 未初始化，仿真时可能为 x end 解决方法：\n使用复位信号初始化 reg。 在仿真中使用 initial 块赋初值。 1 2 3 4 5 6 7 reg [3:0] count; always @(posedge clk or posedge rst) begin if (rst) count \u0026lt;= 4\u0026#39;b0000; else count \u0026lt;= count + 1; end 误解 reg 的硬件含义\n误区：reg 不一定对应硬件中的寄存器，取决于代码的具体描述。\n1 2 3 4 reg a; always @(*) begin a = b \u0026amp; c; // 组合逻辑中的 reg，实际硬件中不生成寄存器 end 解释：在组合逻辑中使用 reg 只是因为需要在过程块内赋值，并不意味着硬件中会有实际的寄存器。\n向量\n在Verilog中，向量类型用于表示多位信号或数据。与单比特（单一的0或1）信号不同，向量可以表示多个比特的组合，如字节（8位）、字（16位或32位）等。向量使得处理和传输多位数据更加便捷和高效。在Verilog中，向量通过指定范围来声明。范围由高位和低位索引组成，格式如下：\n1 2 3 4 5 6 7 8 9 10 11 wire [高位:低位] 信号名; reg [高位:低位] 信号名; //具体来说 wire [7:0] data_bus; // 8位数据总线，位7到位0 wire [3:0] nibble; // 4位半字节，位3到位0 reg [31:0] address; // 32位地址信号，位31到位0 //Verilog允许指定向量的高位到低位，也可以反向指定 wire [7:0] data_bus; // 位7是最高位，位0是最低位 wire [0:7] data_bus_rev; // 位0是最高位，位7是最低位 在verilog中，还允许对向量进行诸多操作，如连接与拼接、位选取与范围选择、位切片、赋值等，详细见下。\n连接与拼接\nVerilog允许通过连接运算符（{}）将多个信号拼接成一个向量，或将一个向量拆分成多个信号。\n1 2 3 4 5 6 7 8 9 10 11 12 wire [3:0] nibble1, nibble2; wire [7:0] byte; // 拼接 assign byte = {nibble1, nibble2}; // 将nibble1作为高位，nibble2作为低位 // 拆分 assign {nibble1, nibble2} = byte; // 从byte中提取高4位到nibble1，低4位到nibble2 //甚至还可以↓ {4{w}} //这等同于{w,w,w,w} {b,{3{a,b}}} //这等同于{b,a,b,a,b,a,b} 位选取与范围选择\nVerilog允许访问向量中的单个比特或一段范围。\n1 2 3 4 5 6 7 8 9 //单比特选取 wire [7:0] data_bus; wire bit7 = data_bus[7]; // 访问第7位 wire bit0 = data_bus[0]; // 访问第0位 //范围选取 wire [15:0] word; wire [7:0] lower_byte = word[7:0]; // 访问低8位 wire [7:0] upper_byte = word[15:8]; // 访问高8位 位切片（Bit Slicing）\n可以通过动态计算的索引来访问向量的一部分，但需要使用生成语句或任务/函数。\n1 2 3 4 5 6 7 wire [31:0] data; wire [7:0] byte0, byte1, byte2, byte3; assign byte0 = data[7:0]; assign byte1 = data[15:8]; assign byte2 = data[23:16]; assign byte3 = data[31:24]; 向量的赋值\n向量可以通过连续赋值（assign）或过程赋值（always块内）进行赋值。\n1 2 3 4 5 6 7 8 9 //连续赋值 wire [7:0] a, b, c; assign c = a \u0026amp; b; // 位与操作 //过程赋值 reg [3:0] counter; always @(posedge clk) begin counter \u0026lt;= counter + 1; end 缩减运算符 1 2 3 4 5 reg [3:0] B; reg C; C = \u0026amp;B; //相当于C =( (B[0]\u0026amp;B[1]) \u0026amp; B[2] ) \u0026amp; B[3]; 整数，实数，时间寄存器变量\n整数（integer）\n整数类型用关键字 integer 来声明。声明时不用指明位宽，位宽和编译器有关，一般为32 bit。reg 型变量为无符号数，而 integer 型变量为有符号数。例如：\n1 2 3 4 5 6 7 8 9 10 reg [31:0] data1 ; reg [7:0] byte1 [3:0]; //数组变量 integer j; //整型变量，用来辅助生成数字电路 always@* begin for (j=0; j\u0026lt;=3;j=j+1) begin byte1[j] = data1[(j+1)*8-1 : j*8]; //把data1[7:0]…data1[31:24] 依次赋值给 byte1[0][7:0]…byte[3][7:0] end end integer 信号 j 作为辅助信号，将 data1 的数据依次赋值给数组 byte1。综合后实际电路里并没有 j 这个信号，j 只是辅助生成相应的硬件电路。\n实数（real）\n实数用关键字 real 来声明，可用十进制或科学计数法来表示。实数声明不能带有范围，默认值为 0。如果将一个实数赋值给一个整数，则只有实数的整数部分会赋值给整数。例如：\n1 2 3 4 5 6 7 8 9 10 real data1 ; integer temp ; initial begin data1 = 2e3 ; data1 = 3.75 ; end initial begin temp = data1 ; //temp 值的大小为3 end 时间（time）\nVerilog 使用特殊的时间寄存器 time 型变量，对仿真时间进行保存。其宽度一般为 64 bit，通过调用系统函数 $time 获取当前仿真时间。例如：\n1 2 3 4 5 time current_time ; initial begin #100 ; current_time = $time ; //current_time 的大小为 100 end 数组\n在 Verilog 中允许声明 reg, wire, integer, time, real 及其向量类型的数组。数组维数没有限制。线网数组也可以用于连接实例模块的端口。数组中的每个元素都可以作为一个标量或者向量，以同样的方式来使用，形如：\u0026lt;数组名\u0026gt;[\u0026lt;下标\u0026gt;]。对于多维数组来讲，用户需要说明其每一维的索引。例如：\n1 2 3 4 5 6 integer flag [7:0] ; //8个整数组成的数组 reg [3:0] counter [3:0] ; //由4个4bit计数器组成的数组 wire [7:0] addr_bus [3:0] ; //由4个8bit wire型变量组成的数组 wire data_bit[7:0][5:0] ; //声明1bit wire型变量的二维数组 reg [31:0] data_4d[11:0][3:0][3:0][255:0] ; //声明4维的32bit数据变量数组 虽然数组与向量的访问方式在一定程度上类似，但不要将向量和数组混淆。向量是一个单独的元件，位宽为 n；数组由多个元件组成，其中每个元件的位宽为 n 或 1。它们在结构的定义上就有所区别。\n存储器\n存储器变量就是一种寄存器数组，可用来描述 RAM 或 ROM 的行为。例如：\n1 2 3 reg membit[0:255] ; //256bit的1bit存储器 reg [7:0] mem[0:1023] ; //1Kbyte存储器，位宽8bit mem[511] = 8\u0026#39;b0 ; //令第512个8bit的存储单元值为0 参数\n参数用来表示常量，用关键字 parameter 声明，只能赋值一次。例如：\n1 2 3 parameter data_width = 10\u0026#39;d32 ; parameter i=1, j=2, k=3 ; parameter mem_size = data_width * 10 ; 字符串\n字符串保存在 reg 类型的变量中，每个字符占用一个字节（8bit）。因此寄存器变量的宽度应该足够大，以保证不会溢出。字符串不能多行书写，即字符串中不能包含回车符。如果寄存器变量的宽度大于字符串的大小，则使用 0 来填充左边的空余位；如果寄存器变量的宽度小于字符串大小，则会截去字符串左边多余的数据。例如，为存储字符串 \u0026ldquo;hello-verilog\u0026rdquo;, 需要 13*8bit 的存储单元：\n1 2 3 4 reg [0: 13*8-1] str ; initial begin str = \u0026#34;hello-verilog\u0026#34;; end Verilog 操作符分类表 操作符类别 操作符 描述 示例 优先级 结合性 算术操作符 + 加法 a + b 14 左结合 - 减法 a - b 14 左结合 * 乘法 a * b 13 左结合 / 除法 a / b 13 左结合 % 取模（取余） a % b 13 左结合 ** 指数（幂运算） a ** b 12 右结合 - (一元) 取负（改变符号） -a 15 右结合 + (一元) 正号（通常不改变值） +a 15 右结合 关系操作符 \u0026lt; 小于 a \u0026lt; b 10 左结合 \u0026lt;= 小于或等于 a \u0026lt;= b 10 左结合 \u0026gt; 大于 a \u0026gt; b 10 左结合 \u0026gt;= 大于或等于 a \u0026gt;= b 10 左结合 等价操作符 == 相等 a == b 9 左结合 != 不相等 a != b 9 左结合 === 严格相等（包括X和Z） a === b 9 左结合 !== 严格不相等（包括X和Z） a !== b 9 左结合 逻辑操作符 \u0026amp;\u0026amp; 逻辑与 a \u0026amp;\u0026amp; b 5 左结合 || 逻辑或 a || b 5 左结合 ! 逻辑非 !a 15 右结合 按位操作符 \u0026amp; 按位与 a \u0026amp; b 8 左结合 | 按位或 a | b 8 左结合 ^ 按位异或 a ^ b 6 左结合 ~ 按位非 ~a 15 右结合 ~\u0026amp; 按位与非（NAND） ~\u0026amp;a 8 左结合 ~ 按位或非（NOR） ~a 8 左结合 ~^ 或 ^~ 按位异或非（XNOR） ~^a 或 ^~a 6 左结合 归约操作符 \u0026amp; 归约与（所有位进行逻辑与） \u0026amp;a - - | 归约或（所有位进行逻辑或） |a - - ^ 归约异或（所有位进行逻辑异或） ^a - - ~\u0026amp; 归约与非（所有位进行逻辑与后取反） ~\u0026amp;a - - ~ 归约或非（所有位进行逻辑或后取反） ~a - - ~^ 或 ^~ 归约异或非（所有位进行逻辑异或后取反） ~^a 或 ^~a - - 移位操作符 \u0026lt;\u0026lt; 逻辑左移 a \u0026lt;\u0026lt; 1 11 左结合 \u0026gt;\u0026gt; 逻辑右移 a \u0026gt;\u0026gt; 1 11 左结合 \u0026lt;\u0026lt;\u0026lt; 算术左移（通常与逻辑左移相同） a \u0026lt;\u0026lt;\u0026lt; 1 11 左结合 \u0026gt;\u0026gt;\u0026gt; 算术右移（保持符号位） a \u0026gt;\u0026gt;\u0026gt; 1 11 左结合 拼接操作符 {} 拼接多个信号或值为一个向量 {a, b} 3 左结合 {{}} 重复拼接（如 {3{a}} 表示 a 重复3次拼接） {{3{a}}} 3 左结合 条件操作符 ?: 条件（三元）操作符，例如 condition ? expr1 : expr2 sel ? a : b 2 右结合 Verilog 数值表示 Verilog HDL 有下列四种基本的值来表示硬件电路中的电平逻辑：\n0：逻辑 0 或 \u0026ldquo;假\u0026rdquo; 1：逻辑 1 或 \u0026ldquo;真\u0026rdquo; x 或 X：未知 z 或 Z：高阻 x 意味着信号数值的不确定，即在实际电路里，信号可能为 1，也可能为 0。\nz 意味着信号处于高阻状态，常见于信号（input, reg）没有驱动时的逻辑结果。例如一个 pad 的 input 呈现高阻状态时，其逻辑值和上下拉的状态有关系。上拉则逻辑值为 1，下拉则为 0 。\n整数数值表示方法\n数字声明时，合法的基数格式有 4 中，包括：十进制(\u0026rsquo;d 或 \u0026lsquo;D)，十六进制(\u0026lsquo;h 或 \u0026lsquo;H)，二进制（\u0026lsquo;b 或 \u0026lsquo;B），八进制（\u0026lsquo;o 或 \u0026lsquo;O）。数值可指明位宽，也可不指明位宽。\n1 2 3 4\u0026#39;b1011 // 4bit 数值 32\u0026#39;h3022_c0de // 32bit 的数值 //下划线 _ 是为了增强代码的可读性 不指明位宽时，一般直接写数字，默认为十进制表示，例如下面的 3 种写法是等效的：\n1 2 3 counter = \u0026#39;d100 ; //一般会根据编译器自动分频位宽，常见的为32bit* counter = 100 ; counter = 32\u0026#39;h64 ; 负数表示，通常在表示位宽的数字前面加一个减号来表示负数。例如：\n1 2 3 4 5 6 -6\u0026#39;d15 -15 //-15 在 5 位二进制中的形式为 5\u0026#39;b10001, 在 6 位二进制中的形式为 6\u0026#39;b11_0001 //需要注意的是，减号放在基数和数字之间是非法的 4\u0026#39;d-2 //非法说明 实数表示方法\n实数表示方法主要有两种方式\n1 2 3 4 5 6 7 8 9 10 //十进制 30.123 6.0 3.0 0.001 //科学计数法 1.2e4 //大小为12000 1_0001e4 //大小为100010000 1E-3 //大小为0.001 字符串表示方法\n字符串是由双引号包起来的字符队列。字符串不能多行书写，即字符串中不能包含回车符。Verilog 将字符串当做一系列的单字节 ASCII 字符队列。例如，为存储字符串 \u0026ldquo;hello-verilog\u0026rdquo;, 需要 13*8bit 的存储单元。\n1 2 3 4 reg [0: 13*8-1] str ; initial begin str = \u0026#34;hello-verilog\u0026#34;; end Verilog 状态机 有限状态机（Finite-State Machine，FSM），简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。状态机不仅是一种电路的描述工具，而且也是一种思想方法，在电路设计的系统级和 RTL 级有着广泛的应用。\n状态机类型\nVerilog 中状态机主要用于同步时序逻辑的设计，能够在有限个状态之间按一定要求和规律切换时序电路的状态。状态的切换方向不但取决于各个输入值，还取决于当前所在状态。 状态机可分为 2 类：Moore 状态机和 Mealy 状态机。\nMoore 型状态机\nMoore 型状态机的输出只与当前状态有关，与当前输入无关。输出会在一个完整的时钟周期内保持稳定，即使此时输入信号有变化，输出也不会变化。输入对输出的影响要到下一个时钟周期才能反映出来。这也是 Moore 型状态机的一个重要特点：输入与输出是隔离开来的。\nMealy 型状态机\nMealy 型状态机的输出，不仅与当前状态有关，还取决于当前的输入信号。Mealy 型状态机的输出是在输入信号变化以后立刻发生变化，且输入变化可能出现在任何状态的时钟周期内。因此，同种逻辑下，Mealy 型状态机输出对输入的响应会比 Moore 型状态机早一个时钟周期。\n状态机设计：3 段式\n众所周知，许多设计规范和最佳实践都推荐使用三段式状态机。这有助于确保设计的一致性和可靠性，尤其在团队合作和大规模项目中尤为重要。将状态机分为三个独立的部分（状态寄存器、下一个状态逻辑、输出逻辑）使得设计更加模块化。每个部分有明确的职责，便于理解和管理。由于各部分职责分明，修改或扩展某一部分（如添加新的状态或修改输出逻辑）不会影响其他部分。这大大提高了设计的可维护性和可扩展性。并且，通过将状态寄存器与状态转移逻辑分离，避免了时序逻辑和组合逻辑的混杂。这有助于减少时序相关的问题，如竞态条件和冒险（hazards）。\n状态机设计如下：\n(0) 首先，根据状态机的个数确定状态机编码。利用编码给状态寄存器赋值，代码可读性更好。 (1) 状态机第一段，时序逻辑，非阻塞赋值，传递寄存器的状态。 (2) 状态机第二段，组合逻辑，阻塞赋值，根据当前状态和当前输入，确定下一个状态机的状态。 (3) 状态机第三代，时序逻辑，非阻塞赋值，因为是 Mealy 型状态机，根据当前状态和当前输入，确定输出信号。 下面给出一个简单的状态机代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 module fsm_example ( input wire clk, input wire rst_n, input wire input_signal, output reg output_signal ); // 状态编码 using localparam localparam [1:0] STATE_IDLE = 2\u0026#39;b00, STATE_ACTIVE = 2\u0026#39;b01, STATE_DONE = 2\u0026#39;b10; // 状态寄存器 reg [1:0] current_state, next_state; // 状态机第一段，时序逻辑，非阻塞赋值，传递寄存器的状态 always @(posedge clk or negedge rst_n) begin if (!rst_n) current_state \u0026lt;= STATE_IDLE; else current_state \u0026lt;= next_state; end // 状态机第二段，组合逻辑，阻塞赋值，根据当前状态和当前输入，确定下一个状态机的状态 always @(*) begin case (current_state) STATE_IDLE: begin if (input_signal) next_state = STATE_ACTIVE; else next_state = STATE_IDLE; end STATE_ACTIVE: begin next_state = STATE_DONE; end STATE_DONE: begin next_state = STATE_IDLE; end default: begin next_state = STATE_IDLE; end endcase end // 状态机第三代，时序逻辑，非阻塞赋值，因为是 Mealy 型状态机， // 根据当前状态和当前输入，确定输出信号 always @(*) begin case (current_state) STATE_IDLE: begin output_signal = 0; end STATE_ACTIVE: begin output_signal = 1; end STATE_DONE: begin output_signal = 0; end default: begin output_signal = 0; end endcase end endmodule Verilog 竞争与冒险 产生原因\n数字电路中，信号传输与状态变换时都会有一定的延时。\n在组合逻辑电路中，不同路径的输入信号变化传输到同一点门级电路时，在时间上有先有后，这种先后所形成的时间差称为竞争（Competition）。 由于竞争的存在，输出信号需要经过一段时间才能达到期望状态，过渡时间内可能产生瞬间的错误输出，例如尖峰脉冲。这种现象被称为冒险（Hazard）。 竞争不一定有冒险，但冒险一定会有竞争。 避免方法\n在编程时多注意以下几点，也可以避免大多数的竞争与冒险问题。\n1）时序电路建模时，用非阻塞赋值。 2）组合逻辑建模时，用阻塞赋值。 3）在同一个 always 块中建立时序和组合逻辑模型时，用非阻塞赋值。 4）在同一个 always 块中不要既使用阻塞赋值又使用非阻塞赋值。 5）不要在多个 always 块中为同一个变量赋值。 6）避免 latch 产生。 时序电路中非阻塞赋值可以消除竞争冒险。由于无法确定 a 与 b 阻塞赋值的操作顺序，就有可能带来竞争冒险。使用非阻塞赋值时，赋值操作是同时进行的，所以就不会带来竞争冒险。\n1 2 3 4 5 6 7 8 9 10 always @(posedge clk) begin a = b ; b = a ; end //不会带来竞争冒险 always @(posedge clk) begin a \u0026lt;= b ; b \u0026lt;= a ; end 我们想实现 C = A\u0026amp;B, F=C\u0026amp;D 的组合逻辑功能，用非阻塞赋值语句如下。两条赋值语句同时赋值，F \u0026lt;= C \u0026amp; D 中使用的是信号 C 的旧值，所以导致此时的逻辑是错误的，F 的逻辑值不等于 A\u0026amp;B\u0026amp;D。而且，此时要求信号 C 具有存储功能，但不是时钟驱动，所以 C 可能会被综合成锁存器（latch），导致竞争冒险。对代码修改后，F = C \u0026amp; D 的操作一定是在 C = A \u0026amp; B 之后，此时 F 的逻辑值等于 A\u0026amp;B\u0026amp;D，符合设计。\n1 2 3 4 5 6 7 8 9 always @(*) begin C \u0026lt;= A \u0026amp; B ; F \u0026lt;= C \u0026amp; D ; end //fix ↓ always @(*) begin C = A \u0026amp; B ; F = C \u0026amp; D ; end 同一个 always 块中建立时序和组合逻辑模型时，用非阻塞赋值。虽然时序电路中可能涉及组合逻辑，但是如果赋值操作使用非阻塞赋值，仍然会导致如规范 1 中所涉及的类似问题。\n1 2 3 4 5 6 7 8 always @(posedge clk or negedge rst_n) if (!rst_n) begin q \u0026lt;= 1\u0026#39;b0; end else begin q \u0026lt;= a \u0026amp; b; //即便有组合逻辑，也不要写成：q = a \u0026amp; b end end 同一个 always 块中不要既使用阻塞赋值又使用非阻塞赋值。always 涉及的组合逻辑中，既有阻塞赋值又有非阻塞赋值时，会导致意外的结果，例如下面代码描述。此时信号 C 阻塞赋值完毕以后，信号 F 才会被非阻塞赋值，仿真结果可能正确。但如果 F 信号有其他的负载，F 的最新值并不能马上传递出去，数据有效时间还是在下一个触发时刻。此时要求 F 具有存储功能，可能会被综合成 latch，导致竞争冒险。\n1 2 3 4 always @(*) begin C = A \u0026amp; B ; F \u0026lt;= C \u0026amp; D ; end 不要在多个 always 块中为同一个变量赋值。此时信号拥有多驱动端（Multiple Driver），是禁止的。当然，也不允许 assign 语句为同一个变量进行多次连线赋值。\nTestbench 概述 定义与目的\n定义：Testbench 是一个独立的Verilog模块，用于仿真和验证被测模块（Module Under Test，MUT）的功能。 目的：提供输入信号、监控输出信号、验证设计的正确性。 基本特点\n不包含端口（port）。 包含信号声明、被测模块实例化、刺激生成和输出监控等部分。 Testbench 的基本组成部分\ntimescale 指令\n1 2 3 4 `timescale \u0026lt;时间单位\u0026gt; / \u0026lt;时间精度\u0026gt; //\u0026lt;时间单位\u0026gt;：定义延迟和仿真时间的基本单位。 //\u0026lt;时间精度\u0026gt;：定义仿真内部时间的最小步长。 // 对于大多数数字电路设计，1ns / 1ps 或 10ns / 1ps 是常见且高效的选择。 时间单位（Time Unit）\n定义：时间单位指定了Verilog中时间延迟和仿真时间的基本单位。例如，1ns 表示1纳秒。 用途：用于定义延迟（如 #5）、仿真时间的显示格式等。 时间精度（Time Precision）\n定义：时间精度指定了时间单位中最小的时间步长。例如，1ps 表示时间步长为1皮秒。 用途：影响仿真中时间的分辨率和计算精度。 模块声明\n使用 module 关键字声明Testbench模块。 不需要端口列表。 1 module testbench_name; 信号声明\n输入信号：通常声明为 reg 类型，因为它们将在过程块中被驱动。 输出信号：声明为 wire 类型，因为它们由被测模块驱动。 1 2 3 4 reg clk; reg rst_n; reg en; wire [3:0] count; 被测模块实例化\n使用被测模块的名称和实例名（如 uut，代表 Unit Under Test）。 通过端口映射（.port_name(signal_name)）连接Testbench中的信号与被测模块的端口。 1 2 3 4 5 6 module_name uut ( .clk(clk), .rst_n(rst_n), .en(en), .count(count) ); 生成输入信号\n时钟信号生成：通常使用 initial 块与 forever 循环结合 # 延迟来实现时钟信号。 复位信号生成：在 initial 块中设置复位信号的初始状态和变化。 其他输入信号生成：使用 initial 或 always 块控制其他输入信号的变化。 1 2 3 4 5 6 7 8 9 initial begin clk = 0; forever #5 clk = ~clk; // 生成时钟，周期10个时间单位 end initial begin rst_n = 0; #10 rst_n = 1; // 释放复位 end 监控输出信号\n使用系统任务如 $monitor、$display 来观察和记录被测模块的输出信号变化。 $monitor：实时监控指定信号的变化。 $display：在特定时刻输出信号信息。 1 2 3 initial begin $monitor(\u0026#34;Time=%0t | rst_n=%b | en=%b | count=%d\u0026#34;, $time, rst_n, en, count); end 结束仿真\n使用 $finish 任务在特定条件下结束仿真。 可以在 initial 块中通过延迟控制仿真结束的时机。 1 2 3 4 initial begin #100; // 仿真时间 $finish; // 结束仿真 end Testbench 的高级功能\n任务（Tasks）和函数（Functions）\n任务：封装重复使用的操作，如复位信号生成。 函数：封装返回值的操作，适用于计算和逻辑判断。 1 2 3 4 5 6 7 task reset; begin rst_n = 0; #10; rst_n = 1; end endtask 生成波形文件\n使用 $dumpfile 和 $dumpvars 生成波形文件（如 .vcd 文件），用于后续波形查看和分析。\n1 2 3 4 initial begin $dumpfile(\u0026#34;testbench.vcd\u0026#34;); // 指定波形文件名 $dumpvars(0, testbench_name); // 记录所有信号 end 断言（Assertions）\n使用 SystemVerilog 的断言功能自动检查设计的正确性，提高测试效率。\n1 2 3 always @(posedge clk) begin assert (count \u0026lt;= 4\u0026#39;d15) else $fatal(\u0026#34;Count overflowed!\u0026#34;); end Testbench 编写步骤总结\n模块声明：不包含端口，使用 module 和 endmodule 包围整个Testbench。 信号声明 使用 reg 类型声明将被驱动的输入信号。 使用 wire 类型声明被测模块的输出信号。 被测模块实例化：使用模块名称和实例名，连接Testbench中的信号与被测模块的端口。 生成输入信号：使用 initial 块和 always 块生成时钟、复位及其他输入信号。 监控输出信号：使用 $monitor、$display 等任务实时观察被测模块的输出。 结束仿真：使用 $finish 任务在合适的时机结束仿真。 常用的系统任务和函数\n$monitor\n持续监控并输出指定信号的变化。 语法：$monitor(\u0026quot;格式字符串\u0026quot;, 参数列表); $display\n在特定时刻输出信号信息。 语法：$display(\u0026quot;格式字符串\u0026quot;, 参数列表); $finish\n结束仿真。 语法：$finish; $dumpfile 和 $dumpvars\n生成波形文件，用于后续的波形分析。 语法： 1 2 $dumpfile(\u0026#34;filename.vcd\u0026#34;); $dumpvars(level, module_name); $stop\n暂停仿真，进入交互模式（部分仿真工具支持）。 语法：$stop; 语法补充 循环语句\n在Verilog HDL中也存在循环语句。可以综合的循环语句为for语句，并且这个for语句和C语句中的for语句使用形式完全一样。for语句的形式为：for (表达式1；表达式2；表达式3）语句；\n也即:for（循环变量赋初值；循环结束条件；循环变量增值）执行语句；上述的展开表达分别解释了表达形式中的表达式1，表达式2和表达式3的含义。可以看到，确实与C语言中的对应的for循环语句完全是一样的。\n对于循环语句来说，不太容易想象得出综合之后的效果，因为这方面并不直观。这一点确实是这样，对于循环来说，综合器处理起来也并不容易，并且对于不同的综合器来说，不一定是可以综合的。相比于前面讨论的语句（赋值和判断语句），for循环语句描述的功能更加高层和抽象（也包括其它类型的循环语句）。虽然写程序容易，但是转化为硬件的难度会更大。即便转化完成，可能所需要的硬件资源也很多，效率不高。因此，除非是一些对语句进行重复设置的情况，尽量不要使用循环语句，以免对综合器造成困扰。\n除了上述的for语句之外，在Verilog HDL中也有其它三个循环语句，分别为forever语句，repeat语句，while语句。其中forever语句会连续执行语句，主要在仿真中使用，用以生成周期性的波形，例如时钟信号。\n1 2 3 4 5 6 7 8 9 10 11 // repeat语句的使用格式为： repeat(循环次数的表达式) begin //语句或者语句块 end //单个语句不需要begin和end // while语句的使用格式为： while(循环执行的条件表达式) begin //语句或者语句块 end //单个语句不需要begin和end 由于repeat语句和while语句的功能实际上都可以通过for语句表达出来，另外也由于for语句在大部分的EDA工具中都是可以综合的，而repeat和while往往是不可综合的，因此在自己编写代码的时，如果需要生成可以综合的代码，尽量使用for语句来实现循环。但是，正如上面所说的一样，综合出来的效率不一定很高，应该谨慎使用。\nVerilog HDL的设计层次与风格 在Verilog HDL中，可以使用不同的方式来进行电路的设计，有的时候也会给初学者很大的困扰。因为语言有很大的灵活性，对于相同的电路可以有不同的设计方法。这种灵活性可能会对初学者来说不太好掌握。下面就通过一个1位全加器的简单例子来说明在Verilog HDL中的不同的设计层次与设计方法。\n1位的全加器的输入包括1位的低位进位cin，两个一位的输入信号a和b，输出则包括了一个当前位的和sum以及向高位的进位cout。全加器的电路图可以直接从1位全加器的真值表中获得，大部分的数字电路以及组成原理的教科书都有1位全加器的例子，可做参考。从1位全加器的真值表可以获得1位全加器的逻辑表达式：\nCarryOut = (¬ABCarryIn)+(A*¬BCarryIn)+(AB*¬CarryIn)+(ABCarryIn)\n=(BCarryIn)+(ACarryIn)+(A*B)\nSum = (¬A*¬BCarryIn) + (¬AB*¬CarryIn) + (A*¬B*¬CarryIn) +（ABCarryIn)\n当然，上面已经完成了一个一位全加器的设计，并且已经设计出了电路。也就是说，真正的工作已经完成了，这实际上就是传统的硬件设计方法。作为例子，在这里首先把一个已经实现的电路绘制出来，然后再逐步讨论使用Verilog HDL进行不同的描述。目的就是展示出真正的硬件之后，再展示不同的描述，用以说明硬件与对应的描述之间的关联。\n在上述的电路中，使用了三个非门（not），四个3输入的与门（and），三个2输入的与门（and），一个4输入的或门（or），一个3输入的或门（or）。这里的非门，与门和或门都是Verilog HDL中的内置的门电路，可以直接使用。这样，可以将上述电路中的每一条线进行命名，然后就可以直接构造出Verilog HDL的结构描述（调用门元件）。当然，如果是直接是输入线，就不必命名了，使用输入的名称即可，同理对于输出线可以使用输出的名称。\n1 2 3 4 5 6 7 8 9 //1位全加器的门级结构描述 module full_adder1(a,b,cin,sum,cout); input a,b,cin; output sum,cout; wire a_n, b_n, cin_n, sum_p1,sum_p2,sum_p3,sum_p4, cout_p1, cout_p2, cout_p3; not(a_n,a),(b_n,b),(cin_n,cin); and(sum_p1,a_n,b_n,cin),(sum_p2,a_n,b,cin_n),(sum_p3,a,b_n,cin_n),(sum_p4,a,b,cin),(cout_p1,b,cin),(cout_p2,a,cin),(cout_p3,a,b); or(sum,sum_p1,sum_p2,sum_p3,sum_p4),(cout,cout_p1,cout_p2,cout_p3); endmodule 可以看到，上述1位全加器的门级结构描述直接描述了各个门之间是如何连线的。由于是文本文件，因此连线不像图形那么直观。但是，从代码中的各个信号线的命名，以及对应于命名完成的信号线作为门级的调用参数来看，各个门原件的连线就比较清楚了。门级结构描述虽然不是最底层的描述（比如直接用晶体管搭建），但是已经非常接近底层的描述，可以被直接综合出来，使用元件进行直接映射即可。但是，门级结构描述的缺点也很明显，要求用户自行完成门级的设计，直接映射到硬件。这种方法一般用于设计比较简单的电路，或者设计非常高效工作的电路，方便综合器直接进行综合。当然，这里的门级结构描述还有一个作用，即其结构描述的方式可用于通过部分的逻辑电路模块来构造更加大型的电路模块。这个等讨论完其它两种描述之后再回顾一下会更加清楚。\n门级结构描述虽然方便了底层的综合器，但是对于编程来说不太方便，开发者希望能够进行更加高层的设计。一个选择就是将上述的逻辑表达式直接写到程序里面这就是数据流描述的方法。\n1 2 3 4 5 6 7 //1位全加器的数据流描述 module full_adder1(a,b,cin,sum,cout); input a,b,cin; output sum,cout; assign sum = (~a\u0026amp;~b\u0026amp;cin)|(~a\u0026amp;b\u0026amp;~cin)|(a\u0026amp;~b\u0026amp;~cin)|(a\u0026amp;b\u0026amp;cin); assign cout = (b\u0026amp;cin)|(a\u0026amp;cin)|(a\u0026amp;b); endmodule 这里的数据流描述方法，说明的就是在组合逻辑中，输出是如何随着输入数据的变化而变化，使用的是持续赋值语句assign。这些持续赋值语句说明了数据流的变化情况以及它们之间的逻辑关系。可以看到，持续赋值语句的抽象层次要比前面的门级结构描述更加抽象。只要有了逻辑表达式关系，直接翻译为Verilog HDL中的运算符即可，而不用仔细考虑底层的门电路构成。但是，数据流描述的抽象层次还不是很高，因为已经很难从给出的数据流中看到这是一个1位的全加器。实际上，对于复杂的硬件逻辑设计来说，使用行为级描述更为妥当，即直接描述出硬件所需要完成的功能，而不需要考虑这些硬件具体是如何实现的。具体如何实现交给EDA综合软件去做。\n1 2 3 4 5 6 7 8 //1位全加器的行为级描述 module full_adder1(a,b,cin,sum,cout); input wire a, b,cin; output reg sum,cout; always @* begin {cout,sum}=a+b+cin; end endmodule 从行为级描述中完全看不到最终的电路是怎样使用元件以及怎样布线的，但是这个程序非常简单明了。这对于开发人员来说极为方便，直接能够看出是一个全加器，因为这一段程序完整的描述了一个全加器所需要完成的功能。\n上面的结构级描述，数据流级描述以及行为级描述就是Verilog HDL开发过程中可以使用的三种不同层次的对于硬件的描述方法。结构级描述直接描述了硬件电路的结构，最为具体，但是不够抽象。数据流描述更加接近传统的逻辑设计，抽象程度中等。行为级描述只需要抽象描述一个硬件的功能单元完成什么样的功能即可，不需要说明硬件是如何构造的，最为抽象。在实际的设计过程中，这三种方式可以互相混合使用，针对不同的电路可以选择不同的描述方式。\n在设计更加大型的硬件电路的时候，使用结构级描述是必不可少的。在前面的例子中，已经看到是如何通过调用门级的基本逻辑单元来完成全加器的功能。这样的方法在设计更加大型的电路中也是相同的。可以设计一些小型的电路模块，然后通过结构的描述来设计出规模更大的电路。下面就通过设计4位的加法器来说明结构描述在设计大型电路时候的作用。\n通过对其中的信号进行命名，可以通过结构描述的方式来描述上述的电路。实际上，这里需要命名的信号就是进位到前一级的进位，可以分别命名为cin1，cin2，cin3。上述电路的描述如下。\n1 2 3 4 5 6 7 8 9 10 11 //1位全加器的行为级描述 module full_adder4(a,b,cin,sum,cout); input cin; input [3:0]a,b; output [3:0]sum; output cout; full_adder1 a0(a[0],b[0],cin,sum[0],cin1); full_adder1 a1(a[1],b[1],cin1sum[1],cin2); full_adder1 a2(a[2],b[2],cin2sum[2],cin3); full_adder1 a3(a[3],b[3],cin3sum[3],cout); endmodule 可以看到，这里的结构级描述与之前的门级结构描述在形式上是完全一样的。例 17.12门级结构描述调用的是语言内建的元件，但是在这里调用的是开发者自己的模块。同时，在这里也看到了调用的另外一种形式，即可以对每一次调用进行命名，分别命名为a0, a1, a2, a3。\n到现在为止，基本上对于Verilog HDL的编程语言进行了一个基本的介绍。当然，这里介绍的是Verilog HDL中的最基本的内容，希望能够帮助读者对Verilog HDL这样一个语言有一个最基本的了解。在实际进行硬件设计的时候，最基本的方法还是自顶向下的方法，对硬件的总体先分成多个互相独立的模块，然后定义模块之间的连线关系。之后，每一个独立的模块可以进行分别设计，连线关系即是它们之间的接口。最终完成的硬件通过结构描述的方式将设计完成的模块连接在一起。\n编程建议与经验 下面的内容与Verilog语言的语法不直接相关，但是会讨论一些经验以及常见的错误。这里的一些常见错误同学平时也会碰到，建议先阅读理解大概的含义，在具体编程碰到问题的时候也可以返回本节查看与理解。也注意多与同学讨论，少走弯路，减少调试难度。另外，一些具体实验中可能碰到的困难与错误已经列举在前面的实验提示部分。\n计算机组成原理的实验是硬件实验，使用的是硬件描述语言Verilog。做实验之前一定要首先熟悉一下Verilog硬件描述语言的语法以及惯用的方法。惯用的方法可以从例子中学习。当然，编写正确代码也需要经验。Verilog代码是硬件描述语言，这个跟软件的代码语言完全不同，不能想当然从软件的角度去思考。Verilog只是描述硬件模块应该达到的功能，但是没有描述内部的结构，不能直接说明内部是如何的。这里关键的一点是所有的硬件模块都是并行执行的，是信号从输入到输出的传递的过程。除了Verilog的语法之外，下面的一些与语言相关的提示需要大家注意一下，会有一些帮助。\n`default_nettype none\n`default_nettype none 这是建议的做法。在Verilog中，所有没有被定义的标记label都被默认认为是wire类型的。但是，这种默认的行为有的是非常危险的，比如在信号名字上出现拼写错误不会被探测出来。因此，建议在所有的源文件的开始加上这一句，取消默认行为。\n锁相环电路\nPLL是FPGA上专用的时钟生成模块，内部是模拟电路。在启动时需要一段时间才能进入稳定状态，所以有个locked信号输出，表示它稳定了。具体的可以参考前面关于锁相环电路的讨论。\n调时序：\n调时序：不同模块之间，由于寄存器的关系，会有相位差，然后需要增加几个空的状态机节拍。哪个path太长了就加寄存器打一拍。\n阻塞赋值语句和非阻塞赋值语句：\n关于=和\u0026lt;=：一般来说，组合逻辑用=，时序逻辑用\u0026lt;=。但是实际上wire和reg仅仅是语法层面的东西，assign的左值必须是wire，always里的左值必须是reg，否则综合就会报错。最终是否综合成触发器，是根据有没有时钟信号决定的。综合器怎么知道哪个是时钟信号呢？通过posedge的描述方法可以知道对应的模块里面需要响应正边沿（posedge）或者是负边沿（negedge），从而会综合出触发器。\n在Verilog的语法上，阻塞的过程赋值语句=，非阻塞的过程赋值语句\u0026lt;=。在always过程里面的赋值语句被称为是过程赋值语句，一般用以对reg类型的变量进行赋值。过程赋值语句分为两种类型，一个是非阻塞赋值语句（\u0026lt;=），一个是阻塞赋值语句（=）。它们之间的区别是：\n1）非阻塞non-blocking赋值语句（\u0026lt;=）在赋值语句出现的地方不是立即发生的，而是等到整个过程块结束的时候才发生。由于不是立即发生的，在过程内的描述中，仿佛这条语句不存在一样，因此被称为是非阻塞的。只是在过程的最后会执行所有的非阻塞赋值语句，在这个执行的过程中，所有的右值会维持原来的值不变。\n2）阻塞blocking赋值语句（=）在赋值语句出现的地方就立即完成赋值操作，左值立刻发生变化。一个块语句中存在多条阻塞赋值语句的话，这些阻塞赋值语句会按照先后顺序关系，一条一条的执行，前面的赋值语句没有执行完，后面的赋值语句不会执行。\n程序的可读性：\n写Verilog的代码和写软件应用程序的代码一样，有一件事情需要特别注意，就是提高程序的可读性，增加程序的可维护性。在选择信号名称的时候需要按照名称选择的惯例，有一些名称是常用的，按照其惯常的用法就可以了，不要改变其名字的用法。下面是一些名字使用的惯例，同学们应当遵守来提高自己程序的可读性。\n_i, _o，分别代表一个模块的输入信号和输出信号。\nn或者_n为后缀，表明这个信号是0使能。\nclk，clock时钟信号，后面或者前面接上频率，可以显示时钟信号的频率。\nrst，reset重置信号，使得信号可以重置，一般在重置响应中写入状态机的初值。\nwe，write enable信号，对应于模块的写入使能。\noe，output entable信号，对应于模块的读取使能。\nce，chip enable，对应于模块的总体使能信号。上述的信号几乎在所有的模块中都会有。（注意信号是正向的还是反向的，即1使能还是0使能。在说明书中，0使能会在信号名称的上面带有横线。）\nselect，sel，这个信号一般用于对芯片的选择。\n其它的信号也会有惯例使用的情况，大家在平时的时候可以多注意学习。\n代码检查工具：\n下面的网址中有一些Verilog的工具可以供大家参考。\nhttps://www.veripool.org/ verilator \u0026ndash;lint-only -Wall [source_files.v]\u0026hellip; 可以帮助做一些检查。\n一些特殊的语法点：\nram_address = pc[2+:21];\n含义就是从第2位开始的21位，就是2:22，就是把最后两位去掉。\n或者写成pc[22:2]也是一样的。\ncase语句可能出现错误的情况：\n在使用case的时候，一定要注意把所有的信号在所有的情况下写全了，要不然有一些信号就没有定义，会造成错误。或者可以灵活使用赋值语句，=，在过程最前面的时候先进行赋值。\n下面的case代码块是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 always @(*) begin ram_we_n = 1\u0026#39;b1; ram_oe_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end endcase end 下面的case语句代码块也是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 always @(*) begin case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_we_n = 1\u0026#39;b1; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end default: begin ram_oe_n = 1\u0026#39;b1; ram_we_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; end endcase end 但是下面的case语句代码块是错误的，为什么？（所有情况没有考虑完善）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 always @(*) begin case(state) STATE_FETCH: begin ram_oe_n = 1\u0026#39;b0; ram_address = pc[22:2]; end STATE_MEM: begin ram_oe_n = mem_op_write; ram_we_n = ~mem_op_write; ram_address = ex_val_o[22:2]; end default: begin ram_oe_n = 1\u0026#39;b1; ram_we_n = 1\u0026#39;b1; ram_address = 21\u0026#39;h0; end endcase end always过程语句的敏感信号\nVerilog规定，always@()中的是指该always块内的所有输入信号的变化为敏感列表，也就是仿真时只有当always@(*)块内的输入信号产生变化，该块内描述的信号才会产生变化。\nWarning: empty statement in seqential block\n如果两个分号放在一起的话\u0026quot;;;\u0026quot;\u0026quot;，就会出现这个警告。删掉一个分号即可。这里一个容易出现的错误是在信号常数定义define的时候，不小心在信号后面跟了一个分号，例如define OP_ADD 4\u0026rsquo;h9; 这里多了一个分号。在模块代码里面直接使用的时候就会出现上面的情况。\nVerilog的实战引入 为了解verilog的语法，此处通过实战引入\n示例程序：4位二进制计数器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // 模块声明，定义模块名和参数 module counter #( parameter WIDTH = 4 // 参数定义，计数器的位宽 )( input wire clk, // 输入信号：时钟 input wire rst_n, // 输入信号：同步复位（低有效） input wire en, // 输入信号：使能 output reg [WIDTH-1:0] count // 输出信号：计数值 ); // 内部信号声明 wire rst = ~rst_n; // 使用wire类型定义内部复位信号 // 计数逻辑 always @(posedge clk or posedge rst) begin if (rst) begin count \u0026lt;= 0; // 复位时计数器清零 end else if (en) begin count \u0026lt;= count + 1; // 使能时计数器加一 end end // 初始块，用于仿真时初始化 initial begin count = 0; end endmodule 语法规则详解\n模块声明 (module):\nmodule counter: 定义一个名为 counter 的模块。#(parameter WIDTH = 4): 定义参数 WIDTH，默认值为4，用于设置计数器的位宽。这使得模块更具通用性。\n端口声明:\ninput wire clk: 定义输入端口 clk，类型为 wire，表示时钟信号。 input wire rst_n: 定义输入端口 rst_n，类型为 wire，表示同步复位信号，低有效。 input wire en: 定义输入端口 en，类型为 wire，表示计数使能信号。 output reg [WIDTH-1:0] count: 定义输出端口 count，类型为 reg，位宽为 WIDTH，用于存储计数值。 内部信号 (wire): wire rst = ~rst_n;: 定义内部信号 rst，通过取反 rst_n 得到高有效的复位信号。\n行为描述 (always 块):\nalways @(posedge clk or posedge rst): 定义一个 always 块，敏感于 clk 的上升沿和 rst 的上升沿。 在always块内部： if (rst): 如果复位信号有效，则将 count 清零。 else if (en): 否则，如果使能信号有效，则将 count 加一。 使用非阻塞赋值 \u0026lt;= 来描述时序逻辑，确保在时钟边沿正确更新信号。 初始块 (initial 块):initial begin count = 0; end: 在仿真开始时，将 count 初始化为0。这在实际硬件中通常由复位信号处理，但在仿真中有助于明确初始状态。 initial过程语句不带出发条件且仅执行一次。initial语句通常用于仿真模块中对激励向量的描述，或者用于给寄存器变量赋初值。它是面向模拟仿真的过程语句，通常不能被逻辑综合工具支持。\n数据类型:\nwire: 用于连续赋值，表示信号之间的连接。 reg: 用于在 always 块中赋值，表示需要在时序逻辑中存储值的信号。 4位二进制计数器testbench 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 // counter_tb.v module counter_tb; // 输入信号 reg clk; reg rst_n; reg en; // 输出信号 wire [3:0] count; // 实例化被测模块 counter uut ( .clk(clk), .rst_n(rst_n), .en(en), .count(count) ); // 生成波形文件 initial begin $dumpfile(\u0026#34;counter_tb.vcd\u0026#34;); // 指定波形文件名 $dumpvars(0, counter_tb); // 记录所有信号 end // 生成时钟信号，周期10个时间单位 initial begin clk = 0; forever #5 clk = ~clk; // 每5个时间单位反转一次 end // 生成复位和使能信号的任务 task reset; begin rst_n = 0; en = 0; #10; rst_n = 1; #10; end endtask task enable_count; begin en = 1; #50; en = 0; end endtask // Testbench 主过程 initial begin // 初始化信号 rst_n = 0; en = 0; // 调用复位任务 reset; // 启动计数 enable_count; // 再次复位 reset; // 结束仿真 #20; $finish; end // 监控信号变化 initial begin $monitor(\u0026#34;Time=%0t | rst_n=%b | en=%b | count=%d\u0026#34;, $time, rst_n, en, count); end endmodule 示例程序：交通信号灯控制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 // 顶层模块声明 module traffic_light_controller ( input wire clk, // 时钟信号 input wire rst_n, // 复位信号，低有效 output reg red, // 红灯输出 output reg yellow, // 黄灯输出 output reg green // 绿灯输出 ); // 状态定义 using localparam localparam [1:0] RED_STATE = 2\u0026#39;b00, GREEN_STATE = 2\u0026#39;b01, YELLOW_STATE = 2\u0026#39;b10; // 当前状态和下一个状态 reg [1:0] current_state, next_state; // 连续赋值使用 assign wire rst = ~rst_n; // 状态转移逻辑 always @(posedge clk or posedge rst) begin if (rst) begin current_state \u0026lt;= RED_STATE; end else begin current_state \u0026lt;= next_state; end end // 下一个状态的组合逻辑 always @(*) begin case (current_state) RED_STATE: begin next_state = GREEN_STATE; end GREEN_STATE: begin next_state = YELLOW_STATE; end YELLOW_STATE: begin next_state = RED_STATE; end default: begin next_state = RED_STATE; end endcase end // 输出逻辑 always @(*) begin // 默认所有灯关闭 red = 0; yellow = 0; green = 0; case (current_state) RED_STATE: begin red = 1; end GREEN_STATE: begin green = 1; end YELLOW_STATE: begin yellow = 1; end endcase end endmodule 语法规则详解\nlocalparam:\n使用 localparam 定义本地参数，用于状态编码，避免在模块外部被修改。例如：\n1 2 3 4 localparam [1:0] RED_STATE = 2\u0026#39;b00, GREEN_STATE = 2\u0026#39;b01, YELLOW_STATE = 2\u0026#39;b10; assign 语句:\n用于连续赋值，适用于组合逻辑。在本例中，将rst定义为rst_n的反转：\n1 wire rst = ~rst_n; always @(\\*) 块:\nalways @(*) 表示组合逻辑块，自动包含所有右边信号作为敏感信号。用于定义下一个状态和输出逻辑：\n1 2 3 always @(*) begin // 组合逻辑内容 end case 语句:\n用于多分支条件选择，类似于编程语言中的 switch 语句。在状态转移和输出逻辑中使用：\n1 2 3 4 5 6 7 8 9 10 case (current_state) RED_STATE: begin next_state = GREEN_STATE; end // 其他状态 default:begin //default分支:在case语句中使用default处理未定义的情况 //.... end endcase 组合逻辑与时序逻辑的分离:\n使用不同的 always 块分别处理时序逻辑（状态转移）和组合逻辑（下一个状态及输出）。\n信号驱动类型:\nreg 类型用于在 always 块中赋值的信号，如 current_state 和输出信号 red、yellow、green。wire 类型用于连续赋值的信号，如内部复位信号 rst。\n示例程序：UART的收发 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 `timescale 1ns / 1ps //该模块实现了一个UART（通用异步收发传输器）接收器，用于接收串行数据并将其转换为并行数据。 //主要功能包括检测起始位、接收数据位、检测停止位，并在接收完成后输出有效信号和接收到的数据。 module uart_recv( input clk, input rst, input din, output reg valid, output reg [7:0] data ); localparam IDLE = 2\u0026#39;b00; localparam START = 2\u0026#39;b01; localparam DATA = 2\u0026#39;b10; localparam STOP = 2\u0026#39;b11; reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; reg din_r1, din_r2; wire rx_data = din_r2; always @(posedge clk or posedge rst) begin if(rst) begin din_r1 \u0026lt;= 1\u0026#39;b1; din_r2 \u0026lt;= 1\u0026#39;b1; end else begin din_r1 \u0026lt;= din; din_r2 \u0026lt;= din_r1; end end assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if(rst) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(current_state == IDLE) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(baud_counter == 14\u0026#39;d0) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else baud_counter \u0026lt;= baud_counter - 14\u0026#39;d1; end always @(posedge clk or posedge rst) begin if(rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end always @(*) begin case(current_state) IDLE: begin if(!rx_data) next_state = START; else next_state = IDLE; end START: begin if(baud_tick) next_state = (!rx_data) ? DATA : IDLE; else next_state = START; end DATA: begin if(baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if(baud_tick) // Sample stop bit next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end always @(posedge clk or posedge rst) begin if(rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(current_state != DATA) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; end always @(posedge clk or posedge rst) begin if(rst) data_reg \u0026lt;= 8\u0026#39;d0; else if(current_state == DATA \u0026amp;\u0026amp; baud_tick) data_reg \u0026lt;= {rx_data, data_reg[7:1]}; end always @(posedge clk or posedge rst) begin if(rst) begin valid \u0026lt;= 1\u0026#39;b0; data \u0026lt;= 8\u0026#39;d0; end else if(current_state == STOP \u0026amp;\u0026amp; baud_tick) begin valid \u0026lt;= 1\u0026#39;b1; data \u0026lt;= data_reg; end else begin valid \u0026lt;= 1\u0026#39;b0; end end endmodule //该模块实现了一个UART发送器，用于将并行数据转换为串行数据进行发送。 //主要功能包括发送起始位、数据位和停止位，并在发送完成后返回到空闲状态。 module uart_send( input clk, input rst, input valid, input [7:0] data, output reg dout ); localparam IDLE = 2\u0026#39;b00; localparam START = 2\u0026#39;b01; localparam DATA = 2\u0026#39;b10; localparam STOP = 2\u0026#39;b11; reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if (rst) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else if (current_state != IDLE) baud_counter \u0026lt;= (baud_counter == 14\u0026#39;d0) ? BAUD_COUNT_MAX - 14\u0026#39;d1 : baud_counter - 14\u0026#39;d1; else baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; end always @(posedge clk or posedge rst) begin if (rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end always @(posedge clk or posedge rst) begin if (rst) data_reg \u0026lt;= 8\u0026#39;d0; else if (valid \u0026amp;\u0026amp; current_state == IDLE) data_reg \u0026lt;= data; end always @(posedge clk or posedge rst) begin if (rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if (current_state == DATA \u0026amp;\u0026amp; baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; else if (current_state == IDLE) bit_cnt \u0026lt;= 3\u0026#39;d0; end always @(*) begin case (current_state) IDLE: begin if (valid) next_state = START; else next_state = IDLE; end START: begin if (baud_tick) next_state = DATA; else next_state = START; end DATA: begin if (baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if (baud_tick) next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end always @(posedge clk or posedge rst) begin if (rst) dout \u0026lt;= 1\u0026#39;b1; else begin case (current_state) IDLE: dout \u0026lt;= 1\u0026#39;b1; START: dout \u0026lt;= 1\u0026#39;b0; DATA: dout \u0026lt;= data_reg[bit_cnt]; STOP: dout \u0026lt;= 1\u0026#39;b1; default: dout \u0026lt;= 1\u0026#39;b1; endcase end end endmodule 设计思想：两者均采用有限状态机（FSM）的设计方法，定义了四个状态：IDLE（空闲）、START（起始）、DATA（数据）、STOP（停止）。这种设计有助于清晰地划分不同的操作阶段，便于管理和扩展。\n时钟生成与计数器：\nbaud_counter：用于生成波特率时钟脉冲。BAUD_COUNT_MAX 定义了一个波特周期的计数值（10416），对应于特定的波特率（如9600波特，假设时钟频率为100MHz）。 baud_tick：当 baud_counter 计数到0时产生一个波特率脉冲，用于状态转换和数据采样。 复位逻辑：在复位信号有效时，将状态机置于 IDLE 状态，并重置相关计数器和寄存器。\nuart_recv 模块详细分析\n信号声明\n1 2 3 4 5 6 7 8 9 10 11 12 reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; reg din_r1, din_r2; wire rx_data = din_r2; 状态相关信号：\ncurrent_state：当前状态。 next_state：下一个状态。 波特率计数器：\nbaud_counter：用于生成波特率脉冲。 BAUD_COUNT_MAX：波特率计数的最大值（10416）。 baud_tick：当 baud_counter 为0时产生脉冲。 数据相关信号：\nbit_cnt：数据位计数器，记录当前接收的数据位。 data_reg：临时存储接收的数据位。 data：输出的并行数据。 输入信号去抖动：\ndin_r1, din_r2：用于同步和去抖动串行输入信号 din。 rx_data：同步后的输入信号。 输入信号同步\n1 2 3 4 5 6 7 8 9 always @(posedge clk or posedge rst) begin if(rst) begin din_r1 \u0026lt;= 1\u0026#39;b1; din_r2 \u0026lt;= 1\u0026#39;b1; end else begin din_r1 \u0026lt;= din; din_r2 \u0026lt;= din_r1; end end 功能：将异步的串行输入 din 同步到本地时钟域，减少亚稳态问题。 初始状态：在复位时，将 din_r1 和 din_r2 设为1（空闲状态，UART线默认为高电平）。 波特率计数器\n1 2 3 4 5 6 7 8 9 10 11 12 assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if(rst) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(current_state == IDLE) baud_counter \u0026lt;= (BAUD_COUNT_MAX \u0026gt;\u0026gt; 1) - 14\u0026#39;d1; else if(baud_counter == 14\u0026#39;d0) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else baud_counter \u0026lt;= baud_counter - 14\u0026#39;d1; end 当在 IDLE 状态时，计数器初始化为 BAUD_COUNT_MAX / 2 - 1，以便在检测到起始位后，等待半个波特周期后开始采样，确保采样在中间位置。 在其他状态，计数器在每个波特周期递减，直到达到0时产生 baud_tick，并重新加载计数值。 状态寄存器更新\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if(rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end 功能：在每个时钟上升沿或复位信号有效时，更新当前状态。 下一个状态逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 always @(*) begin case(current_state) IDLE: begin if(!rx_data) next_state = START; else next_state = IDLE; end START: begin if(baud_tick) next_state = (!rx_data) ? DATA : IDLE; else next_state = START; end DATA: begin if(baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if(baud_tick) // Sample stop bit next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end IDLE： 等待检测到起始位（!rx_data）。 检测到起始位后，进入 START 状态。 START： 等待一个波特周期（baud_tick），确认起始位的稳定性。 如果仍然检测到低电平，进入 DATA 状态；否则，返回 IDLE。 DATA： 每个波特周期采样一个数据位。 计数器达到7（接收完8位数据）后，进入 STOP 状态；否则，继续 DATA 状态。 STOP： 等待一个波特周期采样停止位。 采样完成后，返回 IDLE 状态。 default： 遇到未定义状态时，返回 IDLE。 数据位计数器\n1 2 3 4 5 6 7 8 always @(posedge clk or posedge rst) begin if(rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(current_state != DATA) bit_cnt \u0026lt;= 3\u0026#39;d0; else if(baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; end 在 DATA 状态下，每个波特周期递增 bit_cnt。 在其他状态（IDLE、START、STOP）复位计数器。 数据寄存器\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if(rst) data_reg \u0026lt;= 8\u0026#39;d0; else if(current_state == DATA \u0026amp;\u0026amp; baud_tick) data_reg \u0026lt;= {rx_data, data_reg[7:1]}; end 在 DATA 状态下，每个波特周期将接收到的位 rx_data 右移并存入 data_reg。 实现串行到并行的数据转换。 输出信号生成\n1 2 3 4 5 6 7 8 9 10 11 always @(posedge clk or posedge rst) begin if(rst) begin valid \u0026lt;= 1\u0026#39;b0; data \u0026lt;= 8\u0026#39;d0; end else if(current_state == STOP \u0026amp;\u0026amp; baud_tick) begin valid \u0026lt;= 1\u0026#39;b1; data \u0026lt;= data_reg; end else begin valid \u0026lt;= 1\u0026#39;b0; end end 在 STOP 状态且 baud_tick 时，表示数据接收完成，设置 valid 为高，并将接收到的数据 data_reg 赋值给输出 data。 其他时间，valid 保持低。 uart_recv 模块通过四状态的有限状态机，实现了UART接收的基本功能，包括检测起始位、采样数据位、验证停止位，并在接收完成后输出有效信号和接收到的数据。关键设计点包括输入信号同步、波特率生成、状态机设计和数据采样等。\nuart_send 模块详细分析\n信号声明\n1 2 3 4 5 6 7 8 9 reg [1:0] current_state; reg [1:0] next_state; reg [13:0] baud_counter; localparam BAUD_COUNT_MAX = 14\u0026#39;d10416; wire baud_tick; reg [2:0] bit_cnt; reg [7:0] data_reg; 状态相关信号： current_state：当前状态。 next_state：下一个状态。 波特率计数器： baud_counter：用于生成波特率脉冲。 BAUD_COUNT_MAX：波特率计数的最大值（10416）。 baud_tick：当 baud_counter 为0时产生脉冲。 数据相关信号： bit_cnt：数据位计数器，记录当前发送的数据位。 data_reg：临时存储待发送的数据位。 dout：串行数据输出。 波特率计数器\n1 2 3 4 5 6 7 8 9 10 assign baud_tick = (baud_counter == 14\u0026#39;d0); always @(posedge clk or posedge rst) begin if (rst) baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; else if (current_state != IDLE) baud_counter \u0026lt;= (baud_counter == 14\u0026#39;d0) ? BAUD_COUNT_MAX - 14\u0026#39;d1 : baud_counter - 14\u0026#39;d1; else baud_counter \u0026lt;= BAUD_COUNT_MAX - 14\u0026#39;d1; end 在非 IDLE 状态下，计数器递减，直到达到0时产生 baud_tick，并重新加载计数值。 在 IDLE 状态，计数器保持在 BAUD_COUNT_MAX - 1，等待发送触发。 状态寄存器更新\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if (rst) current_state \u0026lt;= IDLE; else current_state \u0026lt;= next_state; end 功能：在每个时钟上升沿或复位信号有效时，更新当前状态。 数据寄存器\n1 2 3 4 5 6 always @(posedge clk or posedge rst) begin if (rst) data_reg \u0026lt;= 8\u0026#39;d0; else if (valid \u0026amp;\u0026amp; current_state == IDLE) data_reg \u0026lt;= data; end 在 IDLE 状态且 valid 为高时，将输入数据 data 载入 data_reg，准备发送。 数据位计数器\n1 2 3 4 5 6 7 8 always @(posedge clk or posedge rst) begin if (rst) bit_cnt \u0026lt;= 3\u0026#39;d0; else if (current_state == DATA \u0026amp;\u0026amp; baud_tick) bit_cnt \u0026lt;= bit_cnt + 3\u0026#39;d1; else if (current_state == IDLE) bit_cnt \u0026lt;= 3\u0026#39;d0; end 在 DATA 状态下，每个波特周期递增 bit_cnt，记录当前发送的数据位。 在 IDLE 状态复位计数器。 下一个状态逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 always @(*) begin case (current_state) IDLE: begin if (valid) next_state = START; else next_state = IDLE; end START: begin if (baud_tick) next_state = DATA; else next_state = START; end DATA: begin if (baud_tick \u0026amp;\u0026amp; bit_cnt == 3\u0026#39;d7) next_state = STOP; else next_state = DATA; end STOP: begin if (baud_tick) next_state = IDLE; else next_state = STOP; end default: next_state = IDLE; endcase end IDLE： 等待发送有效信号（valid）。 接收到 valid 后，进入 START 状态。 START： 等待一个波特周期（baud_tick），然后进入 DATA 状态。 DATA： 在每个波特周期发送一个数据位。 发送完第8位数据后，进入 STOP 状态。 STOP： 等待一个波特周期采样停止位，然后返回 IDLE 状态。 default： 遇到未定义状态时，返回 IDLE。 串行数据输出逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 always @(posedge clk or posedge rst) begin if (rst) dout \u0026lt;= 1\u0026#39;b1; else begin case (current_state) IDLE: dout \u0026lt;= 1\u0026#39;b1; START: dout \u0026lt;= 1\u0026#39;b0; DATA: dout \u0026lt;= data_reg[bit_cnt]; STOP: dout \u0026lt;= 1\u0026#39;b1; default: dout \u0026lt;= 1\u0026#39;b1; endcase end end IDLE：dout 保持高电平（UART空闲状态）。 START：发送低电平起始位。 DATA：按位发送数据，按照 bit_cnt 从 data_reg 中取出对应位。 STOP：发送高电平停止位。 default：默认保持高电平。 uart_send 模块通过四状态的有限状态机，实现了UART发送的基本功能，包括发送起始位、数据位和停止位。关键设计点包括数据寄存器加载、波特率生成、状态机设计和数据位发送等。\n","date":"2024-11-20T00:00:00Z","image":"https://echudet.github.io/p/intro-verilog/bg_hu_c9f3ded94099f5a2.png","permalink":"https://echudet.github.io/p/intro-verilog/","title":"Intro Verilog"},{"content":"Lecture 23 RCU 课前预习 论文速览 论文链接 在Introduction (简介)部分，介绍了Read-Copy Update (RCU)作为Linux内核中一种高性能的同步机制。它支持并发的读写操作，且自引入以来，RCU的使用不断扩大，成为Linux内核中的关键组件。文章回顾了RCU的发展背景，并说明了其高效性能在内核中带来的巨大优势。\n在RCU Requirements (RCU要求)部分提到，RCU在Linux内核中的实现需要满足三个关键要求：支持更新时的并发读取；低计算和存储开销；确定的操作完成时间，尤其是在实时响应和系统工程方面具有重要意义。\n在RCU Design (RCU设计)部分，详细介绍了RCU的设计，包括核心的两个基本操作：\nRCU读侧临界区：用来保证线程可以安全读取数据结构。 RCU同步：用来确保所有并发的读操作完成后再进行更新操作。通过这些机制，RCU允许多线程并发读取数据，同时其他线程可以更新数据。 在Using RCU (RCU的使用)部分，详细阐述了RCU在Linux内核中的多种使用模式：\n4.1 Wait for Completion (等待操作完成)：RCU用于等待预先存在的操作完成，以确保安全地移除数据。 4.2 Reference Counting (引用计数)：RCU作为引用计数的替代方案，提供了更高效的内存释放机制。 4.3 Type Safe Memory (类型安全内存)：确保在多线程操作中，被释放的内存不会被错误地重新分配。 4.4 Publish-Subscribe (发布-订阅模式)：提供一种安全发布数据的方式，确保并发读取操作安全。 4.5 Read-Write Lock Alternative (读写锁的替代方案)：RCU作为读写锁的高效替代方案，在并发读取场景下提供了更好的性能。 在Algorithmic Transformations (算法转换)部分，讨论了为了适应RCU而需要进行的几种常见算法转换：\n5.1 Impose Level of Indirection (引入间接层次)：通过间接访问数据结构来保证数据的一致性。 5.2 Mark Obsolete Objects (标记过时对象)：对数据对象进行标记，防止读取到已过时的数据。 5.3 Retry Readers (读者重试)：当检测到数据已被更新时，读线程可以选择重试操作。 在RCU Usage Statistics (RCU使用统计)部分，提供了RCU在不同Linux子系统中的使用统计数据，展示了自RCU引入以来的使用增长趋势，尤其是在网络栈、System V IPC以及其他内核子系统中的广泛应用。\n在Related Work (相关工作)部分，作者回顾了与RCU相关的其他并发控制机制，包括类似RCU的机制和不同的内存回收技术。这些机制在不同操作系统中被广泛使用。\n在Conclusions (结论)部分，总结了RCU在Linux内核中作为一种关键同步机制的成功，讨论了RCU的多种应用模式和转换方法，并展望了未来在内核中更多RCU应用的可能性。\nRCU的工作原理 RCU在Linux内核中的设计与工作原理，核心在于两个基本操作：\nRCU读侧临界区：使用rcu_read_lock和rcu_read_unlock实现。进入读侧临界区时，线程调用rcu_read_lock以防止被抢占，确保其对共享数据的读取不会与其他更新操作冲突。当线程退出临界区时调用rcu_read_unlock。 RCU同步：用 synchronize_rcu实现。这个操作确保在调用时所有正在执行的读侧临界区已结束，但不阻止新的读操作。该操作常用于等待之前的读操作完成，从而安全地进行数据更新或释放内存。 假设我们需要从目录缓存中删除一个dentry结构体（文件目录缓存项）。删除过程包含以下步骤：\n一个线程在读取dentry时会进入RCU读侧临界区，使用rcu_read_lock保护数据读取。 当需要删除dentry时，系统首先将其从目录缓存中移除（但不立即释放内存），然后调用synchronize_rcu等待当前所有对该dentry的读取操作完成。 在synchronize_rcu返回后，系统可以安全地释放这个dentry的内存，因为所有之前的读操作都已完成。 这种方法通过允许读者线程在临界区内访问数据，而更新线程可以异步等待所有读操作完成，从而避免锁定导致的性能损失和死锁。\n这种设计实现了RCU对高性能、并发访问和低开销的要求。在Linux内核中，这种机制广泛应用于诸如网络栈和文件系统等高并发场景。\nsynchronize_rcu 在Linux内核中，synchronize_rcu是一个重要的同步原语，用于确保所有现存的RCU读操作完成之后才继续执行接下来的更新或内存释放操作。下面我将详细解释synchronize_rcu的工作机制、使用场景及其背后的工作原理，并通过示例来说明其实际应用。\n在RCU (Read-Copy-Update) 中，读者线程可以无锁地访问共享数据，而写者线程在更新数据时则需要等待所有并发的读者线程完成当前的读取操作，以避免数据不一致或内存被过早释放。\n当写者线程想要修改或删除某个共享数据结构时，它不能直接对数据结构进行操作，因为可能有其他线程正在读取这个数据结构。为了避免这种情况，写者线程需要调用synchronize_rcu，该函数确保在它返回之前，所有使用过旧版本数据的读者线程都已经完成它们的临界区。\nsynchronize_rcu的基本工作流程是等待所有处理器（CPU）执行至少一次上下文切换。每个RCU的读侧临界区通过rcu_read_lock和rcu_read_unlock进行标记，当rcu_read_lock被调用时，它会禁止内核调度器抢占线程。这样，在临界区内，读者线程可以安全地读取数据结构，而不必担心写操作导致的并发问题。\nsynchronize_rcu不会阻止新的RCU读侧临界区的进入，而是等待所有当前活跃的RCU读侧临界区结束。一旦所有读者线程退出临界区，synchronize_rcu函数就会返回，此时可以保证之前版本的数据不会再被读取，从而可以安全地进行数据更新或内存释放。\n简化版的实现示例：\n1 2 3 4 void synchronize_rcu(void) { for_each_cpu(int cpu) run_on(cpu); } 这个伪代码的基本含义是让调用线程在每个CPU上运行一次，以确保所有CPU上的上下文切换已完成。这种设计避免了在每个读者线程之间进行显式的通信，极大地简化了同步机制的复杂性和性能开销。\nsynchronize_rcu主要用于以下几种典型场景：\n等待数据更新安全完成：当写者线程想要删除或更新一个数据结构时，它会首先从共享数据结构中移除旧数据（但不立即释放内存），并调用synchronize_rcu确保所有并发的读者线程完成对该数据的读取。只有在synchronize_rcu返回之后，旧数据的内存才可以被安全地释放。 实现类型安全的内存管理：在多线程环境下，写者线程在释放旧对象的内存时，需要确保不会有线程继续访问该对象。通过synchronize_rcu，写者线程可以等待所有的读操作结束，保证该内存块不再被引用，然后安全地释放内存。 引用计数替代方案：传统的引用计数需要在每次引用时都对共享计数器进行原子操作，这在高并发情况下性能较差。而RCU提供了一种替代方案：只需确保在删除对象前所有引用操作都完成，无需每次修改计数器。synchronize_rcu就是这种场景下的核心机制，确保对象被安全释放。 为了更好地理解synchronize_rcu的实际应用，以下是一个文件系统目录项缓存（dentry）的删除示例：\n在Linux的虚拟文件系统（VFS）中，目录项（dentry）缓存是一个共享的数据结构，频繁用于文件路径解析。假设某个线程要删除一个目录项，具体步骤如下：\n读取操作：读者线程通过rcu_read_lock进入读侧临界区，开始读取dentry结构的数据。在此过程中，它不会阻塞其他读者线程，也不受写者线程的影响。 删除操作：写者线程想要删除一个目录项时，首先将该目录项从缓存中移除，但不会立即释放内存。接下来，它会调用synchronize_rcu。 等待同步：synchronize_rcu会等待所有当前正在读取该目录项的读者线程完成读取操作。这是通过等待所有CPU上的上下文切换来实现的。 释放内存：当synchronize_rcu返回时，表示所有读操作都已经完成。此时，写者线程可以安全地释放该目录项的内存。 通过这样的设计，读者线程在访问共享数据时不需要加锁，从而避免了锁竞争和死锁的风险，提升了系统的并发性能。而写者线程则通过synchronize_rcu确保更新操作的安全性。\n优势 局限性 高性能：synchronize_rcu的最大优势是它允许读操作完全无锁化，极大地提高了高并发场景下的读取性能。 写操作开销较高：虽然读操作几乎是无开销的，但写操作（特别是synchronize_rcu的调用）可能会带来较高的延迟，尤其是在高并发的系统中。 延迟友好：在实时系统中，synchronize_rcu的确定性完成时间有助于避免不可预测的长时间延迟。 实时性限制：虽然有快速同步版本（如call_rcu），但在某些对实时性要求极高的系统中，等待所有CPU的上下文切换可能仍然不足够快。 简单的同步模型：写者只需等待所有读者完成操作，而不需要直接和每个读者进行通信。 \u0026hellip; synchronize_rcu是Linux内核中一种强大的同步工具，它通过等待所有读者线程完成操作，确保共享数据在更新或删除时的安全性。尽管它引入了一定的写操作开销，但它极大地提升了并发读取场景中的性能。因此，RCU及其synchronize_rcu函数在诸如网络栈、文件系统和虚拟文件系统等读密集型子系统中得到了广泛的应用。\n关于 synchronize_rcu 的深入探讨 在Linux内核的RCU机制中，synchronize_rcu 是关键的一环，负责确保数据的安全更新。它的作用是在修改某个数据结构之前，等待所有现存的读取操作完成，以便写操作可以在不干扰正在进行的读操作的前提下进行。\n当一个线程调用rcu_read_lock进入读侧临界区时，它被标记为正在读取一个RCU保护的数据结构。写者线程不能阻塞正在读取的线程，因此rcu_read_lock和rcu_read_unlock提供了一种轻量级的机制来避免锁竞争。\n在需要修改或删除数据结构时，写者线程调用synchronize_rcu，该函数会等待所有活跃的RCU读侧临界区结束。只有当所有现有的读操作完成后，synchronize_rcu才会返回，这样可以确保没有读者再访问旧版本的数据结构。\nsynchronize_rcu 通过等待内核中的每个CPU执行至少一次上下文切换来确认所有的RCU临界区都已经完成。这意味着当每个CPU完成一次任务切换时，所有当前的读侧临界区都会终止，确保任何线程不再访问旧数据。\n在实现层面，Linux并没有直接创建额外的线程在每个CPU上运行（尽管一些初步的实现看起来像是在这样做），而是通过一种更优化的方式来检查上下文切换是否完成。这种方式显著减少了等待开销，而不需要额外的系统资源去处理CPU间的调度。\n关于时间开销的问题是合理的：如果我们简单地让调用线程在每个CPU上运行一次，确实会产生较高的延迟。这个问题在早期的系统设计中显得尤为重要，特别是在高并发的系统中，涉及大量的CPU核心和繁忙的处理任务。\n不过，synchronize_rcu 的实现已经针对这一问题进行了优化，它不需要在每个CPU上调度一个独立的线程来执行上下文切换。实际上，它利用了系统的调度器和现有的上下文切换事件来跟踪CPU的状态。通过监视调度器的状态变化，synchronize_rcu 可以检测到每个CPU是否已经完成了必要的上下文切换。因此，它在性能上有较大的提升，而不必实际等待每个CPU独立运行。\n为了进一步减少延迟，Linux还提供了更快速的同步版本，称为call_rcu。call_rcu 是 synchronize_rcu的异步版本，允许线程在不需要同步等待的情况下发起一个回调函数来释放内存。当所有的读者线程退出RCU临界区后，这个回调函数会被调用，完成最终的内存清理工作。\n异步的call_rcu可以极大地减少线程的阻塞时间，尤其是在高并发的场景下。这对于内核中的许多时间敏感的子系统，例如网络栈和文件系统，都是至关重要的。\n一个实际例子：文件目录项（dentry）删除\n通过一个常见的例子来说明synchronize_rcu的实际应用：删除文件目录项（dentry）缓存。\n读侧操作：假设某个线程正在通过rcu_read_lock进入读侧临界区，读取文件目录项的相关信息。读操作不会被阻塞，它继续无锁地访问共享数据。 写侧操作：当需要删除一个dentry时，写者线程首先从目录缓存中移除该dentry，但不会立即释放它所占用的内存。为了确保安全，写者线程随后调用synchronize_rcu，等待所有并发的读者线程完成操作。 等待同步：在synchronize_rcu的执行期间，系统将等待所有正在执行的读操作完成。这是通过上下文切换跟踪来实现的。它不会阻塞新读者的进入，只是确保在调用时已经存在的读者完成任务。 安全释放内存：一旦synchronize_rcu返回，意味着没有线程还在访问被删除的dentry，此时系统可以安全地释放它所占用的内存。 synchronize_rcu 是Linux内核中处理并发更新操作的关键工具。尽管它需要等待系统中所有读者线程完成操作，但其设计通过上下文切换检测机制有效地避免了额外的系统开销。它在提升并发性能的同时，还为内核的许多子系统提供了稳定和确定的响应时间。对于高性能网络和文件系统等场景，它尤其具有显著的优势。\n异步版本的 synchronize_rcu: call_rcu call_rcu 是 Linux 内核 RCU 机制中用于延迟内存释放或资源清理的异步函数。与同步等待所有现有读操作完成的 synchronize_rcu 不同，call_rcu 会安排一个回调函数在所有 CPU 都完成至少一次上下文切换后执行。这种机制允许写者线程无需阻塞，继续执行其他操作，从而提高系统的并发性和性能。\ncall_rcu 的函数原型如下：\n1 void call_rcu(struct rcu_head *head, rcu_callback_t func); struct rcu_head *head：这是嵌入在要被释放或修改的对象中的 RCU 头结构，用于跟踪延迟操作。 rcu_callback_t func：指定的回调函数，当所有 RCU 临界区完成后调用这个函数。通常，这个回调函数包含释放内存或清理资源的代码。 在内核中，call_rcu 主要用于延迟资源回收，例如内存释放。以下是一个常见的使用场景，涉及删除数据结构中的对象：\n读取数据（读侧临界区）： 多个读者线程可能会在 RCU 读侧临界区内访问数据结构，通过 rcu_read_lock() 和 rcu_read_unlock() 确保不会被中断。 删除对象（写侧操作）： 当一个线程需要删除某个对象时，它会从数据结构中将该对象移除，但不立即释放内存。随后，调用 call_rcu，指定一个回调函数来在所有读者操作完成后释放内存。 1 2 3 4 5 6 7 8 9 10 11 void my_callback(struct rcu_head *head) { my_data_t *data = container_of(head, my_data_t, rcu); free(data); // 安全地释放内存 } void remove_data(my_data_t *data) { // 从数据结构中移除 data call_rcu(\u0026amp;data-\u0026gt;rcu, my_callback); // 异步安排内存清理 } 回调函数执行： 当所有 CPU 都完成至少一次上下文切换时，系统将调用 my_callback，安全地释放内存。 call_rcu 的优点\n非阻塞执行： 相比于 synchronize_rcu，call_rcu 不会让调用线程等待，因此在高并发系统中能够更高效地进行资源管理。 降低延迟： 对于实时或性能敏感的应用程序，call_rcu 的异步特性可以减少阻塞时间，避免长时间等待所有读者完成操作。 批处理更新： 由于 call_rcu 是非阻塞的，可以同时处理大量更新。内核的 RCU 实现会将多个 call_rcu 的调用批量处理，从而减少上下文切换的开销。 call_rcu 通过每个 CPU 的数据结构维护回调列表。当所有 CPU 都完成至少一次上下文切换时，系统会调用这些回调函数，执行延迟的清理操作。其具体的工作流程为：\n标记上下文切换：当调用 call_rcu 时，它不会立即执行回调，而是记录一个待处理的回调，并开始跟踪所有 CPU 的上下文切换。 检测上下文切换：RCU 系统周期性检查每个 CPU 是否完成了至少一次上下文切换，这个过程经过优化，避免了不必要的性能开销。 执行回调：当所有 CPU 都完成至少一次上下文切换时，系统会在安排的 CPU 上执行回调，进行内存释放或资源清理。 在实践中，call_rcu 具有极高的可扩展性，可同时处理数千个回调请求。通过批量处理上下文切换的检测，系统大大降低了每次调用的开销。\ncall_rcu 的一个重要应用案例是在 Linux 网络栈中。网络栈通常需要高吞吐量和低延迟，使用同步等待的方式效率不高。例如，在删除 IP 选项或更新路由表时，call_rcu 能够在所有读者完成后异步释放旧数据，从而避免了不必要的阻塞。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 void udp_sendmsg(sock_t *sock, msg_t *msg) { ip_options_t *opts; opts = rcu_dereference(sock-\u0026gt;opts); if (opts != NULL) { // 处理 IP 选项 } rcu_read_unlock(); } void setsockopt(sock_t *sock, int opt, void *arg) { if (opt == IP_OPTIONS) {s ip_options_t *old = sock-\u0026gt;opts; ip_options_t *new = arg; rcu_assign_pointer(sock-\u0026gt;opts, new); if (old != NULL) call_rcu(\u0026amp;old-\u0026gt;rcu, kfree_rcu); } } 在此示例中，call_rcu 确保在释放旧的 IP 选项之前，所有可能使用这些选项的线程都已经完成操作。\ncall_rcu 是一种延迟执行的异步机制，通过在所有 CPU 完成上下文切换后执行指定的回调函数，解决了高并发场景下的内存释放和资源管理问题。相比于 synchronize_rcu，它具有显著的性能优势和延迟优化，特别适用于网络栈、文件系统等读操作占优的高性能系统。\nLinux NMI Linux 的 NMI (Non-Maskable Interrupt) 系统是处理一种高优先级硬件中断的机制，这种中断无法被屏蔽或忽略。NMI 通常用于非常关键的任务，比如检测系统错误、硬件问题或执行高精度的性能监控。它的优先级高于普通中断，在某些情况下，即使系统中断被禁用或屏蔽，NMI 依然会被处理。其主要应用有如下几点：\n硬件错误检测：NMI 被广泛用于报告严重的硬件错误，比如内存纠错（ECC）错误、CPU 错误或其他硬件问题。当硬件检测到这些严重的错误时，它会触发 NMI，系统会立即进入 NMI 处理程序，快速响应这些错误。 性能监控：NMI 系统常用于高精度的性能监控工具，如 perf 或 OProfile，这些工具依赖 NMI 来收集系统性能数据。通过 NMI，这些工具能够定期中断 CPU，获取系统状态和性能计数器的精确信息。 看门狗定时器：NMI 还常用于看门狗定时器，当系统进入死锁或长时间无响应时，看门狗定时器会触发 NMI 来执行系统恢复操作，防止系统完全崩溃。 在 NMI 系统中，Linux 内核通常会结合 RCU (Read-Copy-Update) 机制来处理并发访问的同步问题，尤其是 NMI 处理程序的动态注册和注销。由于 NMI 具有非常高的优先级，并且可以在任何时间打断系统操作，传统的锁机制在此场景中可能会引发死锁或性能问题。因此，RCU 提供了一种无锁机制来管理 NMI 处理程序列表。RCU 在 NMI 系统中的具体作用：\n动态注册和注销 NMI 处理程序：在 NMI 系统中，处理程序可能会根据需要动态注册和注销。在注销处理程序时，需要确保没有 CPU 正在执行该处理程序。通过 RCU，可以在注销 NMI 处理程序后调用 synchronize_rcu 来等待所有 NMI 处理操作完成，确保不会有 CPU 再次访问已被删除的处理程序。 高并发性能：RCU 提供了一种无锁机制，允许多个 CPU 高效访问 NMI 处理程序列表。这对于依赖频繁调用 NMI 的应用（如性能监控工具）至关重要，可以避免因频繁的锁竞争而导致的性能瓶颈。 确定性的执行时间：在高优先级的 NMI 系统中，使用 RCU 机制可以保证进入和退出 NMI 处理程序的执行时间是确定的，这对于实时系统尤为关键。相比于读写锁等阻塞同步机制，RCU 能避免不确定的执行时间问题。 以下是一个使用 RCU 进行 NMI 处理程序注册和注销的简化代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 rcu_list_t nmi_list; spinlock_t nmi_list_lock; void handle_nmi() { rcu_read_lock(); // 进入 RCU 临界区 rcu_list_for_each(\u0026amp;nmi_list, handler_t cb) { cb(); // 调用每个 NMI 处理程序 } rcu_read_unlock(); // 退出 RCU 临界区 } void register_nmi_handler(handler_t cb) { spin_lock(\u0026amp;nmi_list_lock); rcu_list_add(\u0026amp;nmi_list, cb); // 注册新的 NMI 处理程序 spin_unlock(\u0026amp;nmi_list_lock); } void unregister_nmi_handler(handler_t cb) { spin_lock(\u0026amp;nmi_list_lock); rcu_list_remove(\u0026amp;nmi_list, cb); // 从列表中移除处理程序 spin_unlock(\u0026amp;nmi_list_lock); synchronize_rcu(); // 等待所有 NMI 操作完成 } 通过上面的代码，NMI 处理程序的注册和注销可以在高并发的环境下安全进行。synchronize_rcu 用于确保注销处理程序时，所有正在执行的 NMI 操作已经结束，防止无效指针访问导致的系统崩溃。\nLinux NMI 系统是用于处理高优先级硬件中断的关键机制，通常用于检测硬件故障、执行高精度性能监控以及管理看门狗定时器。结合 RCU 机制，NMI 系统能够实现动态的处理程序管理，并提高系统的并发性能和可靠性。\n传统锁与 RCU 在多线程或多处理器系统中，如何有效地同步共享数据的访问是一个关键问题。传统的锁机制和 RCU（Read-Copy-Update）是两种广泛使用的同步机制。以下是对比这两种同步方法的详细分析。\n传统锁的核心原理是通过互斥锁来确保只有一个线程可以访问某个共享资源，而其他线程必须等待，直到该资源被释放。这种方式包括多种类型的锁，比如互斥锁（mutex）、读写锁（read-write lock）等。\n互斥锁（Mutex）：互斥锁确保同一时刻只有一个线程能够访问共享数据。它会阻塞其他试图访问相同资源的线程，直到锁被释放。 读写锁（Read-Write Lock）：允许多个读者线程并发访问资源，但写者线程必须独占该资源。读者之间不会相互阻塞，但写者和其他读者之间互斥。 传统锁优点 传统锁缺点 简单直观：传统锁的使用方法非常简单明确，只需要在进入临界区前加锁，退出时解锁即可。 性能瓶颈：传统锁的一个主要问题是它可能导致性能瓶颈，尤其是在高并发读操作的场景中。读者线程和写者线程之间的互斥可能导致大量等待，进而拖慢整个系统的性能。 对写密集型操作有优势：传统锁对于写操作频繁的场景非常有效，因为它通过强制序列化写操作来确保数据一致性。 死锁风险：不正确的锁顺序或嵌套锁的使用可能导致死锁问题，即多个线程彼此等待锁释放，从而永久阻塞。 RCU 是 Linux 内核中一种非常高效的同步机制，专为读多写少的场景设计。它通过允许读者线程和写者线程同时访问共享资源，而不会彼此阻塞，从而极大地提高了并发性。\nRCU 的基本原理是读者线程不需要锁来访问数据，而写者线程在更新数据时，会首先创建数据的副本，然后通过同步机制（如 synchronize_rcu 或 call_rcu）来确保所有当前正在读取该数据的线程完成后，才会删除或释放旧数据。\nRCU优点 RCU缺点 高并发性：RCU 允许读者线程无锁访问数据，因此在大量读操作场景下，RCU 提供了极高的并发性。多个读者线程可以同时读取数据，而不会发生互相等待。 写操作的复杂性：虽然 RCU 在读操作上效率极高，但写操作需要复杂的同步机制，如 synchronize_rcu 来确保所有读者线程完成后再进行更新。这意味着在写密集型场景中，RCU 的性能可能并不优于传统锁。 低开销：RCU 不涉及复杂的锁操作，避免了传统锁中因加锁和解锁带来的性能开销。 内存消耗：RCU 的设计要求在更新数据时创建数据副本，因此在高频率更新的场景下，可能会导致内存使用量增加。 确定性时间：RCU 中的读操作具有确定性时间，不会像传统锁那样因锁争用而导致不确定的等待时间。 \u0026hellip; RCU 和传统锁在多核系统上的性能差异较为明显，尤其是在读操作占多数的场景下。\n读写锁 vs RCU\n读写锁 RCU 读操作 使用传统的读写锁来保护同样的读操作，在同样的系统中，获取和释放读锁的开销为 89 纳秒。当 CPU 核心数量增加时，读写锁的开销显著增加。在 16 核心的系统上，获取和释放读锁的时间增至 6654 纳秒。 RCU 的读操作不需要任何锁机制，因此非常快速。在一个典型的多核系统中，进入和退出 RCU 临界区（调用 rcu_read_lock 和 rcu_read_unlock）的开销大约是 6 纳秒，无论系统中有多少 CPU 核心，RCU 的读操作开销基本保持不变。 并发性 读写锁会在多个读者竞争锁时导致缓存失效，降低并发性能。 RCU 的并发读操作不会相互影响，读者线程之间不需要等待。 死锁风险 传统锁，特别是嵌套锁和多个锁的场景中，容易导致死锁。例如，如果一个线程获取了一个锁后被中断，而另一个线程同时尝试获取相同的锁，那么死锁可能发生。 RCU 由于其无锁的读操作，不存在读者线程之间的锁竞争，也不会出现读者线程和写者线程之间的死锁问题。只有在某些特殊情况下，写者线程可能会因为等待 RCU 临界区的完成而阻塞，但这不会导致传统意义上的死锁。 RCU 和传统锁的应用场景有所不同。\n适合使用传统锁的场景 适合使用 RCU 的场景 写操作占主导：在写操作频繁的场景下，传统锁机制更加适合，因为它能够通过强制互斥确保数据的一致性，避免复杂的同步机制。 读操作占主导：RCU 特别适合读操作占多数的场景，尤其是内核中的文件系统、网络栈等场景。在这些场景中，读者线程不需要等待其他读者或写者，系统能够提供极高的并发性能。 实时系统的精确控制：在某些实时系统中，需要严格的顺序保证，并且必须确保某些操作独占资源。在这种场景下，传统锁可能提供更好的控制。 高并发性和低延迟：对于对并发性和低延迟有较高要求的系统，RCU 是理想的选择。由于其读操作几乎无阻塞，因此可以减少延迟，提升系统的整体响应速度。 传统锁和 RCU 各有其优缺点，适合不同的应用场景。传统锁提供了简单、可靠的互斥访问方式，适用于写操作频繁的场景，但在高并发读操作下性能表现不佳。相比之下，RCU 是一种高效的并发同步机制，能够在读密集型场景中大幅提高性能，但它在写操作频繁的场景中表现稍弱。\nRCU 替代读写锁的典型场景\nLinux 内核中的 PID 哈希表 就是使用 RCU 替代读写锁的一个典型例子。PID 哈希表用于将进程标识符（PID）映射到进程信息。在这个例子中，读取者通过调用 rcu_read_lock 来获取进程信息，读取完成后调用 rcu_read_unlock 释放锁。而写者线程通过自旋锁来同步写操作。\n以下是一个简化的代码示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 process_t *pid_lookup(int pid) { process_t *p; rcu_read_lock(); // 进入 RCU 临界区 p = pid_table[pid_hash(pid)].process; if (p) atomic_inc(\u0026amp;p-\u0026gt;ref); // 增加引用计数 rcu_read_unlock(); // 退出 RCU 临界区 return p; } void pid_remove(int pid) { process_t **p; spin_lock(\u0026amp;pid_table[pid_hash(pid)].lock); // 使用自旋锁同步写操作 p = \u0026amp;pid_table[pid_hash(pid)].process; rcu_assign_pointer(p, NULL); // 安全发布 NULL spin_unlock(\u0026amp;pid_table[pid_hash(pid)].lock); if (*p) call_rcu(pid_free, *p); // 使用 RCU 异步释放旧数据 } 在这个例子中，RCU 的读操作是并发的，且不需要阻塞写操作。写操作通过自旋锁保护，并在合适的时间点调用 call_rcu 异步释放旧数据。\nLinux 的 slab 分配器 Linux 的 slab 分配器是一种内存管理机制，用于高效地管理和分配内核中频繁使用的小对象内存。它通过将一块大的内存页面分割成多个相同大小的对象，并维护对象的生命周期，以便快速分配和释放内存。与常规的动态内存分配器相比，slab 分配器有以下几个特点：\n高效性：slab 分配器通过减少内存碎片和频繁的内存分配开销来提高性能，特别是在需要频繁创建和销毁对象的场景中。 类型安全：slab 分配器为每种类型的对象创建特定的内存缓存区，每个缓存区中包含相同类型和大小的对象。因此，这些对象的内存分配和释放在类型上是安全的，避免了对象混淆。 缓存复用：对象在释放后，slab 不会立即将其内存返回给系统，而是将该对象保留在缓存中，供下次分配时复用。这样可以避免频繁的内存分配和释放操作。 类型安全 在 slab 分配器中，开发者可以设置一个特殊的 SLAB_DESTROY_BY_RCU 标志，这个标志会确保在 RCU 同步完成之前，不会将内存重新分配给不同类型的对象。如果使用 SLAB_DESTROY_BY_RCU 标志，意味着对象的内存不会在 RCU 同步完成之前被销毁或重新分配。这就实现了“类型安全内存”的效果。举个例子：\n内存分配：当内核使用 slab 分配器分配对象时，slab 会从一个页面（page）中分割出相同类型的对象，并在该页面中分配所需的内存。 内存回收：当一个页面中的所有对象都被释放后，该页面可以返回给系统。如果设置了 SLAB_DESTROY_BY_RCU 标志，那么在释放这些对象之前，RCU 会确保没有线程仍然在访问这些对象的内存。 类型安全：在这种方式下，即使某个线程在 RCU 临界区内仍然引用了一个对象，系统也能确保在对象的内存被重新分配给另一个对象类型之前完成同步，避免了类型混乱的问题。 反向页表是一个典型的使用类型安全内存的例子。反向页表用于管理物理页和虚拟地址之间的映射。在 Linux 内核中，每个物理页通过 page_t 表示，虚拟地址映射则通过 anon_vma_t 来管理。为了防止在线程引用的 anon_vma_t 被释放时重新分配给其他类型的对象，内核可以通过 RCU 机制实现类型安全的内存分配。具体来说：\n当一个物理页被解映射时，anon_vma_t 对象可能会被释放，但 RCU 确保在释放之前，所有引用该对象的线程已经完成了其操作。 如果某个线程仍然在读取 anon_vma_t，它可以通过检测对象的状态来避免访问无效的内存。 类型安全内存的防护：传统方式与RCU机制的比较 在多线程或多处理器系统中，内存管理和对象生命周期管理是非常复杂的任务。尤其是在对象释放过程中，如果某个对象被多个线程共享访问，那么在某个线程释放该对象内存的同时，其他线程可能仍然在使用该对象，这就会导致悬空指针（dangling pointer）和类型安全问题。为了解决这些问题，系统在没有 RCU 机制的情况下，通常采用传统的防护方式来实现类型安全内存，而引入 RCU 之后，内存管理的复杂性和性能问题得到了很大程度的简化。\n在没有 RCU 机制时，内核通常使用以下几种方法来确保类型安全内存：\n引用计数（Reference Counting）： 工作机制：通过在对象结构中维护一个引用计数器，来跟踪当前对象被多少个线程引用。每当一个线程开始使用对象时，引用计数加 1，当线程停止使用时，引用计数减 1。只有当引用计数降至 0 时，才能安全地释放该对象的内存。 防护方式：通过引用计数，系统能够确保在所有线程都不再引用该对象时，才释放内存，避免了悬空指针的问题。这种机制可以有效防止某个线程访问已释放对象的问题。 缺点：引用计数的更新需要原子操作（如 atomic_inc 和 atomic_dec），在高并发的场景中，频繁的计数器更新会带来性能瓶颈。此外，在某些情况下（如循环引用），引用计数机制可能会失效，导致内存无法及时释放。 互斥锁（Mutex）和自旋锁（Spinlock）： 工作机制：使用锁机制来保护共享对象的访问。在访问或修改对象前，线程必须先获取锁，操作完成后释放锁。这样可以防止多个线程同时访问对象导致数据不一致或非法内存访问。 防护方式：锁机制可以确保在访问和释放对象时，不会有其他线程正在使用该对象，从而避免内存重用时出现类型不匹配的问题。 缺点：锁机制容易导致性能问题，特别是在高并发场景下，频繁的加锁和解锁会带来显著的性能开销。此外，锁的使用不当可能引发死锁和优先级反转等问题。 延迟释放（Deferred Freeing）： 工作机制：在对象释放时，使用某种机制延迟内存的真正释放，直到确定没有线程仍在访问该对象。例如，通过设置标志位或使用线程等待等方式，确保内存释放的安全性。 防护方式：延迟释放可以防止悬空指针，但需要额外的同步操作和检测机制来判断对象是否可以安全释放，这增加了系统的复杂性。 缺点：延迟释放机制可能导致内存占用增多，如果管理不当，甚至会导致内存泄漏。 RCU（Read-Copy-Update）是一种高效的同步机制，可以大大简化类型安全内存管理中的问题。RCU 的核心思想是允许读者线程在无锁的情况下访问数据，同时写者线程可以异步地进行数据更新，并在合适的时机释放旧数据。这种机制极大地降低了传统方式中同步操作的复杂性和性能开销。\nRCU 的类型安全内存防护机制 类型安全内存指的是当对象被释放后，其内存不会立即被重用为其他类型的对象，这样可以防止访问到已释放对象的线程遇到类型不匹配的问题。 在 RCU 中，为了实现类型安全内存，通常会结合**SLAB_DESTROY_BY_RCU 标志**来使用 slab 分配器（内核中一种高效的内存分配器）。这个标志会确保对象在 RCU 同步完成之前，其内存不会被重新分配为不同类型的对象。 在实际应用中，RCU 通过以下方式实现类型安全内存： 读者线程在 rcu_read_lock 和 rcu_read_unlock 之间无锁地访问对象，写者线程在更新或删除对象时，会调用 call_rcu 或 synchronize_rcu 来等待所有读操作完成，然后安全地释放内存。 在使用 SLAB_DESTROY_BY_RCU 标志时，内存分配器会确保对象内存在 RCU 临界区完成之前，不会被其他类型的对象重用。 RCU 机制的优势 高效性：RCU 的读操作几乎无锁，避免了传统锁机制中因加锁解锁带来的性能损失。在高并发读操作场景中，RCU 提供了极高的并发性和低延迟。 低内存开销：RCU 的类型安全内存管理通过 SLAB_DESTROY_BY_RCU 的延迟释放机制，避免了引用计数器和锁带来的额外内存开销。 避免死锁：RCU 的读操作和写操作分离机制有效地避免了传统锁机制中可能发生的死锁问题。 以下是一个简化的内核中 RCU 和 引用计数 两种类型安全内存防护方式的比较：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // 引用计数方式实现类型安全内存 struct my_object { atomic_t refcount; // 数据字段 }; void acquire_object(struct my_object *obj) { atomic_inc(\u0026amp;obj-\u0026gt;refcount); // 增加引用计数 } void release_object(struct my_object *obj) { if (atomic_dec_and_test(\u0026amp;obj-\u0026gt;refcount)) { free(obj); // 当引用计数为 0 时，释放对象 } } // RCU 方式实现类型安全内存 struct my_rcu_object { struct rcu_head rcu; // 数据字段 }; void free_rcu_object(struct rcu_head *head) { struct my_rcu_object *obj = container_of(head, struct my_rcu_object, rcu); free(obj); // 释放对象 } void release_rcu_object(struct my_rcu_object *obj) { call_rcu(\u0026amp;obj-\u0026gt;rcu, free_rcu_object); // 安排异步释放 } 在传统引用计数方式中，每次对象的引用和释放都需要更新引用计数，这在高并发环境中可能带来性能瓶颈。 而在 RCU 方式中，call_rcu 可以异步地释放对象，在释放前无需同步等待所有引用完成，避免了频繁的锁操作，提高了系统性能。 总的来说\n没有 RCU 时：传统的类型安全内存防护主要依赖于引用计数和锁机制。这些方法能够确保在释放对象前，所有引用该对象的线程已经完成访问，从而避免悬空指针和类型安全问题。然而，这种方式在高并发环境中容易带来性能瓶颈和复杂的死锁问题。 引入 RCU 之后：RCU 提供了更高效的类型安全内存防护机制。通过 RCU 的异步释放机制，可以在不阻塞读者线程的情况下安全释放对象，并结合 slab 分配器的 SLAB_DESTROY_BY_RCU 标志，确保内存不会被重新分配给不同类型的对象。这种机制在内核中得到了广泛的应用，特别是在文件系统和网络栈等高并发读操作的场景中。 RCU 的引入显著提升了内核的性能和并发性，简化了类型安全内存管理的复杂性，并减少了传统锁机制带来的问题。\nreverse-mapping 在Linux内核的虚拟内存管理中，**反向映射（Reverse Mapping）**是一个重要机制，用于管理物理内存页与虚拟地址空间之间的映射关系。具体来说，反向映射允许系统通过物理页找到哪些进程正在引用该页以及对应的虚拟地址是什么。\n反向映射的基本功能：\n通过物理页找到虚拟地址：传统的映射是从虚拟地址空间查找对应的物理页，但反向映射的目的是反过来，通过物理页找到所有引用该物理页的虚拟地址。这对于一些内存管理操作非常关键，特别是在共享内存页或分页回收时。 多进程共享页的管理：在现代操作系统中，多个进程可以共享相同的物理页，比如当多个进程使用同一段共享内存或代码段时。反向映射帮助内核追踪这些共享情况，确保在回收或修改页时，可以正确处理所有共享关系。 反向映射的具体用途：\n分页回收：当内存不足时，操作系统会将不常使用的页交换到磁盘上。这时，内核需要知道哪些页可以被安全回收，以及哪些进程的页表需要更新。反向映射使内核能够通过物理页找到对应的虚拟地址，并确保在回收物理页时正确更新相应进程的页表。 内存管理优化：反向映射还能帮助优化内存回收策略。通过了解哪些物理页被多个进程共享，内核可以避免不必要的交换操作（即不轻易将共享频繁的页回收），从而提高系统性能。 页表更新：当物理页的内容发生变化或需要被换出到磁盘时，反向映射允许内核找到所有引用该页的虚拟地址，以便相应地更新这些进程的页表。 在Linux内核中，反向映射的实现依赖于page structure和anon_vma等数据结构：\npage structure每个物理页对应一个page结构体，其中包含了有关该页的信息，包括哪些进程引用了该页。page 是内核为每个物理内存页维护的数据结构。每个物理页都对应一个 page 结构，其中包含了关于该页的元数据，包括该页的状态、引用计数以及通过反向映射找到引用该页的进程。 anon_vma匿名映射（比如进程的堆或栈）使用anon_vma来追踪虚拟内存区域。通过anon_vma，系统可以追踪到所有引用该物理页的虚拟地址。在匿名内存映射（例如进程的堆和栈）中，anon_vma 结构用来管理虚拟内存区域。它包含了一个链表，链表中的每一项都对应一个 vm_area_struct，这些结构用于表示该匿名页映射的进程及其虚拟地址范围。通过 anon_vma，内核能够从一个物理页找到所有引用该页的虚拟地址。 当需要通过一个物理页找到它对应的虚拟地址时，反向映射机制的工作流程如下：\n首先，内核通过 page 结构找到所有引用该页的 anon_vma 结构。 然后，遍历 anon_vma 中的链表，获取所有引用该物理页的 vm_area_struct 结构。 最后，vm_area_struct 中包含了该物理页对应的虚拟地址范围，内核可以根据这些信息对所有引用该物理页的虚拟内存地址进行操作。 反向映射在提高内存管理效率的同时也引入了一些开销。因为内核需要维护额外的数据结构来记录每个物理页与虚拟地址空间的关系，所以需要一定的内存开销。此外，在多进程大量共享内存的情况下，反向映射的管理会增加系统复杂度。为了优化反向映射，Linux 内核引入了多种优化技术，例如：\n多级映射：通过分层次的映射结构减少对所有进程的遍历时间。 延迟更新：当一个物理页的虚拟地址映射发生改变时，内核可以选择延迟更新页表，这样可以减少频繁的页表更新带来的性能开销。 总的来说，反向映射的主要作用是在物理内存和虚拟内存之间提供一个高效的查询机制，特别是在需要从物理页追溯到对应的虚拟地址时。它在内存回收、页表更新和内存共享管理等方面起着重要作用。在现代操作系统中，反向映射是内存管理不可或缺的一部分，能够提升系统的整体性能并确保内存操作的正确性。\n内存回收（Memory Reclamation） 在操作系统中，内存回收是一个非常重要的机制，用于在系统内存资源紧张时释放不再需要的内存。4.5 章节重点讨论了**页面回收（page reclaim）**的机制，以及如何通过反向映射来辅助高效的内存回收。\n页面回收是操作系统的一种内存管理机制，当系统内存资源紧张时，操作系统会通过扫描内存，找到那些不再需要频繁访问的页面（如被交换到磁盘的页面或不活跃的页面）并将它们回收以释放物理内存。回收的页面可能会被重新分配给其他进程或操作系统使用。\n内存回收机制主要包括以下几类操作：\n交换（Swap）：将不活跃的页面写入磁盘，以释放物理内存。 页面缓存回收：回收不再需要的缓存页面，例如文件系统缓存页面。 匿名页回收：回收匿名内存页（如进程的堆和栈）中的不活跃页面。 反向映射对于页面回收至关重要。当内核决定回收一个物理页时，它需要通过反向映射找到所有引用该页的进程，并更新这些进程的页表。这是确保页面回收不会破坏系统的一致性和正确性的重要步骤。\n具体而言，当系统决定回收一个物理页时，反向映射提供了以下关键信息：\n哪些进程引用了该页：反向映射帮助内核找到所有使用该物理页的进程。 该页在进程中的虚拟地址：通过虚拟地址，内核能够更新页表，确保这些进程不会继续访问已回收的页面。 Linux 内核中的内存回收策略依赖于多种因素，包括页面的访问频率、共享情况和物理内存的使用情况。主要的内存回收策略包括：\nLRU（Least Recently Used）算法：内核使用 LRU 算法来判断哪些页面最不常使用，并优先回收这些页面。LRU 算法会将不常使用的页面移动到内存回收列表的尾部，以便被优先回收。 优先回收共享少的页面：通过反向映射，内核能够找到那些被少数进程共享的页面。这些页面通常更容易被回收，因为它们的影响面较小。 Swap 优化：当内存非常紧张时，内核可能需要将一些页面交换到磁盘上。反向映射机制在这时能够提供有用的信息，以确保被交换的页面不会在短时间内被频繁访问，从而减少交换操作的开销。 在内存回收中，一个主要的挑战是确保系统性能不受太大影响。频繁的页面回收和页表更新操作可能导致系统性能下降，尤其是在多进程共享大量内存的场景中。因此，内核引入了多种优化措施，以确保内存回收过程的高效性：\n批量回收：内核可以通过批量回收多个不活跃的页面，减少每次回收的开销。 异步回收：为了减少对应用程序的影响，内存回收通常以异步方式进行。内核会在后台逐步回收内存，而不会阻塞正在执行的用户进程。 逐步更新页表：当系统决定回收某个页时，内核可以延迟对页表的更新，以减少频繁的页表更新操作。 内存回收在释放系统资源的同时，也带来了一定的性能开销。内核必须在释放内存和保持系统性能之间找到平衡。如果内存回收策略过于激进，可能会导致频繁的页面置换和磁盘交换操作，从而拖慢系统性能。反之，如果内存回收不足，则系统可能会遇到内存不足的问题，影响应用程序的正常运行。\n发布-订阅模型 (Publish-Subscribe) 在发布-订阅（Publish-Subscribe）模型中，写入者（发布者）会初始化一个数据项，然后通过 rcu_assign_pointer 来发布一个指向该数据项的指针。读取者（订阅者）通过 rcu_dereference 来获取并读取这个指针指向的数据项。这两个原语分别保证了数据的发布和读取过程的正确性。\nrcu_assign_pointer: 该函数负责将一个新初始化的指针赋值给全局可见的指针，确保在写操作完成之后，该指针所指向的数据才对读取者可见。 rcu_dereference: 读取者在访问 RCU 临界区中的指针时使用 rcu_dereference，确保数据在正确的时间顺序下被访问。 这种模式常常与存在性保证（existence guarantees）结合使用，以便发布新版本的数据项、并在保证旧版本不被并发访问的情况下回收它们。这一模式允许系统中的多个读取者在并发的情况下访问数据，同时允许写入者在后台安全地更新或替换数据项。\nLinux 内核中的一个经典例子是 动态替换系统调用。在某些体系结构（例如 PowerPC Cell 架构）中，系统会在运行时向系统调用表追加新的系统调用。这个扩展表的指针通过 rcu_assign_pointer 发布，确保扩展表的初始化完成后，读取者才能正确访问扩展表中的内容。\n在执行系统调用时，内核会首先调用 rcu_read_lock 进入 RCU 临界区，然后通过 rcu_dereference 来读取系统调用表的指针，确保系统调用表的扩展部分在被使用时已完全初始化。如果需要修改扩展表（例如撤回系统调用），内核会先将扩展表的指针设置为 NULL，确保不再有线程访问扩展表中的内容，随后调用 synchronize_rcu，等待所有正在执行的 RCU 临界区结束。这样确保没有线程会继续执行那些已经被移除的系统调用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 syscall_t *table; spinlock_t table_lock; int invoke_syscall(int number, void *args...) { syscall_t *local_table; int r = -1; rcu_read_lock(); local_table = rcu_dereference(table); // 获取最新的系统调用表 if (local_table != NULL) r = local_table[number](args); // 执行系统调用 rcu_read_unlock(); return r; } void retract_table() { syscall_t *local_table; spin_lock(\u0026amp;table_lock); local_table = table; rcu_assign_pointer(\u0026amp;table, NULL); // 将系统调用表指针设置为 NULL spin_unlock(\u0026amp;table_lock); synchronize_rcu(); // 等待所有读取操作结束 kfree(local_table); // 释放旧的系统调用表 } 发布-订阅模式关键在于：\n同步与并发保证: 发布的指针在 RCU 临界区内能够被读取者并发访问，而不会导致数据竞态问题。使用 rcu_assign_pointer 和 rcu_dereference 能够确保在多处理器系统上数据的发布和读取过程中的内存屏障和编译器指令正确执行。 旧版本的数据管理: 当写入者需要发布新版本的数据时，旧版本的数据仍可能被读取者访问，因此 RCU 提供了确保在读取者完成访问后安全地回收旧数据的机制。 高效性: 发布-订阅模式结合 RCU 能够在极少锁开销的情况下实现高并发的读写分离操作，大大提高系统的吞吐量。 Cache miss 在多处理器系统中，Cache miss 是影响性能的重要因素。传统的同步方法（如互斥锁、读写锁等）和 RCU（Read-Copy-Update） 方法在处理数据结构时，产生 Cache miss 的概率差异主要来源于两者对共享数据访问的方式和内存一致性的处理方式不同。\n传统同步方法，尤其是使用锁机制的场景中，多个线程或处理器为了读取或修改共享数据，需要通过加锁来保护数据的完整性。在这种情况下，容易产生 Cache miss，主要有以下几个原因：\n写-写竞争：多个处理器试图同时写入共享变量时，缓存一致性协议需要通过无效化其他处理器的缓存来保证一致性。比如，一个处理器写入某个缓存行的数据后，其他处理器需要重新加载该缓存行，导致 写 miss 频繁出现。 写-读竞争：一个处理器正在写某个共享变量时，其他处理器可能正试图读取这个变量。每当写操作发生时，读取该变量的处理器需要重新加载最新的数据到它的缓存中，造成 Cache miss。 锁的开销：当多个处理器竞争同一个锁时，锁变量本身也会被频繁读写，导致锁所在的缓存行失效，从而引发 Cache miss。 这些竞争加剧了缓存行在多个处理器之间的传输，增加了缓存失效（Cache miss）的可能性。\nRCU 提供了一种优化读操作的机制，减少了缓存一致性协议带来的负面影响。它的工作原理是通过“读者不阻塞写者，写者不阻塞读者”的方式来管理并发数据访问，具体来说有以下几种特性：\n读者不加锁：RCU 允许多个读者同时读取数据，而不需要对共享数据加锁。这样，读操作就不会引发任何缓存一致性问题，也不会导致缓存行失效，因为读者访问的是共享数据的快照，不会修改数据。这大大减少了读操作的 Cache miss。 写操作延迟并且分阶段进行：RCU 在写操作时，会先创建一个数据的副本（Copy），然后修改这个副本。当没有读者在访问旧的数据时，才会进行数据的更新（Update），将新版本替换旧版本。由于读者和写者之间不直接竞争缓存行，写操作发生时不会影响读者，从而减少了 Cache miss。 批量更新：RCU 通常会将多个更新操作批量执行，而不是频繁地对共享数据进行修改。这种批量操作降低了写操作触发的缓存失效，从而降低了 Cache miss 的概率。 传统同步方法 由于频繁的读写竞争、缓存一致性维护，以及锁的使用，导致缓存失效较多，从而增加了 Cache miss 的频率。 RCU 方法 通过减少读操作的干扰和写操作的延迟更新，避免了读写竞争，因而大幅减少了 Cache miss。 RCU 通过“读-写分离”和“数据复制”机制有效地减少了处理器间的缓存竞争，尤其是在多线程环境中，大量的读操作不会引发 Cache miss，这使得它在高并发场景中表现出色。\n间接访问（indirection） 间接访问（indirection）在 RCU 设计中是为了确保 原子性和一致性，使得更新操作对读者看来是原子的，而不会暴露出“更新中的状态”。\n在直接存储的模式下，如果对某个对象进行写操作，比如在 PID 表中直接存储 process_t 实例，那么更新 process_t 的各个字段就会发生在原对象上。这样一来，读者可能会在对象更新还未完成时，读取到尚未完全初始化的对象，这就是所谓的“中间状态”。\n举个例子，假设一个 process_t 对象有多个字段需要逐一更新。如果 PID 表直接存储了这个对象的实例：\n当写者正在修改 process_t 的字段时，某个读者可能正在读取该对象，读取的字段可能只被部分更新。 这会导致不一致的读取情况，比如某个字段是旧值，而另一个字段是新值。这在多线程或并发环境中可能会引发严重的问题，因为读者期望读取的是一个一致的、完全更新的对象。 为了避免这种情况，RCU 使用间接访问：PID 表存储的是指向 process_t 的指针，而不是 process_t 实例本身。这种方式确保更新过程可以通过更换指针来实现，从而使得新对象的发布是原子的：\n更新副本：当需要更新 process_t 时，写者不会直接修改现有的 process_t，而是创建一个新的 process_t 副本，完全更新新对象的所有字段。 发布新指针：在新对象的所有字段都更新完毕后，写者将 PID 表中指向 process_t 的指针更新为新对象的地址。这一操作是原子的：所有的字段更新都是一次性发布的，不会暴露出中间状态。 在这个过程中，指针充当了一个“指向新状态”的媒介。由于指针的更换是原子的，任何新的读者都会看到完整的新对象，而不是一个正在更新的对象。示例：\n初始状态：PID 表中的指针 ptr 指向 process_t_A。 写操作：创建 process_t_B 副本并更新所有字段。 指针更新：将 ptr 更新为指向 process_t_B。此时，所有读者读取到的都是 process_t_B，不可能再读取到部分更新的 process_t_A。 总而言之，通过间接访问：\n可以确保数据的更新对读者来说是一次性的、原子的，避免了不一致的读取情况。 读者在读取数据时，始终会看到一个完全初始化的数据对象，而不会被暴露给“部分更新”或“中间状态”。 在多线程编程中，使用间接访问为数据一致性提供了重要保障，这也解释了为什么在 RCU 中间接访问是一个常见的设计模式。\n正课部分 使用锁带来的问题 今天的话题是如何在多核CPU计算机上获得好的性能，这是一个非常有趣，深入且令人着迷的话题。今天我们只会涉及这个话题的很小的一个部分，也就是在面对内核中需要频繁读但是不需要频繁写的共享数据时，如何获得更好的性能。\n在不同的场景下有不同的方法可以在多核CPU的机器上获得更好的性能，我们今天要看的是Linux的RCU，它对于需要频繁读的内核数据来说是一种非常成功的方法。\n如果你有一个现代的计算机，或许包含了4、8、16、64个并行运行的CPU核，这些CPU核共享了内存数据，操作系统内核将会是一个并行运行的程序。如果你想要获得好的性能，你需要确保内核能尽可能的在多个CPU核上并行的完成它的工作。\n如果你能将内核并行的运行在8个CPU核上，并且它们都能完成有效的工作，那么相比运行在单个CPU核上，你就能获得8倍的性能。从理论上来说，这明显是可能的。\n如果你在内核中有大量的进程，那就不太用担心，在不做任何额外工作的前提下，这些进程极有可能是并行运行的。另一方面，如果你有很多应用程序都在执行系统调用，很多时候，不同的应用程序执行的不同系统调用也应该是相互独立的，并且在很多场景下应该在相互不影响的前提下运行。\n例如，通过fork产生的两个进程，或者读取不同pipe的两个进程，或者读写不同文件的两个进程。表面上看，这些进程之间没有理由会相互影响，也没有理由不能并行运行并获得n倍的吞吐量。\n但问题是内核中包含了大量的共享数据。\n出于一些其他的原因，内核共享了大量的资源，例如内存，CPU，磁盘缓存，inode缓存，这些东西都在后台被不同的进程所共享。这意味着，即使两个完全不相关的进程在执行两个系统调用，如果这两个系统调用需要分配内存或使用磁盘缓存或者涉及到线程调度决策，它们可能最终会使用内核中相同的数据结构，因此我们需要有办法能让它们在使用相同数据的同时，又互不影响。\n在过去的许多年里，人们付出了巨大的努力来让内核中的这些场景能更快的运行。\n我们之前看过其中一种可以保证正确性的方法，也就是spinlock。\nspinlock很直观，它的工作就是当两个进程可能会相互影响时，阻止并行运行。所以spinlock的直接效果就是降低性能。它使得正确性有了保障，但是又绝对的阻止了并行执行，这并不总是能令人满意。\n今天我们会关注需要频繁读的数据，也就是说你的数据主要是在被读取，相对来说很少被写入。我将使用单链表来作为主要的例子。\n对于单链表，会存在一个指向头指针（head）的全局变量，之后是一些链表元素，每个链表元素都包含了一个数据，假设是字符串。第一个链表元素包含了“hello”。每个链表元素还包含了一个next指针，指向了下一个链表元素。最后一个链表元素的next指针指向空指针。\n接下来我们假设对于这个链表的大部分操作是读，比如说内核线程大部分时候只会扫描链表来找到某些数据，而不会修改链表。假设一个写请求都没有的话，我们就根本不必担心这个链表，因为它是完全静态的，它从来都不会更新，我们可以自由的读它。\n但是接下来我们假设每隔一会，一些其他的线程会来修改链表元素中的数据；删除一个链表元素；又或者是在某个位置插入链表元素。所以尽管我们关注的主要是读操作，我们也需要关心写操作，我们需要保证读操作在面对写操作时是安全的。\n在XV6中，我们是通过锁来保护这个链表。\n在XV6中，不只是修改数据的线程，读取数据的线程也需要获取锁，因为我们需要排除当我们在读的时候某人正在修改链表的可能，否则的话会导致读取数据的线程可能读到更新一半的数据或者是读到一个无效的指针等等，所以XV6使用了锁。\n但是使用锁有个缺点，如果通常情况下没有修改数据的线程，那么意味着每次有一个读取数据的线程，都需要获取一个排他的锁。XV6中的spinlock是排他的，即使只是两个读取数据的线程也只能一次执行一个线程。\n所以一种改进这里场景的方法是使用一种新的锁，它可以允许多个读取线程和一个写入线程。接下来我们来看看这种锁，不仅因为它是有趣的，也因为它的不足促成了对于RCU的需求。\n读写锁 (Read-Write Lock) 这种锁被称为读写锁（Read-Write Lock），它的接口相比spinlock略显复杂。\n如果只是想要读取数据，那么可以调用r_lock，将锁作为参数传入，同样的还会有个r_unlock，数据的读取者使用这些接口。数据的写入者调用w_lock和w_unlock接口。\n这里的语义是，要么你可以有多个数据的读取者获取了读锁，这样可以获得并行执行读操作的能力；要么你只能有一个数据写入者获取了写锁。但是不能两种情况同时发生，读写锁排除了某人获取了数据的写锁，同时又有别人获取读锁的可能性。\n你要么只有一个数据写入者，要么有多个数据读取者，不可能有别的可能。\n学生提问：当某人持有了读锁时，读写锁是采用什么方案来阻止其他人写入数据的？\nRobert教授：并没有什么方案，这就像XV6的锁一样。我们这里讨论的是由值得信赖且负责的开发人员编写的内核代码，所以就像XV6的spinlock一样，如果使用锁的代码是不正确的，那么结果就是不正确的，这是内核代码典型的编写方式，你只能假设开发内核的人员遵循这里的规则。\n如果我们有一个大部分都是读取操作的数据结构，我们会希望能有多个用户能同时使用这个数据结构，这样我们就可以通过多核CPU获得真正的性能提升。\n如果没有其他问题的话，那么读写锁就可以解决今天的问题，我们也没有必要读RCU这篇论文 。但实际上如果你深入细节，你会发现当你使用读写锁时，尤其对于大部分都是读取操作的数据结构，会有一些问题。为了了解实际发生了什么，我们必须看一下读写锁的代码实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //A simplified version of Linux\u0026#39;s read/write lock. //n=0 -\u0026gt; not locked //n=-1 -\u0026gt; locked by one writer //n\u0026gt;0 -\u0026gt; locked by n readers struct rwlock{ int n; }: r_lock(l): while 1: x = l-\u0026gt;n if x \u0026lt; 0 continue if CAS(\u0026amp;l-\u0026gt;n,X,×+1) return //CAS(p,a,b)is atomic compare-and-swap instruction //if *p ==a,set *p = b,return true //else return false w_lock(1): while 1: if CAS(\u0026amp;l-\u0026gt;n,0,-1) return Linux实际上有读写锁的实现，上面是一种简化了的Linux代码。首先有一个结构体是rwlock，这与XV6中的lock结构体类似。rwlock结构体里面有一个计数器n，\n如果n等于0那表示锁没有以任何形式被被任何人持有 如果n等于-1那表示当前有一个数据写入者持有写锁 如果n大于0表示有n个数据读取者持有读锁。我们需要记录这里的数字，因为我们只有在n减为0的时候才能让数据写入者持有写锁。 r_lock函数会一直在一个循环里面等待数据写入者释放锁。\n首先它获取读写锁中计数器n的拷贝，如果n的拷贝小于0的话那意味着存在一个数据写入者，我们只能继续循环以等待数据写入者退出。\n如果n的拷贝不小于0，我们会增加读写锁的计数器。但是我们只能在读写锁的计数器仍然大于等于0的时候，对其加1。\n所以我们不能直接对n加1，因为如果一个数据写入者在我们检查n和我们增加n之间潜入了，那么我们有可能在数据写入者将n设置为-1的同时，将n又加了1。所以我们只能在检查完n大于等于0，且n没有改变的前提下，将其加1。\n人们通过利用特殊的原子指令来实现这一点，我们之前在看XV6中spinlock的实现时看过类似的指令（注，详见10.7中的test_and_set指令）。\n其中一个使用起来很方便的指令是compare-and-swap（CAS）。CAS接收三个参数，第一个参数是内存的某个地址，第二个参数是我们认为内存中该地址持有的数值，第三个参数是我们想设置到内存地址的数值。\nCAS的语义是，硬件首先会设置一个内部的锁，使得一个CAS指令针对一个内存地址原子的执行；之后硬件会检查当前内存地址的数值是否还是x；如果是的话，将其设置为第三个参数，也就是x+1，之后CAS指令会返回1；如果不是的话，并不会改变内存地址的数值，并返回0。\n这里必须是原子性，因为这里包含了两个操作，首先是检查当前值，其次是设置一个新的数值。\n学生提问：有没有可能计算x的过程中，发生了一个中断？\nRobert教授：你是指我们在执行CAS指令之前计算它的第三个参数的过程中发生中断吗？CAS实际上是一个指令，如果中断发生在我们计算x+1的过程中，那么意味着我们还没有调用CAS，这时包括中断在内的各种事情都可能发生。\n如果我们在最初读取n的时候读到0，那么不管发不发生中断，我们都会将1作为CAS的第三个参数传入，因为中断并不会更改作为本地变量的x，所以CAS的第二个和第三个参数会是0和1。如果n还是0，我们会将其设置为1，这是我们想看到的；如果n不是0，那么CAS并不会更新n。\n如果这里没有使用本地变量x，那么就会有大问题了，因为n可能在任意时间被修改，所以我们需要在最开始在本地变量x中存储n的一个固定的值。\n上面介绍了w_lock与r_lock同时调用的场景。多个r_lock同时调用的场景同样也很有趣。\n假设n从0开始，当两个r_lock同时调用时，我们希望当两个r_lock都返回时，n变成2，因为我们希望两个数据读取者可以并行的使用数据。两个r_lock在最开始都将看到n为0，并且都会通过传入第二个参数0，第三个参数1来调用CAS指令，但是只有一个CAS指令能成功。\nCAS是一个原子操作，一次只能发生一个CAS指令。不管哪个CAS指令先执行，将会看到n等于0，并将其设置为1。另一个CAS指令将会看到n等于1，返回失败，并回到循环的最开始，这一次x可以读到1，并且接下来执行CAS的时候，第二个参数将会是1，第三个参数是2，这一次CAS指令可以执行成功。\n最终两次r_lock都能成功获取锁，其中一次r_lock在第一次尝试就能成功，另一次r_lock会回到循环的最开始再次尝试并成功。\n学生提问：如果开始有一堆数据读取者在读，之后来了一个数据写入者，但是又有源源不断的数据读取者加入进来，是不是就轮不到数据写入者了？\nRobert教授：如果多个数据读取者获取了锁，每一个都会通过CAS指令将n加1，现在n会大于0。如果这时一个数据写入者尝试要获取锁，它的CAS指令会将n与0做对比，只有当n等于0时，才会将其设置为-1。\n但是因为存在多个数据读取者，n不等于0，所以CAS指令会失败。数据写入者会在w_lock的循环中不断尝试并等待n等于0，如果存在大量的数据读取者，这意味着数据写入者有可能会一直等待。这是这种锁机制的一个缺陷。\n学生提问：在刚刚两个数据读取者要获取锁的过程中，第二个数据读取者需要再经历一次循环，这看起来有点浪费，如果有多个数据读取者，那么它们都需要重试。\nRobert教授：你说到了人们为什么不喜欢这种锁的点子上了。即使没有任何的数据写入者，仅仅是在多个CPU核上有大量的数据读取者，r_lock也可能会有非常高的代价。\n在一个多核的系统中，每个CPU核都有一个关联的cache，也就是L1 cache。每当CPU核读写数据时，都会保存在cache中。除此之外，还有一些内部连接的线路使得CPU可以彼此交互，因为如果某个CPU核修改了某个数据，它需要告诉其他CPU核不要去缓存这个数据，这个过程被称为(cache) invalidation。\n如果有多个数据读取者在多个CPU上同时调用r_lock，它们都会读取读写锁的计数l-\u0026gt;n，并将这个数据加载到CPU的cache中，它们也都会调用CAS指令，但是第一个调用CAS指令的CPU会修改l-\u0026gt;n的内容。作为修改的一部分，它需要使得其他CPU上的cache失效。\n所以执行第一个CAS指令的CPU需要通过线路发送invalidate消息给其他每一个CPU核，之后其他的CPU核在执行CAS指令时，需要重新读取l-\u0026gt;n，但是这时CAS指令会失败，因为l-\u0026gt;n已经等于1了，但x还是等于0。\n之后剩下的所有数据读取者都会回到循环的最开始，重复上面的流程，但这一次还是只有一个数据读取者能成功。\n假设有n个数据读取者，那么每个r_lock平均需要循环n/2次，每次循环都涉及到O(n)级别的CPU消息，因为至少每次循环中所有CPU对于l-\u0026gt;n的cache需要被设置为无效。\n这意味着，对于n个CPU核来说，同时获取一个锁的成本是O(n^2)，当你为一份数据增加CPU核时，成本以平方增加。\n这是一个非常糟糕的结果，因为你会期望如果有10个CPU核完成一件事情，你将获得10倍的性能，尤其现在还只是读数据并没有修改数据。\n你期望它们能真正的并行运行，当有多个CPU核时，每个CPU核读取数据的时间应该与只有一个CPU核时读取数据的时间一致，这样并行运行才有意义，因为这样你才能同时做多件事情。但是现在，越多的CPU核尝试读取数据，每个CPU核获取锁的成本就越高。\n对于一个只读数据，如果数据只在CPU的cache中的话，它的访问成本可能只要几十个CPU cycle。但是如果数据很受欢迎，由于O(n^2)的效果，光是获取锁就要消耗数百甚至数千个CPU cycle，因为不同CPU修改数据（注，也就是读写锁的计数器）需要通过CPU之间的连线来完成缓存一致的操作。\n所以这里的读写锁，将一个原本成本很低的读操作，因为要修改读写锁的l-\u0026gt;n，变成了一个成本极高的操作。如果你要读取的数据本身就很简单，这里的锁可能会完全摧毁任何并行带来的可能的性能提升。\n读写锁糟糕的性能是RCU存在的原因，因为如果读写锁足够有效，那么就没有必要做的更好。\n除了在有n个CPU核时，r_lock的成本是O(n^2)之外，这里的读写锁将一个本来可以缓存在CPU中的，并且可能会很快的只读的操作，变成了需要修改锁的计数器l-\u0026gt;n的操作。\n如果我们写的是可能与其他CPU核共享的数据，写操作通常会比读操作成本高得多。因为读一个未被修改的数据可以在几个CPU cycle内就从CPU cache中读到，但是修改可能被其他CPU核缓存的数据时，需要涉及CPU核之间的通信来使得缓存失效。\n不论如何修改数据结构，任何涉及到更改共享数据的操作对于性能来说都是灾难。\n1 2 3 4 5 6 7 r_lock(l): while 1: x = l-\u0026gt;n //THIS if x \u0026lt; 0 continue if CAS(\u0026amp;l-\u0026gt;n,X,×+1) return 所以r_lock中最关键的就是它对共享数据做了一次写操作。所以我们期望找到一种方式能够在读数据的同时，又不需要写数据，哪怕是写锁的计数器也不行。这样读数据实际上才是一个真正的只读操作。\nRCU实现(1) - 基本实现 一种可能的解决方案是：数据读取者完全不使用锁。\n在有些场景数据读取者可以直接读数据，只有数据的写入者才需要锁。我们接下来快速的看一下能不能让数据读取者在不上锁的时候直接读取链表。\n假设我们有个链表，链表元素中存的数据是字符串，我们将读取链表中的数据。如果没有数据的写入者，那么不会有任何问题。\n接下来我们看一下存在数据写入者时的三种可能场景：\n首先是数据的写入者只修改了链表元素的内容，将链表元素中的字符串改成了其他的字符串。 第二种场景是数据写入者插入了一个链表元素。 第三种场景是数据写入者删除了一个链表元素。 因为RCU需要分别考虑这三种场景，我们将会分别审视这三种场景并看一下同时发生数据的读写会有什么问题？\n如果数据写入者想要修改链表元素内的字符串，而数据读取者可能正在读取相同字符串。如果不做任何特殊处理，数据读取者可能会读到部分旧的字符串和部分新的字符串。这是我们需要考虑的一个问题。\n如果数据写入者正在插入一个链表元素，假设要在链表头部插入一个元素，数据写入者需要将链表的头指针指向新元素，并将新元素的next指针指向之前的第一个元素。\n这里的问题是，数据的写入者可能在初始化新元素之前，就将头指针指向新元素，也就是说这时新元素包含的字符串是无效的并且新元素的next指针指向的是一个无效的地址。这是插入链表元素时可能出错的地方。\n如果数据写入者正在删除一个链表元素，我们假设删除的是第一个元素，所以需要将链表的头指针指向链表的第二个元素，之后再释放链表的第一个元素。\n这里的问题是，如果数据读取者正好在读链表的第一个元素，而数据写入者又释放了这个元素，那么数据读取者看到的是释放了的元素，这个链表元素可能接下来被用作其他用途，从数据读取者的角度来说看到的是垃圾数据。\n如果我们完全不想为数据读取者提供任何锁，那么我们需要考虑这三个场景。我将不会讨论数据写入者对应的问题，因为在整个课程中我将会假设数据写入者在完成任何操作前，都会使用类似spinlock的锁。\n我们不能直接让数据读取者在无锁情况下完成读取操作，但是我们可以修复上面提到的问题，这就带出了RCU（Read Copy Update）这个话题。\nRCU是一种实现并发的特殊算法，它是一种组织数据读取者和写入者的方法，通过RCU数据读取者可以不用使用任何锁。RCU的主要任务就是修复上面的三种数据读取者可能会陷入问题的场景，它的具体做法是让数据写入者变得更加复杂一些，所以数据写入者会更慢一些。\n除了锁以外它还需要遵循一些额外的规则，但是带来的好处是数据读取者因为可以不使用锁、不需要写内存而明显的变快。\n在之前讨论的第一个场景中，数据写入者会更新链表元素的内容。RCU将禁止这样的行为，也就是说数据写入者不允许修改链表元素的内容。假设我们有一个链表，数据写入者想要更新链表元素E2。\n现在不能直接修改E2的内容，RCU会创建并初始化一个新的链表元素。所以新的内容会写到新的链表元素中，之后数据写入者会将新链表元素的next指针指向E3，之后在单个的写操作中将E1的next指针指向新的链表元素。\n所以这里不是修改链表元素的内容，而是用一个包含了更新之后数据的新链表元素代替之前的链表元素。对于数据读取者来说，如果遍历到了E1并正在查看E1的next指针：\n要么看到的是旧的元素E2，这并没有问题，因为E2并没有被改变； 要么看到的是新版本的E2，这也没有问题，因为数据写入者在更新E1的next指针之前已经完全初始化好了新版本的E2。 不管哪种情况，数据读取者都将通过正确的next指针指向E3。这里核心的点在于，数据读取者永远也不会看到一个正在被修改的链表元素内容。\n学生提问：旧的E2和E3之间的关系会被删除吗？\nRobert教授：会被保留。这是个好问题，并且这也是RCU中较为复杂的主要部分，现在我们假设旧的E2被保留了。\n学生提问：我们并不用担心E2和E3之间的关系，因为在普通的实现中，E2也会被释放，就算没有RCU我们也不用担心这里的关系，是吗（注，这里应该说的是GC会回收E2）？\nRobert教授：这里的问题是，在我们更新E1的next指针时，部分数据读取者通过E1的旧的next指针走到了旧的E2，所以当完成更新时，部分数据读取者可能正在读取旧的E2，我们最好不要释放它。\n这里将E1的next指针从旧的E2切换到新的E2，在我（Robert教授）脑海里，我将其称为committing write。\n这里能工作的部分原因是，单个committing write是原子的，从数据读取者的角度来说更新指针要么发生要么不发生。通过这一条不可拆分的原子指令，我们将E1的next指针从旧的E2切换到的新的E2。写E1的next指针完成表明使用的是新版本的E2。\n这是对于RCU来说一个非常基本同时也是非常重要的技术，它表示RCU主要能用在具备单个committing write的数据结构上。这意味着一些数据结构在使用RCU时会非常的奇怪，例如一个双向链表，其中的每个元素都有双向指针，这时就不能通过单个committing write来删除链表元素，因为在大多数机器上不能同时原子性的更改两个内存地址。所以双向链表对于RCU来说不太友好。\n相反的，树是一个好的数据结构。\n如果我们要更新图中的节点，我们可以构造树的虚线部分，然后再通过单个committing write更新树的根节点指针，切换到树的新版本。\n数据写入者会创建树中更新了的那部分，同时再重用树中未被修改的部分，最后再通过单个committing write，将树的根节点更新到新版本的树的根节点。\n但是对于其他的数据结构，就不一定像树一样能简单的使用RCU。以上就是实现RCU的第一部分。\nRCU实现(2) - Memory barrier 在前一部分介绍的方法中，存在一个问题。在前一部分中，如果要更新E2的内容，需要先创建一个E2‘ 并设置好它的内容，然后将E2’ 的next指针指向E3，最后才会将E1的next指针指向E2’。\n你们或许还记得在XV6中曾经介绍过（注，详见10.8），许多计算机中都不存在“之后”或者“然后”这回事，通常来说所有的编译器和许多微处理器都会重排内存操作。如果我们用C代码表示刚才的过程：\n1 2 3 e = alloc() e-\u0026gt; next = E3 E1-\u0026gt;next = e 如果你测试这里的代码，它可能可以较好的运行，但是在实际中就会时不时的出错。\n这里的原因是编译器或者计算机可能会重排这里的写操作，也有可能编译器或者计算机会重排数据读取者的读操作顺序。如果我们在初始化E2’的内容之前，就将E1的next指针设置成E2‘，那么某些数据读取者可能就会读到垃圾数据并出错。\n所以实现RCU的第二个部分就是数据读取者和数据写入者都需要使用memory barriers，这里背后的原因是因为我们这里没有使用锁。对于数据写入者来说，memory barrier应该放置在committing write之前，\n1 2 3 4 e = alloc() e-\u0026gt; next = E3 BARRIER E1-\u0026gt;next = e 这样可以告知编译器和硬件，先完成所有在barrier之前的写操作，再完成barrier之后的写操作。所以在E1设置next指针指向E2‘的时候，E2’必然已经完全初始化完了。\n对于数据读取者，需要先将E1的next指针加载到某个临时寄存器中，我们假设r1保存了E1的next指针，之后数据读取者也需要一个memory barrier，然后数据读取者才能查看r1中保存的指针。\n1 2 3 4 r1 = E1 -\u0026gt; next BARRIER r1 -\u0026gt; x -\u0026gt; next 这里的barrier表明的意思是，在完成E1的next指针读取之前，不要执行其他的数据读取，这样数据读取者从E1的next指针要么可以读到旧的E2，要么可以读到新的E2‘。\n通过barrier的保障，我们可以确保成功在r1中加载了E1的next指针之后，再读取r1中指针对应的内容。\n因为数据写入者中包含的barrier确保了在committing write时，E2’已经初始化完成。如果数据读取者读到的是E2‘，数据读取者中包含的barrier确保了可以看到初始化之后E2’的内容。\n学生提问：什么情况下才可能在将E1的next指针加载到r1之前，就先读取r1中指针指向的内容？\nRobert教授：我觉得你难住我了。一种可能是，不论r1指向的是什么，它或许已经在CPU核上有了缓存，或许一分钟之前这段内存被用作其他用途了，我们在CPU的缓存上有了E1-\u0026gt;next对应地址的一个旧版本。\n我不确定这是不是会真的发生，这里都是我编的，如果r1-\u0026gt;x可以使用旧的缓存的数据，那么我们将会有大麻烦。\n说实话我不知道这个问题的答案，呵呵。我课下会想一个具体的例子。\nRCU实现(3) - 读写规则 前面有同学也提到过，数据写入者会将E1的next指针从指向旧的E2切换到指向新的E2‘，但是可能有数据读取者在切换之前读到了旧的E2，并且仍然在查看旧的E2。\n我们需要在某个时候释放旧的E2，但是最好不要在某些数据读取者还在读的时候释放。所以我们需要等待最后一个数据读取者读完旧的E2，然后才能释放旧的E2。\n这就是RCU需要解决的第三个问题，数据写入者到底要等待多久才能释放E2？你可以想到好几种方法来实现这里的等待。\n例如，我们可以为每个链表元素设置一个引用计数，并让数据读取者在开始使用链表元素时对引用计数加1，用完之后对引用计数减1，然后让数据写入者等待引用计数为0。\n但是我们会第一时间就否定这个方案，因为RCU的核心思想就是在读数据的时候不引入任何的写操作，因为我们前面看过了，如果有大量的数据读取者同时更新引用计数，相应的代价将十分高。所以我们绝对不会想要有引用计数的存在。\n另一种可能是使用自带垃圾回收（Garbage Collect）的编程语言。\n在带有GC的编程语言中，你不用释放任何对象，相应的GC会记住是否有任何线程或者任何数据结构对于某个对象还存在引用。如果GC发现对象不可能再被使用时，就会释放对象。\n这也是一种可能且合理的用来释放链表元素的方法。但是使用了RCU的Linux系统，并不是由带有GC的编程语言编写，并且我们也不确定GC能不能提升性能，所以这里我们也不能使用一个标准GC来释放E2。\nRCU使用的是另一种方法，数据读取者和写入者都需要遵循一些规则，使得数据写入者可以在稍后再释放链表元素。规则如下：\n数据读取者不允许在context switch（注，是指线程切换的context switch，详见11.4）时持有一个被RCU保护的数据（也就是链表元素）的指针。所以数据读取者不能在RCU critical 区域内出让CPU。 对于数据写入者，它会在每一个CPU核都执行过至少一次context switch之后再释放链表元素。 这里的第一条规则也是针对spin lock的规则，在spin lock的加锁区域内是不能出让CPU的。\n第二条规则更加复杂点，但是相对来说也更清晰，因为每个CPU核都知道自己有没有发生context switch，所以第二条规则是数据写入者需要等待的一个明确条件。\n数据写入者或许要在第二条规则上等待几个毫秒的时间才能确保没有数据读取者还在使用链表元素，进而释放链表元素。\n人们创造了多种技术来实现上面第二条规则中的等待，论文 里面讨论的最简单的一种方法是通过调整线程调度器，使得写入线程简短的在操作系统的每个CPU核上都运行一下，这个过程中每个CPU核必然完成了一次context switching。\n因为数据读取者不能在context switch的时候持有数据的引用，所以经过这个过程，数据写入者可以确保没有数据读取者还在持有数据。\n所以数据写入者的代码实际上看起来是这样的：\n首先完成任何对于数据的修改 之后调用实现了上面第二条规则synchronize_rcu函数 最后才是释放旧的链表元素 synchronize_rcu迫使每个CPU核都发生一次context switch，所以在synchronize_rcu函数调用之后，由于前面的规则1，任何一个可能持有旧的E1 next指针的CPU核，都不可能再持有指向旧数据的指针，这意味着我们可以释放旧的链表元素。\n你们可能会觉得synchronize_rcu要花费不少时间，可能要将近1个毫秒，这是事实并且不太好。\n其中一种辩解的方法是：对于RCU保护的数据来说，写操作相对来说较少，写操作多花费点时间对于整体性能来说不会太有影响。\n对于数据写入者不想等待的场景，可以调用另一个函数call_rcu，将你想释放的对象和一个执行释放的回调函数作为参数传入，RCU系统会将这两个参数存储到一个列表中，并立刻返回。\n之后在后台，RCU系统会检查每个CPU核的context switch计数，如果每个CPU核都发生过context switch，RCU系统会调用刚刚传入的回调函数，并将想要释放的对象作为参数传递给回调函数。这是一种避免等待的方法，因为call_rcu会立即返回。\n但是另一方面不推荐使用call_rcu，因为如果内核大量的调用call_rcu，那么保存call_rcu参数的列表就会很长，这意味着需要占用大量的内存，因为每个列表元素都包含了一个本该释放的指针。\n在一个极端情况下，如果你不够小心，大量的调用call_rcu可能会导致系统OOM，因为所有的内存都消耗在这里的列表上了。所以如果不是必须的话，人们一般不会使用call_rcu。\n学生提问：这里的机制阻止了我们释放某些其他人还在使用的对象，但是并没有阻止数据读取者看到更新了一半的数据，对吗？\nRobert教授：23.3中的基本实现阻止了你说的情况，在23.3中，我们并不是在原地更新链表元素，如果是的话绝对会造成你说的那种情况。\nRCU不允许在原地更新数据，它会创建一个新的数据元素然后通过单个committing write替换原有数据结构中的旧数据元素。因为这里的替换是原子的，所以数据读取者看不到更新了一半的数据。\n学生提问：上面提到的条件1，是不是意味着我们必须关注在RCU read crtical区域内的代码执行时间，因为它限制了CPU核在这个区域内不能context switch？\nRobert教授：是的，在RCU区域内，数据读取者会阻止CPU发生context switch，所以你会想要让这个区域变得较短，这是个需要考虑的地方。\nRCU使用的方式是，在Linux中本来有一些被普通锁或者读写锁保护的代码，然后某人会觉得锁会带来糟糕的性能问题，他会将Locking区域替换成RCU区域，尽管实际中会更加复杂一些。\nLocking区域已经尽可能的短了，因为当你持有锁的时候，可能有很多个CPU核在等待锁，所以普通锁保护的区域会尽量的短。因为RCU区域通常是用来替代Lock区域，它也趋向于简短，所以通常情况下不用担心RCU区域的长短。\n这里实际的限制是，数据读取者不能在context switch时持有指针指向被RCU保护的数据，这意味着你不能读磁盘，然后在等读磁盘返回的过程中又持有指针指向被RCU保护的数据。\n所以通常的问题不是RCU区域的长短，而是禁止出让CPU。\nRCU用例代码 为了巩固前面介绍的内容，接下来看一段使用了RCU的简单代码。上半段是读取被RCU保护的链表 ，下半段代码是替换链表的第一个元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //list reader: rcu_read_lock() e = head while(p){ e = rcu_dereference(e) look at e-\u0026gt;x .. e = e-\u0026gt;next } rcu_read_unlock() //replace the first list element: acquire(lock) old = head e = alloc() e-\u0026gt;x = ... e-\u0026gt;next = head-\u0026gt;next rcu_assign_pointer(\u0026amp;head,e) release(lock) synchronize_rcu() free(old) 数据读取位于rcu_read_lock和rcu_read_unlock之间，这两个函数几乎不做任何事情。\nrcu_read_lock会设置一个标志位，表明如果发生了定时器中断，请不要执行context switch，因为接下来要进入RCU critical区域。\n所以rcu_read_lock会设置一个标志位来阻止定时器中断导致的context switch，中断或许还会发生，但是不会导致context switch（注，也就是线程切换）。rcu_read_unlock会取消该标志位。\n所以这是一个集成在RCU critical区域的计数器。rcu_read_lock和rcu_read_unlock因为几乎不做任何工作所以极其的快。\n注，这里有个问题，23.2中描述的读写锁慢的原因是因为在读数据的时候引入了写计数器的操作，这里同样也是需要额外的写操作，为什么这里不会有问题？这是因为读写锁的计数器是所有CPU共享的，而这里的标志位是针对每个CPU的，所以修改这里的标志位并不会引起CPU之间的缓存一致消息\n其中的while循环会扫描链表，rcu_dereference函数会插入memory barrier，它首先会从内存中拷贝e，触发一个memory barrier，之后返回指向e的指针。之后我们就可以读取e指针指向的数据内容，并走向下一个链表元素。数据读取部分非常简单。\n数据写入部分更复杂点。\nRCU并不能帮助数据写入者之间避免相互干扰，所以必须有一种方法能确保一次只能有一个数据写入者更新链表。这里我们假设我们将使用普通的spinlock，所以最开始数据写入者获取锁。 如果我们要替换链表的第一个元素，我们需要保存先保存链表第一个元素的拷贝，因为最后我们需要释放它，所以有old=head。 接下来的代码执行的是之前介绍的内容，首先是分配一个全新的链表元素，之后是设置该链表元素的内容，设置该链表元素的next指针指向旧元素的next指针。 之后的rcu_assign_pointer函数会设置一个memory barrier，以确保之前的所有写操作都执行完，再将head指向新分配的链表元素e。 之后就是释放锁。 之后调用synchronize_rcu确保任何一个可能持有了旧的链表元素的CPU都执行一次context switch，因此这些CPU会放弃指向旧链表元素的指针。 最后是释放旧的链表元素。 这里有件事情需要注意，在数据读取代码中，我们可以在循环中查看链表元素，但是我们不能将链表元素返回。例如，我们使用RCU的时候，不能写一个list_lookup函数来返回链表元素，也不能返回指向链表元素中数据的指针，也就是不能返回嵌入在链表元素中的字符串。\n我们必须只在RCU critical区域内查看被RCU保护的数据，如果我们写了一个通用的函数返回链表元素，或许我们能要求这个函数的调用者也遵循一些规则，但是函数的调用者还是可能会触发context switch。\n如果我们在函数的调用者返回之前调用了rcu_read_unlock，这将会违反23.5中的规则1，因为现在定时器中断可以迫使context switch，而被RCU保护的数据指针仍然被持有者。所以使用RCU的确会向数据读取者增加一些之前并不存在的限制。\n学生提问：这样是不是说我们不可能返回下标是i的元素所包含的内容？\nRobert教授：可以返回一个拷贝，如果e-\u0026gt;x是个字符串，那么我们可以返回一个该字符串的拷贝，这是没有问题的。\n但是如果我们直接返回一个指针指向e-\u0026gt;x，那就违反了RCU规则。实际上返回e中的任何指针都是错误的，因为我们不能在持有指向RCU保护数据的指针时，发生context switch。通常的习惯是直接在RCU critical区域内使用这些数据。\n接下来我将再简短的介绍性能。\n如果你使用RCU，数据读取会非常的快，除了读取数据本身的开销之外就几乎没有别的额外的开销了。如果你的链表有10亿个元素，读取链表本身就要很长的时间，但是这里的时间消耗并不是因为同步（注，也就是类似加锁等操作）引起的。\n所以你几乎可以认为RCU对于数据读取者来说没有额外的负担。唯一额外的工作就是在rcu_read_lock和rcu_read_unlock里面设置好不要触发context switch，并且在rcu_dereference中设置memory barrier，这些可能会消耗几十个CPU cycle，但是相比锁来说代价要小的多。\n对于数据写入者，性能会更加的糟糕。\n首先之前使用锁的时候所有的工作仍然需要做，例如获取锁和释放锁。其次，现在还有了一个可能非常耗时的synchronize_rcu函数调用。实际上在synchronize_rcu内部会出让CPU，所以代码在这不会通过消耗CPU来实现等待，但是它可能会消耗大量时间来等待其他所有的CPU核完成context switch。\n所以基于数据写入时的多种原因，和数据读取时的工作量，数据写入者需要消耗更多的时间完成操作。如果数据读取区域很短（注，这样就可以很快可以恢复context switch），并且数据写入并没有很多，那么数据写入慢一些也没关系。所以当人们将RCU应用到内核中时，必须要做一些性能测试来确认使用RCU是否能带来好处，因为这取决于实际的工作负载。\n你们应该已经看到了RCU并不是广泛通用的，你不能把所有使用spinlock并且性能很差的场景转化成使用 RCU，并获得更好的性能。\n主要的原因是RCU完全帮不到写操作，甚至会让写操作更慢，只有当读操作远远多于写操作时才有可能应用RCU。\n因为RCU有这样的限制：代码不能在sleep的时候持有指针指向被RCU保护的数据，所以这会使得一些代码非常奇怪。当一定要sleep的时候，在sleep结束之后需要重新进入RCU critical区域再次查找之前已经看过的数据，前提是这些数据还存在。所以RCU使得代码稍微复杂了一些。\n另一方面可以直接应用RCU的数据结构在更新时，需要能支持单个操作的committing write。\n你不能在原地更新数据，而是必须创建一个新的链表元素对象来替代之前的元素对象。所以单链表，树是可以应用RCU的数据结构，但是一些复杂的数据结构不能直接使用RCU。\n论文 里面提到了一些更复杂的方法，例如sequence lock，可以在允许原地更新数据的同时，又不用数据读取者使用锁。但是这些方法要复杂一些，并且能够提升性能的场景也是受限的。\n另一个小问题是，RCU并没有一种机制能保证数据读取者一定看到的是新的数据。\n因为如果某些数据读取者在数据写入者替换链表元素之前，获取了一个指针指向被RCU保护的旧数据，数据读取者可能会在较长的时间内持有这个旧数据。大部分时候这都无所谓，但是论文提到了在一些场景中，人们可能会因为读到旧数据而感到意外。\n作为一个独立的话题，你们或许会想知道对于一个写操作频繁的数据该如何提升性能。\nRCU只关心读操作频繁的数据，但是这类数据只代表了一种场景。在一些特殊场景中，写操作频繁的数据也可以获取好的性能，但是我还不知道存在类似RCU这样通用的方法能优化写操作频繁的数据。不过仍然有一些思路可以值得借鉴。\n最有效的方法就是重新构造你的数据结构，这样它就不是共享的。有的时候共享数据完全是没必要的，一旦你发现数据共享是个问题，你可以尝试让数据不共享。\n但是某些时候你又的确需要共享的数据，而这些共享数据并没有必要被不同的CPU写入。\n实际上你们已经在lab中见过这样的数据，在locking lab的kalloc部分，你们重构了free list使得每个CPU核都有了一个专属的free list，这实际上就是将一个频繁写入的数据转换成了每个CPU核的半私有数据。\n大部分时候CPU核不会与其他CPU核的数据有冲突，因为它们都有属于自己的free list。唯一的需要查看其他CPU核的free list的场景是自己的free list用光了。\n有很多类似的例子用来处理内核中需要频繁写入的数据，例如Linux中的内存分配，线程调度列表。对于每个CPU核都有一套独立的线程对象以供线程调度器查看（注，详见11.8，线程对象存储在struct cpu中）。CPU核只有在自己所有工作都完成的时候才会查看其他CPU核的线程调度列表。\n另一个例子是统计计数，如果你在对某个行为计数，但是计数变化的很频繁，同时又很少被读出，你可以重构你的计数器，使得每个CPU核都有一个独立的计数器，这样每个CPU核只需要更新属于自己的计数器。当你需要读取计数值时，你只需要通过加锁读出每个CPU核的计数器，然后再加在一起。\n这些都是可以让写操作变得更快的方法，因为数据写入者只需要更新当前CPU核的计数器，但是数据读取者现在变得更慢了。如果你的计数器需要频繁写入，实际上通常的计数器都需要频繁写入，通过将更多的工作转换到数据读取操作上，这将会是一个巨大的收益。\n这里想说的是，即使我们并没有太讨论，但是的确存在一些技术在某些场合可以帮助提升需要频繁写入数据的性能。\n最后总结一下，论文中介绍的RCU对于Linux来说是一个巨大的成功。\n它在Linux中各种数据都有使用，实际中需要频繁读取的数据还挺常见的，例如block cache基本上就是被读取，所以一种只提升读性能的技术能够应用的非常广泛。\n尽管已经有了许多有趣的并发技术，同步（synchronization）技术，RCU还是很神奇，因为它对数据读取者完全去除了锁和数据写入（注，这里说的数据写入是指类似读写锁时的计数值，但是RCU在读数据的时候还是需要写标志位关闭context switch，只是这里的写操作代价并不高），所以相比读写锁，RCU是一个很大的突破。\nRCU能工作的核心思想是为资源释放（Garbage Collection）增加了grace period，在grace period中会确保所有的数据读取者都使用完了数据。所以尽管RCU是一种同步技术，也可以将其看做是一种特殊的GC技术。\n学生提问：为什么数据读取者可以读到旧数据呢？在RCU critical区域里，你看到的应该就是实际存在的数据啊？\nRobert教授：通常来说这不是个问题。通常来说，你写代码，将1赋值给x，之后print ”done“。\n在print之后，如果有人读取x，可能会看到你在将1赋值给x之前x的数值，这里或许有些出乎意料。而RCU允许这种情况发生，如果我们在使用RCU时，并将数据赋值改成list_replace，将包含1的元素的内容改成2。\n在函数结束后，我们print ”done“。\n如果一些其他的数据读取者在查看链表，它们或许刚刚看到了持有1的链表元素，之后它们过了一会才实际的读取链表元素内容，并看到旧的数值1（注，因为RCU是用替换的方式实现更新，数据读取者可能读到了旧元素的指针，里面一直包含的是旧的数值）。\n所以这就有点奇怪了，就算添加memory barrier也不能避免这种情况。不过实际上大部分场景下这也没关系，因为这里数据的读写者是并发的，通常来说如果两件事情是并发执行的，你是不会认为它们的执行顺序是确定的。\n但是论文中的确举了个例子说读到旧数据是有关系的，并且会触发一个实际的问题，尽管我并不太理解为什么会有问题。\n学生提问：RCU之所以被称为RCU，是因为它的基本实现对吧？\nRobert教授：Read-Copy-Update，是的我认为是因为它的基本实现，它不是在原地修改数据，你是先创建了一个拷贝再来更新链表。\n学生提问：在介绍读写锁时，我们讨论了为了实现缓存一致需要O(n^2)时间。对于spinlock这是不是也是个问题，为什么我们在之前在介绍spinlock的时候没有讨论这个问题，是因为spinlock有什么特殊的操作解决了这个问题吗？\nRobert教授：并没有，锁的代价都很高。如果没有竞争的话，例如XV6中的标准spinlock会非常快。但是如果有大量的CPU核在相同的时候要获取相同的锁就会特别的慢。存在一些其他的锁，在更高负载的时候性能更好，但是在更低负载的时候性能反而更差。这里很难有完美的方案。\n学生提问：或许并不相关，可能存在不同操作系统之间的锁吗？\nRobert教授：在分布式系统中，有一种锁可以存在于多个计算机之间。\n一个场景是分布式数据库，你将数据分发给多个计算机，但是如果你想要执行一个transaction，并使用分布在多个计算机上的数据，你将需要从多个计算机上收集锁。\n另一个场景是，有一些系统会尝试在独立的计算机之间模拟共享内存，比如说一个计算机使用了另一个计算机的内存，背后需要有一些工具能够使得计算机之间能交互并请求内存。\n这样就可以在一个集群的计算机上运行一些现有的并行程序，而不是在一个大的多核计算机上，这样成本会更低。\n这时需要对spinlock或者任何你使用的锁做一些额外的处理，人们发明了各种技术来使得锁能很好的工作，这些技术与我们介绍的技术就不太一样了，尽管避免性能损失的压力会更大。\n","date":"2024-10-08T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08123rcu/bg_hu_4d83e8134cb7c5ba.jpg","permalink":"https://echudet.github.io/p/mit6s08123rcu/","title":"MIT6S081：23RCU"},{"content":"Lecture 22 Meltdown 课前预习 论文速览 论文链接 摘要部分概述了Meltdown漏洞的关键概念。Meltdown利用现代处理器的乱序执行特性来从用户空间读取内核内存数据。它不依赖于任何软件漏洞，而是利用硬件层面的设计缺陷。这一漏洞广泛影响了个人计算机、云服务器中的虚拟机等设备。文章还特别提到KAISER防御机制在阻止Meltdown方面的意外效果。漏洞的严重性在于，它打破了内存隔离的安全保证，使攻击者能够读取其他进程或虚拟机的内存。\n引言部分深入讨论了现代操作系统中内存隔离的重要性，特别是用户进程和内核之间的隔离。文章指出，Meltdown漏洞破坏了这种隔离机制，使攻击者能够访问内核内存。这一漏洞不依赖操作系统或软件漏洞，而是通过利用处理器硬件特性来实施攻击。引言还强调了Meltdown可以在不需要权限的情况下读取敏感数据，这对个人计算机用户和云计算环境中的用户都构成了重大威胁。\n背景部分介绍了Meltdown攻击所依赖的关键技术，包括乱序执行、地址翻译和缓存攻击。乱序执行是现代处理器的性能优化机制，它允许CPU提前执行指令以提高效率，然而这种执行方式会带来缓存的副作用，成为信息泄露的基础。地址翻译部分解释了虚拟地址如何映射到物理地址，并描述了内核地址如何与用户空间隔离。缓存攻击则涉及利用缓存的时序差异来推断内存中的数据。\n玩具示例提供了一个简单的代码示例，用以演示乱序执行如何通过缓存的副作用泄露数据。代码中，虽然某些指令会由于异常而未实际执行，但其带来的缓存变化仍然能够被利用来推测数据。这一部分的核心在于展示了即使未完成的指令也可能对微架构状态产生影响，而这正是Meltdown攻击的基础。\n攻击组成部分描述了Meltdown的两个主要组成部分。首先是“瞬态指令”的执行，这些指令是由于乱序执行而提前执行的，尽管它们在架构层面上没有影响，但它们可以通过微架构层次泄露数据。其次是通过隐蔽通道将泄露的信息传递到外部世界。文章详细解释了如何通过这些瞬态指令访问和泄露内核中的数据。\nMeltdown攻击部分具体介绍了如何在实际系统中实施攻击。通过利用乱序执行，攻击者可以访问操作系统的内核内存，包括敏感的用户数据、密码等。Meltdown能够在多种操作系统上进行攻击，包括Linux、Windows和Android。文中还描述了在这些系统中读取整个物理内存的方法，并讨论了攻击的速度和成功率。\n评估部分通过实验验证了Meltdown的攻击效果。实验在不同的硬件平台上进行了攻击测试，结果表明攻击的成功率和速度与缓存状态密切相关。评估数据展示了在最佳情况下，攻击者可以以数百KB每秒的速度读取内核内存数据，并且攻击在不同操作系统上的表现也有所不同。\n局限性部分讨论了Meltdown在ARM和AMD处理器上的效果。虽然Meltdown能够在大多数Intel处理器上成功运行，但在某些ARM和AMD架构上，Meltdown无法被复现。文章推测这是由于这些处理器在处理权限检查和内存访问时的不同实现方式所致。\n防御措施部分概述了针对Meltdown的可能防御手段。主要提到了通过硬件和软件改进来阻止攻击的方式。硬件设计的改变可以有效阻止此类漏洞的再次出现，例如通过严格的内存访问权限检查。文章特别提到，KAISER补丁是目前最有效的短期解决方案，因为它阻止了攻击者访问内核内存。\n相关工作部分回顾了与Meltdown相关的研究，包括之前的缓存侧信道攻击和推测执行攻击。文中强调了Meltdown与这些攻击的区别，并说明了Meltdown在攻击范围和成功率上的独特之处。\n结论部分总结了Meltdown漏洞对现代计算系统的严重威胁，并呼吁立即采取防御措施，特别是部署KAISER补丁。文章还指出，长期的解决方案可能需要从硬件设计的角度进行修复，以防止未来类似漏洞的产生。\n侧信道攻击（Side-Channel Attack） 侧信道攻击（Side-Channel Attack）是一类通过分析计算机系统中硬件、软件执行时产生的物理信息或微观行为特征来获取敏感信息的攻击手段，而不是通过传统的软件漏洞或直接的破解加密算法等方式进行攻击。侧信道攻击通常利用处理器执行时的时序、功耗、电磁辐射、缓存访问行为等外部特征来推测出某些敏感数据。\n在论文中，讨论的是乱序处理器（out-of-order processor）的缓存侧信道攻击，并且是与Meltdown漏洞相关。这涉及到现代处理器中复杂的执行优化技术，以及这些技术在某些极端情况下可能暴露的信息安全隐患。\nMeltdown 漏洞是什么？Meltdown 是在 2018 年公开的一类严重的硬件漏洞，主要影响英特尔的处理器（也有部分 AMD 和 ARM 处理器）。该漏洞利用了现代处理器的乱序执行机制，允许攻击者在未授权的情况下读取内核空间或其他进程中的敏感数据。\n乱序执行是处理器为了提高性能，提前执行一些看似可以立即进行的指令，即使这些指令的前置条件（如内存访问权限检查）还未完成。而 Meltdown 恰恰利用了这个特性：虽然最终系统会撤销未经授权的操作，但在执行期间，数据可能已经进入了处理器的缓存中，从而留下痕迹。\n缓存侧信道攻击是什么？侧信道攻击常与缓存的行为密切相关。缓存是一种快速存取的数据存储区域，存放处理器频繁访问的数据。不同级别的缓存（L1、L2、L3）会极大地影响程序的执行效率和数据访问时延。\n在 Meltdown 攻击中，攻击者利用乱序执行机制，通过分析内存访问时引发的缓存行为，泄露出敏感信息。具体来说，缓存侧信道攻击可以通过缓存命中与失效的时序差异来判断某些内存单元是否已被访问（即是否已加载到缓存中）。这可以通过类似于Flush+Reload或Prime+Probe这样的攻击技术实现：\nFlush+Reload：攻击者使用 clflush 指令清空特定的缓存行，然后观察该缓存行的加载时延。如果攻击目标的某个内存地址被加载回缓存，攻击者通过时延测量可以推断出相关数据。 Prime+Probe：攻击者在缓存中占据一定的空间，然后测量其他程序对该缓存区域的影响，进而推测程序行为。 Meltdown 漏洞与缓存侧信道攻击的结合非常关键。Meltdown 攻击本质上是通过乱序执行让处理器提前访问到本不应被访问的数据，虽然这些数据无法直接被读取或使用，但它们会短暂存储在缓存中。然后，攻击者可以通过侧信道分析技术（如缓存时序分析）推测出这些数据。\n例如，攻击者在利用 Meltdown 读取未经授权的内核空间数据时，可以在数据进入缓存后，使用缓存时序分析来判断访问了哪些内存地址，以及这些地址存放了什么数据。\n所以，在论文讨论的对乱序处理器缓存进行侧信道攻击，是在描述一种通过缓存行为监控的方式，利用处理器在乱序执行过程中暴露的安全漏洞（如 Meltdown），从而窃取敏感数据的攻击手段。\n这种攻击方式的流程可能如下：\n攻击者诱使处理器触发乱序执行。 在处理器访问未经授权的内核空间或其他进程的数据时，这些数据会暂时存入缓存中。 攻击者通过缓存侧信道技术（如 Flush+Reload 或 Prime+Probe）分析缓存行为，获取敏感数据的相关信息。 这种类型的攻击利用了硬件的特性（如乱序执行、缓存行为）以及微架构中的信息泄漏漏洞，超越了传统的安全模型，因此在公开时引发了广泛的关注和讨论。\nMeltdown + 侧信道攻击 的完整过程 从攻击的准备开始，逐步展示攻击者是如何利用处理器的乱序执行特性、缓存行为以及侧信道分析来泄露内核或其他敏感数据的。\n攻击准备：隔离的内存空间\n在现代操作系统中，用户进程和内核空间（包括操作系统的关键数据和其他进程的内存）是隔离的。通常情况下，用户进程不能直接访问内核空间的数据，因为这种访问会触发页面故障（page fault）并**作系统阻止。\n但是，Meltdown 攻击利用处理器的乱序执行特性，使得在某些情况下，即使内核数据不应被用户态进程访问，处理器仍可能提前执行读取内核内存的指令——即使最终这些指令会因为权限问题被回滚。\n利用乱序执行访问未经授权的内核数据\nMeltdown 的核心在于处理器的乱序执行机制。乱序执行允许处理器为了优化性能，提前执行一些指令，而不必等待前面的指令完成。例如，当处理器等待较慢的内存访问完成时，它可以提前执行某些后续指令。\n在 Meltdown 攻击中，攻击者可以诱使处理器访问属于内核空间的数据，尽管这类访问通常是不被允许的。这可以通过构造如下代码实现：\n1 unsigned char kernel_data = *(unsigned char*)kernel_address; 此代码试图从内核地址 kernel_address 读取数据。尽管内核数据通常不能被用户态程序直接访问，处理器可能会通过乱序执行提前读取该数据。但是，一旦处理器发现该读取操作无权限，它将撤销此操作，并触发页面故障异常。但在 Meltdown 攻击中，尽管读取的结果不能被直接使用，该数据已被暂时存入缓存中。\n数据进入缓存，留下痕迹\n乱序执行导致的数据被存储到缓存，即使随后这些数据被撤销且处理器不允许访问它们。然而，这些内存访问留下了一个副作用：数据被加载到了缓存中。此时，数据的物理内容可能无法被直接访问，但缓存的行为泄露了这些信息。\n通过缓存侧信道攻击获取数据\n接下来，攻击者利用缓存侧信道技术，如 Flush+Reload，来提取数据。这个步骤关键在于，通过测量缓存的访问时延，攻击者可以推测哪些数据已经加载到了缓存中，从而间接推测内核数据。\nFlush+Reload 攻击机制的具体步骤：\n步骤 1：清空缓存 攻击者首先使用 clflush 指令清空缓存中的特定内存行。通过清空缓存，攻击者确保之后的访问会重新加载数据到缓存中，从而使得缓存命中与失效的时间差异显著。 步骤 2：诱导乱序执行读取内核数据 通过前面提到的代码，诱导处理器乱序执行去读取未经授权的内核数据。尽管最终读取会被操作系统阻止，但在短时间内该数据已经被加载到缓存中。 步骤 3：测量时延，推测数据 攻击者再次尝试访问缓存中被加载的数据，通过测量访问的时间差，可以推测出这些数据是否已经在缓存中。如果数据已经被缓存，访问时间会非常短（命中缓存）；如果数据没有被缓存，访问时间则会显著增加（缓存未命中，需要从内存加载）。通过这种时延分析，攻击者可以逐字节推测内核数据的值。 整个流程的总结\n从上面的步骤可以看出，Meltdown + 侧信道攻击的关键点在于处理器的乱序执行和缓存行为。攻击者利用乱序执行机制读取到不应访问的内核数据，并利用缓存时延差异通过侧信道技术来间接推测数据的内容。这个攻击过程分为以下主要阶段：\n触发乱序执行：攻击者构造代码，迫使处理器访问未经授权的内核数据。 数据进入缓存：虽然访问被撤销，但数据暂时存入缓存。 缓存时延分析：通过 Flush+Reload 等侧信道攻击技术，利用缓存命中与失效的时间差，攻击者间接获取敏感数据。 例子：泄露内核空间中的一个字节\n假设攻击者想要泄露内核地址 0xFFFF0000 中的数据，该地址包含一个字节的敏感数据。\n首先，攻击者清空缓存中的某些内存行，确保后续缓存行为是可测量的。\n通过乱序执行读取内核数据：\n攻击者构造代码，如：\n1 2 unsigned char kernel_data = *(unsigned char*)0xFFFF0000; array[kernel_data * 4096] = 1; 这个操作试图从内核地址读取一个字节，并将其值用于访问一个数组。这段代码会被乱序执行提前处理，而敏感数据会被存入缓存。\n缓存中的数据被测量：\n由于内存访问时敏感数据已经进入缓存，攻击者通过测量缓存时延来判断 array[kernel_data * 4096] 是否已经在缓存中。 通过多个猜测，最终可以推断出 kernel_data 的实际值。 总而言之，Meltdown 攻击利用了处理器的乱序执行，允许攻击者访问通常无法读取的内核数据，而缓存侧信道攻击则为这些数据的提取提供了有效手段。通过缓存命中和未命中的时延差异，攻击者能够逐步推测出内核空间中的数据，尽管系统本身阻止了未经授权的访问。\n数据读取过程（微架构状态如何转换为架构状态） 虽然侧信道可以让我们知道缓存的某个位置是否被加载了数据，但如何从缓存中真正推测出数据的值似乎还不够清晰。确实，侧信道攻击通过分析缓存访问时延，能够推测哪些数据被缓存，但问题是如何利用这些信息来推断具体的数值。在下面，我将进一步细化这个过程，以解释数据具体是如何从缓存中读取出来的。\n关键点：使用缓存时延推测数据值\n侧信道攻击本质上不是直接读取数据，而是通过缓存行为推测出数据的内容。在 Meltdown + 侧信道攻击中，缓存的命中或失效是由我们控制的代码触发的，而我们可以利用这个控制来获取敏感数据。\n为了更直观地理解，一个具体的例子将详细解释，如何通过缓存时延分析逐字节提取敏感数据。\n攻击环境准备，假设攻击者正在尝试通过 Meltdown 漏洞访问内核空间中某个敏感字节的值。为了实现这一目的，攻击者构建如下代码：\n1 2 unsigned char kernel_data = *(unsigned char*)kernel_address; array[kernel_data * 4096] = 1; kernel_address 是攻击者试图访问的敏感内核数据的地址。\narray[] 是一个攻击者可控的数组，大小足够大，通常为 256 * 4096 字节，这样可以利用每个缓存行（通常是 64 字节）的不同位置来探测数据。\n关键在于 kernel_data 是从内核空间读取的未经授权的数据。虽然处理器最终会拒绝这一访问并触发页面故障（权限不允许访问），但在触发异常之前，乱序执行可能已经将数据暂时加载到缓存中，并用于后续的代码执行。\n使用乱序执行加载数据，当处理器提前执行这段代码时，kernel_data 的值会用于数组 array[] 的索引计算。即使最终这个操作因为权限问题被撤销，处理器已经使用了 kernel_data 的值，并将它反映在缓存行为中。\n测量缓存时延来推测数据，关键步骤来了：攻击者如何通过缓存侧信道推测出 kernel_data 的具体值？这就是通过测量缓存时延来完成的，通常使用类似 Flush+Reload 或 Prime+Probe 技术。以 Flush+Reload 为例，具体操作步骤如下：\n清空缓存（Flush）：在执行上述代码之前，攻击者先通过 clflush 指令清空缓存中 array[] 的所有内容。这样可以确保接下来的操作导致的数据访问都会重新加载。\n执行代码，触发乱序执行：处理器在运行乱序执行时，提前读取了 kernel_address 并且把这个读取的结果 kernel_data 作为索引用于访问 array[kernel_data * 4096]。此时，虽然实际的 kernel_data 未被攻击者直接读取到，但它的值影响了数组访问的缓存命中。\n测量时延（Reload）：代码执行后，攻击者尝试依次访问 array[] 的不同部分（每个间隔 4096 字节），并测量每次访问的时延。由于缓存命中时访问会比缓存失效时更快，攻击者可以通过时延分析发现哪一部分的 array[] 访问速度明显较快。\n如果某个地址 array[kernel_data * 4096] 的访问非常快（即缓存命中），攻击者就可以推测 kernel_data 的值为该部分的索引。 具体例子：逐字节提取数据。假设 kernel_address 指向的内存地址包含一个字节的值，例如 0x41（ASCII 对应 \u0026lsquo;A\u0026rsquo;）。此时，kernel_data = 0x41。攻击者通过以下方式逐步推测出 kernel_data 的值：\n处理器执行代码 array[kernel_data * 4096] = 1;，此时 array[0x41 * 4096] 被访问，导致这部分数据加载到缓存中。 攻击者接着通过 clflush 清除缓存，并开始依次访问 array[0 * 4096], array[1 * 4096], ..., array[255 * 4096]，测量每个访问的时延。 当攻击者访问 array[0x41 * 4096] 时，发现该访问非常快（缓存命中），因此可以推测出 kernel_data = 0x41，即这个字节的值是 0x41。 整个攻击过程的核心是：通过乱序执行使得敏感数据进入缓存，并通过缓存时延分析侧信道技术，间接推测出这些敏感数据的值。\n侧信道分析并不是直接读取缓存中的数据，而是通过控制访问和测量缓存时延，来推测哪些数据在缓存中被命中了，从而一步步推导出 kernel_data 的值。\n完整过程回顾\n诱导乱序执行：通过访问内核数据并构建某种操作，使得数据暂时存入缓存。 清空缓存：在测量之前清除缓存，以确保后续时延测量的准确性。 测量缓存时延：通过访问特定缓存行并测量访问时间差，推测缓存中已命中的数据值。 逐字节推测内核数据：通过这种方式，可以字节级地推测出敏感数据。 通过这种缓存时延分析，Meltdown 能够从无法直接读取的内存区域中逐字节地提取出数据。\n预测执行 为了确保 预测执行（或称为乱序执行）一定发生，特别是在你提到的循环中，可以采取一些策略，这些策略能够诱导处理器执行提前加载（prefetching）或预测性执行，以提高成功的概率。在详细解释之前，我们先定义一些相关概念：\n预测执行与乱序执行\n预测执行（Speculative Execution）是处理器在不确定某个条件是否成立时，提前执行可能需要执行的指令。比如，在条件判断（如 if 或 while）之前，处理器可能会提前执行某些后续的指令，假设条件为真或假。\n乱序执行（Out-of-Order Execution）是指处理器根据当前的资源可用性、指令的依赖关系等，提前执行不依赖于当前正在等待的指令的部分操作。这种执行机制有助于提高处理器的吞吐量。\n如何确保预测执行一定发生，为了确保预测执行在循环（或其他控制结构）中尽可能发生，可以设计代码，使处理器尽量进入预测执行路径。以下是常用的策略：\n使用条件分支和预测，预测执行经常发生在条件分支的地方，特别是 if-else 或 switch 语句。处理器通过预测某个条件将会成立（例如分支预测器认为 if 条件为真），进而提前执行某些指令。为了确保预测执行可以发生，你可以故意引入复杂的条件判断： 1 2 3 4 5 for (int i = 0; i \u0026lt; 1000; i++) { if (condition) { // 关键指令在这里执行 } } 这种情况下，处理器会根据历史记录和分支预测算法，决定是否提前执行 if 分支中的代码。你可以通过构造复杂且动态的条件，让预测执行变得更加可能。\n控制指令顺序，确保代码中有足够多的指令可以并行执行，特别是在循环体中。当前面的指令可能导致内存访问等需要等待的操作时，处理器会试图执行后续的指令。这种环境下，乱序执行也会发生。你可以在循环中引入一些独立的指令，从而确保处理器在等待某些内存访问时，仍然可以预测性地执行其他指令。 prefetch 的含义，Prefetch 是指提前加载数据到缓存中，目的是减少内存访问时的延迟。现代处理器会自动进行预取操作，但程序员也可以手动触发预取指令，将将来可能需要的数据提前加载到缓存中。它的主要作用是提高程序的性能，避免因访问主内存而产生的高延迟。\n在具体操作中，预取可以通过硬件自动完成，也可以通过显式的指令实现，比如 __builtin_prefetch()，它允许程序员手动指定要提前加载的数据。这在需要频繁访问大量数据的情况下尤其有用。\n1 __builtin_prefetch(\u0026amp;data, 0, 3); 这里 \u0026amp;data 是预取的数据地址，0 表示这是只读预取，3 是预取的优先级。通过预取，程序可以在需要数据之前将它加载到缓存中，避免内存访问延迟。\n如何结合预取与预测执行，在使用 Meltdown 或其他类似攻击时，如果你希望诱导处理器提前加载某些数据（例如敏感数据）到缓存中，你可以：\n利用循环和条件分支来确保乱序执行或预测执行。\n结合预取，让处理器将目标内存数据提前加载到缓存中。\n例如，你可以在一个循环中使用 __builtin_prefetch() 来让处理器提前加载内核中的某些内存地址。尽管这些内核地址在用户态不允许直接访问，但利用 Meltdown 类漏洞，攻击者可以通过侧信道技术推测它们的内容。\n假设你有一个想要预取的内核地址 kernel_address，你可以在循环中手动预取该地址：\n1 2 3 4 for (int i = 0; i \u0026lt; 1000; i++) { __builtin_prefetch((void*)kernel_address, 0, 3); // 后续操作触发缓存侧信道攻击 } 总结\n预测执行可以通过复杂的条件判断和控制指令顺序来诱发，特别是在循环和分支结构中，它能提高触发 Meltdown 类攻击的可能性。 乱序执行是处理器自动进行的一种优化，程序设计中可以通过提供独立的操作来诱导它提前执行某些指令。 Prefetch 是提前将数据加载到缓存的手段，可以手动指定特定的内存地址进行预取，从而在攻击中加快对缓存状态的推测。 Flush+Reload Flush+Reload 是一种典型的缓存侧信道攻击方法，广泛应用于如 Meltdown、Spectre 等漏洞的利用中。它主要利用了共享内存（如共享库或页面）中的缓存行为，结合缓存命中与失效的时延差异，来推测内存中的数据内容。Flush+Reload 的核心在于通过清除缓存和测量缓存命中时的时延，获取受害者程序对特定内存区域的访问情况。\nFlush+Reload 攻击依赖于现代处理器的多级缓存体系和内存共享机制。其基本工作流程如下：\nFlush（清空缓存）：攻击者首先通过 clflush 指令将目标内存地址对应的缓存行从缓存中移除，确保接下来对该地址的访问会从主内存重新加载，而不是从缓存读取。 Victim 访问（受害者访问）：受害者程序运行期间，可能会访问这个内存地址。当受害者访问该地址时，数据会被加载到缓存中，而这个缓存行为是攻击者所无法直接观察到的。 Reload（重新加载并测量时延）：接着，攻击者再次访问该地址，并测量访问的时延。如果访问时延较短，说明数据已经在缓存中（受害者程序访问过该地址）；如果时延较长，说明缓存未命中，数据需要从主内存加载。 Flush+Reload 的详细流程\n准备阶段：确定共享内存区域。Flush+Reload 攻击通常需要一个共享的内存区域，攻击者和受害者程序都可以访问这一内存区域。这可以是一个共享的库（如 .so 文件），或者是一个共享的内存页面。在 Meltdown 攻击中，攻击者可能会试图访问内核空间的某个地址。\nFlush：清除缓存行。攻击者首先使用 clflush 指令将目标地址对应的缓存行从缓存中清除。clflush 是一个处理器指令，它会强制清空某个内存地址所在的缓存行，无论该地址是否在 L1、L2 或 L3 缓存中。\n1 clflush(\u0026amp;shared_memory_address); // 清空 shared_memory_address 对应的缓存行 通过这一步操作，攻击者确保缓存中没有目标地址的数据，确保接下来对该地址的访问一定会引发缓存行为（缓存命中或未命中）。\n受害者程序执行。在攻击者执行 clflush 后，受害者程序继续执行。在它的执行过程中，如果受害者程序访问了之前被清空缓存的那个地址，数据会被重新加载到缓存中。\n这一步对于攻击者是不可见的，攻击者无法直接知道受害者程序是否访问了该地址，但这正是 Flush+Reload 要通过测量时延来推测的部分。\nReload：访问并测量时延。攻击者接下来会重新访问刚刚清空缓存的那个内存地址，并测量访问的时延。这里使用类似 rdtsc（读取时间戳计数器）的指令来精确计量该地址的访问时延。\n缓存命中：如果受害者程序访问了该地址，数据已经被加载到缓存中，那么攻击者的访问会非常快，通常在几个 CPU 周期内完成。\n缓存未命中：如果受害者程序没有访问该地址，数据没有被加载到缓存中，攻击者的访问会较慢，因为处理器需要从主内存重新加载数据。\n代码示例：\n1 2 3 4 start_time = rdtsc(); access(shared_memory_address); // 访问目标内存地址 end_time = rdtsc(); time_diff = end_time - start_time; 通过访问时延的差异，攻击者可以推测受害者程序是否访问了该地址。如果访问时间短，则说明该地址已经在缓存中（缓存命中）；如果访问时间长，则说明该地址未被缓存（缓存未命中）。\n多次重复，推测数据。Flush+Reload 攻击的威力在于，它可以通过多次采样，并结合对多个地址的访问时延，逐步推测出受害者程序访问了哪些数据，并且可能进一步推测这些数据的值。\n在某些攻击场景中，例如 Meltdown，攻击者会构造某种数据依赖关系，使得内核数据（或其他敏感数据）加载到缓存中，通过时延差异来推测这些数据。\nFlush+Reload 的应用场景\n加密算法泄露：Flush+Reload 经常被用来攻击基于内存访问模式的加密算法（如 AES）。攻击者可以监控加密算法在执行过程中对内存的访问，从而推测出加密密钥的部分或全部。 Meltdown/Spectre 攻击：Flush+Reload 在 Meltdown 和 Spectre 攻击中被广泛使用。Meltdown 攻击诱导处理器乱序执行访问不该访问的内核空间数据，而 Flush+Reload 则用来通过缓存时延分析间接提取这些数据。 共享库攻击：攻击者可以通过 Flush+Reload 分析共享库的缓存行为，监控其他程序对共享库函数的调用情况，从而推测出程序的行为或数据。 优点 局限 精确性高：Flush+Reload 的缓存时延测量非常精确，可以区分出极小的访问时间差异。 需要共享内存：Flush+Reload 依赖于攻击者和受害者程序共享某个内存区域，才能有效地监控缓存行为。 高效性：因为只需通过测量缓存行为进行推测，Flush+Reload 可以在无需直接访问内存数据的情况下泄露信息。 防御机制有效：现代操作系统和处理器在修复 Meltdown、Spectre 等漏洞时，采取了一些防御机制，如缓存隔离、限制缓存刷新指令的使用等，可能会降低 Flush+Reload 的效果。 总的来说，Flush+Reload 是一种强大的缓存侧信道攻击方法，利用缓存时延分析推测内存访问行为。它通过 clflush 指令清空缓存、受害者程序访问内存、攻击者重新加载数据并测量时延，来判断受害者程序是否访问了特定的内存区域。这种方法在加密算法泄露和 Meltdown、Spectre 等漏洞的利用中具有广泛的应用，但它也需要依赖共享内存和较高精度的时延测量技术。\nFlush+Reload 查询对象 在 Flush+Reload 攻击中，查询的是缓存中的数据，而不是直接查询页表或页表中的缓存行。Flush+Reload 的核心是基于缓存行的状态（命中或未命中），而不涉及具体的页表结构。\n为了更清楚地解释这一点，让我们深入了解 Flush+Reload 的运作机制：\n缓存与内存的关系\n在现代处理器中，内存访问的速度相对较慢，而缓存（L1、L2、L3）是一种高速的存储层，用于存放处理器频繁使用的数据。缓存以 缓存行（通常 64 字节）的单位进行管理。\n当处理器访问某个内存地址时，它首先会检查该地址是否已经在缓存中：\n命中（Cache Hit）：如果该数据已经在缓存中，处理器可以直接读取，速度极快。\n未命中（Cache Miss）：如果缓存中没有该数据，处理器需要从更慢的主内存中加载，造成较大的时延。\nFlush+Reload 的基本原理\nFlush+Reload 是一种缓存侧信道攻击技术，它的工作原理基于对特定缓存行的命中与未命中状态的监控。具体操作步骤如下：\nFlush：首先，攻击者使用 clflush 指令，将目标内存地址对应的缓存行从缓存中移除，确保缓存行处于未命中的状态。这个目标地址通常指向一个共享的内存区域，比如一个共享的库或页面（在 Meltdown 攻击中，可能是敏感数据的地址）。\nVictim 访问：然后，受害者程序可能会访问这个内存地址。这会导致数据被重新加载到缓存中，但攻击者此时不知道受害者是否真的访问了这个地址。\nReload：攻击者随后再访问相同的内存地址，并测量访问时延。如果访问速度非常快（缓存命中），攻击者可以推测该地址已经被受害者程序访问并加载到缓存中。\n与物理页表无关\nFlush+Reload 攻击关注的是内存地址对应的缓存行，而不是查询或直接操作物理页表或页表缓存。\n页表是操作系统管理虚拟内存和物理内存映射的结构。它决定虚拟地址映射到哪个物理地址。然而，Flush+Reload 的核心关注点是物理内存地址的缓存状态，并不涉及页表的具体内容或查询页表中的缓存行。\n攻击对象是缓存行，而非页表\n在 Flush+Reload 中：\nFlush 清除的是目标内存地址的缓存行。\nReload 通过测量时延，检查是否该地址对应的缓存行已经被加载到缓存中。\n虽然页表决定了虚拟地址和物理地址之间的映射，但在缓存系统中，数据是按照物理地址进行缓存的。因此，当处理器访问某个内存地址时，处理器会先查找这个物理地址对应的缓存行。\nFlush+Reload 主要是查询内存地址对应的缓存行状态（命中或未命中），而不是直接查询物理页表或页表中的缓存行。\n缓存未命中复习 当缓存未命中时，加载的内容不是物理页表本身，而是内存地址对应的实际数据。具体来说：\n未命中时，处理器会从主内存中将目标内存地址对应的数据块（即缓存行）加载到缓存中，而不是加载物理页表。缓存行通常是内存中某个物理地址的一小块（比如 64 字节）。 页表的作用是将虚拟地址映射到物理地址。当处理器需要访问某个虚拟地址时，它首先通过页表查找该虚拟地址对应的物理地址。这个查找过程会通过 TLB（Translation Lookaside Buffer） 来加速。 如果TLB 命中，处理器可以快速得到物理地址，然后访问该物理地址并加载相应的数据到缓存中。 如果TLB 未命中，处理器需要从页表中查找对应的物理地址，这会导致页表的查找过程。而这个过程通常涉及访问页表（页表存放在主内存中），然后将查找到的物理地址对应的数据加载到缓存中。 缓存行（Cache Line）：\n缓存是按块（即缓存行）来管理的，每个缓存行对应内存中连续的一段数据。 例如，当处理器访问某个内存地址时，如果该地址所在的缓存行未命中，则处理器会从主内存中加载整个缓存行（如 64 字节）到缓存，而不是只加载单个字节或单个指令。 空间局部性原理：\n缓存设计通常基于空间局部性原理，这意味着如果处理器访问了某个内存地址，它很有可能会在接下来访问相邻的地址。因此，处理器每次加载一个较大的数据块（即缓存行）到缓存中，而不是只加载一个特定字节或单个数据项。 假设处理器访问内存地址 0x1000，该地址对应的缓存行未命中：\n如果缓存行大小为 64 字节，处理器会从 0x1000 开始加载一段内存，可能从 0x1000 到 0x103F（64 字节的范围）。 整个缓存行（64 字节）会被存储在缓存中，之后处理器对这些地址范围内的数据访问就可以直接从缓存中读取，避免再次访问主内存。 缓存行大小的选择:\n常见缓存行大小：典型的缓存行大小为 64 字节，但某些系统可能会有不同的缓存行大小，如 32 字节或 128 字节。这种大小通常是处理器设计时的固定参数，旨在平衡性能和内存带宽使用。 优势：一次加载较大的数据块有助于提高缓存命中率，并减少处理器与主内存之间的数据传输开销，因为后续访问同一缓存行的相邻数据不需要再从主内存读取。 加载整个缓存物理页表的地方，指的是 TLB（Translation Lookaside Buffer，翻译后备缓冲区），它与虚拟地址和物理地址映射过程中的缓存机制有关。TLB 是一种专用缓存，用于加速虚拟地址到物理地址的转换。\nTLB 是处理器内的一个小型高速缓存，它存储了最近使用的虚拟地址到物理地址的映射，即页表条目。当处理器访问虚拟内存时，TLB 可以直接提供虚拟地址对应的物理地址，而不必每次都查找完整的页表。这样极大地提升了地址转换的效率。\nTLB 缓存的是页表的部分条目，通常是页表中的一组映射关系，而不是整个页表本身。具体来说，每个 TLB 条目包含虚拟页面号（VPN）和物理页面号（PPN）之间的映射关系。每当 TLB 未命中时，处理器需要从内存中的页表中查找该虚拟地址对应的物理地址，并将这一映射加载到 TLB 中，以便加速后续的内存访问。典型的 TLB 加载过程：\nTLB 命中：处理器首先查找 TLB。如果命中，虚拟地址直接被转换为物理地址，继续执行程序。\nTLB 未命中: 如果未命中，处理器会访问内存中的页表结构，找到对应的物理地址映射，然后将这一映射加载到 TLB 中。\n页表是分层的，因此处理器可能需要多次内存访问来查找页表，尤其是在多级页表的系统中。 一旦找到对应的物理页表条目，处理器会将其加载到 TLB 中。 “加载一张缓存的物理页表”是指 TLB 在加载页表条目时，起到了页表缓存的作用。它并不缓存整个页表，而是缓存最近使用的页表条目。因为页表查找相对耗时，通过 TLB 缓存，可以避免每次都进行完整的页表查找。\n假设系统使用的是 4KB 的页（常见的页面大小）：\n虚拟地址访问：处理器访问一个虚拟地址时，首先会查询 TLB 是否存在该地址对应的物理地址映射。 TLB 未命中：如果 TLB 没有该虚拟地址的映射，处理器需要通过页表查找物理地址（这涉及访问内存中的多级页表）。 更新 TLB：查找到物理地址后，该映射会被存入 TLB，后续对该虚拟地址范围内的访问会直接命中 TLB，不再需要查页表。 TLB 通常非常小，可能只有几十到几百个条目，但它的存在可以显著减少处理器访问完整页表的频率。通常情况下，每个 TLB 条目会缓存一整个页面的映射信息。例如，如果页面大小是 4KB，那么一次 TLB 加载可以加速对该 4KB 内存范围内的所有地址访问。\n总的来说，当缓存未命中时，处理器加载的是目标物理地址对应的数据块，即缓存行，而不是直接加载页表。页表的作用是在虚拟地址到物理地址的转换过程中使用，而最终缓存的内容是物理地址对应的数据本身，不是页表结构。一般情况下，处理器在缓存未命中时会从主内存加载一个缓存行的数据，大小通常为 64 字节，这是缓存设计中常用的大小。\n“加载一张缓存物理页表”的地方指的是 TLB，它缓存的是页表中的部分条目，即最近使用的虚拟地址到物理地址的映射。TLB 的作用是加速地址转换，避免每次访问虚拟内存时都查找页表，而是通过缓存常用的页表条目来提高效率。\nMeltdown 的 异常 0 值 在 Meltdown + 侧信道攻击 中，出现“异常读取到 0 值”的情况，通常与以下几个方面有关：\n权限问题导致的异常访问：由于内存访问违规（试图访问不允许的地址，如内核空间），导致读取的数据无效，处理器返回 0 值或触发异常。 缓存行为异常：某些情况下，处理器在非法访问内存时，会导致数据未能正确加载到缓存中，进而在侧信道攻击中读取时得到错误的 0 值。 为了避免在这种攻击中读取到无效的 0 值，需要从以下几个方面进行调整：\n避免异常提前终止执行。Meltdown 攻击依赖于处理器的乱序执行，即使触发了非法内存访问异常，处理器仍然会提前执行部分指令。因此，以下措施可以帮助减少读取到 0 值的可能性：\n限制异常处理\n在进行 Meltdown 攻击时，尽量减少异常的影响。例如，通过构造某种条件，使得异常发生后处理器仍然可以继续执行读取操作。可以使用信号处理机制来捕捉异常，并避免程序在异常发生后立即崩溃。\n方法：使用类似以下代码结构捕捉异常：\n1 2 3 4 5 6 7 8 if (setjmp(buf) == 0) { // 这里执行 Meltdown 攻击的代码 unsigned char kernel_data = *(unsigned char*)kernel_address; array[kernel_data * 4096] = 1; } else { // 异常发生时的处理逻辑 // 可以重试或者记录错误 } 通过 setjmp 和 longjmp，程序可以在发生非法访问异常后返回并继续执行。\n多次采样，减少误差\n由于缓存命中与失效的时间差异较小，有时单次攻击可能因数据加载失败或异常读取到 0。为了解决这个问题，可以使用多次采样的方式：\n多次尝试访问同一个地址，并记录多次访问的结果。\n统计访问时的时延，并通过取平均值或最大值的方式，确定数据是否正确。\n通过多次采样，可以减少单次访问异常带来的误差，增加推测数据的准确性。\n优化侧信道分析。当缓存行为异常，或者数据没有正确加载到缓存时，侧信道技术可能读取到无效数据。在这种情况下，可以优化缓存时延的测量方法，确保对缓存命中状态的判断更准确。\n更精准的时延测量\n有时，测量时延的精度不够高，导致读取的结果不准确。可以通过更精准的计时器（如 rdtsc 指令）来测量缓存访问的时间差，确保能够正确区分缓存命中与未命中。\n设置更明确的触发条件\n通过调整缓存的刷新和测量时机，可以确保攻击中处理器将数据正确加载到缓存中，避免读取到无效的 0 值。例如，控制 clflush 指令的调用时机，以确保在读取敏感数据之前缓存被正确刷新。\n避免处理器的零填充机制。某些现代处理器在检测到非法内存访问时，可能会出于安全考虑返回 0（或其他固定值），而不是读取实际的数据。这是一种防止侧信道攻击的硬件机制。为了应对这种情况，攻击者可以：\n使用较旧的硬件\n某些较旧的处理器没有这种安全机制，因此可以使用这些处理器进行攻击，从而避免数据被零填充。\n探测并绕过零填充\n在硬件中如果存在零填充机制，可以通过探测不同的访问模式，分析哪些数据读取到的 0 值是由于这个机制引起的，进而在攻击逻辑中规避这些地址。\n处理异常返回的零值。在攻击过程中，针对出现的异常情况返回 0 值，可以通过重试策略来解决。每当读取到 0 值时，可以重新尝试访问同一个内存地址，直到获取到非 0 值为止。例如：\n1 2 3 4 unsigned char result; do { result = attempt_to_read_data(kernel_address); } while (result == 0); // 重试直到获取到非 0 的数据 在 Meltdown + 侧信道攻击 中，读取到 0 值的原因可能是由于异常处理、缓存行为或硬件机制。要减少这种情况的发生，可以通过捕捉异常、增加多次采样、优化时延测量等手段改进攻击的准确性。此外，了解处理器的硬件特性也是关键，尤其是在现代处理器中，某些硬件防护机制可能会主动返回 0 或其他无效数据。\n区分内核值 0 与异常 0 在 Meltdown + 侧信道攻击 中，确实存在一种情况：内核中的值本身可能为 0，而你又可能因为异常处理（如权限问题）得到了一个 0。那么，如何区分这两种 0，即区分真实的内存值为 0 和异常导致的 0，是侧信道攻击中的一个挑战。\n可以通过多次采样和时延分析等技术来区分这两种情况。具体的方法如下：\n多次采样。通过多次采样可以有效地减少由于异常返回 0 的影响。基本思路是，如果数据在多次采样过程中稳定地返回 0，那么它更可能是内存中的真实值；而如果采样结果波动不定（有时返回 0，有时返回其他值或异常），那么就可能是由于异常导致的。步骤：\n对同一个内存地址进行多次攻击，测量每次的访问结果。\n如果多次采样都返回 0，可以初步判断内核中的值可能确实是 0。\n如果 0 只是偶尔出现，并且伴随有其他非 0 的返回值，则可以推测该 0 可能是由于异常处理机制导致的。\n缓存时延分析。为了进一步提高区分的精度，缓存时延分析仍然是关键。你可以结合时延测量来帮助判断：\n真实的内存值为 0 时，读取缓存中的 0 值通常伴随着缓存命中，此时访问的时延应该是很短的。\n异常导致返回 0 时，由于处理器未能正常加载该内存地址的数据，缓存未命中时的访问时延通常会更长。\n通过时延测量，攻击者可以识别出缓存命中的情况，从而区分正常的内存访问（真实的 0）和异常情况导致的返回值 0。假设要判断 kernel_address 的值是否为 0：\n刷新缓存：使用 clflush 指令清除缓存中 kernel_address 对应的缓存行。\n执行读取操作：触发 Meltdown 攻击，尝试从 kernel_address 读取数据。\n测量时延：使用 rdtsc（读取时间戳计数器）测量对 kernel_address 的访问时延。\n多次采样: 重复步骤 1-3，采集多个结果。\n如果时延总是很短且返回 0，则说明内存中的真实值可能就是 0。\n如果时延较长，或是时延不稳定、偶尔返回 0，则很可能是由于异常导致的。\n信号处理与异常控制。如前面提到的，你可以通过 setjmp 和 longjmp 等机制捕捉异常并确保程序在发生异常时不会崩溃。这种方法不仅可以减少因异常直接返回 0，还可以通过异常处理来重试访问，进一步确保结果的准确性。\n当你捕获到异常时，可以记录异常次数和触发位置。如果某个内存地址频繁触发异常且返回 0，则这个 0 可能与异常有关。\n随机访问测试。另一个技术是对目标内存地址周围的地址进行随机访问。如果周围地址的访问正常且没有返回异常的 0，但目标地址却持续返回 0，这更有可能表明目标地址中的数据确实是 0。可以通过以下策略进行随机测试：\n对比 kernel_address 的相邻地址（例如 kernel_address - 1 和 kernel_address + 1）的访问情况。\n如果相邻地址的数据返回正常，但目标地址频繁出现异常 0，则该 0 可能是由于异常引起的。\n重试机制。为了应对异常返回 0 的情况，可以设计一个重试机制，即每次读取到 0 时，进行多次重试，直到结果稳定。如果经过多次重试结果仍然是 0，则该 0 更有可能是内存中的真实值。\n1 2 3 4 5 6 unsigned char result; int retries = 5; // 设定重试次数 do { result = attempt_to_read_data(kernel_address); retries--; } while (result == 0 \u0026amp;\u0026amp; retries \u0026gt; 0); // 重试，直到获取到非 0 的数据或达到重试上限 总而言之，要区分 Meltdown + 侧信道攻击中读取到的 0 是内存中的真实值还是异常导致的返回值，可以通过以下几种方法：\n多次采样：多次读取同一地址，判断 0 是否稳定出现。 缓存时延分析：通过测量缓存访问时延，判断是否发生了缓存命中，命中意味着数据可能是真实的。 信号处理与异常控制：捕捉异常，记录异常次数，并重试访问来确认结果。 随机访问测试：对目标地址附近的地址进行测试，通过对比结果进一步验证。 重试机制：当读取到 0 时，尝试重试多次，以确保返回值的准确性。 通过以上手段，可以大大提高攻击的成功率，并有效区分异常导致的 0 和内存中的真实值。\nMeltdown调整内存值大小 在 Meltdown 的侧信道攻击中，要调整每次能得到的内存值的大小，通常涉及到以下几个技巧：\n控制访问粒度。一般来说，Meltdown 攻击可以逐字节读取数据。如果你想调整每次读取的数据大小，可以通过以下方式：\n逐字节访问\n典型实现：构造攻击代码，使每次读取一个字节。\n示例代码：\n1 2 3 4 for (size_t offset = 0; offset \u0026lt; data_size; offset++) { unsigned char value = read_byte(kernel_address + offset); // 处理读取的 value } 逐多字节访问\n如果希望每次读取多字节，可以将多次字节读取合并，例如使用memcpy优化攻击代码：\n1 2 3 4 5 for (size_t offset = 0; offset \u0026lt; data_size; offset += n) { unsigned char buffer[n]; memcpy(buffer, kernel_address + offset, n); // 处理读取的 buffer } 这个方法需要确保攻击代码能连续加载所需字节到缓存。\n优化缓存行为。缓存行是以固定大小（通常 64 字节）的块进行管理的。调整数据读取大小时，需要考虑缓存行对齐的问题：\n缓存对齐：确保读取的数据恰好在一个或多个完整的缓存行中。这样能减少多余的内存访问和缓存未命中。\n预取优化：通过 prefetch 指令提前加载大块数据，帮助提高获取率。\n使用批量处理。如果希望每次读取更多数据，考虑使用批量攻击来同时处理多个字节：\n结合流水线和乱序执行：调整内存访问模式，让处理器能够提前执行多个字节加载。\n批量缓存刷新：在攻击中，批量刷新缓存行以准备加载更多字节。\n利用高级指令。某些处理器架构支持高级指令，可以用于更大范围的内存操作。有助于提高每次读取的字节量。\n异常处理调整。在 Meltdown 攻击中，如果要提高每次读取数据的粒度，可以结合异常处理调整策略：\n细化异常捕获：确保每个读取操作的边界条件和异常处理得当，以避免数据丢失或错误。\n调整 setjmp 和 longjmp 的使用：确保在更大连续内存读取中保持执行连续性。\n要在 Meltdown 的侧信道攻击中调整每次能得到的内存值大小，可以通过控制访问粒度、优化缓存行为、使用批量处理和高级指令来实现。同时调整异常处理策略，确保读取准确和有效。通过这些方法，可以更灵活地控制每次读取的数据量。\nKAISER KAISER 是一种改进的内存隔离技术，旨在强化用户空间和内核空间之间的隔离。其主要步骤包括：\n内存映射调整：KAISER 会确保在用户空间中不映射任何内核内存，只保留一些x86架构必须的部分（例如中断处理程序）。 消除直接物理映射：用户空间无法访问物理内存或内核内存，因此无法通过地址解析来泄露这些内存。 防止漏洞利用：通过不在用户空间映射敏感内核内存，阻止像Meltdown这样的攻击从内核中泄露信息。 假设一个攻击者正在尝试定位某个关键的内存处理函数，该函数在用户空间中只留下了微小的踪迹（比如只有几千字节），而整体用户空间的大小可能是数十GB。同时，如果这些内存位置随着KASLR（内核地址空间布局随机化）不断随机化，攻击者需要经历极多次尝试才能有效猜测到这些位置，因此增加了攻击的难度。\nKAISER通过以下方式保护这个攻击面：\n使用跳板函数：对必要的内核指针位置使用\u0026quot;跳板（trampoline）\u0026ldquo;函数。这意味着中断处理程序和其他必要的内核代码不直接调用内核，而是通过一个单独的跳板函数进行。 跳板函数内存分离：确保这些跳板函数只被映射到内核中，并且与其他内核代码使用不同的随机偏移量进行随机化。 隔离作用：即使Meltdown攻击成功，攻击者只能获取到跳板代码的指针，而无法获取到内核的其余随机化偏移，从而保护内核地址的随机化信息。 假设一个系统需要在用户空间中保持中断处理程序的映射，以正常操作。\n传统方法：可能直接映射中断处理程序，容易让攻击者通过Meltdown攻击获取其地址并推断其他内核信息。 通过KAISER防护： 将中断处理程序地址指向一个跳板函数。 这个跳板函数的地址在内核中被随机化。 当需要访问中断处理程序时，首先通过被完全随机化了的跳板函数进行处理，以覆盖整个内核的直接地址。 通过这种方式，KAISER减少了直接暴露给用户空间的内核地址数量，即使在攻击者可以通过Meltdown获得一些地址，也无法得知内核的具体随机化偏移和其他信息。这种方式在安全和性能之间找到了妥协点，保护了“剩余攻击面”。\nMemory-Type Range Registers (MTRRs) MTRRs 是一个特性，允许系统定义内存区域的缓存特性。通过这些寄存器，系统可以指定某些内存区域是可缓存的、不可缓存的、写合并等。这对于优化系统性能和特定硬件行为很重要。\nMicroarchitectural state 和 architectural state \u0026ldquo;Microarchitectural state\u0026rdquo; 和 \u0026ldquo;architectural state\u0026rdquo; 是计算机体系结构中两个不同的概念，涉及处理器如何管理和表现数据。\n微架构状态（Microarchitectural state）：这是处理器内部的细节层次，它包括缓存、分支预测器、管道、加载/存储队列等部件的状态。这些状态反映了处理器如何在硬件层面优化执行指令，但这些信息通常对软件程序不可见，也不会直接影响程序执行的逻辑结果。 架构状态（Architectural state）：这是处理器从程序员或系统的角度所能看到的状态，主要包括寄存器、内存中的数据、程序计数器等。这些状态决定了程序的执行逻辑和可见结果。 “Microarchitectural state is translated into an architectural state” 指的是微架构中的某些临时状态，最终可能影响到架构状态，也就是说，处理器内部的一些优化行为（例如缓存命中或未命中）可能在某些情况下改变程序的实际执行结果或可见结果。\n缓存和微架构攻击：\n在微架构层面上，缓存中的数据是可见的，但这不属于程序的架构状态。然而，通过侧信道攻击（如Meltdown或Spectre），攻击者可以通过观察缓存行为（例如缓存命中与未命中）推断出某些本应不可见的架构状态。 假设一个程序访问了一个数据地址，如果这个地址在缓存中（微架构状态），访问速度会快。如果攻击者能够利用侧信道推断出这个缓存访问时间的差异，他们可以推断出该数据地址的位置或内容，进而影响架构状态（例如，暴露寄存器或内存中的数据）。 微架构状态（如缓存中的数据或管道中的指令）通常对程序不可见，但某些侧信道攻击能够利用这些状态的变化，最终推导出影响程序执行的架构状态。这种“转换”通常并不是直接的，而是通过观察微架构行为来推断出架构级别的信息。\n正课部分 Meltdown发生的背景 今天讲的是Meltdown，之所以我会读这篇论文，是因为我们在讲解如何设计内核时总是会提到安全。内核提供安全性的方法是隔离，用户程序不能读取内核的数据，用户程序也不能读取其他用户程序的数据。\n我们在操作系统中用来实现隔离的具体技术是硬件中的User/Supervisor mode，硬件中的Page Table，以及精心设计的内核软件，例如系统调用在使用用户提供的指针具备防御性。\n但是同时也值得思考，如何可以破坏安全性？实际上，内核非常积极的提供隔离性安全性，但总是会有问题出现。\n今天的论文 讨论的就是最近在操作系统安全领域出现的最有趣的问题之一，它发表于2018年。包括我在内的很多人发现对于用户和内核之间的隔离进行攻击是非常令人烦恼的，因为它破坏了人们对于硬件上的Page Table能够提供隔离性的设想。这里的攻击完全不支持这样的设想。\n同时，Meltdown也是被称为Micro-Architectural Attack的例子之一，这一类攻击涉及利用CPU内隐藏的实现细节。通常来说CPU如何工作是不公开的，但是人们会去猜，一旦猜对了CPU隐藏的实现细节，就可以成功的发起攻击。\nMeltdown是可被修复的，并且看起来已经被完全修复了。然后它使得人们担心还存在类似的Micro-Architectural Attack。所以这是最近发生的非常值得学习的一个事件。\n让我从展示攻击的核心开始，之后我们再讨论具体发生了什么。\n1 2 3 4 5 6 char buf [8192] r1 = \u0026lt;a kernel virtual address\u0026gt; r2 = *r1 r2 = r2 \u0026amp; 1 r2 = r2 * 4096 r3 = buf [r2] 这是论文中展示攻击是如何工作的代码的简化版。\n如果你是攻击者，出于某种原因你可以在计算机上运行一些软件，这个计算机上有一些你想要窃取的数据。虽然你不能直接访问这些数据，但是这些数据还是位于内存中，或许是内核内存，或许是另一个进程的内存。\n你可以在计算机上运行一个进程，或许因为你登录到了分时共享的机器，也或许你租用了运行在主机上的服务。你可以这样发起攻击：\n在程序中你在自己的内存中声明了一个buffer，这个buffer就是普通的用户内存且可以被正常访问。 然后你拥有了内核中的一个虚拟内存地址，其中包含了一些你想要窃取的数据。 这里的程序是C和汇编的混合，第3行代码的意思是你拥有了内核的虚拟内存地址，你从这个内存地址取值出来并保存在寄存器r2中。 第4行获取寄存器r2的低bit位，所以这里这种特定的攻击只是从内核一个内存地址中读取一个bit。 第5行将这个值乘以4096，因为低bit要么是1，要么是0，所以这意味着r2要么是4096，要么是0。 第6行中，我们就是读取前面申请的buffer，要么读取位置0的buffer，要么读取位置4096的buffer。 这就是攻击的基本流程。\n这里的一个问题是，为什么这里不能直接工作？在第3行，我们读取了内核的内存地址指向的数据，我们可以直接读取内核的内存地址吗？并不能，我们相信答案是否定的。\n如果我们在用户空间，我们不可能直接从内核读取数据。我们知道CPU不能允许这样的行为，因为当我们使用一个内核虚拟地址时，这意味着我们会通过Page Table进行查找，而Page Table有权限标志位，我们现在假设操作系统并没有在PTE中为内核虚拟地址设置标志位来允许用户空间访问这个地址，这里的标志位在RISC-V上就是pte_u标位置。\n因此这里的读取内核内存地址指令必然会失败，必然会触发Page Fault。实际中如果我们运行代码，这些代码会触发Page Fault。如果我们在代码的最后增加printf来打印r3寄存器中的值，我们会在第3行得到Page Fault，我们永远也走不到printf。这时我们发现我们不能直接从内核中偷取数据。\n然而，如论文展示的一样，这里的指令序列是有用的。虽然现在大部分场景下已经不是事实了，但是论文假设内核地址被映射到了每个用户进程的地址空间中了。也就是说，当用户代码在运行时，完整的内核PTE也出现在用户程序的Page Table中，但是这些PTE的pte_u比特位没有被设置，所以用户代码在尝试使用内核虚拟内存地址时，会得到Page Fault。\n在论文写的时候，所有内核的内存映射都会在用户程序的Page Table中，只是它们不能被用户代码使用而已，如果用户代码尝试使用它们，会导致Page Fault。操作系统设计人员将内核和用户内存地址都映射到用户程序的Page Table中的原因是，这使得系统调用非常的快，因为这使得当发生系统调用时，你不用切换Page Table。\n切换Page Table本身就比较费时，同时也会导致CPU的缓存被清空，使得后续的代码执行也变慢。所以通过同时将用户和内核的内存地址都映射到用户空间可以提升性能。但是上面的攻击依赖了这个习惯。我将会解释这里发生了什么使得上面的代码是有用的。\n学生提问：能重复一下上面的内容吗？\nRobert教授：在XV6中，当进程在用户空间执行时，如果你查看它的Page Table，其中包含了用户的内存地址映射，trampoline和trap frame page的映射，除此之外没有别的映射关系，这是XV6的工作方式。\n而这篇论文假设的Page Table不太一样，当这篇论文在写的时候，大部分操作系统都会将内核内存完整映射到用户空间程序。所以所有的内核PTE都会出现在用户程序的Page Table中，但是因为这些PTE的pte_u比特位没有被设置，用户代码并不能实际的使用内核内存地址。\n这么做的原因是，当你执行系统调用时，你不用切换Page Table，因为当你通过系统调用进入到内核时，你还可以使用同一个Page Table，并且因为现在在Supervisor mode，你可以使用内核PTE。\n这样在系统调用过程中，进出内核可以节省大量的时间。所以大家都使用这个技术，并且几乎可以肯定Intel也认为一个操作系统该这样工作。\n在论文中讨论的攻击是基于操作系统使用了这样的结构。最直接的摆脱攻击的方法就是不使用这样的结构。但是当论文还在写的时候，所有的内核PTE都会出现在用户空间。\n学生提问：所以为了能够攻击，需要先知道内核的虚拟内存地址？\nRobert教授：是的。或许找到内存地址本身就很难，但是你需要假设攻击者有无限的时间和耐心，如果他们在找某个数据，他们或许愿意花费几个月的时间来窃取这个数据。\n有可能这是某人用来登录银行账号或者邮件用的密码。这意味着攻击者可能需要尝试每一个内核内存地址，以查找任何有价值的数据。\n或许攻击者会研究内核代码，找到内核中打印了数据的地址，检查数据结构和内核内存，最后理解内核是如何工作的，并找到对应的虚拟内存地址。因为类似的攻击已经存在了很长的时间，内核实际上会保护自己不受涉及到猜内核内存地址的攻击的影响。\n论文中提到了Kernal address space layout randomization。所以现代的内核实际上会将内核加载到随机地址，这样使得获取内核虚拟地址更难。这个功能在论文发表很久之前就存在，因为它可以帮助防御攻击。\n在这个攻守双方的游戏中，我们需要假设攻击者最后可以胜出并拿到内核的虚拟内存地址。所以我们会假设攻击者要么已经知道了一个内核虚拟地址，要么愿意尝试每一个内核虚拟内存地址。\n我们会好奇，上面的代码怎么会对攻击者是有用的？如果CPU如手册中一样工作，那么这里的攻击是没有意义的，在第三行会有Page Fault。但是实际上CPU比手册中介绍的要复杂的多，而攻击能生效的原因是一些CPU的实现细节。\n这里攻击者依赖CPU的两个实现技巧，一个是Speculative execution（预测执行），另一个是CPU的缓存方式。\nSpeculative execution(1) 首先来看Speculative execution（预测执行），这里也有一个示例代码。\n1 2 3 4 5 6 7 8 r0 = \u0026lt;something\u0026gt; r1 = valid //r1 is a register; valid is in RAM if(r1 == 1){ r2 = *r0 r3 = r2 + 1 } else{ r3 = 0 } 现在我并没有讨论安全性，Speculative execution是一种用来提升CPU性能的技术，所以这是CPU使用的一些优化技巧。假设我们在运行这里的代码：\n在r0寄存器保存了一个内存地址，地址可能是有效的也可能是无效的，这取决于我代码的逻辑。 我们假设内存中还保存了一个valid变量。在使用r0中保存地址之前，我们会先将valid从内存中加载到r1。 并且只有当valid等于1时，才使用r0中的地址。如果valid等于0，我们将不会使用r0中的地址。 如果valid等于1，我们会将r0的地址指向的内容加载到r2。 并对r2寄存器加1，保存在r3寄存器中。 在一个简单的CPU实现中，在代码的第2行，你会将valid从内存中加载到r1，这里对应了从内存中读取数据的load指令。任何一个需要从内存中读取数据的load指令都会花费2GHZ CPU的数百个CPU cycle。CPU最多可以在每个cycle执行一条指令，如果我们需要在代码的第2行等待几百个CPU cycle，那么机器会闲置数百个CPU cycle。\n这是一个明显的降低性能的地方，因为如果一切都正常的话，CPU可以在每个cycle内执行一条指令，而不是每几百个cycle才执行一条指令。\n所有现在的CPU都使用了叫做branch prediction的功能。第3行的if语句是一个branch，如果我们将其转换成机器指令，我们可以发现这里有一个branch，并且这是一个带条件的branch用来测试r1寄存器是否等于1。\nCPU的branch prediction会至少为每个最近执行过的branch保存一个缓存，并记住这个branch是否被选中了，所以这里可能是基于上次branch的选择的预测。\n但是即使CPU没有足够的信息做预测，它仍然会选择一个branch，并执行其中的指令。也就是说在CPU在知道第3行代码是否为true之前，它会选择某一个branch并开始执行。或许branch选错了，但是CPU现在还不知道。\n所以在上面的代码中，或许在第2行代码的load结束之前，也就是在知道valid变量的值之前，CPU会开始执行第4行的指令，并通过load指令读取r0指向的内存地址的内容。\n而r0中的内存地址或许是，也或许不是一个有效的指针。一旦load指令返回了一些内容，在代码的第5行对返回内容加1并设置到r3寄存器中。\n或许很久之后，第2行的load指令终于完成了，现在我们知道valid变量的值。如果valid等于1，那么一切都好，如果valid等于0，CPU会取消它执行第4、5行代码的效果，并重新执行合适的分支代码，也就是第7行代码。\n这里在确定是否应该执行之前就提前执行分支代码的行为，被称作预测执行。这是为了提升性能，如果CPU赌对了，那么它就可以超前执行一些指令，而不用等待费时的内存加载。\nCPU中为了支持预测执行的硬件及其复杂，CPU里面有大量的设计来让这里能工作，但是没有一个设计被公开了，这些都是Intel的内部信息，并且不在手册中。所以在Meltdown Attack时，涉及到大量有关CPU是如何工作的猜测来确保攻击能生效。\n为了能回滚误判的预测执行，CPU需要将寄存器值保存在别处。虽然代码中第4行，第5行将值保存在了r2，r3，但是实际上是保存在了临时寄存器中。\n如果CPU赌对了，那么这些临时寄存器就成了真实寄存器，如果赌错了，CPU会抛弃临时寄存器，这样代码第4，5行就像从来没有发生过一样。\n在这里的代码中，我们需要考虑如果r0中是有效的指针会发生什么，如果不是有效的指针，又会发生什么。如果我们在超前执行代码第4行，并且r0中是有效的指针，那么CPU会真实的加载指针的内容到r2寄存器的临时版本中。\n如果r0中的指针指向的内容位于CPU的cache中，那么必然可以将内容拷贝到r2寄存器的临时版本。如果CPU的cache中没有包含数据，我并不清楚CPU是否会会从内存中读取r0中指针指向的内容。\n对于我们来说，更有趣的一个问题是，如果r0中的指针不是一个有效的指针，会发生什么？如果r0中的指针不是一个有效的地址，并且我们在超前执行代码第4行，机器不会产生Fault。\n机器或许知道r0是无效的地址，并且代码第4行尝试使用一个无效的地址，但是它不能产生Page Fault，因为它不能确定代码第4行是否是一个正确的代码分支，因为有可能CPU赌错了。\n所以直到CPU知道了valid变量的内容，否则CPU不能在代码第4行生成Page Fault。也就是说，如果CPU发现代码第4行中r0内的地址是无效的，且valid变量为1，这时机器才会生成Page Fault。如果r0是无效的地址，且valid变量为0，机器不会生成Page Fault。\n所以是否要产生Page Fault的决定，可能会推迟数百个CPU cycle，直到valid变量的值被确定。\n当我们确定一条指令是否正确的超前执行了而不是被抛弃了这个时间点，对应的技术术语是Retired。\n所以当我们说一个指令被超前执行，在某个时间点Retired，这时我们就知道这条指令要么会被丢弃，要么它应该实际生效，并且对机器处于可见状态。\n一条指令如果是Retired需要满足两个条件，首先它自己要结束执行，比如说结束了从内存加载数据，结束了对数据加1；其次，所有之前的指令也需要Retired。\n所以上面代码第4行在直到valid变量被从内存中加载出来且if被判定之前不能Retired，所以第4行的Retirement可能会延后数百个CPU cycle。\n这是Meltdown攻击非常关键的一个细节。\nSpeculative execution(2) 1 2 3 4 5 6 7 8 r0 = \u0026lt;something\u0026gt; r1 = valid //r1 is a register; valid is in RAM if(r1 == 1){ r2 = *r0 r3 = r2 + 1 } else{ r3 = 0 } 如果r0中的内存地址是无效的，且在Page Table中完全没有映射关系，那么我也不知道会发生什么。如果r0中的内存地址在Page Table中存在映射关系，只是现在权限不够，比如说pte_u标志位为0，那么Intel的CPU会加载内存地址对应的数据，并存储在r2寄存器的临时版本中。\n之后r2寄存器的临时版本可以被代码第5行使用。所以尽管r0中的内存地址是我们没有权限的内存，比如说一个内核地址，它的数据还是会被加载到r2，之后再加1并存储在r3中。\n之后，当代码第4行Retired时，CPU会发现这是一个无效的读内存地址行为，因为PTE不允许读取这个内存地址。这时CPU会产生Page Fault取消执行后续指令，并回撤对于r2和r3寄存器的修改。\n所以，在这里的例子中，CPU进行了两个推测：一个是CPU推测了if分支的走向，并选择了一个分支提前执行；除此之外，CPU推测了代码第4行能够成功完成。\n对于load指令，如果数据在CPU缓存中且相应的PTE存在于Page Table，不论当前代码是否有权限，Intel CPU总是能将数据取出。如果没有权限，只有在代码第4行Retired的时候，才会生成Page Fault，并导致预测执行被取消。\n学生提问：我对CPU的第二个预测，也就是从r0中保存的内存地址加载数据有一些困惑，这是不是意味着r0对应的数据先被加载到了r2，然后再检查PTE的标志位？\nRobert教授：完全正确。在预测的阶段，不论r0指向了什么地址，只要它指向了任何东西，内存中的数据会被加载到r2中。\n之后，当load指令Retired时才会检查权限。如果我们并没有权限做操作，所有的后续指令的效果会被取消，也就是对于寄存器的所有修改会回滚。同时，Page Fault会被触发，同时寄存器的状态就像是预测执行的指令没有执行过一样。\n学生提问：难道不能限制CPU在Speculative execution的时候，先检查权限，再执行load指令吗？看起来我们现在的问题就是我们在不知道权限的情况下读取了内存，如果我们能先知道权限，那么Speculative execution能不能提前取消？\nRobert教授：这里有两个回答。首先，Intel芯片并不是这样工作的。其次，是的，我相信对于Intel来说如果先做权限检查会更简单，这样的话，在上面的例子中，r2寄存器就不会被修改。\n你们或许注意到论文中提到，尽管AMD CPU的手册与Intel的一样，它们有相同的指令集，Meltdown Attack并不会在AMD CPU上生效。普遍接受的观点是，AMD CPU在Speculative execution时，如果没有权限读取内存地址，是不会将内存地址中的数据读出。\n这就是为什么Meltdown Attack在AMD CPU上不生效的原因。最近的Intel CPU明显也采用了这种方法，如果程序没有权限，在Speculative execution的时候也不会加载内存数据。\n这里使用哪种方式对于性能来说没有明显区别，或许在指令Retired的时候再检查权限能省一些CPU的晶体管吧。这里我要提醒一下，这里有很多内容都是猜的，不过我认为我说的都是对的。Intel和AMD并没有太披露具体的细节。\n这里有一些重要的术语。你可以从CPU手册中读到的，比如说一个add指令接收两个寄存器作为参数，并将结果存放在第三个寄存器，这一类设计被称为CPU的Architectural，或者通告的行为。\n如果你读取一个你没有权限的内存地址，你会得到一个Page Fault，你不允许读取这个内存地址，这就是一种通告的行为。\nCPU的实际行为被称作Micro-Architectural，CPU的通告行为与实际行为是模糊不清的。比如说CPU会悄悄的有Speculative execution。\nCPU设计者在设计Micro-Architectural时的初衷是为了让它是透明的。的确有很多行为都发生在CPU内部，但是结果看起来就像是CPU完全按照手册在运行。\n举个例子，在上面代码的第4行，或许Intel的CPU在读取内存时没有检查权限，但是如果权限有问题的话，在指令Retired的时候，所有的效果都会回滚，你永远也看不到你不该看到的内存内容。\n所以看起来就跟CPU的手册一样，你不允许读取你没有权限的内存地址。这里Architectural和Micro-Architectural的区别是Meltdown Attack的主要攻击点。这里的攻击知道CPU内部是如何工作的。\nCPU caches 接下来我将介绍Micro-Architectural的另一个部分，也就是缓存。我知道大家都知道CPU有cache，但是缓存或多或少应该是也透明的。\n让我画个图描述一下cache，因为我认为cache与Meltdown最相关。\n首先，你有CPU核，这是CPU的一部分，它会解析指令，它包含了寄存器，它有加法单元，除法单元等等。所以这是CPU的执行部分。\n当CPU核需要执行load/store指令时，CPU核会与内存系统通信。\n内存系统一些cache其中包含了数据的缓存。首先是L1 data cache，它或许有64KB，虽然不太大，但是它特别的快。\n如果你需要的数据在L1 cache中，只通过几个CPU cycle就可以将数据取回。L1 cache的结构包含了一些线路，每个线路持有了可能是64字节的数据。\n这些线路是个表单，它们通过虚拟内存地址索引。如果一个虚拟内存地址在cache中，并且cache为这个虚拟内存地址持有了数据，那么实际中可以认为L1 cache中也包含了来自对应于虚拟内存地址的PTE的权限。\nL1 cache是一个表单，当CPU核执行load指令时，首先硬件会检查L1 cache是否包含了匹配load指令的虚拟内存地址，如果有的话，CPU会直接将L1 cache中的数据返回，这样可以很快完成指令。\n如果不在L1 cache，那么数据位于物理内存中，所以现在我们需要物理内存地址，这里需要Translation Lookaside Buffer（TLB），TLB是PTE的缓存。\n现在我们会检查load指令中的虚拟内存地址是否包含在TLB中。如果不在TLB，我们就需要做大量的工作，我们需要从内存中读取相关的PTE。\n让我们假设TLB中包含了虚拟内存地址对应的物理内存Page地址，我们就可以获取到所需要的物理内存地址。通常来说会有一个更大的cache（L2 cache），它是由物理内存地址索引。\n现在通过TLB我们找到了物理内存地址，再通过L2 cache，我们有可能可以获取到数据。\n如果我们没有在L2 cache中找到物理内存地址对应的数据。我们需要将物理内存地址发送给RAM系统。这会花费很长的时间，当我们最终获得了数据时，我们可以将从RAM读取到的数据加入到L1和L2 cache中，最终将数据返回给CPU核。\n以上就是CPU的cache。\n如果L1 cache命中的话可能只要几个CPU cycle，L2 cache命中的话，可能要几十个CPU cycle，如果都没有命中最后需要从内存中读取那么会需要几百个CPU cycle。\n一个CPU cycle在一个2GHZ的CPU上花费0.5纳秒。所以拥有cache是极其有利的，如果没有cache的话，你将会牺牲掉几百倍的性能。所以cache对于性能来说是非常关键的。\n在Meltdown Attack的目标系统中，如果我们运行在用户空间，L1和L2 cache可以既包含用户数据，也包含内核数据。L2 cache可以包含内核数据因为它只是物理内存地址。\nL1 cache有点棘手，因为它是虚拟内存地址，当我们更换Page Table时，L1 cache的内容不再有效。因为更换Page Table意味着虚拟内存地址的意义变了，所以这时你需要清空L1 cache。不过实际中会有更多复杂的细节，可以使得你避免清空L1 cache。\n论文中描述的操作系统并没有在内核空间和用户空间之间切换的时候更换Page Table，因为两个空间的内存地址都映射在同一个Page Table中了。这意味着我们不必清空L1 cache，也意味着L1 cache会同时包含用户和内核数据，这使得系统调用更快。\n如果你执行系统调用，当系统调用返回时，L1 cache中还会有有用的用户数据，因为我们在这个过程中并没与更换Page Table。\n所以，当程序运行在用户空间时，L1 cache中也非常有可能有内核数据。L1 cache中的权限信息拷贝自TLB中的PTE，如果用户空间需要访问内核内存数据，尽管内核数据在L1 cache中，你也不允许使用它，如果使用的话会触发Page Fault。\n尽管Micro-Architectural的初衷是完全透明，实际中不可能做到，因为Micro-Architectural优化的意义在于提升性能，所以至少从性能的角度来说，它们是可见的。也就是说你可以看出来你的CPU是否有cache，因为如果没有的话，它会慢几百倍。\n除此之外，如果你能足够精确测量时间，那么在你执行一个load指令时，如果load在几个CPU cycle就返回，数据必然是在cache中，如果load在几百个CPU cycle返回，数据可能是从RAM中读取，如果你能达到10纳秒级别的测量精度，你会发现这里区别还是挺大的。\n所以从性能角度来说，Micro-Architectural绝对不是透明的。我们现在讨论的分支预测，cache这类功能至少通过时间是间接可见的。\n所以尽管Micro-Architectural设计的细节都是保密的，但是很多人对它都有强烈的兴趣，因为这影响了很多的性能。比如说编译器作者就知道很多Micro-Architectural的细节，因为很多编译器优化都基于人们对于CPU内部工作机制的猜测。\n实际中，CPU制造商发布的优化手册披露了一些基于Micro-Architectural的技巧，但是他们很少会介绍太多细节，肯定没有足够的细节来理解Meltdown是如何工作的。所以Micro-Architectural某种程度上说应该是透明的、隐藏的、不可见的，但同时很多人又知道一些随机细节。\n学生提问：L1 cache是每个CPU都有一份，L2 cache是共享的对吧？\nRobert教授：不同CPU厂商，甚至同一个厂商的不同型号CPU都有不同的cache结构。今天普遍的习惯稍微有点复杂，在一个多核CPU上，每一个CPU核都有一个L1 cache，它离CPU核很近，它很快但是很小。每个CPU核也还有一个大点的L2 cache。除此之外，通常还会有一个共享的L3 cache。\n另一种方式是所有的L2 cache结合起来，以方便所有的CPU共用L2 cache，这样我可以非常高速的访问我自己的L2 cache，但是又可以稍微慢的访问别的CPU的L2 cache，这样有效的cache会更大。\n所以通常你看到的要么是三级cache，或者是两级cache但是L2 cache是合并在一起的。典型场景下，L2和L3是物理内存地址索引，L1是虚拟内存地址索引。\n学生提问：拥有物理内存地址的缓存有什么意义？\nRobert教授：如果同一个数据被不同的虚拟内存地址索引，虚拟内存地址并不能帮助你更快的找到它。而L2 cache与虚拟内存地址无关，不管是什么样的虚拟内存地址，都会在L2 cache中有一条物理内存地址记录。\n学生提问：MMU和TLB这里位于哪个位置？\nRobert教授：我认为在实际中最重要的东西就是TLB，并且我认为它是与L1 cache并列的。如果你miss了L1 cache，你会查看TLB并获取物理内存地址。MMU并不是一个位于某个位置的单元，它是分布在整个CPU上的。\n学生提问：但是MMU不是硬件吗？\nRobert教授：是的，这里所有的东西都是硬件。CPU芯片有数十亿个晶体管，所以尽管是硬件，我们讨论的也是使用非常复杂的软件设计的非常复杂的硬件。所以CPU可以做非常复杂和高级的事情。所以是的，它是硬件，但是它并不简单直观。\n学生提问：Page Table的映射如果没有在TLB中命中的话，还是要走到内存来获取数据，对吧？\nRobert教授：从L2 cache的角度来说，TLB miss之后的查找Page Table就是访问物理内存，所以TLB需要从内存中加载一些内存页，因为这就是加载内存，这些内容可以很容易将Page Table的内容缓存在L2中。\n虚拟地址到物理地址的映射表主要存储在内存中。不过，为了提高查询效率，系统使用了一个称为转换后备缓冲区（Translation Lookaside Buffer, TLB）的缓存装置。\nTLB：\n存储最近使用的虚拟地址到物理地址的映射。 是一种小型的、高速的缓存，能加快地址转换过程。 内存：\n完整的虚拟地址到物理地址映射表（页表）存储在内存中。 当TLB未命中时，需访问内存中的页表来完成地址转换。 L1, L2, L3 缓存：\n主要用于缓存数据和指令，不存储地址映射表。 总结：映射表主要在内存中，TLB用于缓存一部分映射以提高查询速度\nFlush and Reload 为什么Cache与Meltdown相关呢？接下来我将讨论一下论文 中使用Cache的主要方法。\n论文中讨论了这种叫做Flush and Reload的技术，这个技术回答了一个问题：一段特定的代码是否使用了特定内存地址的数据？这个技术本身并不是一个直接的安全漏洞，因为它只能基于你有权限的内存地址工作。\n所以如果你是用户代码，你可以使用属于你的用户空间内存，并且你现在要调用一个你自己的函数，你可以使用Flush and Reload来知道你刚刚执行的函数是否使用了某个属于你自己的内存。\n你不能直接使用这种技术来获取其他进程的私有内存。进程之间有时候会共享内存，你还是可以访问这部分共享的内存。所以Flush and Reload回答了这个问题，特定的函数是否使用了特定内存地址？\n它的具体工作步骤如下：\n第一步，假设我们对地址X感兴趣，我们希望确保Cache中并没有包含位于X的内存数据。\n实际中，为了方便，Intel提供了一条指令，叫做clflush，它接收一个内存地址作为参数，并确保该内存地址不在任何cache中。\n这超级方便，不过即使CPU并没有提供这样的指令，实际中也有方法能够删除Cache中的数据，举个例子，如果你知道Cache有64KB，那么你load 64KB大小的随机内存数据，这些数据会被加载到Cache中，这时Cache中原本的数据会被冲走，因为Cache只有64KB大小。所以即使没有这个好用的指令，你仍然可以清空Cache中的所有数据。\n第二步，如果你对某段可能使用了内存地址X的代码感兴趣，你可以调用这个函数，先不管这个函数做了什么，或许它使用了内存地址X，或许没有。\n现在，你想要知道X是否在Cache中，如果是的话，因为在第一步清空了Cache，必然是因为第二步的函数中load了这个内存地址。所以你现在想要执行load，但是你更想知道load花费了多长时间，而且我们这里讨论的是纳秒级别的时间，比如5个纳秒或者100个纳秒，那么我们该怎样达到这种测量精度呢？\n这是个困难的任务。Intel CPU会提供指令来向你返回CPU cycle的数量，这被称为rdtsc。所以这里我们会执行rdtsc指令，它会返回CPU启动之后总共经过了多少个CPU cycle。如果是2GHZ的CPU，这意味着通过这个指令我们可以得到0.5纳秒的测量精度。\n现在我们会将内存地址X的数据加载到junk对象中。\n然后再通过rdtsc读取时间。如果两次读取时间的差是个位数，那么上一步的load指令走到了cache中，也就是第二步的函数中使用了内存地址X的数据。\n如果两次读取时间的差别超过100，这意味着内存地址X不在cache中，虽然这并不绝对，但是这可能代表了第二步的函数中并没有使用内存X的数据。因为函数中可能使用了内存地址X，然后又用了其他与X冲突的数据，导致内存地址X又被从cache中剔除了。\n但是对于简单的情况，如果两次时间差较大那么第二步的函数没有使用内存地址X，如果两次时间差较小那么第二步函数使用了内存地址X。\n现在还没有涉及到攻击，因为这里我们需要能够访问到内存地址X，所以这是我们可以访问的内存地址。\n以上就是有关Meltdown的前置知识。\nMeltdown Attack 接下来让我们回到Meltdown。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 char buf[8192] // L1 // the Flush of Flush+Reload clflush buf[0] clflush buf[4096] // L5 \u0026lt;some expensive instruction like divide\u0026gt; r1 = \u0026lt;a kernel virtual address\u0026gt; r2 = *r1 // L10 r2 = r2 \u0026amp; 1 // speculated r2 = r2 * 4096 // speculated r3 = buf[r2] // speculated \u0026lt;handle the page fault from \u0026#34;r2 = *r1\u0026#34;\u0026gt; // the Reload of Flush+Reload a = rdtsc // L18 r0 = buf[0] b = rdtsc r1 = buf[4096] // L21 c = rdtsc if b-a \u0026lt; c-b: low bit was probably a 1 // L24 这段代码比22.1里面的代码更加完整，这里是一个更完整的Meltdown攻击代码，这里我们增加了Flush and Reload代码。\n首先我们声明了一个buffer，现在我们只需要从内核中窃取1个bit的数据，我们会将这个bit乘以4096，所以我们希望下面的Flush and Reload要么看到buffer[0]在cache中，要么看到buffer[4096]在cache中。\n为什么要有这么的大的间隔？是因为硬件有预获取。如果你从内存加载一个数据，硬件极有可能会从内存中再加载相邻的几个数据到cache中。\n所以我们不能使用两个非常接近的内存地址，然后再来执行Flush and Reload，我们需要它们足够的远，这样即使有硬件的预获取，也不会造成困扰。所以这里我们将两个地址放到了两个内存Page中（注，一个内存Page 4096）。\n现在的Flush部分直接调用了clflush指令（代码第4第5行），来确保我们buffer中相关部分并没有在cache中。\n代码第7行或许并不必要，这里我们会创造时间差。我们将会在第10行执行load指令，它会load一个内核内存地址，所以它会产生Page Fault。但是我们期望能够在第10行指令Retired之前，也就是实际的产生Page Fault并取消这些指令效果之前，再预测执行（Speculative execution）几条指令。\n如果代码第10行在下面位置Retired，那么对我们来说就太早了。实际中我们需要代码第13行被预测执行，这样才能完成攻击。\n所以我们希望代码第10行的load指令尽可能晚的Retired，这样才能推迟Page Fault的产生和推迟取消预测执行指令的效果。因为我们知道一个指令只可能在它之前的所有指令都Retired之后，才有可能Retired。\n所以在代码第7行，我们可以假设存在一些非常费时的指令，它们需要很长时间才能完成。或许要从RAM加载一些数据，这会花费几百个CPU cycle；或许执行了除法，或者平方根等。\n这些指令花费了很多时间，并且很长时间都不会Retired，因此也导致代码第10行的load很长时间也不会Retired，并给第11到13行的代码时间来完成预测执行。\n现在假设我们已经有了内核的一个虚拟内存地址，并且要执行代码第10行。我们知道它会生成一个Page Fault，但是它只会在Retired的时候才会真正的生成Page Fault。我们设置好了使得它要过一会才Retired。\n因为代码第10行还没有Retired，并且在Intel CPU上，即使你没有内存地址的权限，数据也会在预测执行的指令中被返回。这样在第11行，CPU可以预测执行，并获取内核数据的第0个bit。第12行将其乘以4096。第13行是另一个load指令，load的内存地址是buffer加上r2寄存器的内容。\n我们知道这些指令的效果会被取消，因为第10行会产生Page Fault，所以对于r3寄存器的修改会被取消。但是尽管寄存器都不会受影响，代码第13行会导致来自于buffer的部分数据被加载到cache中。取决于内核数据的第0bit是0还是1，第13行会导致要么是buffer[0]，要么是buffer[4096]被加载到cache中。\n之后，尽管r2和r3的修改都被取消了，cache中的变化不会被取消，因为这涉及到Micro-Architectural，所以cache会被更新。\n第15行表示最终Page Fault还是会发生，并且我们需要从Page Fault中恢复。用户进程可以注册一个Page Fault Handler（注，详见Lec17），并且在Page Fault之后重新获得控制。论文还讨论了一些其他的方法使得发生Page Fault之后可以继续执行程序。\n现在我们需要做的就是弄清楚，是buffer[0]还是buffer[4096]被加载到了cache中。现在我们可以完成Flush and Reload中的Reload部分了。第18行获取当前的CPU时间，第19行load buffer[0]，第20行再次读取当前CPU时间，第21行load buffer[4096]，第22行再次读取当前CPU时间，第23行对比两个时间差。\n哪个时间差更短，就可以说明内核数据的bit0是0还是1。如果我们重复几百万次，我们可以扫描出所有的内核内存。\n学生提问：在这里例子中，如果b-a\u0026lt;c-b，是不是意味着buffer[0]在cache中？\nRobert教授：是的，你是对的。\n学生提问：在第9行之前，我们需要if语句吗？\nRobert教授：并不需要，22.2中的if语句是帮助我展示Speculative execution的合理理由：尽管CPU不知道if分支是否命中，它还是会继续执行。\n但是在这里，预测执行的核心是我们并不知道第10行的load会造成Page Fault，所以CPU会在第10行load之后继续预测执行。理论上，尽管这里的load可能会花费比较长的时间（例如数百个CPU cycle），但是它现在不会产生Page Fault，所以CPU会预测执行load之后的指令。如果load最终产生了Page Fault，CPU会回撤所有预测执行的效果。\n预测执行会在任何长时间执行的指令，且不论这个指令是否能成功时触发。例如除法，我们不知道是否除以0。一旦触发预测执行，所有之后的指令就会开始被预测执行。不管怎样，真正核心的预测执行从第10行开始，但是为了让攻击更有可能成功，我们需要确保预测执行从第7行开始。\n学生提问：在这个例子中，我们只读了一个bit，有没有一些其他的修改使得我们可以读取一整个寄存器的数据？\nRobert教授：有的，将这里的代码运行64次，每次获取1个bit。\n学生提问：为什么不能一次读取64bit呢？\nRobert教授：如果这样的话，buffer需要是2^64再乘以4096，我们可能没有足够的内存来一次读64bit。或许你可以一次读8个bit，然后buffer大小是256*4096。\n论文中有相关的，因为这里主要的时间在第17行到第24行，也就是Flush and Reload的Reload部分。如果一次读取一个字节，那么找出这个字节的所有bit，需要256次Reload，每次针对一个字节的可能值。\n如果一次只读取一个bit，那么每个bit只需要2次Reload。所以一次读取一个bit，那么读取一个字节只需要16次Reload，一次读取一个字节，那么需要256次Reload。所以论文中说一次只读取一个bit会更快，这看起来有点反直觉，但是又好像是对的。\n学生提问：这里的代码会运行在哪？会运行在特定的位置吗？\nRobert教授：这取决于你对于机器有什么样的权限，并且你想要窃取的数据在哪了。\n举个例子，你登录进了Athena（注，MIT的共享计算机系统），机器上还有几百个其他用户 ，然后你想要窃取某人的密码，并且你很有耐心。\n在几年前Athena运行的Linux版本会将内核内存映射到每一个用户进程的地址空间。那么你就可以使用Meltdown来一个bit一个bit的读取内核数据，其中包括了I/O buffer和network buffer。如果某人在输入密码，且你足够幸运和有耐心，你可以在内核内存中看见这个密码。\n实际中，内核可能会映射所有的物理内存，比如XV6就是这么做的，这意味着你或许可以使用Meltdown在一个分时共享的机器上，读取所有的物理内存，其中包括了所有其他进程的内存。\n这样我就可以看到其他人在文本编辑器的内容，或者任何我喜欢的内容。这是你可以在一个分时共享的机器上使用Meltdown的方法。其他的场景会不太一样。\n分时共享的机器并没有那么流行了，但是这里的杀手场景是云计算。如果你使用了云服务商，比如AWS，它会在同一个计算机上运行多个用户的业务，取决于AWS如何设置它的VMM或者容器系统，如果你购买了AWS的业务，那么你或许就可以窥探其他运行在同一个AWS机器上的用户软件的内存。我认为这是人们使用Meltdown攻击的方式。\n另一个可能有用的场景是，当你的浏览器在访问web时，你的浏览器其实运行了很多不被信任的代码，这些代码是各种网站提供的，或许是以插件的形式提供，或许是以javascript的形式提供。\n这些代码会被加载到浏览器，然后被编译并被运行。有可能当你在浏览网页的时候，你运行在浏览器中的代码会发起Meltdown攻击，而你丝毫不知道有一个网站在窃取你笔记本上的内容，但是我并不知道这里的细节。\n学生提问：有人演示过通过javascript或者WebAssembly发起攻击吗？\nRobert教授：我不知道。人们肯定担心过WebAssembly，但是我不知道通过它发起攻击是否可行。对于javascript我知道难点在于时间的测量，你不能向上面一样获取到纳秒级别的时间，所以你并不能使用Flush and Reload。或许一些更聪明的人可以想明白怎么做，但是我不知道。\n实际中Meltdown Attack并不总是能生效，具体的原因我认为论文作者并没有解释或者只是猜测了一下。如果你查看论文的最后一页，\n你可以看到Meltdown Attack从机器的内核中读取了一些数据，这些数据里面有一些XXXX，这些是没能获取任何数据的位置，也就是Meltdown Attack失败的位置。\n论文中的Meltdown Attack重试了很多很多次，因为在论文6.2还讨论了性能，说了在某些场景下，获取数据的速率只有10字节每秒，这意味着代码在那不停的尝试了数千次，最后终于获取到了数据，也就是说Flush and Reload表明了两个内存地址只有一个在Cache中。\n所以有一些无法解释的事情使得Meltdown会失败，从上图看，Meltdown Attack获取了一些数据，同时也有一些数据无法获得。\n据我所知，人们并不真的知道所有的成功条件和失败条件，最简单的可能是如果内核数据在L1 cache中，Meltdown能成功，如果内核数据不在L1 Cache中，Meltdown不能成功。如果内核数据不在L1 cache中，在预测执行时要涉及很多机制，很容易可以想到如果CPU还不确定是否需要这个数据，并不一定会完成所有的工作来将数据从RAM中加载过来。\n你可以发现实际中并没有这么简单，因为论文说到，有时候当重试很多次之后，最终还是能成功。所以这里有一些复杂的情况，或许在CPU内有抢占使得即使内核数据并不在Cache中，这里的攻击偶尔还是可以工作。\n论文的最后也值得阅读，因为它解释了一个真实的场景，比如说我们想要通过Meltdown窃取Firefox的密码管理器中的密码，你该怎么找出内存地址，以及一个攻击的完整流程，我的意思是由学院派而不是实际的黑客完成的一次完整的攻击流程。尽管如此，这里也包含了很多实用的细节。\nMeltdown Fix 我最后想讨论的是Meltdown的修复，你们实际已经接触了一些了。当论文 发表的时候，它获取了很多的关注。实际中还有另一篇论文，也是由这篇论文的部分作者参与完成，另一篇论文讨论了一种使用了CPU内一种叫做Spectre的不同的预测执行的不同攻击方法。这一对论文的同时出现让人非常兴奋。\n所以人们现在发现危害太大了，因为现在我们讨论的是操作系统的隔离性被破坏了。这里的技术破坏了Page Table的保护，这是我们用来实现用户和内核间隔离的技术，所以这是一个非常基础的攻击，或者至少以一种非常通用的方式破坏了安全性非常重要的一个部分。所以人们非常非常迫切的想要修复Meltdown。\n很多操作系统在这篇论文发表之后数周内就推出的一个快速修复，这是一个叫做KAISER，现在在Linux中被称为KPTI的技术（Kernel page-table isolation）。\n这里的想法很简单，也就是不将内核内存映射到用户的Page Table中，相应的就像XV6一样，在系统调用时切换Page Table。所以在用户空间时，Page Table只有用户内存地址的映射，如果执行了系统调用，会有类似于XV6中trampoline的机制，切换到拥有内核内存映射的另一个Page Table中，这样才能执行内核代码。\n这会导致Meltdown不能工作，因为现在你会切换Page Table，本来代表内核虚拟内存地址的r1寄存器不仅是没有权限，并且也没有意义了，因为现在的用户Page Table并没有包含对它的翻译，所以CPU并不知道该如何处理这个内存地址。\n现在这个虚拟内存地址不会存在于cache中，甚至都不会出现在TLB中。所以当在用户空间发起Meltdown Attack时，也就没有办法知道对应这个虚拟内存地址的数据是什么。这个虚拟内存地址并不是非法的，只是在用户空间没有意义了，这样会导致Meltdown Attack不能工作。\nKAISER的缺点是，系统调用的代价更高了，因为如果不做任何事情的话，切换Page Table会导致TLB被清空，因为现在TLB中的映射关系都是前一个Page Table的。同时也会导致L1 cache被清空，因为其中对应的虚拟内存地址对于新的Page Table也没有意义了。在一些机器上，切换Page Table会使得系统调用明显变慢。\n最近的CPU拥有叫做PCID（process-context identifiers）的技术，它可以帮助你在切换Page Table时避免清空Cache，尽管它还是要花费一些时间。\n如果你上网看的话，当时人们有很多顾虑，当时人们认为这种两个Page Table的方案是不可接受的慢。但是实际中这并不是一个严重的问题，你上网看的话就可以发现人们有对于工作负载的整体影响的评估，因为毕竟程序也不是一直在进出内核，这里的影响大概是5%，所以这并不是一个坏的主意。\n人们非常快的采用了这种方案，实际上在论文发表时，已经有内核采用了这种方案来抵御其他的攻击。\n除此之外，还有一个合理的硬件修复。我相信Intel在最近的处理器上已经添加了这个修复，AMD之前就已经有这个修复。\n这是Cache的结构，当指令从L1 cache中加载某个数据时，比如说我们想要窃取的内核数据，人们认为数据的权限标志位就在L1 cache中，所以CPU完全可以在获取数据的时候检查权限标志位。\n实际中，AMD CPU和最近的Intel CPU会在很早的时候检查权限标志位。如果检查不能通过，CPU不会返回数据到CPU核中。所以没有一个预测执行指令可以看到不该看到的数据。\n学生提问：为什么你觉得Intel会做这个呢？对我来说这里像是个讨论，我们应该为预测执行指令检查权限标志位吗？Intel的回答是不，为什么要检查呢？\nRobert教授：是的，为什么要检查呢？反正用户也看不到对应的数据。如果更早的做权限检查，会在CPU核和L1 cache之间增加几个数字电路门，而CPU核和L1 cache之间路径的性能对于机器来说重要的，如果你能在这节省一些数字电路门的话，这可以使得你的CPU节省几个cycle来从L1 cache获取数据，进而更快的运行程序。\n所以很容易可以想到如果过早的检查权限，会在电路上增加几个晶体管。因为毕竟所有的预测执行指令都会Retired，并不是说过早的检查权限就可以节省一些后续的工作，在指令Retired的时候还是要触发Page Fault。我这里只是猜测，这里做一些权限检测并不能带来什么优势。\n学生提问：既然Intel已经从CPU上修复了这个问题，有没有哪个内核计划取消KAISER来提升性能？\nRobert教授：我知道在很多内核上，这个是可选项，但是我并不完全清楚Intel修复的具体内容。我很确定他们有一些修复，但是具体内容我并不知道。\nFrans教授：我认为Linux中你可以查询哪些硬件修复已经存在，并根据返回要求Linux修改从软件对于硬件问题的规避。你可以在你的笔记本上运行一个Linux命令来查看它包含了哪些问题的修复，哪些问题已经在硬件中规避了。\nRobert教授：你是说如果CPU包含了修复的话，Linux实际会使用combined Page Table（注，也就是将内核内存映射到用户Page Table中）？\nFrans教授：是的，我99%相信是这样的，虽然我最近没有再看过了，但是我认为还是这样的。\n学生提问：人们是在干什么的时候发现这个的？\nRobert教授：当人们尝试入侵一个计算机的时候。谁知道人们真正想要干什么呢？论文是由学院派写的，或许他们在研究的时候发现了一些安全问题。\nFrans教授：我认为很长时间他们的一个驱动力是，他们想破解Address Space Layout Randomization，他们有一些更早的论文，看起来在这个领域有一些研究者。我认为最开始的时候，人们来自不同的领域。 就像Robert说过的，人们在这个领域工作了几十年来找到可以理解和攻击的Bug。\n学生提问：有多大的可能还存在另一种Meltdown？\nRobert教授：非常有可能。CPU制造商在几十年间向CPU增加了非常非常多酷炫的技术，以使得CPU运行的可以更快一些。人们之前并没有太担忧或者没有觉得这会是一个严重的安全问题。\n现在人们非常清楚这可能会是非常严重的安全问题，但是我们现在使用的CPU已经包含了30年的聪明思想，实际上在论文发表之前，已经存在很多基于Micro-Architectural的这一类攻击。我认为还需要一段时间才能把这一类问题完全消除。\nFrans教授：如果你查看过去两年的安全相关的会议，每个会议基本都有一个session是有关探索预测执行属性，来看看能不能发起一次攻击。\nRobert教授：或许这是一个更大的问题，是不是我们解决了有限的问题就没事了，又或者是上层设计方法出现问题了。这可能太过悲观了，但是你知道的，人们对于操作系统的隔离寄托了太多期望，可以非常合理的认为隔离可以工作。\n并且我们会在这种假设下设计类似于云计算，在浏览器中运行Javascript等等场景。但是现在这种假设实际并不成立，曾经人们认为操作系统的隔离性足够接近成立，但是这一整套基于Micro-Architectural的攻击使得这里的故事不再让人信服。\n学生提问：CPU设计者可以做到什么程度使得不使用Micro-Architectural又能保持高性能，同时也有很好的安全性？\nRobert教授：有些内容明显是可以修复的，比如这节课介绍的Meltdown Attack是可以被修复的，并且不会牺牲任何性能。对于一些其他的攻击，并不十分确定你可以在不损伤性能的前提下修复它们。\n有些问题隐藏的非常非常的深，现在有很多共享的场景，例如分时共享的计算机，云计算。假设在你的云主机上有一个磁盘驱动和一个网卡驱动，你或许可以仅仅通过监测别人的流量是怎么影响你的流量的，这里的流量包括了网络流量和磁盘流量，来获取同一个主机上的其他用户信息。\n我不知道这是否可行，但是对于很多东西，人们都能发现可以攻击的点。\n所以很多这里的Micro-Architectural带来的问题可以在不损伤性能的前提下清除掉，但是也或许不能。\n","date":"2024-10-07T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08122meltdown/19_hu_6fa48e3aa5e521c2.jpg","permalink":"https://echudet.github.io/p/mit6s08122meltdown/","title":"MIT6S081：22Meltdown"},{"content":"Lecture 21 Networking 课前预习 论文速览 论文链接 论文《Eliminating Receive Livelock in an Interrupt-driven Kernel》分为多个部分，每部分都有其核心内容，以下是各部分的详细总结：\n在引言（Introduction）部分首先介绍了接收活锁（receive livelock）问题。传统的中断机制适用于早期的流量控制系统，但在面对现代非流量控制协议（如多媒体和RPC客户端-服务器系统）时，这种机制容易导致活锁问题。接收活锁是指系统在处理中断时耗尽所有资源，却无法执行其他关键任务，从而使系统进程停滞。\n在激励应用（Motivating Applications）部分，作者提供了几种容易受到活锁影响的应用示例：\n基于主机的路由（Host-based Routing）：许多现代路由器在面对非流量控制的高负载流量时，容易陷入活锁状态。 被动网络监控（Passive Network Monitoring）：处于混杂模式的网络接口在监控流量时，易遭受大规模的数据包输入，从而面临活锁。 网络文件服务（Network File Services）：如NFS服务器，在处理大量RPC流量时，也可能受到活锁的影响。 在调度网络任务的需求（Requirements for Scheduling Network Tasks）部分谈到，为了避免活锁并确保系统性能，操作系统必须合理调度网络任务。该部分讨论了系统在面对不同负载时，如何保证吞吐量、延迟、抖动（延迟波动）的合理控制，同时确保公平分配CPU资源，以避免系统过载。\n在中断驱动的调度及其后果（Interrupt-driven Scheduling and Its Consequences）部分中，中断驱动系统的缺陷被详细讨论，包括：\n接收活锁：系统在处理中断时，忽略了其他任务，导致吞吐量降为零。 延迟增加：在高负载下，系统推迟了数据包的交付，延迟大幅增加。 传输饥饿：过多的输入处理会阻止数据包的传输，导致系统仅接收数据包，却无法发送。 在改进调度避免活锁（Avoiding Livelock Through Better Scheduling）部分中作者提出了一些避免活锁的方法：\n限制中断到达速率：当系统超载时，暂时禁用中断，以便系统处理更高层的任务。 轮询（Polling）机制：通过轮询机制代替中断，确保公平分配CPU资源，避免活锁。 避免抢占：防止系统在处理中断时被中断，这样可以降低活锁的风险。 在 BSD路由器中的活锁问题（Livelock in BSD-based Routers）部分中，作者通过实验展示了BSD系统中的活锁问题。在负载超过一定限度后，系统因处理中断耗尽资源，导致性能急剧下降。作者详细讨论了为何接收活锁会出现在这些系统中，并指出这些系统在面对高负载时会陷入活锁状态。\n在确保用户进程的进展（Guaranteeing Progress for User-level Processes）部分，作者讨论了如何在高网络负载下，仍然为用户级进程保留足够的CPU时间。提出了一些机制，如CPU周期限制，确保系统在处理中断时，不会完全忽略用户进程。\n相关工作（Related Work），该部分回顾了先前与轮询机制、中断驱动设计等相关的工作，讨论了现有技术如何减少中断开销，但这些技术仍不足以完全消除活锁问题。\n总结与结论（Summary and Conclusions），论文总结了避免活锁的关键点：\n活锁在中断驱动系统中是一个严重的问题，尤其是在高网络负载下。 通过引入新的调度策略（如中断限速、轮询机制和CPU时间管理），系统可以有效避免活锁，并维持良好的性能。 这些改进方法使得系统能够在高负载情况下，依然保持合理的响应速度和稳定的性能。\nfeedback 在实验中，作者发现当系统处理接收的数据包时，如果筛选队列（例如供 screend 使用的筛选队列）已满，那么继续接收数据包会导致系统资源浪费。这些数据包最终会在被完全处理前被丢弃，浪费了宝贵的处理时间。为了应对这一问题，作者在内核中添加了反馈机制：当检测到筛选队列已满时，系统会停止进一步的输入处理，直到队列有了足够的空间。\n具体来说，反馈机制会在系统中检测到特定队列（例如筛选队列）满时，立即暂停接收新的数据包，暂时禁用输入中断。系统会定期检查队列状态，当队列中的数据包处理到一定程度（比如达到队列容量的25%）时，重新允许输入中断恢复工作。这一机制确保了系统不会因过多的接收任务导致资源浪费，并且能够腾出时间处理现有的队列中的数据包。\n在实验中，作者选择的筛选队列大小为32个数据包，并在队列填满75%时停止输入处理，只有当队列数据量减少至25%时才重新启用输入。这种高、低水位线的策略能够帮助系统有效地管理资源并保持一定的吞吐量。队列反馈机制的主要作用包括：\n避免活锁：系统在高负载下避免无谓的数据包处理，当筛选队列满时直接暂停输入，确保资源集中处理现有数据包。 提高资源利用率：反馈机制确保了系统可以充分利用输入队列，在队列资源消耗完之前处理到位，从而减少数据包丢弃情况，提升整体处理效率。 稳定系统性能：通过合理地分配资源，该机制确保系统在高负载下依然能够以一定的速度处理和转发数据包，从而避免系统在过载情况下崩溃或陷入死循环。 在使用 screend 程序的实验中，通过引入反馈机制，系统在高负载时能够显著提升峰值吞吐量，避免了活锁问题。通过这些测试，作者还发现不同的配额设置（即每次轮询处理的包数量）对系统性能影响较大，小配额（如10-20个包）能有效避免活锁并保持系统稳定。\n总体来说，第6.6节的反馈机制通过实时监控队列状态，并在队列容量达到一定程度时暂停输入，实现了更高效的资源管理，确保系统在高负载下稳定运行并防止活锁现象。\n操作系统的网络功能 操作系统的网络功能在底层确实使用了类似上面提到的编程接口（API）来处理网络通信，但这些功能背后涉及操作系统中的网络协议栈、驱动程序和硬件接口等复杂的机制。操作系统通过提供一系列的系统调用或库函数接口，使应用程序可以方便地进行网络通信。\n这些接口可以分为以下几类：\n套接字接口（Socket API）\n这是操作系统最常见的网络编程接口，主要用于实现不同网络协议（如 TCP 和 UDP）的网络通信。大多数现代操作系统提供的网络功能都是基于套接字的。操作系统通过系统调用（如 socket()、connect() 等）为用户提供操作网络连接的接口。\n以下是常用的套接字接口：\nsocket()：创建一个套接字，指定协议族（如 IPv4、IPv6）和传输协议（如 TCP、UDP）。 bind()：将套接字与本地地址（IP 和端口）绑定，用于服务端监听特定的网络接口。 listen()：将套接字设置为被动监听模式，等待客户端连接。 accept()：服务端接受客户端连接，返回新的套接字用于通信。 connect()：客户端用来连接服务器的套接字。 send() 和 recv()：分别用于发送和接收数据。 sendto() 和 recvfrom()：用于无连接的 UDP 协议，直接向指定地址发送数据或从指定地址接收数据。 close()：关闭套接字，终止连接。 网络设备接口\n操作系统提供了对网络设备（如网卡、无线适配器）的抽象，允许应用程序通过接口访问这些设备。例如，通过系统调用和ioctl命令来控制设备状态，修改网络配置。\n常见的操作有：\nioctl()：控制网络接口，配置地址、网络掩码、路由表等。 ifconfig：在类 Unix 系统中，用于配置和管理网络接口的命令（或对应的 API）。 ethtool：查看和控制以太网设备的属性。 数据链路层接口\n一些操作系统允许开发人员直接操作数据链路层，通过特定的接口发送和接收原始的以太网帧（Raw Sockets）。这通常用于开发低级别的网络工具或网络协议的实现。\n接口如：\nAF_PACKET：Linux 中允许直接在链路层上发送/接收数据包的套接字类型，用于实现自定义协议。 网络配置接口\n操作系统还提供了一些接口，用于用户或应用程序动态配置网络，如修改 IP 地址、设置路由、配置防火墙等。\n常见的工具和接口：\nnetstat：查看网络连接和端口状态。 route：查看和修改系统的路由表。 ip：现代 Linux 系统中，用于配置 IP 地址、路由、防火墙等。 iptables：设置和管理 Linux 系统的防火墙规则。 异步 I/O 接口\n现代操作系统提供的网络接口通常支持异步 I/O，允许应用程序在等待网络事件时不阻塞。这对于高并发应用和事件驱动的网络程序非常重要。\n常用的异步接口：\nselect() 和 poll()：检查多个套接字的状态，确定哪些套接字可读、可写或发生错误。 epoll()（Linux） 和 kqueue()（BSD 系统）：更高效的事件通知机制，用于大规模并发连接。 异步 I/O 框架：如 Linux 的 AIO 或 Windows 的 IOCP，允许更高效的非阻塞 I/O 操作。 高级网络协议接口\n除了底层套接字接口，操作系统往往通过用户态库提供对更高级网络协议的支持，如：\nHTTP：通过操作系统提供的 API 或高级网络库（如 libcurl）支持高层协议。 TLS/SSL：如 OpenSSL，提供安全传输层的加密通信支持。 虚拟网络接口\n操作系统还可以提供虚拟网络设备，用于虚拟机网络通信、VPN 实现等，如：\nTUN/TAP 设备：Linux 中用于创建用户态虚拟网络接口的设备。TUN 用于三层网络（如 IP），TAP 用于二层网络（如以太网帧）。 网络管理接口\n现代操作系统通过管理接口提供动态的网络资源管理，应用程序可以通过这些接口调整网络行为，如 QoS、流量管理等。\n常见的网络管理接口包括：\nNetworkManager：Linux 上的网络管理服务，允许动态切换网络配置（Wi-Fi、有线网络等）。 Windows Sockets（Winsock）：Windows 操作系统上用于网络通信的接口。 防火墙和安全接口\n操作系统提供了用于设置网络防火墙和安全策略的接口。例如：\niptables（Linux）和 firewalld：用于定义网络防火墙规则。 Windows Firewall API：Windows 操作系统提供的防火墙控制接口。 示例代码（基于 C 语言的 POSIX 套接字）\n操作系统底层通过 C 语言的 POSIX 标准为程序员提供系统调用来访问网络功能。以下是一个基于 C 的简单 TCP 服务器示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; int main() { int server_fd, new_socket; struct sockaddr_in address; int opt = 1; int addrlen = sizeof(address); char buffer[1024] = {0}; // 创建套接字 server_fd = socket(AF_INET, SOCK_STREAM, 0); if (server_fd == 0) { perror(\u0026#34;socket failed\u0026#34;); exit(EXIT_FAILURE); } // 绑定地址和端口 address.sin_family = AF_INET; address.sin_addr.s_addr = INADDR_ANY; address.sin_port = htons(8080); bind(server_fd, (struct sockaddr *)\u0026amp;address, sizeof(address)); // 监听 listen(server_fd, 3); printf(\u0026#34;Server is listening on port 8080\\n\u0026#34;); // 接受客户端连接 new_socket = accept(server_fd, (struct sockaddr *)\u0026amp;address, (socklen_t*)\u0026amp;addrlen); // 读取数据 read(new_socket, buffer, 1024); printf(\u0026#34;Received: %s\\n\u0026#34;, buffer); // 发送响应 char *response = \u0026#34;Hello from server\u0026#34;; send(new_socket, response, strlen(response), 0); // 关闭连接 close(new_socket); close(server_fd); return 0; } 总的来说，操作系统通过各种网络接口向用户提供网络功能，从底层的套接字系统调用到更高级的网络管理工具和库。应用程序通过这些接口可以实现网络通信、数据传输、网络设备配置等功能。\nLivelock Livelock 是一种计算机系统中常见的同步问题，类似于死锁（Deadlock），但有所区别。在 livelock 情况下，多个进程或线程虽然没有完全停止运行（也就是说它们仍然在“活动”中），但无法继续执行有意义的操作，因为它们彼此不断地改变状态或采取相互回避的行为，导致始终无法达到目标状态。二者区别如下：\n死锁（Deadlock） 活锁（Livelock） 进程或线程由于资源竞争或互相等待而完全停滞，不会再有任何进展。 进程或线程仍在运行，但因互相妥协或状态改变，始终无法完成预期的任务，表现为一种不断尝试和回退的循环。 举一个例子来形容livelock，即两个人同时尝试从狭窄的走廊中通过，并相互避让。人 A 往左，人 B 也往左。然后人 A 发现两人撞上了，决定往右，而人 B 也往右。他们就这样不断相互避让，但永远无法成功通过。\n计算机系统中的例子：在多线程编程中，如果两个线程使用自旋锁（spinlock），并不断尝试解除锁，但总是因为其他线程在做相同的事情而陷入循环，则可能形成 livelock。\n造成Livelock可能有多个原因：\n不恰当的资源释放策略：线程或进程在资源竞争中频繁地释放和请求资源。 优先级反转和调度问题：当线程的优先级动态变化，导致调度策略始终无法满足任务需求时。 不良的避免冲突策略：如多个线程在检测冲突后不断地相互回避，却无法找到合适的行动策略。 相应的，可以预防 Livelock的方法有如下：\n引入随机化：通过在冲突时引入随机等待或随机操作策略，可以避免相互的“镜像”行为。 限制重试次数：设定尝试的最大次数或时间限制，超过后采取其他方式或放弃操作。 引入优先级机制：在多线程竞争时，设置适当的优先级，以确保重要操作能够顺利完成。 要识别 Livelock，通常需要对系统的状态进行监控和分析，查看是否存在某些进程或线程在做无效的、重复的动作而没有实际进展。可以通过以下方法进行调试：\n日志分析：检查重复操作的模式。 CPU 占用率监控：看是否存在线程持续占用 CPU 但没有实际产出。 执行路径跟踪：追踪线程执行的具体路径，确认是否进入了一个无效循环。 Livelock 是一种由于资源协调不当而导致的“忙碌等待”现象，其表现为系统中某些进程或线程虽然在活动，但因重复的避让行为而无法完成有效的任务。与死锁不同，活锁需要更复杂的策略来处理，如引入随机化、调整优先级等。\nReceive Livelock Receive Livelock 是网络系统中的一种活锁现象，通常发生在高流量环境下，当系统过度忙于处理接收的数据包，但无法有效传递数据到更高层的应用或协议栈时。它本质上是网络设备或操作系统在网络包接收过程中被不断打断，导致没有足够时间处理或传递这些数据包，使得传输效率极低。\n在网络中，尤其是高流量下，网络接口会迅速收到大量数据包。由于每个接收的包都可能触发一个中断，CPU会频繁响应这些中断，导致以下问题：\n频繁的中断：系统一旦收到网络包就产生中断，迫使 CPU 停止当前任务去处理网络包，这会使应用层或协议栈无法获取足够的时间来处理这些包。 资源争用：由于系统忙于响应中断，几乎所有 CPU 资源都用于接收和调度数据包，使得系统其他任务无法执行，包括转发或处理这些接收的数据包。 处理延迟：不断的中断和无效处理会让系统无法在每次中断间完成一个完整的数据包处理流程，进而形成 livelock。 系统表现出接收很多包但没有真正处理的情况。例如：\n高 CPU 占用率：系统在处理中断，但 CPU 时间主要耗在接收上，而非有效处理。 低传输效率：虽然网络接口忙于接收，但传输的数据量不成比例，表现为低吞吐量。 应用层数据包少：应用程序无法接收到足够的有效数据，影响服务性能。 为了应对 receive livelock，可以采取以下措施：\n减少中断频率：采用“中断聚合”（Interrupt Coalescing）技术，将多个中断合并为一个，从而减少 CPU 处理中断的频率，让系统有时间有效处理数据包。 使用 NAPI（New API）技术：NAPI 是 Linux 内核中的一种中断处理机制，用于网络包的批量处理。它将接收从中断驱动切换到轮询驱动，当流量过高时，系统进入轮询模式，这样系统可以避免频繁中断，从而减轻 CPU 负担。 分布式处理：使用多核 CPU 并合理分配网络任务，让多个核心共同处理接收和转发数据包。 限流策略：控制接收的数据包速率，例如限制每秒接收的包数，避免超过系统处理能力。 为确认是否发生 receive livelock，可以通过以下步骤检测：\n监控 CPU 占用和中断频率：观察是否有单个 CPU 核心被高频中断占满。 流量分析：检查网络流量与应用层收到的有效数据包量是否存在显著差距。 应用层性能测试：通过对比不同网络负载下的应用性能，分析是否存在高负载下的明显性能下降。 Receive Livelock 是网络系统因高频中断而导致的活锁现象，会严重影响系统的数据处理和传输效率。通过减少中断、优化处理机制和引入流量控制策略，可以有效减轻其影响。\n正课部分 计算机网络概述 今天我想讨论一下Networking以及它与操作系统的关联。今天这节课的很多内容都与最后一个lab，也就是构建一个网卡驱动相关。\n在这节课，我们首先会大概看一下操作系统中网络相关的软件会有什么样的结构，之后我们会讨论今天的论文Livelock 。Livelock展示了在设计网络协议栈时可能会出现的有趣的陷阱。\n首先，让我通过画图来描述一下基本的网络场景。网络连接了不同的主机，这里的连接有两种方式：\n相近的主机连接在同一个网络中。例如有一个以太网设备，可能是交换机或者单纯的线缆，然后有一些主机连接到了这个以太网设备。这里的主机可能是笔记本电脑，服务器或者路由器。在设计网络相关软件的时候，通常会忽略直接连接了主机的网络设备。\n这里的网络设备可能只是一根线缆（几十年前就是通过线缆连接主机）；也可能是一个以太网交换机；也可能是wifi无线局域网设备（主机通过射频链路与网络设备相连），但是不管是哪种设备，这种直接连接的设备会在网络协议栈的底层被屏蔽掉。\n每个主机上会有不同的应用程序，或许其中一个主机有网络浏览器，另一个主机有HTTP server，它们需要通过这个局域网来相互通信。\n一个局域网的大小是有极限的。局域网（Local Area Network）通常简称为LAN。\n一个局域网需要能让其中的主机都能收到彼此发送的packet。有时，主机需要广播packet到局域网中的所有主机。当局域网中只有25甚至100个主机时，是没有问题的。但是你不能构建一个多于几百个主机的局域网。\n所以为了解决这个问题，大型网络是这样构建的。首先有多个独立的局域网，假设其中一个局域网是MIT，另一个局域网是Harvard，还有一个很远的局域网是Stanford，在这些局域网之间会有一些设备将它们连接在一起，这些设备通常是路由器Router。\n其中一个Router接入到了MIT的局域网，同时也接入到了Harvard的局域网。\n路由器是组成互联网的核心，路由器之间的链路，最终将多个局域网连接在了一起。\n现在MIT有一个主机需要与Stanford的一个主机通信，他们之间需要经过一系列的路由器，路由器之间的转发称为Routing。\n所以我们需要有一种方法让MIT的主机能够寻址到Stanford的主机，并且我们需要让连接了MIT的路由器能够在收到来自MIT的主机的packet的时候，能够知道这个packet是发送给Harvard的呢，还是发送给Stanford的。\n从网络协议的角度来说，局域网通信由以太网协议决定。而局域网之上的长距离网络通信由Internet Protocol协议决定。以上就是网络的概述。\n接下来我想介绍一下，在局域网和互联网上传递的packet有什么样的结构，之后再讨论在主机和路由器中的软件是如何处理这些packet。\n二层网络 \u0026mdash; Ethernet 让我从最底层开始，我们先来看一下一个以太网packet的结构是什么。当两个主机非常靠近时，或许是通过相同的线缆连接，或许连接在同一个wifi网络，或许连接到同一个以太网交换机。\n当局域网中的两个主机彼此间要通信时，最底层的协议是以太网协议。你可以认为Host1通过以太网将Frame发送给Host2。\nFrame是以太网中用来描述packet的单词，本质上这就是两个主机在以太网上传输的一个个的数据Byte。以太网协议会在Frame中放入足够的信息让主机能够识别彼此，并且识别这是不是发送给自己的Frame。\n每个以太网packet在最开始都有一个Header，其中包含了3个数据。Header之后才是payload数据。Header中的3个数据是：目的以太网地址，源以太网地址，以及packet的类型。\n每一个以太网地址都是48bit的数字，这个数字唯一识别了一个网卡。\npacket的类型会告诉接收端的主机该如何处理这个packet。接收端主机侧更高层级的网络协议会按照packet的类型检查并处理以太网packet中的payload。整个以太网packet，包括了48bit+48bit的以太网地址，16bit的类型，以及任意长度的payload这些都是通过线路传输。\n除此之外，虽然对于软件来说是不可见的，但是在packet的开头还有被硬件识别的表明packet起始的数据（注，Preamble + SFD），在packet的结束位置还有几个bit表明packet的结束（注，FCS）。packet的开头和结束的标志不会被系统内核所看到，其他的部分会从网卡送到系统内核。\n如果你们查看了这门课程的最后一个lab，你们可以发现我们提供的代码里面包括了一些新的文件，其中包括了kernel/net.h，这个文件中包含了大量不同网络协议的packet header的定义。\n上图中的代码包含了以太网协议的定义。我们提供的代码使用了这里结构体的定义来解析收到的以太网packet，进而获得目的地址和类型值（注，实际中只需要对收到的raw data指针强制类型转换成结构体指针就可以完成解析）。\n学生提问：硬件用来识别以太网packet的开头和结束的标志是不是类似于lab中的End of Packets？\nRobert教授：并不是的，EOP是帮助驱动和网卡之间通信的机制。\n这里的开头和结束的标志是在线缆中传输的电信号或者光信号，这些标志位通常在一个packet中是不可能出现的。以结束的FCS为例，它的值通常是packet header和payload的校验和，可以用来判断packet是否合法。\n有关以太网48bit地址，是为了给每一个制造出来的网卡分配一个唯一的ID，所以这里有大量的可用数字。\n这里48bit地址中，前24bit表示的是制造商，每个网卡制造商都有自己唯一的编号，并且会出现在前24bit中。后24bit是由网卡制造商提供的任意唯一数字，通常网卡制造商是递增的分配数字。\n所以，如果你从一个网卡制造商买了一批网卡，每个网卡都会被写入属于自己的地址，并且如果你查看这些地址，你可以发现，这批网卡的高24bit是一样的，而低24bit极有可能是一些连续的数字。\n虽然以太网地址是唯一的，但是出了局域网，它们对于定位目的主机的位置是没有帮助的。如果网络通信的目的主机在同一个局域网，那么目的主机会监听发给自己的地址的packet。\n但是如果网络通信发生在两个国家的主机之间，你需要使用一个不同的寻址方法，这就是IP地址的作用。\n在实际中，你可以使用tcpdump来查看以太网packet。这将会是lab的一部分。下图是tcpdump的一个输出：\ntcpdump输出了很多信息，其中包括：\n接收packet的时间 第一行的剩下部分是可读的packet的数据 接下来的3行是收到packet的16进制数 如果按照前面以太网header的格式，可以发现packet中：\n前48bit是一个广播地址，0xffffffffffff。广播地址是指packet需要发送给局域网中的所有主机。 之后的48bit是发送主机的以太网地址，我们并不能从这个地址发现什么，实际上这个地址是运行在QEMU下的XV6生成的地址，所以地址中的前24bit并不是网卡制造商的编号，而是QEMU编造的地址。 接下来的16bit是以太网packet的类型，这里的类型是0x0806，对应的协议是ARP。 剩下的部分是ARP packet的payload。 二/三层地址转换 \u0026mdash; ARP 下一个与以太网通信相关的协议是ARP。\n在以太网层面，每个主机都有一个以太网地址。但是为了能在互联网上通信，你需要有32bit的IP地址。\n为什么需要IP地址呢？因为IP地址有额外的含义。\nIP地址的高位bit包含了在整个互联网中，这个packet的目的地在哪。所以IP地址的高位bit对应的是网络号，虽然实际上要更复杂一些，但是你可以认为互联网上的每一个网络都有一个唯一的网络号。\n路由器会检查IP地址的高bit位，并决定将这个packet转发给互联网上的哪个路由器。IP地址的低bit位代表了在局域网中特定的主机。\n当一个经过互联网转发的packet到达了局域以太网，我们需要从32bit的IP地址，找到对应主机的48bit以太网地址。这里是通过一个动态解析协议完成的，也就是Address Resolution Protocol，ARP协议。\n当一个packet到达路由器并且需要转发给同一个以太网中的另一个主机，或者一个主机将packet发送给同一个以太网中的另一个主机时，发送方首先会在局域网中广播一个ARP packet，来表示任何拥有了这个32bit的IP地址的主机，请将你的48bit以太网地址返回过来。\n如果相应的主机存在并且开机了，它会向发送方发送一个ARP response packet。\n下图是一个ARP packet的格式：\n它会出现在一个以太网packet的payload中。所以你们看到的将会是这样的结构：首先是以太网header，它包含了48bit的目的以太网地址，48bit的源以太网地址，16bit的类型；之后的以太网的payload会是ARP packet，包含了上图的内容。\n接收到packet的主机通过查看以太网header中的16bit类型可以知道这是一个ARP packet。在ARP中类型值是0x0806。通过识别类型，接收到packet的主机就知道可以将这个packet发送给ARP协议处理代码。\n有关ARP packet的内容，包含了不少信息，但是基本上就是在说，现在有一个IP地址，我想将它转换成以太网地址，如果你拥有这个IP地址，请响应我。\n同样的，我们也可以通过tcpdump来查看这些packet。\n在网络的lab中，XV6会在QEMU模拟的环境下发送IP packet。所以你们可以看到在XV6和其他主机之间有ARP的交互。\n下图中第一个packet是我的主机想要知道XV6主机的以太网地址，第二个packet是XV6在收到了第一个packet之后，并意识到自己是IP地址的拥有者，然后返回response。\ntcpdump能够解析出ARP packet，并将数据打印在第一行。对应ARP packet的格式，在第一个packet中，10.0.2.2是SIP，10.0.2.15是DIP。在第二个packet中，52:54:00:12:34:56对应SHA。\n同时，我们也可以自己分析packet的原始数据。对于第一个packet：\n前14个字节是以太网header，包括了48bit目的以太网地址，48bit源以太网地址，16bit类型。 从后往前看，倒数4个字节是TIP，也就是发送方想要找出对应以太网地址的IP地址。每个字节对应了IP地址的一块，所以0a00 020f对应了IP地址10.0.2.15。 再向前数6个字节，是THA，也就是目的地的以太网地址，现在还不知道所以是全0。 再向前数4个字节是SIP，也就是发送方的IP地址，0a000202对应了IP地址10.0.2.2。 再向前数6个字节是SHA，也就是发送方的以太网地址。 剩下的8个字节表明了我们感兴趣的是以太网和IP地址格式。 第二个packet是第一个packet的响应。\n学生提问：ethernet header中已经包括了发送方的以太网地址，为什么ARP packet里面还要包含发送方的以太网地址？\nRobert教授：我并不清楚为什么ARP packet里面包含了这些数据，我认为如果你想的话是可以精简一下ARP packet。\n或许可以这么理解，ARP协议被设计成也可以用在其他非以太网的网络中，所以它被设计成独立且不依赖其他信息，所以ARP packet中包含了以太网地址。\n现在我们是在以太网中发送ARP packet，以太网packet也包含了以太网地址，所以，如果在以太网上运行ARP，这些信息是冗余的。\n但是如果在其他的网络上运行ARP，你或许需要这些信息，因为其他网络的packet中并没有包含以太网地址。\n学生提问：tcpdump中原始数据的右侧是什么内容？\nRobert教授：这些是原始数据对应的ASCII码，“.”对应了一个字节并没有相应的ASCII码，0x52对应了R，0x55对应了U。当我们发送的packet包含了ASCII字符时，这里的信息会更加有趣。\n我希望你们在刚刚的讨论中注意到这一点，网络协议和网络协议header是嵌套的。我们刚刚看到的是一个packet拥有了ethernet header和ethernet payload。\n在ethernet payload中，首先出现的是ARP header，对于ARP来说并没有的payload。\n但是在ethernet packet中还可以包含其他更复杂的结构，比如说ethernet payload中包含一个IP packet，IP packet中又包含了一个UDP packet，所以IP header之后是UDP header。\n如果在UDP中包含另一个协议，那么UDP payload中又可能包含其他的packet，例如DNS packet。\n所以发送packet的主机会按照这样的方式构建packet：DNS相关软件想要在UDP协议之上构建一个packet；UDP相关软件会将UDP header挂在DNS packet之前，并在IP协议之上构建另一个packet；IP相关的软件会将IP heade挂在UDP packet之前；最后Ethernet相关的软件会将Ethernet header挂在IP header之前。所以整个packet是在发送过程中逐渐构建起来的。\n类似的，当一个操作系统收到了一个packet，它会先解析第一个header并知道这是Ethernet，经过一些合法性检查之后，Ethernet header会被剥离，操作系统会解析下一个header。\n在Ethernet header中包含了一个类型字段，它表明了该如何解析下一个header。\n同样的在IP header中包含了一个protocol字段，它也表明了该如何解析下一个header。\n软件会解析每个header，做校验，剥离header，并得到下一个header。一直重复这个过程直到得到最后的数据。这就是嵌套的packet header。\nDNS UDP IP ETH 三层网络 \u0026mdash; Internet Ethernet header足够在一个局域网中将packet发送到一个host。\n如果你想在局域网发送一个IP packet，那么你可以使用ARP获得以太网地址。但是IP协议更加的通用，IP协议能帮助你向互联网上任意位置发送packet。下图是一个IP packet的header，你们可以在lab配套的代码中的net.h文件找到。\n如果IP packet是通过以太网传输，那么你可以看到，在一个以太网packet中，最开始是目的以太网地址，源以太网地址，以太网类型是0x0800，之后是IP header，最后是IP payload。\nPAYLOAD IP T=0x0800 S D 在一个packet发送到世界另一端的网络的过程中，IP header会被一直保留，而Ethernet header在离开本地的以太网之后会被剥离。\n或许packet在被路由的过程中，在每一跳（hop）会加上一个新的Ethernet header。但是IP header从源主机到目的主机的过程中会一直保留。\nIP header具有全局的意义，而Ethernet header只在单个局域网有意义。\n所以IP header必须包含足够的信息，这样才能将packet传输给互联网上遥远的另一端。对于我们来说，关键的信息是三个部分，目的IP地址（ip_dst），源IP地址（ip_src）和协议（ip_p）。\n目的IP地址是我们想要将packet送到的目的主机的IP地址。地址中的高bit位是网络号，它会帮助路由器完成路由。IP header中的协议字段会告诉目的主机如何处理IP payload。\n如果你们看到过MIT的IP地址，你们可以看到IP地址是18.x.x.x，虽然最近有些变化，但是在很长一段时间18是MIT的网络号。所以MIT的大部分主机的IP地址最高字节就是18。全世界的路由器在看到网络号18的时候，就知道应该将packet路由到离MIT更近的地方。\n接下来我们看一下包含了IP packet的tcpdump输出。\n因为这个IP packet是在以太网上传输，所以它包含了以太网header。呃……，实际上这个packet里面有点问题，我不太确定具体的原因是什么，但是Ethernet header中目的以太网地址不应该是全f，因为全f是广播地址，它会导致packet被发送到所有的主机上。\n一个真实网络中两个主机之间的packet，不可能出现这样的以太网地址。所以我提供的针对network lab的方案，在QEMU上运行有点问题。不管怎么样，我们可以看到以太网目的地址，以太网源地址，以及以太网类型0x0800。0x0800表明了Ethernet payload是一个IP packet。\nIP header的长度是20个字节，所以中括号内的是IP header，\n从后向前看：\n目的IP地址是0x0a000202，也就是10.0.2.2。 源IP地址是0x0a00020f，也就是10.0.2.15。 再向前有16bit的checksum，也就是0x3eae。 再向前一个字节是protocol，0x11对应的是10进制17，表明了下一层协议是UDP 其他的就是我们不太关心的一些字段了，例如packet的长度。 IP header中的protocol字段告诉了目的主机的网络协议栈，这个packet应该被UDP软件处理。\n四层网络 \u0026mdash; UDP IP header足够让一个packet传输到互联网上的任意一个主机，但是我们希望做的更好一些。每一个主机都运行了大量需要使用网络的应用程序，所以我们需要有一种方式能区分一个packet应该传递给目的主机的哪一个应用程序，而IP header明显不包含这种区分方式。\n有一些其他的协议完成了这里的区分工作，其中一个是TCP，它比较复杂，而另一个是UDP。TCP不仅帮助你将packet发送到了正确的应用程序，同时也包含了序列号等用来检测丢包并重传的功能，这样即使网络出现问题，数据也能完整有序的传输。\n相比之下，UDP就要简单的多，它以一种“尽力而为”的方式将packet发送到目的主机，除此之外不提供任何其他功能。\nUDP header中最关键的两个字段是sport源端口和dport目的端口。\n当你的应用程序需要发送或者接受packet，它会使用socket API，这包含了一系列的系统调用。一个进程可以使用socket API来表明应用程序对于特定目的端口的packet感兴趣。\n当应用程序调用这里的系统调用，操作系统会返回一个文件描述符。每当主机收到了一个目的端口匹配的packet，这个packet会出现在文件描述符中，之后应用程序就可以通过文件描述符读取packet。\n这里的端口分为两类，一类是常见的端口，例如53对应的是DNS服务的端口，如果你想向一个DNS server发请求，你可以发送一个UDP packet并且目的端口是53。\n除此之外，很多常见的服务都占用了特定的端口。\n除了常见端口，16bit数的剩下部分被用来作为匿名客户端的源端口。比如说，我想向一个DNS server的53端口发送一个packet，目的端口会是53，但是源端口会是一个本地随机选择的端口，这个随机端口会与本地的应用程序的socket关联。\n所以当DNS server向本地服务器发送一个回复packet，它会将请求中的源端口拷贝到回复packet的目的端口，再将回复packet发送回本地的服务器。本地服务器会使用这个端口来确定应该将packet发送给哪个应用程序。\n接下来我们看一下UDP packet的tcpdump输出。\n首先，我们同样会有一个以太网Header，以及20字节的IP header。IP header中的0x11表明这个packet的IP协议号是17，这样packet的接收主机就知道应该使用UDP软件来处理这个packet。\n接下来的8个字节是UDP header。\n这里的packet是由lab代码生成的packet，所以它并没有包含常见的端口，源端口是0x0700，目的端口是0x6403。\n第4-5个字节是长度，第6-7个字节是校验和。XV6的UDP软件并没有生成UDP的校验和。\nUDP header之后就是UDP的payload。在这个packet中，应用程序发送的是ASCII文本，所以我们可以从右边的ASCII码看到，内容是“a.message.from.xv6”。\n所以ASCII文本放在了一个UDP packet中，然后又放到了一个IP packet中，然后又放到了一个Ethernet packet中。最后发布到以太网上。\n学生提问：当你发送一个packet给一个主机，但是你又不知道它的以太网地址，这个packet是不是会被送到路由器，之后再由路由器来找到以太网地址？\nRobert教授：如果你发送packet到一个特定的IP地址，你的主机会先检查packet的目的IP地址来判断目的主机是否与你的主机在同一个局域网中。\n如果是的话，你的主机会直接使用ARP来将IP地址翻译成以太网地址，再将packet通过以太网送到目的主机。更多的场景是，我们将一个packet发送到互联网上某个主机。\n这时，你的主机会将packet发送到局域网上的路由器，路由器会检查packet的目的IP地址，并根据路由表选择下一个路由器，将packet转发给这个路由器。这样packet一跳一跳的在路由器之间转发，最终离目的主机越来越近。\n学生提问：对于packet的长度有限制吗？\nRobert教授：有的。这里有几个不同的限制，每一个底层的网络技术，例如以太网，都有能传输packet的上限。\n今天我们要讨论的论文基于以太网最大可传输的packet是1500字节。最新的以太网可以支持到9000或者10000字节的最大传输packet。\n为什么不支持传输无限长度的packet呢？这里有几个原因：\n发送无限长度的packet的时间可能要很长，期间线路上会有信号噪音和干扰，所以在发送packet的时候可能会收到损坏的bit位。\n基本上每一种网络技术都会在packet中带上某种校验和或者纠错码，但是校验和也好，纠错码也好，只能在一定长度的bit位内稳定的检测错误。\n如果packet长度增加，遗漏错误的可能性就越来越大。所以一个校验和的长度，例如16bit或者32bit，限制了传输packet的最大长度。\n另一个限制是，如果发送巨大的packet，传输路径上的路由器和主机需要准备大量的buffer来接收packet。\n这里的代价又比较高，因为较难管理一个可变长度的buffer，管理一个固定长度的buffer是最方便的。而固定长度的buffer要求packet的最大长度不会太大。\n所以，以太网有1500或者9000字节的最大packet限制。\n除此之外，所有的协议都有长度字段，例如UDP的长度字段是16bit。所以即使以太网支持传输更大的packet，协议本身对于数据长度也有限制。\n以上就是UDP的介绍。在lab的最后你们会通过实验提供的代码来向谷歌的DNS server发送一个查询，收到回复之后代码会打印输出。你们需要在设备驱动侧完成以太网数据的处理。\n网络协议栈（Network Stack） 与packet的协议和格式对应的是运行在主机上的网络协议栈。人们有各种各样的方式来组织网络软件，接下来我会介绍最典型的，并且至少我认为是最标准的组织方式。\n假设我们现在在运行Linux或者XV6，我们有一些应用程序比如浏览器，DNS服务器。这些应用程序使用socket API打开了socket layer的文件描述符。\nSocket layer是内核中的一层软件，它会维护一个表单来记录文件描述符和UDP/TCP端口号之间的关系。同时它也会为每个socket维护一个队列用来存储接收到的packet。\n我们在networking lab中提供的代码模板包含了一个非常原始的socket layer。\n在socket layer之下是UDP和TCP协议层。UDP软件几乎不做任何事情，它只是检查收到的packet，获取目的端口号，并将UDP payload传输给socket layer中对应的队列。\nTCP软件会复杂的多，它会维护每个TCP连接的状态，比如记录每个TCP连接的序列号，哪些packet没有被ACK，哪些packet需要重传。所以TCP的协议控制模块会记录大量的状态，但是UDP中不会记录任何状态。\nUDP和TCP通常被称为传输层。networking lab提供的代码中有一个简单的UDP层，但是没有TCP的代码。\n在TCP/UDP之下是IP层，IP层的软件通常很简单。虽然我不确定是在同一层还是下一层，与IP层在一起的还有ARP层。\n再往下的话，我们可以认为还会有一层以太网。但是通常并没有一个独立的以太网层。通常来说这个位置是一个或者多个网卡驱动，这些驱动与实际的网卡硬件交互。网卡硬件与局域网会有实际的连接。\n当一个packet从网络送达时，网卡会从网络中将packet接收住并传递给网卡驱动。网卡驱动会将packet向网络协议栈上层推送。\n在IP层，软件会检查并校验IP header，将其剥离，再把剩下的数据向上推送给UDP。UDP也会检查并校验UDP header，将其剥离，再把剩下的数据加入到socket layer中相应文件描述符对应的队列中。\n所以一个packet在被收到之后，会自底向上逐层解析并剥离header。当应用程序发送一个packet，会自顶向下逐层添加header，直到最底层packet再被传递给硬件网卡用来在网络中传输。所以内核中的网络软件通常都是被嵌套的协议所驱动。\n这里实际上我忘了一件重要的事情，在整个处理流程中都会有packet buffer。所以当收到了一个packet之后，它会被拷贝到一个packet buffer中，这个packet buffer会在网络协议栈中传递。\n通常在不同的协议层之间会有队列，比如在socket layer就有一个等待被应用程序处理的packet队列，这里的队列是一个linked-list。通常整个网络协议栈都会使用buffer分配器，buffer结构。\n在我们提供的networking lab代码中，buffer接口名叫MBUF。\n以上就是一个典型的网络协议栈的分层图。\nRing Buffer 对于今天的论文，了解packet的控制流程是如何工作的还是比较重要，这里的控制流程与前一节介绍的分层网络协议栈还不太一样。\n有关网络协议栈，通常会有多个独立的actor会处理packet，解析packet并生成输出。\n出于各种各样的原因，这些不同的actor之间是解耦的，这样它们可以并发的运行，并且连接不同的packet队列。这对于今天的论文来说，是非常重要的前提。\n现在我们有了一张网卡，有了一个系统内核。当网卡收到了一个packet，它会生成一个中断。系统内核中处理中断的程序会被触发，并从网卡中获取packet。\n因为我们不想现在就处理这个packet，中断处理程序通常会将packet挂在一个队列中并返回，packet稍后再由别的程序处理。所以中断处理程序这里只做了非常少的工作，也就是将packet从网卡中读出来，然后放置到队列中。\n在一个传统的网络协议栈中，我们之所以想要快速的将packet从网卡中读出并存放于软件队列中，是因为通常来说网卡中用来存储packet的内存都非常小，而在计算机的RAM中，会有GB级别的内存，所以计算机的内存要大得多。\n如果有大量的packet发送到网卡，网卡可能会没有足够的内存来存储packet，所以我们需要尽快将packet拷贝到计算机的内存中。\n之后，在一个独立的线程中，会有一个叫做IP processing thread的程序。它会读取内存中的packet队列，并决定如何处理每一个packet。其中一个可能是将packet向上传递给UDP，再向上传递给socket layer的某个队列中，最后等待某个应用程序来读取。\n通常来说，这里的向上传递实际上就是在同一个线程context下的函数调用。\n另一种可能就是，这个主机实际上是个路由器，packet从一个网卡进来，经过路由需要从另一个网卡出去。\n通过例如Linux操作系统构建路由器是非常常见的。如果你买一个wifi路由器，或者一个有线调制解调器，非常有可能里面运行的就是Linux系统，并且使用了Linux网络协议栈，因为Linux的协议栈实现了完整的路由协议。\n所以，如果IP process thread查看了packet的目的IP地址，并决定将packet从另一个网卡转发出去，它会将packet加入到针对发送网卡的发送队列中。\n通常来说网卡会有发送中断程序，当网卡发送了一个packet，并且准备好处理更多packet的时候，会触发一个中断。所以网卡的发送中断也很重要。\n在这个结构中，有一点非常重要，这里存在一些并发的组件，它们以不同的方式调度。中断处理程序由网卡的发送或者接受中断触发。\nIP processing thread就是一个内核线程。在一个处理器上，IP processing thread不能与中断处理程序同时运行，因为中断处理程序的优先级最高，不过在多核处理器上，并发度可能会更高。\n最后，应用程序要能够读取socket layer中的packet，应用程序又是另一个独立调度的组件。所有这些组件都会参与到CPU的调度中。\n缓存队列经常会被提到，在上图中，总共有3个队列。这里的队列的作用是，一个独立的组件会向队列中添加packet，其他的组件会从队列中读取packet。\n在网络系统中，这样的队列很常见，主要出于以下几个原因：\n其中一个原因是可以应对短暂的大流量。\n比如，IP processing thread只能以特定的速度处理packet，但是网卡可能会以快得多的速度处理packet。对于短暂的大流量，我们想要在某个位置存储这些packet，同时等待IP processing来处理它们，这是网卡的接收方向。\n在网卡的发送方向，我们可能需要在队列中存储大量的packet，这样网卡可以在空闲的时候一直发送packet。有的时候100%利用网卡的发送性能还是很重要的。\n第三个原因是，队列缓存可以帮助组件之间解耦。\n我们不会想要IP processing thread或者应用程序知道中断处理程序的具体实现。在一个传统的操作系统中，IP processing thread并不必须知道中断是什么时候发生，或者应用程序怎么运行的。\n学生提问：同一个网卡可以即是接收方又是发送方吗？\nRobert教授：可以的。比如说我的笔记本只有一个网卡连接到了wifi，packet会从一个网卡进入并发出。双网卡通常用在路由器中。\n比如说我家里的wifi路由器，它就有两张网卡，其中一个网卡连接到线缆并进一步连接到整个互联网，另一个网卡是wifi网卡。\n有很多服务器也有多个网卡，尤其是对于web服务器来说，会有一个网卡连接互联网，另一个网卡连接你的私有的敏感的数据库信息。两个网卡连接的是完全不同的网络。\n学生提问：所以多网卡的场景在于想要连接不同的网络？\nRobert教授：是的。如果你想要连接不同的网络，那么你需要有多块网卡。\n我想再讨论一下当packet送到网卡时，网卡会做什么操作？这与networking lab非常相关。\n对于一个网卡的结构，会有一根线缆连接到外面的世界。网卡会检查线缆上的电信号，并将电信号转换成packet。网卡会接入到一个主机上，主机会带有网卡的驱动软件。我们需要将网卡解码出来的packet传递给主机的内存，这样软件才能解析packet。\n网卡内有许多内置的内存，当packet到达时，网卡会将packet存在自己的缓存中，并向主机发送中断，所以网卡内部会有一个队列。而主机的驱动包含了一个循环，它会与网卡交互，并询问当前是否缓存了packet。\n如果是的话，主机的循环会逐字节的拷贝packet到主机的内存中，再将内存中的packet加到一个队列中。\n这是我们今天要看的论文中网卡的工作方式：网卡驱动会负责拷贝网卡内存中的数据到主机内存。这在30年前还是有意义的，但是今天通过驱动中的循环来从硬件拷贝数据是非常慢的行为。\n即使是在同一个计算机上，外设到CPU之间的距离也非常的长，所以它们之间的交互需要的时间比较长。所以人们现在不会这么设计高速接口了。\n接下来我将讨论一下E1000网卡的结构，这是你们在实验中要使用的网卡。\nE1000网卡会监听网线上的电信号，但是当收到packet的时候，网卡内部并没有太多的缓存，所以网卡会直接将packet拷贝到主机的内存中，而内存中的packet会等待驱动来读取自己。所以，网卡需要事先知道它应该将packet拷贝到主机内存中的哪个位置。\nE1000是这样工作的，主机上的软件会格式化好一个DMA ring，ring里面存储的是packet指针。所以，DMA ring就是一个数组，里面的每一个元素都是指向packet的指针。\n当位于主机的驱动初始化网卡的时候，它会分配一定数量，例如16个1500字节长度的packet buffer，然后再创建一个16个指针的数组。\n为什么叫ring呢？因为在这个数组中，如果用到了最后一个buffer，下一次又会使用第一个buffer。\n主机上的驱动软件会告诉网卡DMA ring在内存中的地址，这样网卡就可以将packet拷贝到内存中的对应位置。\n当网卡收到packet时，网卡还会记住当前应该在DMA ring的哪个位置并通过DMA将packet传输过去。\n传输完成之后，网卡会将内部的记录的指针指向DMA ring的下一个位置，这样就可以拷贝下一个packet。\n刚才说的都是接收packet，对应的是RX ring。类似的，驱动还会设置好发送buffer，也就是TX ring。驱动会将需要网卡传输的packet存储在 TX ring中，网卡也需要知道TX ring的地址。\n你们在networking lab中的主要工作就是写驱动来处理这些ring。\n你们在networking lab中的主要工作就是写驱动来处理这些ring。\n学生提问：E1000与生产环境的高性能场景使用的网卡有什么区别吗？\nRobert教授：E1000曾经是最优秀的网卡，没有之一，并且它也曾经使用在生产环境中，但这是很多年前的事了。现代的网卡更加的“智能”，但是我们这里介绍的DMA ring结构并没有太多的变化，现在你仍然可以发现网卡使用DMA来传输packet，内存中对应的位置是由ring buffer的位置决定。\n现代的网卡更加“智能”在以下几个方面：\nE1000只能与一个RX ring传输数据，而现代网卡可以与多个RX ring同时传输数据。\n比如说你可以告诉一张现代的网卡，将接受到的packet分别传输给21个RX ring，网卡会根据packet的内容，决定将packet送到哪个RX ring。\n人们在很多地方都使用了这个特性，比如说在主机上运行了多个虚拟机，你可以使用这个特性将虚拟机对应的packet送到虚拟机对应的RX ring中，这样虚拟机可以直接读取相应的RX ring。（注，也就是网卡多队列）\n现代网卡更加“智能”的体现是，它们会完成一些TCP的处理，最常见的就是校验和计算。（注，各种TCP offload）\n所以，现代的网卡有与E1000相似的地方，但是更加的“智能”。\n学生提问：在接下来的networking lab中，IP层和驱动之间没有队列，是吗？\nRobert教授：是的，lab中的网络栈已经被剥离到了最小，它比实际的网络协议栈简单的多\n学生提问：那这样的话，性能会不会很差？\nRobert教授：我不知道，我没有在实际环境中运行过这些代码。在写networking lab的代码时，我们没有关注过性能。大多数情况下，性能不是问题，lab中的代码可以完成一个网络协议栈95%的功能，例如处理多网卡，处理TCP。\n学生提问：为了让网卡能支持DMA，需要对硬件做一些修改吗？在E1000之前的网卡中，所有的数据传输都是通过CPU进行传输。\nRobert教授：我们在介绍E1000之前的网卡时，网卡并不能访问内存。我认为这里最重要的问题是，当网卡想要使用主机内存中的某个地址时，虚拟内存地址是如何翻译的。我不知道这里是如何工作的。\n网卡通过总线，并经过一些可编程芯片连接到了DRAM，我认为在现代的计算机中，你可以设置好地址翻译表，这样网卡可以使用虚拟内存地址，虚拟内存地址会由网卡和DRAM之间的硬件翻译，这对于一些场景还是很有价值的。\n另一方面，如果网卡需要读写一些内存地址，而内存数据现在正在CPU的cache中，那么意味着内存对应的最新数据位于CPU cache中，而不是在RAM。这种情况下，当网卡执行DMA时，我们希望网卡能读取CPU的cache而不是RAM。\n在Intel的机器上，有一些精心设计的机制可以确保当网卡需要从内存读取数据而最新的内存数据在CPU cache中时，CPU cache而不是RAM会返回数据。一些软件基于这种机制来获得高性能。对于写数据同样的也适用，网卡可以直接将数据写到CPU cache中，这样CPU可以非常快的读到数据。\n我们介绍的E1000的结构非常简单，但是实际中的网卡机制非常的复杂。\nReceive Livelock 接下来我们看一下今天的论文 。因为我们已经介绍了很多论文相关的背景知识，我们直接来看一下论文的图1。我们之后根据论文中的图来开展讨论。\n这张图是一个路由器的性能图。这是一个有两张网卡的路由器，它的工作是从一个网卡接收packet，再从另一个网卡送出 。\nX轴是接收速率，也就是接收端网卡的收到packet的速率。Y轴是发送速率，也就是观察到的发送端网卡发送packet的速率。\n我们关心的是实心圆对应的曲线，它先上升，再下降。所以即使还不知道任何上下文，看到这个图之后我们会问自己，为什么这个曲线先上升，再下降？曲线的转折点有什么特殊之处？是什么决定了曲线的上升斜率和下降斜率？即使不知道任何背景知识，我们还是可以问出这么多问题。\n首先，为什么这条曲线开始会上升？\n学生回答：在到达处理的瓶颈之前，路由器可以处理更多的接收方向的packet，也可以处理更多的发送发向的packet。\n完全正确，在出现错误之前，对于每个接收到的packet，路由器都可以转发出去。比如说当packet以2000pps的速度接收时，路由器直接将packet从输入网卡拷贝到输出网卡，所以路由器的发送速率与接收速率一样，都是2000pps，所以这里X轴与Y轴的值相等。这种状态一直保持，直到曲线到达转折点。\n那么为什么曲线不是一直上升的呢？\n学生回答：是不是因为中断不能被处理导致的？\nRobert教授：这个其实是为什么曲线会下降的原因。我这里的问题是为什么曲线在某个点之后就不再上升了。假设这里的设计是合理的，对于一个合理的系统，对应的曲线会一直上升吗？\n学生回答：我认为不会，就算系统能足够快的处理packet，对于足够多的packet，还是可能触发系统的瓶颈。\n是的，CPU的算力并不是无限的，CPU最多每秒执行一定数量的指令。\n对于每个packet，IP软件会查看packet的header，检查校验和，根据目的地址查找转发表等等，这个过程会消耗数百甚至数千条CPU指令时间来处理一个packet。所以，我们不能期望曲线能一直向上走，它必然会在某个位置停止向上。\n上面的图中，曲线在5000的位置就停止不再上升了，这告诉我们这台机器处理每个packet要消耗200微秒。所以，曲线的转折点隐含的包含了处理一个packet需要的时间信息。\n虽然这只是一个猜想，但是通常与真实的结果非常相近。或许我们可以修改软件使其更加的高效，我们可以优化到处理每个packet只需要150微秒，我们或许可以将曲线的转折点向上移一些，但是在到达了这台机器每秒能处理的packet数量的极限时，我们还是会到达曲线的转折点。\n除了CPU的性能，还有一些不是必然存在的瓶颈需要注意一下。最明显的一个就是网络的性能。如果你使用的网络只有10Mb/s，那么底层的网路硬件最多就能按照这个速率传输数据，这也有可能构成一个限制。\n所以也有可能是因为网络传输的速率决定了曲线的顶点是在5000pps这个位置。论文中并没有说明究竟是CPU还是网速是这里的限制因素，但是对于一个10Mb/s的网络，如果你传输小包的话，是可以达到10-15 Kpps，这实际上是网线的能达到的极限，而上图中转折点对应的5Kpps远小于10-15Kpps，所以几乎可以确定限制是来自CPU或者内存，而不是网络本身。\n在一个设计良好的路由器中，如果处理每个packet要200微秒，那么我们期望看到的是不论负载多高，路由器至少每秒能处理5000个packet。所以我们期望看到的曲线在5000pps之后是一条水平线，路由器每秒处理5000个packet，并丢弃掉其他的packet。\n但是我们实际拥有的曲线会更加的糟糕，当收到的packets超过5000pps时，成功转发的packets随着收到的packet的增多反而趋向于0。为什么曲线会下降呢？前面有同学已经提到了。\n论文作者给出的原因是，随着packet接收速率的增加，每个收到的packet都会生成一个中断，而这里的中断的代价非常高，因为中断涉及到CPU将一个packet从网卡拷贝到主机的内存中。如果我们知道packet将会以10K每秒的速率到达，并且我们知道我们不能处理这么多packet，那么我们可以期望的最好结果就是每秒转发5000个packet，并且丢弃5000个packet之外的其他packet。\n但是实际上，5000个packet之外的其他packet，每个都生成了一个昂贵的中断，收到的packet越多，生成的中断就越多。而中断有更高的优先级，所以每一个额外的packet都会消耗CPU时间，导致更少的CPU时间可以用来完成packet的转发。最后，100%的CPU时间都被消耗用来处理网卡的输入中断，CPU没有任何时间用来转发packet。\n这里曲线的下降被称为中断的Livelock，这是一个在很多系统中都会出现的现象。\n这里背后的原因是有两个独立的任务，比如这里的两个任务是输入中断和转发packet程序。由于调度的策略，输入中断的优先级更高，使得转发packet的任务可能分配不到任何CPU时间。\n几乎在任何需要处理输入的系统中，如果输入速率过高，都有可能出现Livelock。\nLivelock不仅会因为CPU耗尽而发生，也可能是其他原因，比如说网卡的DMA耗尽了RAM的处理时间，那么网卡占据了RAM导致CPU不能使用RAM。所以，即使你拥有大量的CPU空闲时间，还是有可能触发Livelock。不管怎样，这曲线的下降被称为Livelock。\n你或许会问，不能处理的packet最后怎么样了？我们回想一下网络协议软件的结构，网卡会通知网卡的接收中断，接收中断将packet拷贝到队列缓存中，之后会有一个线程处理队列缓存中的packet。\n所以packet会在队列缓存中丢失。队列缓存有一个最大的长度，至少RAM的大小是有限制大，但是队列缓存的大小会远小于RAM的大小。如果网卡的接收中断从网卡获得了一个packet，并且发现队列缓存的长度已经是最长了，接收中断程序会丢弃packet。\n如何解决Livelock 论文 作者对于Livelock提出了一种解决方法。这种解决方法的最直接结果就是，当packet的输入速率达到了5000pps，随着输入速率的增加，转发性能维持在5000pps。\n曲线后半部分的水平是一种完美的non-livelock性能曲线。之所以是水平的，是因为受CPU的限制，最多只能处理5000pps的转发。\n在这个解决方案中，还是存在处理packet的线程和中断处理程序。当网卡第一次触发中断时，会导致中断处理函数的运行。但是中断处理函数并不会从网卡拷贝packet，相应的，它会唤醒处理packet的线程，并且关闭网卡的中断，这样接下来就收不到任何中断了。\n处理packet的线程会有一个循环，在循环中它会检查并从网卡拉取几个packet，论文中我记得是最多拉取5个packet，之后再处理这些packet。\n所以现在处理packet的线程是从网卡读取packet，而不是从中断处理程序读取。如果网卡中没有等待处理的packet，那么处理线程会重新打开网卡中断，并进入sleep状态。\n因为最后打开了中断，当下一个packet到达时，中断处理程序会唤醒处理packet线程，线程会从sleep状态苏醒并继续处理packet。这就是论文介绍的解决Livelock的方法。\n这里的处理方式实际上是将中断模式（Interrupt Scheme）转变成了轮询模式（Polling Scheme）。\n在高负载的情况下，中断会被关闭，并且CPU会一直运行这里的循环中，不断读取packet并处理packet。因为中断被关闭了，CPU用来运行主线程的时间不会被中断占据。在低负载的情况下，中断会被打开，在收到packet之后，线程会被中断处理程序直接唤醒。\n学生提问：这里的循环会检查所有的设备吗？还是只会检查产生中断的设备？\nRobert教授：这是个好问题，如果存在多个网卡，我并不知道这里的循环会怎么工作。\n一个非常合理的设计是，packet处理线程需要记录每个网卡是在中断模式还是在轮询模式，然后只对轮询模式的网卡。。。等一下，因为中断处理程序现在不从网卡读取packet，所以线程中的循环可以直接检查所有网卡，如果网卡中有待处理的packet，就读取几个packet并处理。\n如果所有的网卡都没有待处理的packet，主循环会打开所有网卡的中断，并进入sleep状态。之后，任何一个网卡的中断都会唤醒packet处理线程。\n学生提问：当处理线程运行的时候，packet是如何进入到一个等待读取的队列中？我觉得网卡上只会有一个packet。\nRobert教授：最开始的时候，packet会在网卡自己的内存中按照队列形式缓存。而处理线程的主循环会询问每个网卡是否在自己的内存中有待处理的packet。如果有的话，主循环会在主机的RAM中申请缓存，再将packet数据从网卡中拷贝到RAM中的缓存，再处理packet。\n学生提问：所以一次可以拷贝多个packet？\nRobert教授：是的，我认为论文中说的是一次拷贝5个packet。即使有100packet在网卡中等待处理，一次也只会读取5个，这样可以避免阻塞输出。\n学生提问：但是这就要求提升网卡的内存容量了吧？\nRobert教授：Well，我不知道要多少内存容量。\n在Livelock曲线的转折点之前，都是靠中断来处理的。在转折点之前，如果网卡收到了一个packet，处理线程会立即被唤醒并读出packet。但是在转折点之后，处理线程就一直在轮询模式而不是中断模式。\n在转折点之后，肯定会有丢包，因为现在输入速率和输出速率之间是有差异的，而这个差异间的packet都被丢弃了。因为这些packet不论如何都会被丢弃，增加网卡的内存并不太能减少这里的丢包，所以不太确定网卡是否需要增加内存容量。\n在论文中，一次会读取最多5个packet，那么网卡必然需要存储5个packet的内存容量，但是更多的packet是否有好处就不太确定了。\n网卡上的buffer大小，对于短暂的高pps有帮助，这样可以保存好packet等处理线程来读取它们。但是我们这里并没有讨论短暂的overload，我们讨论的是持续的overload。所以增加网卡的buffer，并不是很有用。\n学生提问：当网卡中断被关闭了，网卡还能在自己的buffer上加入新的packet吗？\nRobert教授：可以的。网卡是自治的，不论中断是打开还是关闭，只要有一个packet到达了网卡，网卡都会将packet加入到自己的缓存队列中。\n当然不同的网卡设计可能非常不一样，但是在论文中网卡不会调用DMA，不会主动访问主机内存。如果网卡上内存都用光了，packet会被丢弃。\n所以，在这里的设计中，丢包发生在网卡内部。在一个overload的场景下，网卡中的队列总是满的，当再收到一个packet时，网卡会直接丢包，这样就不会浪费CPU时间。网卡可以在不消耗CPU时间的前提下直接丢包，是避免Livelock的直接原因。\n学生提问：有没有这种可能，CPU从网卡读取packet，但是处理线程内部的队列满了？\nRobert教授：当然。在其他地方肯定也有瓶颈，例如对于收到的packet，需要交给监听了socket的应用程序去处理，如果应用程序并没有以足够快的速度读取packet，相应的socket buffer会满，那么packet会在处理线程中丢包，而这也可能导致Livelock。\nLivelock发生的根本原因是我们浪费时间处理了一些最终会被丢弃的packet，这里的处理是徒劳。\n另一种发生Livelock的可能是，当负载增加时，我们可能会消耗100%的CPU时间在packet处理线程上，而留给应用程序的CPU时间为0，这时还是会发生Livelock。\n论文在第六节中有相应的介绍，如果一个packet将要被传输给本地的应用程序，网络线程会查看应用程序的socket buffer，如果socket buffer过满的话，网络线程会停止从网卡读取packet，直到socket buffer变小。\n这意味着网络线程会停止运行，并给应用程序机会运行并处理packet，所以如果你不够小心的话，你可能会在任何阶段都经历类似Livelock的问题。\n","date":"2024-10-06T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08121networking/bg_hu_b858e8595de3a184.jpg","permalink":"https://echudet.github.io/p/mit6s08121networking/","title":"MIT6S081：21Networking"},{"content":"Lecture 20 Kernels and HLL 课前预习 论文速览 论文链接 这篇论文的标题是《The Benefits and Costs of Writing a POSIX Kernel in a High-Level Language》，通过梳理论文的结构和内容，以下是根据各部分标题总结的主要内容：\n在引言 (Introduction)部分，作者探讨了在高性能操作系统内核中使用高级语言（如Go）的可行性，特别是与传统C语言的对比。他们构建了一个名为Biscuit的内核，该内核使用Go语言编写，支持POSIX系统调用子集，并通过实验评估其性能与安全性。主要目标是研究高级语言在编写内核时带来的性能开销、实现挑战、以及编程效率和安全性。\n在相关工作 (Related Work)部分，本节回顾了与高级语言编写操作系统内核相关的先前研究，包括早期基于高级语言的内核如Pilot和Lisp machine，以及现代的高级系统编程语言如Rust和Go。还探讨了内核中自动内存管理（垃圾回收）和其他内存分配方案的研究。\n在 动机 (Motivation)部分，这里讨论了选择C语言和高级语言的利弊。C语言在内核编写中被广泛使用，主要因为它的低级内存管理能力能够实现高性能。而高级语言（如Go）的优势则在于减少了编程复杂度和内存管理相关的错误，如缓冲区溢出和内存释放错误。\n在概述 (Overview)部分，这一部分详细介绍了Biscuit内核的结构和设计，包括Go运行时的修改、进程调度、虚拟内存、文件系统和网络栈的实现。Biscuit使用了Go语言的多核支持、并发机制（如Go的goroutines和channels）来处理系统调用和设备驱动。\n在垃圾回收 (Garbage Collection)部分，高级语言的垃圾回收机制是本研究的关键挑战之一。本节描述了Go语言的并行标记-清除垃圾回收器以及Biscuit如何配置垃圾回收来最小化对系统性能的影响。研究表明，在内核中使用垃圾回收虽然增加了一定的CPU开销，但通过合理的设计可以将其控制在可接受范围内。\n在避免堆耗尽 (Avoiding Heap Exhaustion)部分，Biscuit通过使用静态分析工具MAXLIVE来计算系统调用可能使用的最大内存量，以避免内核堆耗尽。每个系统调用在启动时会预留内存，如果没有足够的内存预留则会进入等待状态，直到释放出足够的内存。此机制使得Biscuit在处理内存分配时比传统的C语言内核更具鲁棒性。\n在 实现 (Implementation)部分，这里列举了Biscuit内核的具体实现细节，包括Go代码行数统计、系统调用支持情况（58个POSIX系统调用）、设备驱动的实现、内核启动过程以及内核的多核并发处理机制。还讨论了对Go运行时的修改，以及如何通过unsafe操作与硬件交互。\n在评估 (Evaluation)部分，本节通过实验数据展示了使用Go语言编写内核的性能代价（HLL tax）。评估了Biscuit在垃圾回收、堆内存大小、系统调用处理上的性能表现，并与Linux内核进行了直接比较。实验结果表明，虽然Biscuit在性能上不及Linux，但依然具有良好的性能表现，尤其是在多核并发处理上，Biscuit得益于Go语言的高效并发模型。\n在讨论与未来工作 (Discussion and Future Work)部分，作者对Biscuit和Go语言在内核开发中的优势和不足进行了总结。虽然Go语言降低了编程复杂度和内存管理的错误风险，但在性能上仍不如C语言。最后，作者指出，Go语言适用于那些强调安全性和快速原型开发的新内核项目，但对于现有的大型C语言内核，切换到Go的成本可能超过其收益。\n这篇论文通过Biscuit内核的案例，深入探讨了高级语言在操作系统内核开发中的可行性和局限性，并提供了定量的实验评估数据。\n处理器架构 x86-64 和 ARM 是两种主要的处理器架构（Instruction Set Architecture, ISA），它们决定了计算机硬件如何与软件进行交互。ISA 定义了处理器能执行的指令集，也就是说，软件和硬件如何沟通、数据如何被处理和存储。\n为了帮助你理解，我会给你列出几种常见的处理器架构，并简单解释它们的特点。这样你就可以看到 x86-64 和 ARM 是如何与其他架构相似或不同的。\nx86-64 (或 x64)\n主要特点：这是 x86 架构的 64 位扩展版本，由 Intel 和 AMD 开发。它广泛应用于桌面电脑、笔记本电脑和服务器。\n应用场景：Windows、Linux 和 macOS 系统广泛支持。大多数个人电脑和企业级服务器使用的就是 x86-64 处理器。\n厂商：Intel 和 AMD。\n优点：向后兼容 32 位 x86 程序，有强大的性能和广泛的软件兼容性。\n缺点：功耗较大，因此在移动设备中应用较少。\nARM (Advanced RISC Machines)\n主要特点：ARM 是一种基于 RISC（精简指令集计算，Reduced Instruction Set Computing）的架构，设计更加注重低功耗和高效能。ARM 处理器的指令集更小、更简单。\n应用场景：广泛应用于智能手机、平板电脑、嵌入式设备以及最近的 Apple M1/M2 处理器系列（用于 MacBook）。\n厂商：苹果（Apple）、高通（Qualcomm）、三星、NVIDIA 等使用 ARM 架构设计自己的处理器。\n优点：低功耗，非常适合电池驱动的设备，架构灵活，授权模式使得多家厂商可以开发自己的实现版本。\n缺点：历史上软件兼容性较弱，尤其是在桌面电脑上，但这一点正在逐渐改进。\nRISC-V\n主要特点：一个开源的 RISC 架构，它的设计简单且可扩展，任何公司或个人都可以免费使用并进行修改。\n应用场景：目前多用于学术界、研究项目、嵌入式系统以及一些创新应用中。\n厂商：SiFive、Microchip 等。\n优点：完全开放，便于创新和定制化，同时也更容易推动硬件开发的标准化。\n缺点：目前的软件生态系统不如 x86 和 ARM 成熟。\nPowerPC (Performance Optimization With Enhanced RISC – Performance Computing)\n主要特点：由 IBM、苹果和摩托罗拉联合开发，基于 RISC 架构。PowerPC 曾广泛应用于 Apple 早期的 Macintosh 电脑，后来逐渐淡出消费市场。\n应用场景：目前多应用于高性能计算和嵌入式设备，例如游戏机（如早期的 Xbox 和 PlayStation 3）、汽车电子系统等。\n厂商：IBM、Freescale 等。\n优点：适合高性能计算，具有较强的并行处理能力。\n缺点：已逐渐被 ARM 和 x86-64 超越，尤其是在消费市场中。\nMIPS (Microprocessor without Interlocked Pipeline Stages)\n主要特点：MIPS 是一种经典的 RISC 架构，历史悠久，简单易用，广泛应用于嵌入式系统中。\n应用场景：路由器、电视机顶盒、网络设备等。\n厂商：早期由 MIPS Technologies 开发，现在由 Wave Computing 维护。\n优点：设计简单，功耗低，非常适合嵌入式系统。\n缺点：在高性能市场中逐渐被其他架构取代。\nSPARC (Scalable Processor Architecture)\n主要特点：一种由 Sun Microsystems 开发的 RISC 架构，专为高性能服务器和工作站设计。 应用场景：多用于服务器和高性能计算环境。 厂商：最著名的厂商是 Oracle（曾经的 Sun Microsystems）。 优点：在其时代，适合并行处理和高负载环境。 缺点：随着 x86 和 ARM 的崛起，SPARC 逐渐被淘汰，市场份额越来越小。 Itanium (IA-64)\n主要特点：由 Intel 开发的 64 位架构，最初是为了取代 x86，但未能成功。\n应用场景：主要应用于高端服务器和一些专用的计算环境。\n厂商：Intel。\n优点：在特定高性能服务器应用中有一定优势。\n缺点：市场接受度较低，最终被 x86-64 和 ARM 架构压制。\n总的来说，这些架构可以分为两大类：\nCISC（复杂指令集计算）：如 x86-64，指令集复杂，但单指令功能强大。 RISC（精简指令集计算）：如 ARM、RISC-V、PowerPC，指令集较简单，但执行效率高。 x86-64 强调性能和向后兼容性，适合高性能设备；而 ARM 注重功耗和效率，成为移动设备和嵌入式设备的首选。每种架构都有其特定的应用场景，取决于性能需求、功耗要求和开发生态。\nGo 语言简介 Go 语言（通常称为 Golang）与 C 语言相比，具有一些显著的不同和优势。Go 是由 Google 开发的，设计目标是解决在现代多核计算和并发编程中的一些问题，并且它非常注重简洁、快速开发和高效执行。以下是对比 Go 和 C 语言的几个重要方面：\nC 语言 Go 语言 设计目标 C 诞生于 1970 年代，主要用于系统编程，特别是操作系统开发（如 UNIX 系统）。它是一种底层语言，直接操控硬件资源，具有很高的执行效率和灵活性。C 的设计强调简洁和可移植性，但编程时需要手动管理内存（如指针、malloc/free）。 Go 于 2009 年发布，设计目标是提高编程效率，特别是对于现代互联网时代的服务器和并发程序。Go 的核心设计理念是简化开发流程，提供更好的并发支持，并减少程序员因复杂性（如内存管理）带来的错误。 并发支持 虽然 C 能够通过多线程和库（如 pthreads）实现并发，但这些实现相对低级，需要程序员手动管理线程、锁和同步机制。这增加了编程复杂性和出错的可能性。 Go 具有内建的并发模型，使用所谓的 goroutine 和 channel 来实现高效的并发处理。Goroutine 是比线程更轻量的实体，Go 运行时负责调度和管理，开发者只需要专注于逻辑。使用 Go 编写并发程序比 C 要简单得多。 内存管理 C 要求程序员手动分配和释放内存（malloc 和 free），虽然这种直接控制提供了很高的灵活性和性能，但也容易引发错误，例如内存泄漏、空指针引用等问题。 Go 有自动的 垃圾回收（Garbage Collection） 机制，它会自动管理内存的分配和释放。这减少了开发者手动管理内存的负担，同时减少了内存相关的错误，虽然代价是稍微降低了一些性能。 编程复杂性 C 作为一种底层语言，接近于汇编语言，提供了极高的灵活性。你可以直接操作内存、硬件寄存器等，但这也带来了较高的复杂性。C 代码通常较为繁琐，调试和维护成本高。 Go 强调 简洁性和可读性，语言本身设计得非常简洁，提供了现代编程所需的内置工具，比如内置的并发支持、标准化的包管理和文档生成工具。与 C 相比，Go 代码往往更简洁，开发速度更快。 类型系统 C 是一种静态类型语言，但类型系统相对简单。C 支持结构体（structs）、枚举（enums）等，但没有内置的面向对象编程支持。 Go 也是静态类型语言，支持基本的数据类型、结构体、接口等。Go 并不直接支持面向对象编程，但通过结构体和接口，可以实现类似 OOP 的风格，避免了一些传统 OOP 语言（如 Java、C++）的复杂性。 生态系统和应用场景 C 是一种通用语言，几乎可以用于任何场景，特别是在系统编程、驱动程序、嵌入式系统和高性能计算等领域应用广泛。C 语言的生态系统非常成熟，有丰富的库和工具支持。 Go 的设计初衷是用于构建高并发、高性能的网络服务器和分布式系统。其生态系统非常适合现代 Web 开发、云计算和微服务架构。许多著名的项目和企业，如 Docker、Kubernetes、Uber、Dropbox 等，都是用 Go 开发的。 编译与性能 C 代码经过编译器优化后，可以生成非常高效的机器代码，因此 C 程序通常具有极高的性能。这使得 C 适用于性能要求极高的应用（如嵌入式系统、操作系统内核等）。 Go 也属于编译型语言，尽管在极端性能上不如 C，但在大多数应用场景中，其性能表现足够优越。Go 的编译器能够进行较多优化，编译速度也非常快。同时，Go 提供了跨平台编译工具，非常适合开发分布式系统和服务器。 错误处理 C 的错误处理依赖于返回值和全局变量（如 errno），这有时可能导致代码难以阅读和维护。 Go 的错误处理通过显式的错误返回值（error type）来实现，虽然这种方式有时显得繁琐，但它使得错误处理更加明确和可靠。Go 强调显式错误处理，这减少了程序中的潜在隐患。 总结\nC 语言：是一种底层、高性能、灵活的系统编程语言，非常适合需要对硬件进行精确控制的场景。 Go 语言：是一种面向现代网络开发、并发处理和分布式系统的高效语言。它简化了许多复杂的系统编程任务，同时保持了较高的执行性能。 如果正在开发需要直接控制硬件、实时性要求高的系统（如嵌入式开发、内核开发），C 是一个理想的选择。但如果在开发现代的 Web 服务、云计算平台或分布式系统，Go 会带来更高的开发效率和更轻松的并发处理。\n代码示例：使用 Go 创建一个简单的 Web 服务器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) // 定义一个结构体来表示 JSON 响应的格式 type Response struct { Message string `json:\u0026#34;message\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` } // 处理根路径请求的处理函数 func helloHandler(w http.ResponseWriter, r *http.Request) { response := Response{ Message: \u0026#34;Welcome to the Go Web Server!\u0026#34;, Timestamp: time.Now(), } // 设置响应的 Content-Type 为 application/json w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // 将结构体编码为 JSON 并返回给客户端 if err := json.NewEncoder(w).Encode(response); err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } } func main() { // 将 / 路由绑定到 helloHandler 函数 http.HandleFunc(\u0026#34;/\u0026#34;, helloHandler) // 启动 Web 服务器，监听 8080 端口 log.Println(\u0026#34;Starting server on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatalf(\u0026#34;Could not start server: %s\\n\u0026#34;, err.Error()) } } 代码说明：\nResponse 结构体：这是用于生成 JSON 响应的数据结构，包含一个 Message 字符串和 Timestamp 时间戳。 helloHandler 函数：这是处理根路径 / 的请求处理器。它创建一个响应结构体，并使用 Go 的 json.NewEncoder() 函数将其编码为 JSON 格式，然后发送给客户端。 http.HandleFunc(\u0026quot;/\u0026quot;, helloHandler)：将根路径 / 绑定到 helloHandler，当客户端访问根路径时，调用此函数。 http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil)：启动 Web 服务器，监听 8080 端口。 如何运行：\n将上述代码保存为一个 .go 文件，例如 main.go。\n在命令行中导航到文件目录，并运行以下命令来编译和运行 Go 服务器：\n1 go run main.go 打开浏览器，访问http://localhost:8080，你会看到如下 JSON 响应：\n1 2 3 4 { \u0026#34;message\u0026#34;: \u0026#34;Welcome to the Go Web Server!\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-10-06T12:34:56.789123456Z\u0026#34; } 扩展思路：\n可以进一步扩展这个服务器，添加更多的路由（如 /about, /users），并处理不同的 HTTP 方法（如 GET, POST）。 也可以引入 Go 的 web 框架如 Gin 或 Echo，以简化路由处理和中间件的使用。 Biscuit Heap Issue Biscuit 的方法，虽然看似没有引入全新的技术，但它的确提供了一种相对简单且有效的解决内核堆内存分配问题的方法。它并不是传统方法的革命性替代，但在特定上下文中有其优势，尤其是通过静态分析预估系统调用所需的内存并进行提前分配，从而避免了许多传统内存管理问题，比如复杂的分配失败恢复和内存死锁。以下是对 Biscuit 方法的更深入分析，并与传统方法对比：\n静态分析与提前预留内存，Biscuit 的创新点在于它使用静态分析来预测每个系统调用可能需要的内存，并在系统调用执行之前预留足够的内存。这样，当系统调用开始时，它可以确保所有需要的内存已经就绪，从而避免了在执行过程中动态分配失败的风险。传统方法的对比：\n动态分配：许多操作系统采用动态分配策略，在内存不够时，可能导致分配失败、等待或触发 OOM Killer 之类的机制。Biscuit 的提前预留内存策略通过分析避免了这些复杂的分配失败处理逻辑。\n内存池：预分配内存池虽然可以提高分配效率，但并不能保证每次分配的内存量足够满足所有调用需求，仍然可能出现内存不足的情况。\n避免分配失败和恢复机制，传统的内存管理系统往往需要应对内存分配失败，这可能涉及到复杂的错误恢复机制。例如，如果系统调用在执行过程中遇到内存分配失败，可能需要回滚操作或进行繁琐的资源释放。Biscuit 的设计则通过预先分配内存避免了这一问题，简化了错误处理。传统方法的对比：\n恢复和回滚：处理内存分配失败通常涉及复杂的恢复机制，特别是在并发环境下，可能会遇到死锁或竞态条件的问题。Biscuit 通过在执行之前保证分配，避免了这种复杂性。\n等待内存释放：传统系统中，当内存不足时，进程可能会阻塞并等待内存释放，这可能会导致性能瓶颈或死锁风险。Biscuit 通过预先确定内存需求，减少了这些问题的发生。\n实现的简洁性，Biscuit 的方法虽然不复杂，但它依赖于一个关键前提：内核中每个系统调用的内存需求可以通过静态分析确定。这种设计思路虽然并不算高明或者复杂，但它的确很实用，尤其是在现代内核中，减少了内存分配失败的风险，并简化了内存管理代码。传统方法的对比：\n传统方法可能会依赖复杂的内存分配器和恢复机制，开发和维护成本较高。Biscuit 的设计则更加直接和简洁，减少了处理分配失败和恢复的需要。 局限性，尽管 Biscuit 通过静态分析来预先分配内存，这种方法并非适用于所有场景。某些复杂的系统调用可能无法准确预测内存需求，特别是在涉及到大量动态数据的情况下。此外，Biscuit 的设计需要在内存紧张的情况下小心处理，否则预分配的策略可能导致系统调用等待较长时间，影响性能。传统方法的优势：\n灵活性：传统的动态分配方法更灵活，能够适应不确定的内存需求。这对于复杂的系统调用或者高度动态的应用环境尤为重要。\n性能：Biscuit 的方法依赖于静态分析和预分配，这在某些情况下可能导致系统调用延迟，特别是当多个系统调用竞争有限的内存资源时。\n总的来说，从技术角度看，Biscuit 的方法并不是高度复杂或特别创新的，但它确实在内核设计中引入了一种相对简单、直接的方式来解决内存分配失败问题。它的优势在于简化了内存管理，避免了复杂的恢复逻辑和潜在的死锁问题。通过提前分析内存需求并进行预分配，它可以有效避免动态分配中常见的瓶颈和风险。\n尽管如此，它的局限性也很明显，尤其是在动态性较高或内存需求不可预测的场景中，这种方法可能会显得不够灵活。因此，Biscuit 的方法是一种针对特定问题的实用而非高明的解决方案。\n静态分析 “静态分析”是一个专业术语，广泛用于软件工程和编译器设计中。它指的是在不运行程序的情况下，通过分析源代码来检查程序的正确性、性能、潜在错误以及代码结构等。静态分析与动态分析（在程序运行时进行的分析）相对。静态分析可以应用在多个层面，以下是它通常会分析的几个方面：\n语法检查（Syntax Checking）\n分析内容：检查源代码是否符合编程语言的语法规则。例如，确保没有语法错误、未闭合的括号或语句等。\n工具：编译器的前端部分通常负责语法检查，如 gcc 在编译 C 代码时会做基础的语法检查。\n类型检查（Type Checking）\n分析内容：确保程序中的类型匹配正确，变量和函数的使用符合其声明的类型。例如，在静态类型语言（如 C、Go、Java）中，确保函数接收和返回的参数类型正确，避免类型转换错误。\n工具：大部分编译器都进行类型检查，像 Go 和 Rust 等现代编程语言有非常严格的类型检查机制。\n数据流分析（Data Flow Analysis）\n分析内容：追踪程序中的变量从定义到使用的路径，分析数据如何在程序中流动。这有助于发现未初始化变量、死代码、常量传播（优化机会）等问题。\n工具：一些优化编译器或代码静态分析工具如 Clang Static Analyzer、SonarQube 等可以执行这种分析。\n控制流分析（Control Flow Analysis）\n分析内容：检查程序中的控制结构（如条件分支、循环、函数调用）的执行路径，确保每个可能的路径都是有效的。它有助于发现潜在的无限循环、死锁或不安全的代码路径。\n工具：编译器和静态分析工具都可以执行控制流分析，用于优化和安全分析。\n内存分析（Memory Analysis）\n分析内容：分析代码中的内存分配、使用和释放，帮助检测内存泄漏、指针错误、越界访问等问题。内存分析也包括堆栈分配的正确性，检测未释放内存或者无效的内存访问。\n工具：Coverity、Cppcheck 等工具可以进行静态内存分析。\n程序依赖分析（Dependency Analysis）\n分析内容：检查函数、变量和模块之间的依赖关系，分析程序模块的耦合程度。这有助于识别出耦合过高的模块，优化代码的可维护性和解耦程度。\n工具：软件开发工具如 SonarQube 和 Understand 可以帮助进行程序依赖的分析。\n并发分析（Concurrency Analysis）\n分析内容：在有并发的程序中，静态分析可以帮助检测潜在的竞态条件、死锁和线程安全问题。通过分析不同线程对共享资源的访问方式，可以发现同步问题。\n工具：像 ThreadSanitizer 等静态分析工具专门用于检测并发相关问题。\n安全漏洞分析（Security Vulnerability Analysis）\n分析内容：静态分析可以用于发现安全漏洞，例如 SQL 注入、缓冲区溢出、未验证的输入、权限提升等。它通过扫描代码，寻找已知的安全模式或问题。\n工具：Fortify、Veracode、OWASP ZAP 等工具用于静态代码安全分析。\n资源使用分析（Resource Usage Analysis）\n分析内容：静态分析可以检测程序在运行过程中可能使用的资源，例如内存、CPU 时间、文件句柄等。在实时系统或嵌入式系统中，这类分析有助于确保程序不会耗尽资源。\n工具：实时系统开发中的专用静态分析工具如 Polyspace 可以用于这类分析。\n循环不变性与优化（Loop Invariants and Optimization）\n分析内容：在代码优化中，静态分析可以帮助发现循环中的不变条件，优化代码性能。例如，通过分析循环不变表达式，将其移出循环，以减少不必要的计算。\n工具：优化编译器（如 LLVM）会执行这种分析。\n论C 语言与 Go 在静态分析中的区别，虽然静态分析在多种编程语言中都可以执行，但 C 语言和 Go 语言由于各自的设计特点，分析的难度和效果有很大的差别：\nC 语言\n指针操作和内存管理：C 语言允许直接的指针操作和手动内存管理，导致内存分析的难度增加。分析工具很难准确追踪指针的生命周期或内存的动态分配。 灵活性：C 语言的灵活性让静态分析更难，因为它允许低级别的操作（如汇编嵌入、直接操作内存、动态类型转换等），这些特性让编译器和分析工具很难精确理解代码的意图。 较少的类型信息：C 语言的类型系统相对简单且宽松，类型检查也不如 Go 严格，增加了分析的复杂性。 Go 语言\n垃圾回收：Go 的自动垃圾回收系统减少了手动内存管理的复杂性，简化了静态内存分析。程序员不需要显式管理内存，分析工具可以更容易跟踪对象的分配和释放。 更强的类型系统：Go 语言的类型系统比 C 更严格，变量的类型和分配更容易被分析工具理解，从而提供更准确的静态分析。 并发模型：Go 的 goroutine 和 channel 使得并发的分析更加简单和明确，分析工具能够更好地理解并发程序的行为。 静态分析是一个强大的工具，广泛应用于软件开发的各个阶段。它可以帮助程序员发现潜在问题，优化代码，提升安全性。在 C 语言中，静态分析受到语言复杂性的限制，但在 Go 等现代语言中，由于其内存管理的简化和更严格的类型系统，静态分析变得更可行和有效。\nBiscuit 的静态分析 Biscuit 能够使用静态分析来预测每个系统调用所需的堆内存，这是因为它采用了Go 语言，而 Go 语言的一些特性使得这种静态分析更为可行和有效。相比之下，C 语言由于语言本身的特性和复杂性，静态分析的难度要大得多。\n静态分析是指在编译时对程序代码进行分析，而不是在运行时进行。它可以通过检查源代码中的函数、变量、内存分配等，来推断程序的行为、性能或者资源需求。对于 Biscuit 来说，静态分析的目标是推断出每个系统调用的最大堆内存使用量，从而在系统调用开始之前预留足够的内存，确保不会在运行时遇到分配失败的问题。\nGo 语言在设计上有一些特性使得静态分析比 C 语言更容易实现：\nGo 语言有内置的垃圾回收机制，开发者不需要手动管理内存分配和释放。这种自动内存管理简化了内存分配的逻辑，减少了复杂的内存管理模式（如手动释放、双重指针、内存池等），使得静态分析工具可以更容易地跟踪和预测内存的使用情况。\n在 C 语言中，手动管理内存意味着程序员可以随时随地调用 malloc() 或 free()，而且可能通过复杂的方式（如指针运算）操作内存。这种灵活性大大增加了静态分析的难度，因为分析工具很难准确判断内存的分配和释放行为。\nGo 的内存模型相对简单。它的堆内存分配通常是通过标准的内存分配器进行，且由于 Go 的类型系统是静态类型的，内存分配的大小在大多数情况下是可以确定的。这使得静态分析工具可以更容易地推断出每个函数的内存需求。\n相比之下，C 语言允许更复杂的内存操作，比如通过指针动态分配、操作和释放内存。这些操作的灵活性和复杂性，使得 C 程序的内存模型变得不可预测，尤其是在使用 malloc() 或 realloc() 等函数时，很难通过静态分析准确预估内存需求。\nGo 语言没有 C 语言那样广泛的指针操作。虽然 Go 也支持指针，但它在设计上限制了对裸指针的直接操作。指针的滥用是 C 语言中一个常见的静态分析障碍，因为指针可以指向任意内存位置，并且程序员可以随时通过指针修改内存。这使得 C 语言的静态分析工具很难跟踪内存的生命周期和使用情况。而 Go 的内存引用机制更加受控，这有助于静态分析工具做出更准确的推断。\nGo 有自己的轻量级并发模型（goroutine 和 channel），它相较于 C 语言中的多线程模型更为简洁和可控。虽然 C 可以通过 pthread 或其他库来实现多线程并发，但这些操作涉及锁、信号量等复杂的同步机制，这会进一步增加静态分析的复杂性。而 Go 语言通过自己的并发原语大大简化了并发操作，使得静态分析工具可以更容易推断 goroutine 的行为和内存使用情况。\nC 语言为什么难以实现静态分析\nC 语言中的手动内存管理（malloc() 和 free()）大大增加了静态分析的复杂性。C 程序员可以在任意位置、任意时间进行动态内存分配，并且释放的时间和方式也完全由程序员控制。这种灵活性使得分析工具很难判断每个系统调用或函数可能会分配多少内存，尤其是在多个模块或库函数之间复杂交互的情况下。\n指针是 C 语言的一个核心特性，允许开发者直接操作内存地址。尽管这种能力非常强大，但它给静态分析带来了极大的困难。通过指针操作，程序可以将内存分配、释放、传递给不同的函数，甚至可以修改完全不可预测的内存区域。因此，C 程序中的指针使得静态分析无法精确追踪内存的使用情况。\nC 的类型系统相对宽松，允许许多类型转换和操作。尤其是指针类型可以自由地转换为不同类型，进一步增加了内存分析的难度。分析工具很难判断内存对象的确切大小和类型，因此难以精确预测内存需求。\nC 语言中广泛使用的宏（#define）和内联函数也给静态分析增加了难度。宏可以改变代码的结构，并且由于它们在预处理阶段展开，静态分析工具在某些情况下难以处理宏的行为。这使得分析工具难以准确理解代码的实际执行路径和行为。\nC 程序员不仅可以使用 malloc() 进行堆分配，还可以使用 alloca() 在栈上动态分配内存，或通过静态分配全局变量。这些不同的分配方式大大增加了内存管理的复杂性，使得静态分析难以统一处理所有的内存分配场景。\n总的来说，Biscuit 能够使用静态分析来解决内核堆内存分配问题，主要得益于 Go 语言的简洁内存模型、自动垃圾回收、类型安全性以及受控的并发模型。这些特性使得 Go 的程序行为在编译时更加可预测，静态分析工具可以较容易地推断出每个系统调用的内存需求。\n相比之下，C 语言的手动内存管理、复杂指针操作和宽松的类型系统，使得静态分析要困难得多。C 语言的灵活性和低级控制带来了更多的自由度，但也增加了代码行为的不确定性，特别是在处理内存分配和管理方面，因此很难实现类似于 Biscuit 的静态分析内存分配策略。\nType-safe语言 Type-safe（类型安全）是一种编程语言特性，它确保数据类型的使用在程序中始终保持一致和正确。换句话说，类型安全的语言能够防止开发者在代码中进行不合法的类型操作，例如将字符串误用为整数、将浮点数错误地当作布尔值等。Go 语言被称为 Type-safe 语言，因为它在编译时严格检查类型，并且不允许不安全的类型转换。\nType-safe 简单来说，就是在编写程序时，不允许混淆不同数据类型的操作，确保代码中每一个变量的类型都在其生命周期内保持一致。比如：\n在 Type-safe 语言中，你不能将整数赋值给一个预期存储字符串的变量。 不能进行不安全的类型转换，比如将指针类型强制转换为整数类型（除非有显式的、明确的机制）。 Go 语言的类型安全性意味着：\n静态类型检查：在编译阶段，Go 会检查每个变量的类型是否正确使用。如果类型不匹配，程序将无法通过编译。例如，不能将 string 类型的值赋给 int 类型的变量。 强制类型转换：如果确实需要进行不同类型之间的转换，必须通过显式的类型转换。例如，int 和 float64 不能直接相互赋值，必须使用类型转换操作。 1 2 3 4 5 6 7 8 var i int = 10 var s string = \u0026#34;hello\u0026#34; // 编译时会报错，因为 i 是整数，而 s 是字符串 s = i // 错误：cannot use i (type int) as type string in assignment // 必须使用显式的转换 s = strconv.Itoa(i) // 正确：将 int 转为 string Type-safe 的作用\n减少错误： 类型安全机制能有效避免许多运行时错误。例如，在动态类型语言（如 Python）中，变量的类型检查发生在运行时，可能会因为类型不匹配导致程序崩溃。而在静态类型安全语言（如 Go、Rust、Java）中，类型错误会在编译时被发现，程序在运行之前就能被修正。 增强代码可读性与可维护性： 类型安全的代码有助于让开发者更明确地理解每个变量的意图和用法。类型明确定义，使得代码在维护和重构时减少了因类型不匹配而引发的潜在问题。 提高编译器优化的可能性： 类型安全的代码可以让编译器更容易优化，因为编译器能够提前了解每个变量的确切类型，从而进行更深入的性能优化。例如，编译器可以针对特定类型生成更高效的机器代码。 安全性： 类型安全性不仅防止程序崩溃，还能防止某些安全漏洞。例如，类型不安全的代码可能会引发缓冲区溢出、内存泄漏等问题，这些问题常常成为系统安全漏洞的来源。 随着软件系统的规模和复杂性不断增长，类型安全在现代软件开发中变得越来越重要，原因如下：\n复杂性增加： 现代软件系统往往非常复杂，涉及大量的模块、库、服务和并发操作。类型安全的语言可以帮助开发者管理复杂的代码结构，减少因数据类型错误带来的潜在错误。 并发和分布式系统： 在现代应用中，并发编程和分布式系统已成为常态。类型安全在这些场景下尤为重要，因为并发操作下的类型错误更难调试和排查。Go 等语言的类型安全机制使得并发编程更为安全和可靠。 大型团队协作： 大型团队中开发人员之间经常共享代码库。类型安全语言可以确保每个开发者按照一致的规则编写代码，减少因为类型误用导致的跨模块错误。这对于持续集成和快速迭代的开发模式来说至关重要。 防止运行时错误： 动态类型语言的一个缺点是，许多错误在运行时才会暴露出来，而这些错误可能导致系统崩溃、数据损坏或者严重的安全漏洞。类型安全语言通过在编译时检查这些错误，减少了系统在生产环境中出现问题的几率。 内存安全与安全漏洞防范： 类型安全与内存安全密切相关。在类型不安全的语言中，错误的类型转换可能导致访问非法内存地址，产生严重的安全漏洞（如缓冲区溢出）。现代软件开发对安全性要求极高，因此类型安全的语言更适合用于构建安全的系统。 许多现代编程语言都非常注重类型安全，例如：\nGo：通过静态类型和强类型系统，Go 确保类型错误在编译时被发现，并提供了简单但强大的类型转换机制。 Rust：Rust 不仅类型安全，还通过借用检查器（borrow checker）确保内存安全，防止常见的指针错误和数据竞争。 TypeScript：作为 JavaScript 的超集，TypeScript 引入了静态类型检查，弥补了 JavaScript 类型安全不足的问题。 Java：长期以来，Java 一直强调静态类型和类型安全，这使得 Java 在大型企业级应用中被广泛使用。 Type-safe 是一种编程语言的关键特性，确保了类型使用的一致性与安全性，减少了潜在错误。它的重要性在现代软件开发中日益提升，尤其是在处理复杂系统、并发操作和大规模协作时。Go 等现代编程语言通过类型安全机制提供了更高的可靠性和可维护性，帮助开发者在代码开发过程中减少错误，提升生产力和系统稳定性。\n用户空间的内核 \u0026ldquo;用户空间的内核\u0026rdquo; 和 \u0026ldquo;虚拟机上的内核\u0026rdquo; 虽然有一些相似的功能，但它们本质上是不同的概念。\n用户空间的内核（User-space Kernel） 在传统的操作系统架构中，内核（Kernel）运行在内核空间，它有访问系统硬件的最高权限，并管理进程调度、内存管理、设备驱动等功能。与此相对，用户应用程序则运行在用户空间，没有直接操作硬件的权限，必须通过系统调用与内核通信。\n用户空间的内核指的是一种特殊的设计，在这种设计中，内核的一部分或者类似于内核的功能在用户空间中运行，而不是在内核空间。这种用户空间的内核主要目的是隔离和安全，减少对主机操作系统内核的直接依赖，从而提高系统的安全性。例如：\ngVisor：gVisor 是一个用户空间内核，它模拟了大量的 Linux 系统 API，让容器可以像在标准 Linux 内核上运行一样运作，但它实际上并没有直接使用主机的 Linux 内核来运行这些容器。这种设计让容器运行时更加安全，因为它限制了容器对主机系统的访问。 关键区别：用户空间的内核运行在宿主操作系统的用户空间，它与宿主系统的内核隔离，通常使用系统调用与宿主内核通信。它的主要功能是提供一种额外的安全层，以限制和管理用户空间进程对系统资源的访问。\n虚拟机上的内核 虚拟机的内核则是另一个概念。在虚拟化环境中，虚拟机模拟了一个完整的物理计算机，这意味着虚拟机有自己的操作系统，这个操作系统有一个标准的内核，通常是 Linux 或 Windows 内核。\n在虚拟机上，虚拟机内核是运行在虚拟机操作系统中的内核。这个内核是完整的，管理虚拟机内部的所有系统资源和进程，就像它在真实硬件上运行一样。 虚拟化层（如 Hypervisor 或 KVM）在底层提供资源的抽象，它将物理硬件的资源分配给多个虚拟机。因此，虚拟机上的内核运行在虚拟化层上，而不是直接运行在真实的硬件上。 关键区别：虚拟机内核是模拟了一个完整的硬件环境，它运行在虚拟机的操作系统中，就像在实际硬件上运行一样；而用户空间的内核（如 gVisor）则是在宿主操作系统的用户空间中运行，它不是独立的操作系统内核，而是通过模拟部分系统功能为应用提供隔离和安全环境。\n相似性与差异\n相似性\n两者都提供了一种隔离机制，目的是增强安全性。 两者都与宿主操作系统隔离，不直接接触宿主内核。 差异\n用户空间内核是运行在用户空间的轻量级内核，它通常不是一个完整的操作系统，而是部分内核功能的实现，用于提供安全隔离（如 gVisor）。 虚拟机内核是运行在虚拟化环境中的完整操作系统内核，它模拟了整个硬件系统，因此是一个完整的操作系统。 虽然用户空间内核和虚拟机内核都有相似的隔离和安全目标，但它们的实现方式不同。用户空间内核（如 gVisor）通过在宿主操作系统上提供内核功能的模拟来隔离应用，而虚拟机内核则是虚拟化技术的一部分，它运行在模拟的硬件环境中，并管理整个虚拟机内部的操作系统和资源。\n死锁风险 在 Linux 内核中，系统调用（system calls）会分配内存给内核来处理各种任务（如创建进程、管理文件系统等）。当内核堆内存耗尽时，系统调用可能无法完成分配所需的内存。Linux 的策略是乐观分配，让系统调用继续进行，直到实际的内存分配失败为止。\n如果分配失败，系统可能会进入等待状态，给 OOM Killer（Out-Of-Memory Killer） 线程一个机会来寻找并终止某个消耗大量内存的进程，从而释放出一些内存供其他进程使用。\n问题在于，当一个线程正在分配内存时，它可能持有某些资源的锁（例如文件系统的锁、进程表的锁等）。如果该线程因内存分配失败而进入等待状态，而此时 OOM Killer 线程杀死了另一个持有锁的进程，可能会导致死锁情况的出现。以下是几个场景：\n情况 1：分配线程持有锁，等待内存\n线程 A 需要分配内存来完成某个系统调用，它已经持有某个锁（例如一个文件系统锁），但是由于内存不足，它进入等待状态。 OOM Killer 开始寻找消耗大量内存的进程，准备杀掉其中一个进程以释放内存。 线程 B 是 OOM Killer 选择的目标，但线程 B 也持有了另一个重要的内核锁（例如进程表锁），并且它正等待线程 A 释放锁。 此时就出现了经典的死锁：线程 A 等待内存释放，而线程 B 等待线程 A 释放锁，而线程 B 可能正被 OOM Killer 杀死。在这种情况下，由于线程 B 持有的锁没有释放，整个系统可能陷入死锁状态。 情况 2：OOM Killer 杀死持有锁的进程\n线程 C 正在执行某个操作，持有某个内核锁，并尝试分配内存。 内存不足，OOM Killer 选择线程 C 作为目标，并强制终止线程 C。 如果线程 C 在被杀死时没有机会释放它持有的锁，那么其他依赖于该锁的线程将永远无法获得该锁，可能导致整个系统的一部分冻结或陷入死锁。 Linux 内核采取了一些措施来减少死锁的风险，但并不能完全消除它。以下是几种常见的策略：\nOOM Killer 的谨慎选择 OOM Killer 通常会避免杀死关键系统进程或持有重要锁的进程。它通过一些启发式规则来选择那些可以安全终止的进程。例如，OOM Killer 会优先选择消耗大量内存但相对不重要的用户进程，而不是那些核心系统进程。\n无等待内存分配策略 在某些情况下，内核代码会被设计成不等待内存分配。如果分配失败，系统调用会立即返回错误，而不是进入无限制的等待状态。这减少了进程在持有锁时因内存不足而进入等待状态的机会，从而减少死锁风险。\n锁的分层与优先级顺序 为了避免锁的相互依赖导致的死锁，Linux 内核通常会设计锁的层级顺序。通过确保不同的锁以一致的顺序被获取，可以避免交叉锁定（即两个线程分别持有彼此需要的锁）的情况。\n分配超时和中断机制 在某些情况下，内核可能会设置一个超时，让内存分配的等待时间有限。如果超过这个时间限制，线程会被强制中断或返回错误，而不是无限期等待内存的释放。这也可以降低持锁线程进入死锁的可能性。\n示例 1：文件系统死锁。假设一个进程（线程 A）正在执行一个文件写入操作，它持有了文件系统的锁（fs_lock）。在这个操作过程中，它需要为某个数据结构分配内存，但此时系统内存不足，于是线程 A 进入等待状态。与此同时，OOM Killer 开始尝试释放内存，并选择了另一个进程（线程 B）作为目标进行杀死。但是线程 B 此时持有了进程表的锁（proc_lock），并且正等待 fs_lock 被释放。此时，fs_lock 被线程 A 持有，线程 A 又在等待内存释放，形成了死锁。\n示例 2：内存分配导致的死锁。假设有一个多线程应用程序，其中一个线程正在尝试分配内存，并持有内存管理相关的锁（如 memory_lock）。当它无法分配内存并进入等待状态时，OOM Killer 选择杀死另一个持有 memory_lock 的线程（因为这个线程消耗了大量的内存）。然而，当该线程被杀死后，由于它在被杀死时持有的 memory_lock 没有释放，导致其他线程无法再获得该锁，系统可能陷入死锁。\n总而言之，死锁的风险主要来自于进程在内存分配失败时持有锁进入等待状态，以及 OOM Killer 杀死持锁进程时导致的资源未释放问题。Linux 内核通过启发式选择进程、锁定顺序、无等待分配等机制来减少死锁发生的概率，但在极端情况下，死锁仍可能发生。\n这些问题表明，在内存管理和多线程编程中，并发、锁管理、内存分配相互交织，导致了一些复杂的挑战。\nruntime \u0026ldquo;runtime\u0026rdquo; 指的是一个程序执行时所依赖的运行时环境，它为程序提供了执行过程中所需的功能和支持。C 语言通常被认为没有复杂的运行时环境，因为它的设计非常接近硬件，程序运行时不需要依赖复杂的框架或机制来管理内存、线程、异常等。\n在编程语言中，运行时环境（runtime environment） 是指在程序执行过程中提供支持的一组库、工具和服务。这些服务可以包括：\n内存管理：如垃圾回收（GC）。 线程管理和调度：处理多线程并发的机制。 异常处理：如处理错误和异常的机制。 类型检查：某些语言可能在运行时进行类型检查或转换。 系统调用接口：为程序提供与操作系统交互的接口。 在一些编程语言中，运行时环境是相对复杂的，因为它不仅要管理内存，还要处理并发、线程同步、异常处理等问题。运行时环境通常伴随着一个虚拟机（如 Java 的 JVM）或某些底层库（如 Python 的解释器）来确保程序在不同环境下都能一致运行。\nC 语言的设计非常简洁，接近硬件层，它的程序不依赖复杂的运行时环境。C 语言提供了直接的内存管理（通过 malloc() 和 free()），而且程序员需要手动管理堆栈、内存分配和释放。它不像其他语言那样依赖自动垃圾回收器或虚拟机。C 语言的运行时环境非常轻量化，主要包括一些标准库函数的支持，但它不提供自动内存管理或复杂的系统抽象。\n相比之下，其他一些现代编程语言通常带有复杂的运行时。例如：\nJava：Java 依赖 Java 虚拟机（JVM） 作为其运行时环境。JVM 提供了垃圾回收、自动内存管理、线程调度、异常处理等功能。因此，Java 程序运行时依赖一个复杂的运行时环境来执行，这也是为什么它具有较好的跨平台能力。 Go：Go 语言虽然编译成机器代码，但它有一个复杂的运行时系统，主要用于管理**并发（goroutine）*和*垃圾回收。Go 的垃圾回收器负责自动管理内存的分配和回收，这使得 Go 程序员不需要手动释放内存，但也带来了一定的性能开销。 Python：Python 是解释型语言，依赖于 Python 解释器 来执行代码。解释器不仅处理代码的解释执行，还管理内存（如垃圾回收）、处理异常，并提供动态类型系统的支持。Python 的运行时环境非常复杂，包含了大量的高层机制。 某些现代编程语言通过复杂的运行时提供了很多高级特性，如：\n自动内存管理：例如 Java、Go、Python 等语言都有垃圾回收器（GC），自动回收不再使用的对象。这种功能可以让开发者不用担心内存泄漏问题，但同时它也增加了运行时的复杂性。 类型安全：一些语言（如 Java 和 Go）有严格的类型检查机制，运行时可能会进行类型检查，确保类型安全，这也需要额外的运行时支持。 并发管理：像 Go 和 Java 有内置的并发机制，Go 的 goroutine 和 Java 的线程管理都依赖于复杂的运行时来调度和管理并发任务。 异常处理：C 语言没有内建的异常处理机制，但像 Java、Python 等语言有复杂的异常处理系统，它们需要运行时来捕捉和处理异常，确保程序的健壮性。 在这段话中，\u0026ldquo;runtime\u0026rdquo; 指的是一个编程语言在程序执行时所依赖的环境支持。C 语言的运行时非常简单，因为它不依赖垃圾回收、虚拟机或复杂的系统抽象，程序员必须手动管理内存和系统资源。而其他语言，如 Java、Go、Python 等，往往依赖复杂的运行时来提供内存管理、并发支持、类型安全和异常处理等高级功能。现代社会中，这种复杂运行时让程序员开发时更加高效、安全，但也增加了一些运行时的开销和复杂性。\nFutex Futexes（fast userspace mutexes，快速用户空间互斥锁）是一种用于高效实现用户空间线程同步的机制，它允许线程在用户空间中完成大部分的互斥锁操作，只有在需要操作内核资源时才会进入内核。这种设计使得线程同步更加高效，因为它避免了频繁的系统调用开销。\nFutex 是一种结合了用户空间和内核空间的锁机制，用于解决多线程环境中的同步问题。它的核心思想是：大部分时间，线程同步操作可以在用户空间中完成，只有当发生竞争或资源不足时，才会通过 futex 机制进入内核进行处理。这种机制帮助减少不必要的上下文切换，提升了性能。主要作用包括：\n用户空间中的锁管理：Futex 允许多个线程在用户空间中通过简单的内存操作来实现锁的获取和释放。这意味着，当锁可用时，线程之间的锁竞争可以不进入内核，直接在用户空间解决，大大降低了系统调用的频率。 内核中的等待机制：当多个线程竞争一个资源时，可能会出现资源不可用的情况。在这种情况下，线程可以调用 futex 系统调用，将自己挂起到内核中的等待队列中，直到资源可用时被唤醒。这样，线程只会在锁被争抢时进入内核，减少了不必要的内核参与。 避免繁重的上下文切换：传统的锁机制（如互斥锁、条件变量等）可能会频繁调用内核进行线程调度，而 futex 在用户空间处理大部分锁竞争时，避免了频繁的内核调度，提升了性能。 Futex 和线程的关联主要体现在线程同步和锁竞争上。在多线程程序中，多个线程常常需要共享资源，而同步机制（如互斥锁、信号量）则是保证多个线程安全访问共享资源的手段。Futex 是一种更高效的线程同步原语，专门为优化线程同步性能而设计。具体关联如下：\n线程间的锁竞争：当多个线程尝试同时访问同一个资源时，它们会竞争获取锁。Futex 允许这种竞争在用户空间中解决，当锁不可用时，线程会进入内核的等待队列等待被唤醒。 线程的挂起和唤醒：当线程无法立即获得锁时，它可以通过 futex 系统调用主动进入内核，并将自己挂起，等待锁被释放。这种机制避免了线程在用户空间中自旋等待（一直忙等），节省了 CPU 资源。 内核支持的同步：Futex 作为用户空间和内核之间的桥梁，当用户空间的锁争抢进入复杂状态时，才会依赖内核进行线程的管理和调度。这种设计结合了用户空间的高效性和内核的强大调度能力。 Futex的典型使用场景有：\nPthread 实现中的基础：许多操作系统（如 Linux）中基于 POSIX 线程（Pthread）的实现都使用 futex 作为底层的同步原语。通过 futex，可以高效实现锁、条件变量等机制。 高性能多线程应用：在高性能多线程应用中，futex 允许程序减少进入内核的系统调用次数，从而提高并发性能，特别是在资源竞争较少的情况下。 总的来说，Futex 是一种设计用于高效线程同步的机制，它允许在用户空间处理大部分的锁操作，只有在锁被争抢或资源不足时才进入内核。这种机制减少了系统调用开销，提升了多线程程序的性能。与线程的关系在于，futex 提供了线程之间的锁竞争、挂起与唤醒机制，是实现高效线程同步的重要工具。在像 Biscuit 这样的系统中，futex 为内核支持的线程同步提供了基础，确保了高效的线程管理。\nFutex（Fast Userspace Mutex）和普通互斥锁（Mutex） Futex（Fast Userspace Mutex）和普通互斥锁（Mutex）都用于实现线程之间的同步和互斥，确保多个线程安全地访问共享资源。然而，它们的实现方式和效率存在差异。接下来，我将分别介绍 futex 和普通 mutex，并通过举例说明它们的使用场景及区别。\n普通互斥锁（Mutex）\n普通互斥锁 是一种最常见的线程同步原语，常用于保护共享资源，确保只有一个线程在特定时间内访问共享资源。它通常依赖内核进行线程调度和管理。其特点为：\n完全内核实现：每次加锁和解锁都会涉及系统调用，进入内核空间进行处理。\n开销较大：由于每次加锁和解锁都需要上下文切换到内核，因此当竞争不激烈时，性能较低。\n简单直接：开发者不需要关注底层实现细节，只需调用相应的加锁和解锁函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //代码举例 #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; pthread_mutex_t lock; void* thread_function(void* arg) { pthread_mutex_lock(\u0026amp;lock); // 加锁，进入临界区 printf(\u0026#34;Thread %d is accessing the critical section\\n\u0026#34;, *(int*)arg); pthread_mutex_unlock(\u0026amp;lock); // 解锁，离开临界区 return NULL; } int main() { pthread_mutex_init(\u0026amp;lock, NULL); // 初始化互斥锁 pthread_t thread1, thread2; int thread1_id = 1, thread2_id = 2; pthread_create(\u0026amp;thread1, NULL, thread_function, \u0026amp;thread1_id); pthread_create(\u0026amp;thread2, NULL, thread_function, \u0026amp;thread2_id); pthread_join(thread1, NULL); pthread_join(thread2, NULL); pthread_mutex_destroy(\u0026amp;lock); // 销毁互斥锁 return 0; } 代码解释：\n上面的代码使用 pthread_mutex_lock() 和 pthread_mutex_unlock() 来对共享资源进行加锁和解锁。\n每次加锁和解锁都会涉及系统调用，即进入内核来确保线程之间的互斥。这在资源竞争不激烈时，可能会造成不必要的性能开销。\nFutex（Fast Userspace Mutex）\nFutex 是一种更加高效的线程同步机制，结合了用户空间和内核空间的优势。大部分情况下，futex 操作只在用户空间中进行，当且仅当锁被争抢时，才需要进入内核。其特点为：\n大部分操作在用户空间完成：如果没有锁竞争，futex 的加锁和解锁操作可以完全在用户空间中完成，避免了系统调用。\n进入内核的条件：只有当锁被争抢时，才会进入内核来挂起等待中的线程，从而减少系统调用的频率。\n性能更高：由于减少了系统调用的次数，futex 在锁竞争较少时能够显著提高性能。\n假设我们在 Linux 上实现了一个 futex 锁机制，可以通过 futex() 系统调用来挂起和唤醒等待中的线程。下面是一个简化版的伪代码示例，展示 futex 的工作原理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;linux/futex.h\u0026gt; #include \u0026lt;syscall.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int futex_word = 0; void futex_wait(int *futex_addr, int expected) { syscall(SYS_futex, futex_addr, FUTEX_WAIT, expected, NULL, NULL, 0); } void futex_wake(int *futex_addr) { syscall(SYS_futex, futex_addr, FUTEX_WAKE, 1); } void lock() { while (__sync_lock_test_and_set(\u0026amp;futex_word, 1)) { // 进入内核等待 futex 被唤醒 futex_wait(\u0026amp;futex_word, 1); } } void unlock() { __sync_lock_release(\u0026amp;futex_word); // 释放锁 futex_wake(\u0026amp;futex_word); // 唤醒等待中的线程 } void* thread_function(void* arg) { lock(); printf(\u0026#34;Thread %d is accessing the critical section\\n\u0026#34;, *(int*)arg); unlock(); return NULL; } int main() { pthread_t thread1, thread2; int thread1_id = 1, thread2_id = 2; pthread_create(\u0026amp;thread1, NULL, thread_function, \u0026amp;thread1_id); pthread_create(\u0026amp;thread2, NULL, thread_function, \u0026amp;thread2_id); pthread_join(thread1, NULL); pthread_join(thread2, NULL); return 0; } 代码解释：\n这里使用 futex_wait() 和 futex_wake() 来模拟基于 futex 的线程同步。\nfutex_wait() 用于将线程挂起到内核的等待队列中，直到某个条件被满足（例如锁被释放），futex_wake() 用于唤醒等待中的线程。\n当锁没有被争抢时，lock() 函数通过原子操作（__sync_lock_test_and_set）直接在用户空间处理锁的获取，无需系统调用。\n只有当锁被争抢时，才会进入内核进行等待和唤醒操作。\nFutex 和 Mutex 的区别\nFutex 普通 mutex 实现机制 大部分操作发生在用户空间，只有在竞争发生时才进入内核，这显著降低了系统调用的次数，减少了开销。 所有加锁、解锁操作都依赖内核，需要频繁进行系统调用，这增加了上下文切换和开销。 性能 futex 在竞争少时性能更高，因为大部分锁操作都可以在用户空间完成。 普通 mutex 在竞争少时效率较低，因为每次锁操作都需要进入内核，带来额外的性能开销。 复杂性 普通 mutex 的使用相对简单，程序员不需要关心底层实现细节。 futex 的实现更复杂，因为它结合了用户空间和内核空间的操作，需要考虑在用户空间完成大部分操作的同时，在需要时进入内核。 应用场景对比 适用于高性能的多线程场景，特别是竞争不激烈时。比如在 Web 服务器、数据库系统中，futex 的高效锁管理可以大大提高并发性能。 适用于竞争较多或锁机制相对简单的场景。比如小规模的并发程序，在锁的竞争比较频繁时，普通 mutex 的开销与 futex 差异不大。 总结\n普通 mutex 是内核实现的锁机制，所有操作都通过系统调用，较为稳定但性能略低。 futex 是一种结合用户空间和内核的同步机制，减少了系统调用的次数，提高了性能，尤其适用于多线程高并发应用中。 Futex 和 xv6 中的睡眠锁（sleeping locks） Futex 和 xv6 中的睡眠锁（sleeping locks）确实有一些相似之处，特别是在它们如何处理线程等待和唤醒的机制上。但两者的实现细节、使用场景以及设计目标有所不同。接下来我会详细对比 futex 和 xv6 的睡眠锁，并解释它们的区别和相似之处。\nFutex（Fast Userspace Mutex）\nFutex 是 Linux 中的一种高效锁机制，允许线程在用户空间完成大部分的锁操作，只有在必要时才进入内核进行等待和唤醒。它的核心机制是将锁竞争和锁的实际管理分为两个阶段：\n用户空间操作：当线程尝试获取锁时，它在用户空间执行操作。如果锁是空闲的，直接获取锁，线程继续执行，无需进入内核。\n进入内核：如果锁被占用，线程进入内核，通过 futex() 系统调用将自己挂起，等待锁释放。当锁释放时，内核通过 futex_wake() 唤醒等待的线程，重新竞争锁。\n这种机制的设计主要为了提高锁竞争较少时的性能，减少频繁的系统调用。由于大部分锁操作在用户空间完成，它在无竞争情况下比完全依赖内核的锁（如传统的互斥锁）效率更高。\nxv6 中的睡眠锁（sleeping locks）\n在 xv6 中，睡眠锁是一种用于确保线程安全访问共享资源的机制，特别适用于长时间持有锁的场景。在 xv6 中，睡眠锁的机制如下：\n获取锁：当线程尝试获取睡眠锁时，如果锁已经被其他线程持有，那么该线程将进入睡眠，并被移出运行队列。这种睡眠通常是通过操作系统内核的调度器来实现的，调度器会挂起该线程，直到锁变为可用。\n唤醒：当持有锁的线程释放锁时，系统会唤醒等待该锁的线程，让它们重新进入调度队列，并在锁空闲时竞争获取。\n睡眠锁的设计理念是，避免忙等待（busy waiting）。在某些情况下，锁可能需要持有较长时间（例如，执行 I/O 操作），因此让线程进入睡眠状态可以节省 CPU 资源，而不是让它们一直自旋等待锁的释放。\nFutex 与 xv6 睡眠锁的相似之处\n等待时挂起线程：两者的核心相似之处在于，当锁不可用时，线程会被挂起，直到锁被释放。这样做的目的是避免忙等待，节省 CPU 资源。\n唤醒机制：当锁空闲时，两者都会唤醒等待的线程，让它们重新参与锁的竞争。\n避免忙等待：两者都旨在避免 CPU 资源浪费，使线程在无法获取锁时有效地让出 CPU。\nFutex 与 xv6 睡眠锁的区别\nFutex xv6 睡眠锁 实现细节 Futex 是基于用户空间和内核的混合机制。锁的获取和释放大部分是在用户空间完成的，只有在竞争激烈或资源不足时才会通过系统调用进入内核。这使得 futex 更加高效，尤其是在低竞争环境中，减少了频繁的内核调用。 xv6 中的睡眠锁则完全依赖内核调度。每当线程无法获取锁时，线程直接进入内核，被挂起到等待队列中，并依赖内核调度器唤醒。这是一种传统的、完全依赖内核的锁机制，没有 futex 那样的用户空间优化。 性能与使用场景 Futex 设计上更为高效，特别适合于多线程高并发场景，因为它避免了每次锁操作都进入内核。如果锁竞争较少，futex 的性能优势明显，因为它大部分时间在用户空间处理。 xv6 的睡眠锁则更加适合于较简单的多线程环境，尤其是长时间持有锁的场景。由于它完全依赖内核进行线程调度，在某些低并发场景下，这种机制是足够的，但在高并发或频繁的锁竞争情况下，性能可能不如 futex。 设计复杂度 Futex 的实现较复杂，因为它需要在用户空间和内核之间协同工作。需要处理用户空间锁竞争的同步，还要处理进入内核时的等待队列管理等操作。 xv6 的睡眠锁设计相对简单，完全依赖内核来进行调度。内核负责管理锁的获取、释放以及线程的睡眠和唤醒操作。 举例说明\n**Futex 示例：**当多个线程尝试获取同一个 futex 锁时，假设锁是空闲的，线程 A 可以直接在用户空间获取锁并执行。线程 B 尝试获取锁时发现锁已经被占用，于是它调用 futex_wait() 进入内核，等待线程 A 释放锁。当线程 A 释放锁时，它通过 futex_wake() 唤醒线程 B，线程 B 重新获得执行机会。\n**xv6 睡眠锁示例：**在 xv6 系统中，假设线程 A 持有了某个共享资源的睡眠锁，正在进行 I/O 操作。线程 B 想要访问同一个资源，但发现锁已被线程 A 占用，于是线程 B 被内核调度器挂起，进入睡眠状态。等到线程 A 完成操作并释放锁时，调度器会唤醒线程 B，线程 B 才能继续执行。\n总结\nFutex 是一种更复杂、更高效的锁机制，主要通过在用户空间解决大部分锁操作来减少系统调用，适用于高性能的多线程并发环境。\nxv6 的睡眠锁 则是一种相对简单的锁机制，完全依赖内核进行线程调度和唤醒，适用于较低并发或长时间持有锁的场景。\n尽管它们在挂起和唤醒线程的机制上有相似之处，但在性能优化、设计复杂度以及使用场景上有显著差异。\nFutex xv6 睡眠锁 用户态与内核态的配合 首先在用户态尝试获取锁，只有当用户态无法获取锁时才进入内核。这种设计大幅减少了用户线程与内核间的切换开销，从而提高了性能。 锁的获取和释放完全在内核中进行。所有与锁相关的操作都依赖于内核，无法在用户态直接操作。 锁状态和线程状态管理 锁的状态管理主要由用户态的锁变量（通常是一个整数）维护，内核只在必要时介入。线程在用户态时不需要依赖内核调度。 锁的状态和线程的状态管理都依赖内核，由内核调度器直接负责线程的挂起和唤醒。 性能与复杂度 Futex 设计上更加复杂，它的混合设计保证了在大多数情况下无需频繁进入内核，从而具有更好的性能表现。 xv6 睡眠锁 设计简单，适合教学与理解，但是每次锁竞争时都需要内核介入，性能上不如 futex 高效。 针对schedule的复习 在 xv6 操作系统中，系统中多个进程如何在多个 CPU 上运行、切换，并在内核态和用户态之间切换的机制。\n假设的场景：\n4 个 CPU：你有一个四核的系统。 3 个进程：系统当前有 3 个用户进程正在运行或准备运行。 在这种情况下，是否存在额外的进程在运行 \u0026ldquo;内核监管的内容\u0026rdquo;，或者这些进程是如何在用户态和内核态之间切换的。\n多进程与 CPU 调度\n在 xv6 中，CPU 的调度由操作系统的内核控制。假设有 3 个进程和 4 个 CPU 时，操作系统将决定这些进程如何分配到不同的 CPU 上运行。如果系统中只有 3 个进程，而有 4 个 CPU，那么其中一个 CPU 会空闲，因为没有足够的进程去占用每个 CPU。每个 CPU 在某个时刻只能运行一个进程。\n在多核环境下，xv6 的内核调度器会根据进程的状态（例如：就绪、运行、阻塞等）决定哪些进程在哪些 CPU 上运行。\n进程的用户态和内核态切换\n每个进程都会在需要时在 用户态 和 内核态 之间切换。这个切换发生在以下几种典型场景下：\n系统调用：当进程调用某些系统资源（如文件系统、网络、内存管理等），需要通过系统调用进入内核态。\n中断：如定时器中断、硬件中断等，进程会暂时进入内核态，处理完中断后再切换回用户态。\n进程调度：当操作系统决定切换运行进程时，当前进程会切换到内核态进行上下文切换，然后另一个进程会被调度并切换到用户态继续运行。\n在这些场景下，虽然同一个进程可以在用户态和内核态之间切换，但这些进程的执行仍然是在这 3 个用户进程中，没有额外的进程在运行专门的内核任务。\n内核与用户态的关系\n用户态进程：用户态的进程是我们平时所运行的应用程序，它们通过系统调用与内核交互。\n内核态：当进程需要访问系统资源或处理中断时，进程会从用户态切换到内核态。此时，虽然执行的是内核的代码，但它仍然是当前的进程在执行，只不过执行的是与内核相关的代码，而不是用户应用代码。\n因此，没有一个单独的进程在运行内核监管的内容。而是每个进程在需要时会进入内核态，执行系统调用或处理中断等任务，然后返回用户态。\n总的来说\n在 4 个 CPU 和 3 个进程的场景下，3 个进程会被内核调度到 CPU 上运行，其中一个 CPU 会空闲。 这些进程在执行时，会在用户态和内核态之间切换（例如通过系统调用或中断），但没有额外的进程专门负责运行内核任务。 每个进程的用户态和内核态的切换是由具体的事件触发的（系统调用、中断等），这些切换不会创建新进程，只是在现有进程中执行内核代码。 针对schedule的复习（2） 在 xv6 中，当 CPU 数量少于进程数时，确实会涉及到进程的调度，这由操作系统内核的**调度器（scheduler）**负责管理和执行。然而，这个调度过程并不会创建一个额外的\u0026quot;调度进程\u0026quot;来控制进程运行，而是由操作系统中的调度逻辑直接在内核态中执行。\n在 xv6 和大多数操作系统中，调度器（scheduler） 是内核中的一部分代码，它并不是一个单独的进程。调度器函数是内核态执行的代码，它负责选择下一个要运行的进程。每个 CPU 会有自己的调度器逻辑，它会检查当前有哪些进程可以运行，选择一个进程，然后将它调度到 CPU 上。\n当进程主动放弃 CPU（例如调用 sleep() 函数）或时间片用尽时，系统会进入内核态，调用调度器。调度器会在所有可以运行的进程中选择下一个要运行的进程。这个过程并不会创建一个新的调度进程，而是当前的进程（或者说当前 CPU）进入内核态，调用内核中的调度代码来进行调度。\n当 CPU 数量少于进程数时，调度器就会负责让每个进程“轮流”占用 CPU。这通常通过以下方式实现：\n时间片（time slice）：每个进程在 CPU 上运行一段时间（称为时间片），当时间片耗尽时，调度器会选择另一个就绪的进程来运行。 主动让出 CPU：进程可能在需要等待某些资源（如 I/O、信号等）时主动进入睡眠状态，调度器会选择另一个可运行的进程接替。 当时间片耗尽或进程需要进入睡眠时，CPU 进入内核态，由当前进程的上下文切换到调度器的上下文，调度器函数在内核态中执行，负责选择下一个进程运行。因此，在这个过程中，没有一个专门的调度进程，调度器逻辑是作为操作系统内核的一部分直接运行的。\n调度器通常在以下情况下被调用：\n时间片用尽：当一个进程在 CPU 上的时间片耗尽时，会发生时钟中断，当前进程进入内核态，调用调度器函数来选择下一个要运行的进程。 进程阻塞：当进程主动调用 sleep()、wait() 等函数时，它进入睡眠状态，并通过调度器选择其他可运行的进程。 进程退出：当进程完成并调用 exit() 时，调度器会选择下一个进程运行。 中断处理：在中断（例如 I/O 完成）期间，进程可能会从睡眠状态唤醒，这时调度器也可能被调用，选择唤醒的进程继续运行。 虽然调度器代码在内核态中执行，但它并不是由用户进程直接控制的。当用户进程运行时，它并不知道调度器的存在。调度过程完全由内核管理，是在操作系统内核态中运行的。换句话说，调度器逻辑是内核的一部分，而不是用户进程的一部分。具体来说：\n当前进程不控制调度：当一个进程的时间片耗尽时，时钟中断发生，进程会被强制切换到内核态，执行调度器逻辑。这个过程由内核自动控制，进程本身并不会主动去调用调度器。 调度是内核的行为：调度器在内核态中运行，切换不同的进程，不依赖任何一个用户进程去执行调度逻辑。 总而言之\n没有额外的调度进程：在 CPU 少于进程数时，不会有一个额外的进程专门执行调度任务。调度过程由操作系统的内核负责，调度器逻辑直接在内核态中执行。 调度逻辑由内核管理：每当需要切换进程时，当前进程会进入内核态，由调度器选择下一个要运行的进程。 CPU 轮换执行进程：调度器确保每个进程轮流占用 CPU，根据时间片或进程状态（如进程阻塞或退出）来决定何时切换进程。 因此，调度器函数是在内核中运行的，不是原来的进程自己运行的。每当系统需要调度时，当前进程会进入内核态，调度器逻辑会在内核中执行，并决定下一个要运行的进程。\nQ:什么叫做在内核中运行的，函数不是只有在运行在CPU上的进程时才能执行么\nA:你的问题涉及到内核态与用户态的概念，以及如何理解 \u0026ldquo;在内核中运行\u0026rdquo; 这一说法。让我详细解释一下。\n在操作系统中，进程的执行可以分为用户态（user mode）和内核态（kernel mode）。这是为了安全性和隔离，防止普通用户程序直接访问硬件或操作系统的核心资源。\n用户态：大部分应用程序在用户态运行，拥有受限的权限，不能直接访问硬件或者操作系统内核的资源。如果需要访问这些资源（比如进行文件读写、网络操作等），需要通过系统调用（system call）请求内核来完成。 内核态：当系统调用或中断发生时，进程进入内核态，执行操作系统的代码，直接与硬件交互。内核态拥有最高权限，可以执行任何操作。调度器（scheduler） 也是在内核态中运行的。 进程确实是运行在 CPU 上的。无论进程是在用户态还是内核态，最终都是由 CPU 执行代码。只不过，用户态的代码和内核态的代码是有所区别的。操作系统通过上下文切换来在不同进程之间切换，而 \u0026ldquo;内核中运行\u0026rdquo; 是指在内核态执行操作系统的代码，而不是运行用户的应用代码。举个例子：\n一个进程在用户态执行一个应用程序的代码，例如访问文件。此时它调用了 read() 系统调用。 这个 read() 系统调用触发进程进入内核态，进程现在开始执行内核代码（例如文件系统的操作）。 当文件读取操作完成后，进程会返回用户态，继续执行它的应用代码。 所以，在内核中运行，并不是指有一个独立的\u0026quot;内核进程\u0026quot;，而是指当前进程切换到了内核态，此时进程执行的是操作系统内核的代码，而不是用户态的代码。\n操作系统的调度器也是在内核态中运行的。当某个进程的时间片耗尽，或者它调用了阻塞操作（如等待 I/O），CPU 会触发一个中断（通常是时钟中断），使当前进程切换到内核态。此时，调度器代码会运行，它会：\n保存当前进程的状态（上下文）。 选择一个新的进程来运行（通过调度算法）。 切换 CPU 的上下文到新进程的上下文。 让 CPU 开始执行新进程的代码。 这个整个过程都是在内核态中发生的，执行的也是内核代码，但依然是通过当前 CPU 的资源运行的。\n当我们说某个函数 \u0026ldquo;在内核中运行\u0026rdquo; 时，实际意思是：\n当前的进程（或 CPU 上正在执行的任务）处于 内核态，执行的是操作系统内核提供的功能（例如文件系统、调度器等），而不是用户程序的代码。 在某个时刻，一个进程可能处于用户态执行用户代码，也可能处于内核态执行内核代码。这都是由同一个进程完成的，只不过切换了执行的权限和代码。 关键点是：内核态和用户态不是两种不同的进程或任务，而是同一个进程在不同的执行状态下执行不同的代码。\n在内核中运行是指进程切换到内核态，执行操作系统的代码，比如系统调用、调度器逻辑等。 调度器在内核态中执行，它负责在进程间进行上下文切换，让不同的进程共享 CPU 资源。调度器运行时，实际也是某个进程在 CPU 上执行内核态的调度逻辑。 没有独立的 \u0026ldquo;内核进程\u0026rdquo; 在执行这些操作，而是当前 CPU 或进程在必要时进入内核态执行系统级的代码。 所以，当你问 \u0026ldquo;函数不是只有在运行在 CPU 上的进程时才能执行么\u0026rdquo;，答案是是的，无论是内核态还是用户态，都是在进程执行时通过 CPU 来运行代码，只不过在内核态下，进程执行的是操作系统内核的功能，而不是用户代码。\n针对schedule的复习（3） 我们可以进一步解释一下进程切换和多核系统中的调度，特别是如何在内核态执行 schedule 以及如何确保不会重复调度已经在其他 CPU 上运行的进程。\n进程切换（上下文切换）\n当一个进程进入内核态并执行调度（schedule() 函数）时，操作系统会选择下一个要运行的进程，并进行上下文切换。上下文切换是将当前进程的状态（如寄存器、程序计数器、栈指针等）保存下来，然后加载下一个进程的状态，使得 CPU 可以继续执行新进程的任务。具体过程如下：\n当前进程的状态保存：当一个进程进入内核态并被调度器决定切换出去时，系统会保存当前进程的上下文，包括寄存器的值、程序计数器、栈指针等信息。这些信息会被保存在内核的数据结构中，以便稍后能够恢复这个进程的执行。 选择下一个进程：schedule() 函数会从就绪队列中选择下一个可以运行的进程。就绪队列（run queue）包含所有等待被调度执行的进程。调度器根据调度策略（如轮转、优先级等）选择下一个进程。 恢复下一个进程的上下文：调度器选定下一个进程后，会加载该进程的上下文，即恢复该进程的寄存器、程序计数器、栈指针等信息。然后，CPU 开始执行这个进程的代码。 退出内核态，回到用户态：完成上下文切换后，新的进程从内核态退出，重新进入用户态，继续它的执行，直到再次被调度器选中或需要进入内核态。 进程调度和多核系统\n在多核系统中，多个 CPU 可以同时运行不同的进程。为了避免同一个进程被分配到多个 CPU 上运行，调度器需要确保在一个 CPU 上选择的进程没有在其他 CPU 上执行。如何避免进程在多个 CPU 上运行？\nCPU 绑定和调度器协调：在多核系统中，调度器使用一个全局的数据结构来管理所有的就绪进程（通常称为全局就绪队列），或者每个 CPU 维护自己的就绪队列。无论采用哪种方式，调度器需要在进程状态中记录该进程当前是否已经在某个 CPU 上运行，防止同一个进程同时被调度到多个 CPU 上。 进程状态：每个进程都有状态，如运行中（running）、就绪（ready）、阻塞（blocked）等。当一个进程正在某个 CPU 上运行时，它的状态会被标记为“运行中”，这意味着其他 CPU 不会再次调度这个进程。 CPU 和进程的关系管理：调度器在分配进程时，会查询每个进程的状态。如果进程正在其他 CPU 上运行，它会跳过该进程并选择其他就绪的进程。这样可以确保每个进程只在一个 CPU 上执行。 调度器使用进程的状态和 CPU 数据结构来管理哪些进程正在运行、哪些进程可以被调度：\n运行队列（runqueue）：每个 CPU 通常有一个自己的运行队列，存放所有可以在该 CPU 上执行的进程。这个队列中可能会有一些进程等待执行。如果进程正在另一个 CPU 上运行，它不会被加入该 CPU 的运行队列。\n进程控制块（PCB）：每个进程都有一个数据结构，称为进程控制块（Process Control Block, PCB），其中包含进程的各种状态信息。每个 CPU 的调度器会检查进程的 PCB，判断该进程是否正在运行，是否可以调度。\n进程的状态与调度流程\n为了确保同一个进程不会在多个 CPU 上运行，操作系统通过以下几个步骤进行协调：\n检查进程状态：调度器在调用 schedule() 时，会检查进程的状态。只有那些处于就绪（ready）状态的进程会被考虑调度。如果进程已经处于运行中状态，则表示它正在其他 CPU 上运行，当前 CPU 不会选择它。 状态标记：一旦调度器决定将某个进程分配给 CPU，它会将该进程的状态标记为运行中，同时更新该进程的 PCB，记录该进程当前在哪个 CPU 上运行。这样，其他 CPU 的调度器在检查时，就会知道这个进程正在执行，不会重复调度。 CPU 核心的独立调度：每个 CPU 都有自己的调度器，负责决定该 CPU 上运行哪些进程。在某些多核系统中，调度器会有全局协调机制，确保多个 CPU 不会重复调度同一个进程。在其他系统中，调度器可以依赖每个 CPU 的独立运行队列来确保进程不会重复分配。 总而言之\n调度器在内核态运行：当一个进程进入内核态并调用 schedule() 函数时，调度器会选择下一个要运行的进程，并执行上下文切换。上下文切换是保存当前进程状态并加载下一个进程状态的过程。 多核调度中的协调：在多核系统中，调度器需要确保一个进程不会同时在多个 CPU 上运行。它通过进程的状态标记（如运行中、就绪）和全局或局部的运行队列来管理多个进程，并避免重复调度。 进程的状态和管理：每个进程有一个控制块（PCB）来记录它的状态和所运行的 CPU。调度器使用这些信息来选择哪些进程可以运行，并避免调度那些已经在其他 CPU 上运行的进程。 通过这些机制，操作系统确保在多核环境中，调度是协调一致的，避免同一个进程在多个 CPU 上同时运行。\n针对schedule的复习（4） 在多核系统中，某些数据结构是所有 CPU 共享的，这些数据使得各个 CPU 能够了解到其他 CPU 上的进程状态，从而避免重复调度同一个进程。\n共享的数据结构\n在操作系统中，进程的状态和调度信息通常存储在一些全局或者共享的数据结构中，以便所有 CPU 可以访问。以下是几个常见的共享数据结构：\n进程控制块（PCB，Process Control Block）\n每个进程都会有一个唯一的进程控制块（PCB），其中存储了该进程的状态信息，例如：\n进程当前的状态（如就绪、运行中、阻塞）。 当前进程在哪个 CPU 上运行。 寄存器值、程序计数器、栈指针等用于上下文切换的信息。 这些 PCB 是系统中的共享数据结构，各个 CPU 的调度器可以查看每个进程的 PCB 来判断它的状态。通过这些信息，CPU 能够了解当前进程是否已经在其他 CPU 上运行，从而避免重复调度。\n运行队列（Run Queue）：每个 CPU 可能会有自己的就绪队列，但在某些系统中，也可以有全局的运行队列（runqueue），该队列存储了所有处于就绪状态、等待被调度的进程。每当一个 CPU 需要调度进程时，它会查看这个共享的运行队列或它自己的局部运行队列，选择一个处于“就绪”状态且尚未运行的进程来执行。\n全局调度器信息：在某些操作系统中，存在一个全局调度器，它能够协调所有 CPU 的进程调度。在这种情况下，全局调度器可以直接管理各个 CPU 的负载，平衡进程在多个 CPU 上的分布。这种全局信息也可以让系统知道每个进程在哪个 CPU 上运行，并避免重复调度。\n进程状态的共享与同步：为了确保多个 CPU 能够正确地共享这些信息，并避免出现竞争条件（例如两个 CPU 同时调度同一个进程），系统通常采用锁机制或者原子操作来保护对这些共享数据的访问。\n原子操作：当需要改变进程的状态（例如从“就绪”变为“运行中”）时，系统会使用原子操作来更新进程的状态，确保状态更新是不可分割的，避免多个 CPU 同时修改状态导致冲突。\n锁机制：如果多个 CPU 需要同时访问共享数据（如全局运行队列），系统会使用锁来确保一次只有一个 CPU 能够修改共享数据。这可以防止竞争条件，但同时也会带来一些额外的性能开销。\n如何避免重复调度：通过共享的 PCB 和进程状态信息，每个 CPU 的调度器能够了解到当前有哪些进程正在运行，以及它们在哪些 CPU 上运行。这是如何避免重复调度的关键：\n在每次调度时，调度器会检查进程的状态。如果进程处于“运行中”状态，且已经在某个 CPU 上运行，则该调度器会跳过这个进程，选择其他处于“就绪”状态的进程。 通过这种方式，各个 CPU 能够同步了解进程的状态变化，确保同一个进程不会同时在多个 CPU 上运行。 总结\n进程的状态信息（如 PCB）和运行队列是所有 CPU 可以访问的共享数据结构。每个 CPU 的调度器通过访问这些共享数据来决定哪些进程可以运行，避免重复调度。\n操作系统使用原子操作和锁机制来保护对这些共享数据的访问，确保多个 CPU 在并行运行时能够正确更新和读取进程的状态。\n这种共享的数据结构确保了在多核系统中，调度器可以协调多个 CPU 之间的进程执行，避免同一个进程同时在多个 CPU 上运行。\n因此，进程状态的数据是可以被所有 CPU 共享的，这也是如何在多核系统中进行有效调度的核心机制。\nschedule的死锁 在操作系统的调度器（scheduler）实现中，锁机制是非常重要的，它用于保护共享资源的访问，防止多个 CPU 或多个线程同时修改共享数据，导致数据不一致。然而，锁的使用不当可能引发一系列问题，如死锁、优先级反转、锁竞争等。下面是调度器中使用锁时常见的一些易产生错误的地方，以及对应的解决办法：\n死锁（Deadlock）\n死锁发生在两个或多个进程相互等待对方持有的锁，导致所有相关进程都无法继续执行。调度器中可能需要对多个资源进行加锁，例如对进程控制块（PCB）、就绪队列、进程状态等进行加锁。在获取多个锁时，可能会出现锁的顺序不一致，进而引发死锁。解决办法有：\n锁定顺序一致性：确保所有锁的获取顺序是一致的。这意味着在不同的代码路径中，如果多个锁需要同时持有，锁的获取顺序必须始终保持一致。\n避免嵌套锁：尽量减少嵌套锁的使用。如果确实需要多个锁，使用分层的锁机制来确保不会形成循环等待。\n死锁检测和恢复：某些操作系统实现了死锁检测机制，通过检测锁的依赖关系，发现可能的死锁情况并强制中断相关进程。\n优先级反转（Priority Inversion）\n优先级反转发生在一个低优先级的进程持有了一个锁，而一个高优先级的进程需要等待这个锁释放。如果有一个中优先级的进程占据了 CPU，低优先级的进程无法运行并释放锁，那么高优先级的进程将被中优先级的进程“饿死”，导致系统性能下降。解决办法有：\n优先级继承（Priority Inheritance）：当高优先级的进程等待一个由低优先级进程持有的锁时，低优先级进程会临时继承高优先级，以加速锁的释放，避免优先级反转问题。这是一个常见的调度器优化技术。\n避免不必要的锁：确保只在必要时使用锁，尽可能缩短锁的持有时间，减少优先级反转的可能性。\n锁竞争和性能瓶颈\n在多核系统中，当多个 CPU 频繁访问同一个共享数据结构时，可能会导致大量的锁竞争。比如在调度器中，所有 CPU 都需要访问共享的运行队列或进程控制块，这会造成频繁的锁获取和释放，进而影响系统性能。解决办法有：\n锁分离和局部队列：通过将全局锁分离为多个锁，减少竞争。例如，每个 CPU 可以有自己的运行队列（local runqueue），避免所有 CPU 竞争访问同一个全局队列。\n读写锁（Read-Write Locks）：对于读多写少的共享数据结构，使用读写锁可以提升并发性能。多个 CPU 可以同时读取数据，而写操作则需要独占锁。\n减少锁的粒度：尽量减少锁的粒度，避免对大范围的操作进行锁定，只对真正需要保护的部分加锁。可以通过锁分割来减少锁的范围，例如针对不同的数据结构使用不同的锁。\n持锁时间过长\n如果进程在持有锁的情况下进行耗时较长的操作，其他等待锁的进程可能会被长时间阻塞，影响系统的并发性和响应能力。在调度器中，这个问题可能出现在当进程持有锁并且执行复杂的调度算法或处理其他任务时。解决办法有：\n短期持锁策略：确保锁的持有时间尽可能短。锁应仅用于保护关键的、最小粒度的代码区域。\n使用自旋锁或休眠锁的结合：在持锁时间较短的情况下可以使用自旋锁（spinlock），这样等待的进程不会进入睡眠状态，而是自旋等待锁释放。对于持锁时间较长的情况，使用休眠锁（sleeping lock）更为合适。\n抢占中断和锁\n如果进程在持有锁时被抢占或发生中断，且中断处理程序或新的被调度的进程也需要获取同一个锁，可能会造成系统的不可预测行为。解决办法有：\n禁用抢占：在某些关键区域（持有锁期间），可以短暂禁用抢占，确保进程不会在持有锁时被打断。\n中断安全的锁机制：确保使用的锁机制在中断上下文中是安全的，例如在自旋锁中禁用中断，防止锁被不安全地抢占。\n双重释放锁或未释放锁\n错误地释放同一个锁两次（双重释放锁）或者未释放锁（锁泄漏）是常见的编程错误。这在调度器的代码中尤其可能发生，尤其是在上下文切换、调度新进程时忘记释放锁。解决办法有：\n使用调试工具：可以使用一些调试工具（如 lockdep，用于 Linux 内核），它能动态检测锁的使用不当，提前发现问题。\n代码审计与测试：在锁操作的代码中进行严密的审查和测试，确保所有加锁的地方都能够正常释放锁，避免逻辑漏洞。\n饥饿问题（Starvation）\n如果调度器中某个低优先级的进程长时间无法获取锁（尤其是当锁被高优先级的进程频繁获取时），该进程可能会遭遇饥饿，即它的执行总是被推迟。解决办法有：\n公平锁（Fair Locking）：通过公平锁机制，确保锁的获取是公平的，按顺序来决定谁能获取锁，防止某些进程总是被饿死。例如使用队列锁或基于优先级的公平调度机制。\n限时锁等待：对等待锁的时间设置一个限制时间，如果超过这个时间还没有获取到锁，则可能选择放弃当前操作或采取其他补救措施。\n递归锁导致的逻辑混乱\n递归锁允许同一个线程多次获得同一把锁，但在调度器中，递归锁的使用如果不加以控制，会导致逻辑混乱或不可预测的行为，特别是在调度复杂的多线程环境中。解决办法有：\n避免使用递归锁：尽量避免在调度器中使用递归锁。如果确实需要嵌套锁，尽量使用普通锁，并对锁的获取和释放顺序进行严格控制。 总而言之，在操作系统的调度器中，使用锁来保护共享资源和数据结构是必不可少的，但错误的锁使用会导致死锁、性能下降、优先级反转等问题。为了解决这些问题，操作系统通常采取以下措施：\n确保锁的获取顺序一致，避免死锁。 使用优先级继承等机制解决优先级反转问题。 使用局部队列、读写锁等机制减少锁竞争。 确保锁的持有时间短，避免影响系统的并发性。 通过这些措施，可以有效地提高调度器的性能和可靠性。\nschedule的中断 在调度（schedule）和上下文切换（context switch）的关键时刻，如果发生中断，确实可能会造成系统状态的不一致或混乱。为此，操作系统（如 xv6 和 Linux）都有相应的机制来处理这种情况，确保调度和上下文切换时的中断不会导致问题。接下来，我们分别看看 xv6 和 Linux 是如何解决这一问题的。\nxv6 中的中断处理\n在 xv6 中，调度和上下文切换是非常重要且关键的操作。为了避免在这些关键时刻被中断打断，xv6 采用了一些简化但有效的机制来处理中断问题。\n在 schedule 和 context switch 等关键的调度场合，xv6 通过禁用中断来保证这些操作的原子性。具体来说，在调度器运行期间，系统会暂时禁用中断，确保不会有外部中断（如时钟中断、设备中断）在关键时刻打断调度过程。\n以下是 xv6 的具体做法：\n在进入调度器前禁用中断：当调度器决定切换进程时，会禁用当前 CPU 上的中断。这意味着在上下文切换完成之前，不会有任何中断能够发生。这确保了上下文切换的安全性。 完成上下文切换后重新启用中断：当上下文切换完成并且新的进程开始执行后，调度器会重新启用中断。这样，系统在调度完成后能够处理外部中断，但不会在关键时刻被打断。 xv6 上下文切换的步骤：\n禁用中断：调度器在切换进程时会禁用中断，确保切换过程不会被打断。 保存当前进程状态：保存当前正在运行进程的 CPU 状态到进程控制块（PCB）。 选择新的进程：从就绪队列中选出下一个要运行的进程。 加载新进程状态：将新进程的状态从 PCB 中恢复到 CPU。 重新启用中断：上下文切换完成后，恢复 CPU 上的中断。 通过在关键操作时禁用中断，xv6 避免了在调度和上下文切换中发生的潜在冲突和数据不一致问题。\nLinux 中的中断处理\nLinux 是一个复杂的多任务操作系统，相比 xv6，它处理调度和中断的机制更加复杂和优化。Linux 需要保证在调度和上下文切换时，能够正确处理中断，同时尽量减少性能开销。Linux 的解决方案主要依赖于更复杂的锁机制、抢占控制和中断屏蔽。\nLinux 使用抢占（preemption）机制来确保高优先级任务能够及时执行，但在某些关键区域，Linux 通过禁用抢占来确保安全的调度和上下文切换：\n禁用抢占：在进入调度器和执行上下文切换时，Linux 内核会临时禁用抢占（preemption）。这意味着当前 CPU 不会被更高优先级的任务打断，直到调度和上下文切换完成。 中断屏蔽：类似于 xv6，Linux 也可以在调度器运行和上下文切换期间临时屏蔽中断。通过屏蔽中断，Linux 确保不会在关键时刻打断调度和切换进程。 局部中断禁用：为了提高系统的实时响应能力，Linux 只禁用当前 CPU 的中断（局部中断），而不是禁用全局中断。这种设计保证了其他 CPU 在调度过程中可以继续处理中断和任务，而不会因为一个 CPU 的上下文切换而影响整个系统。 Linux 还依赖**各种锁（如自旋锁、互斥锁）**来保护关键的共享数据结构。调度器和上下文切换期间，操作系统必须访问和修改一些关键的数据结构，如进程控制块（PCB）、调度队列等。这些访问通常通过锁来保护，防止多个 CPU 同时访问同一个数据结构。\n自旋锁：在某些短暂的、快速完成的操作中，Linux 使用自旋锁。在持有自旋锁期间，CPU 不会进入睡眠状态，而是自旋等待锁的释放。对于上下文切换等需要快速完成的任务，自旋锁是一个合适的选择。 互斥锁：对于较长的操作，Linux 使用互斥锁来避免多个进程同时修改相同的数据结构。 Linux 上下文切换的步骤：\n禁用抢占：Linux 在上下文切换期间禁用抢占，防止其他进程在此过程中抢占 CPU。 保存当前进程状态：保存当前进程的状态，包括 CPU 寄存器、栈指针等到进程控制块（PCB）。 选择下一个进程：通过调度器算法（如 CFS，完全公平调度器），从就绪队列中选择下一个要运行的进程。 加载下一个进程状态：将下一个进程的上下文信息从 PCB 中恢复到 CPU。 重新启用抢占：上下文切换完成后，重新启用抢占和中断处理。 关键差异总结：xv6 vs Linux\n特性 xv6 Linux 中断处理 在调度和上下文切换期间禁用中断 在调度和上下文切换期间禁用局部中断和抢占，避免冲突 抢占机制 没有复杂的抢占机制 使用抢占控制，在必要时禁用抢占，允许实时调度 锁机制 锁机制简单，主要用于保护关键数据 复杂的锁机制，如自旋锁和互斥锁，用于保护内核数据 上下文切换 通过简单的状态保存和恢复进行切换 使用调度器算法和复杂的状态管理，支持多核和高并发 在调度（schedule）和上下文切换过程中，避免中断打断关键操作是非常重要的。xv6 通过简单地禁用中断来确保上下文切换的安全性，适合简单的单 CPU 或较少核心的系统。Linux 则更复杂，它通过禁用抢占、局部中断以及使用锁机制来处理多核系统中的复杂并发和中断问题，同时确保高效的系统响应能力。\nBiscuit 操作系统中断处理 Device Interrupt Handlers in Biscuit “A Biscuit device interrupt handler marks an associated device-driver goroutine as runnable and then returns”\n当设备（例如硬件设备）发出中断时，操作系统的**中断处理程序（interrupt handler）**会被触发。\n在 Biscuit 中，中断处理程序的主要作用是\n标记与该设备相关的设备驱动程序的 goroutine 为“可运行”（runnable）\nGoroutine 是 Go 语言中的轻量级线程。 Marking as runnable 意味着操作系统不会在中断处理程序中做大量工作，而是通知相关的 goroutine 可以运行，然后返回。中断处理程序不会长时间运行，也不会在中断处理过程中直接处理复杂的设备逻辑。 中断处理程序执行完这个标记操作后，立即返回，不会进行更多操作。这是一种常见的设计，可以提高中断响应效率，并减少由于中断处理程序长时间运行可能带来的问题（如性能降低或资源死锁）。\nWhy Do Interrupt Handlers Return Quickly? “Interrupt handlers cannot do much more without risk of deadlock, because the Go runtime does not turn off interrupts during sensitive operations such as goroutine context switch”\n这里提到，中断处理程序不能做太多事情，否则可能导致死锁问题。这是因为： Go 运行时不会在某些敏感操作期间关闭中断，例如在 goroutine 上下文切换 的时候。 如果中断处理程序在这些敏感时期执行复杂操作，可能会干扰 Go 运行时对 goroutine 的调度，从而引发死锁（即程序无法继续执行的状态）。 为了避免死锁，Biscuit 采用了一种保守的设计——中断处理程序只做简单的标记操作，然后快速返回，让真正的工作交给相应的 goroutine 来完成。 Handlers for System Calls and Faults “Handlers for system calls and faults from user space can execute any Go code”\n相较于设备中断处理程序，\n系统调用和用户态故障（faults）的处理程序可以执行任意的 Go 代码\n系统调用（system call）：指用户程序请求操作系统服务的调用（如读写文件、分配内存等）。Biscuit 中，系统调用处理程序是在与用户线程相关联的 goroutine 上执行的，因此它们不受设备中断处理那种约束。 Faults from user space：这些故障（如页面错误或非法内存访问）也会由相应的 handler 来处理，这些 handler 同样可以执行复杂的 Go 代码。 Execution Context for System Calls and Faults “Biscuit executes this code in the context of the goroutine that is associated with the current user thread”\n当处理系统调用或用户态故障时，Biscuit 在与当前用户线程（user thread）相关联的 goroutine 中执行处理代码。这意味着： 执行上下文是当前的 goroutine（这 goroutine 是与发起系统调用或触发故障的用户线程相关联的）。 由于 Go 运行时对 goroutine 的管理，Biscuit 可以在这些情况下执行任意的 Go 代码，而不必担心中断相关的死锁问题。 这段话解释了 Biscuit 操作系统如何处理中断和系统调用。具体来说：\n设备中断处理程序只标记相关设备驱动的 goroutine 为“可运行”，并迅速返回，以避免与 Go 运行时的敏感操作（如上下文切换）发生冲突，从而避免死锁。 与此相反，系统调用和用户态故障处理程序可以执行任意的 Go 代码，因为它们是在用户线程相关联的 goroutine 上执行的，不受设备中断处理程序那样的限制。 在 Biscuit 系统中，中断处理程序不能直接处理复杂的逻辑，尤其是涉及到 goroutine 的操作。主要原因在于 Go 运行时的敏感操作和死锁风险，具体解释如下：\nGo 运行时的特性： Go 运行时管理着 goroutine 的调度，这涉及到 goroutine 的上下文切换和状态管理。而上下文切换是一种敏感操作，这意味着它不能被中断打扰，因为打断这种操作可能导致调度状态的不一致，进而导致整个调度系统陷入混乱。 死锁的来源： 如果中断处理程序（interrupt handler）在这些敏感时期执行了复杂的操作，例如尝试操作一个资源或者等待某个锁的释放，很有可能与 Go 运行时本身的锁定机制发生冲突，产生 死锁。比如，假设某个中断在 Go 运行时正忙于调度 goroutine 时发生，如果中断处理程序尝试与调度器共享资源，就可能出现双方相互等待的情况——调度器等待中断处理完成，中断处理等待调度器释放锁，这就形成了死锁。 因此，Biscuit 系统选择了一种保守的设计方式，即中断处理程序 尽可能少地做事情，它只是标记一个与设备相关联的 goroutine 为“可运行”，然后迅速返回。这样一来，中断处理程序不会与 Go 运行时的调度逻辑产生直接冲突，避免了在上下文切换时可能的死锁。\n这里的中断不包括 trap。\n中断 (Interrupt)：通常由硬件设备触发，用来通知 CPU 需要执行某个设备的服务。这种中断通常是异步的（随时可能发生），比如网卡有新数据到达、硬盘请求完成等等。 Trap：一种特殊的中断，通常是由软件触发的，例如系统调用（system call）或程序发生异常（如非法指令或内存访问错误）。trap 由 CPU 执行指令时检测到条件后发出，可以看作是 同步中断。 Biscuit 中提到的设备中断处理程序是特指硬件中断，而 trap 处理（例如系统调用和用户态故障）被赋予了更多的自由，因为它们发生在 系统调用上下文中，可以在与用户线程相关联的 goroutine 上执行任意的 Go 代码，不涉及中断期间的敏感情况。因此，trap 并不受同样的限制，且其处理程序可以执行更复杂的逻辑。\n中断处理程序只做标记的方式非常类似于经典的 top-half 和 bottom-half 机制。\n在传统操作系统中，top-half 和 bottom-half 的设计是为了提高中断处理的效率，同时保证系统的稳定性和响应性。简单介绍它们：\nTop-Half： 是中断发生后由中断控制器立即触发的部分。 主要目的是快速处理紧急的任务，例如确认接收到的数据或者告诉设备继续传输。 保持尽可能短的执行时间，以便快速让出 CPU，避免阻塞其他中断。 Bottom-Half： 由 top-half 触发或标记，稍后由系统调度进行执行。 主要用于处理一些相对复杂且时间消耗大的任务，因为这些任务不需要立即响应。 这样可以减少中断关闭时间，提高系统整体的中断响应性能。 在 Biscuit 中，中断处理程序只做一个简单的标记操作，类似于 top-half 的行为，它不会在中断期间做大量工作。标记设备驱动程序的 goroutine 为“可运行”后，这部分工作会在系统调度后由这个 goroutine 来完成，这类似于 bottom-half 的工作方式——即，复杂的设备处理逻辑被推迟到一个更合适的时机进行处理。\n总结来说：\nBiscuit 系统的中断处理程序做标记并迅速返回，是为了避免在中断期间与 Go 运行时的调度机制产生冲突，从而防止死锁。 设备中断和 trap 不同，trap 的处理可以执行复杂的 Go 代码，因为它们是在安全的上下文中处理的。 这种设计非常类似 top-half 和 bottom-half 机制，将紧急、短暂的操作放在 top-half，复杂、耗时的工作放在 bottom-half，由底层调度来完成，从而提高系统响应效率和稳定性。 Biscuit速记 Biscuit 使用页表硬件实现按需分配内存、写时复制和按需文件映射。这些机制通过页面缺失处理来填充页表，提升内存使用效率。 Biscuit 通过紧凑记录连续的内存映射，减少管理开销，提高性能。 Biscuit 使用引用计数来跟踪物理页面的共享情况，确保物理内存的正确管理和回收。 Biscuit 操作系统设备中断处理程序只标记相关设备驱动的 goroutine 为“可运行”，并迅速返回，以避免与 Go 运行时的敏感操作（如上下文切换）发生冲突，从而避免死锁。 Biscuit操作系统与此相反，系统调用和用户态故障处理程序可以执行任意的 Go 代码，因为它们是在用户线程相关联的 goroutine 上执行的，不受设备中断处理程序那样的限制。 Biscuit 实现了从硬件到软件的完整网络通信堆栈： TCP/IP 协议栈：用 Go 语言实现了支持 TCP 和 IP 协议的网络协议栈，负责处理网络中的数据传输、连接管理和错误处理等功能。 网络硬件驱动：实现了支持 Intel PCI-Express 以太网网卡的驱动，使用 DMA 来加速数据传输，使用 MSI 中断来提高网络事件处理的效率。 POSIX 套接字接口：提供了标准化的系统调用接口，使得应用程序可以方便地通过熟悉的 POSIX 套接字 API 进行网络编程。 Biscuit 不支持交换或页出，这意味着它的内存管理相对简单，所有数据必须保留在物理内存中，无法使用磁盘来扩展内存空间。 Biscuit 不支持反向映射，因此它无法有效地回收映射的页面，在内存管理上有所限制。 Biscuit 缺乏许多现代操作系统的安全功能，包括用户管理、访问控制列表和地址空间随机化，这使得它在安全性方面不如其他操作系统健全。 Biscuit 的物理内存分配是通过一个独立的分配器进行的，不依赖于 Go 堆内存管理。 由于物理内存资源有限，页面分配可能会失败，因此内核代码需要检查分配结果，并在分配失败时采取措施（如返回错误）。 Biscuit 内核堆有固定大小，称为 M，用于管理系统调用的内存。 在每次系统调用执行前，内核必须为系统调用保留足够的内存（s），以保证调用能够成功运行。 即使堆内存紧张，Biscuit 会调用垃圾回收器回收超出部分的内存，确保系统调用可以继续执行。 保守估算：由于堆中存活数据的精确大小无法实时获取，Biscuit 使用三个计数器（g、c、n）来保守估算内存使用量，并通过 L = g + c + n 进行判断。 内存不足时的行为：如果内存不足，Biscuit 会唤醒 killer 线程，执行垃圾回收或杀死进程以释放内存，确保系统调用有足够的资源继续执行。 Biscuit 的预留内存机制通过确保每个系统调用在执行前已经预留了足够的内存，避免了系统调用在运行过程中遇到内存不足的问题。这消除了内存资源不足引发的阻塞或循环等待情况，因此系统调用在该机制下无需担心死锁。如果内存不足以启动系统调用，Biscuit 会通过垃圾回收或进程终止来释放资源，从而保证系统的正常运行。 静态分析工具 MAXLIVE Biscuit 操作系统如何通过静态分析工具 MAXLIVE 来确定每个系统调用所需的最大内存保留量（即 s）。这个过程涉及分析 Biscuit 内核代码和所使用的 Go 包，以识别系统调用在执行过程中可能需要的最大活动内存量。让我们详细解释其工作原理。\nMAXLIVE 工具的目标是通过静态分析，计算出每个系统调用需要保留的内存大小 s。这对于确保系统调用能够成功执行，并防止内存不足导致的错误至关重要。\n静态分析的核心挑战\n“The core challenge is detecting statically when allocated memory can no longer be live”\n核心挑战：主要难点在于静态分析中检测出已分配的内存何时不再存活（即不再被使用），这是确定 s 的关键。许多系统调用会为短暂的用途分配内存（如临时变量或数据缓冲区），在某些时候这些内存不再需要，因此可以释放。但静态分析需要精确判断这些时机，确保计算出的 s 是足够保守但不过度的。 其他挑战： 循环：系统调用中的循环可能具有非固定的边界，导致内存分配量难以确定。静态分析必须能够处理这种不确定性，确保保守估计 s。 后台活动：有些内核活动与特定的系统调用无关（如后台任务），但它们仍然可能消耗内存。静态分析工具需要考虑这些活动对内存的影响。 通过事件驱动结构简化分析\n“We address these challenges by exploiting the characteristic event-handler-style structure of most kernel code”\nBiscuit 内核代码大多是事件驱动的，系统调用和内核任务通常只进行少量的工作，然后返回或进入空闲状态。这种结构化的、分阶段的工作模式使得内存的生命周期更加清晰，从而简化了静态分析的复杂度。 系统调用的结构化实现：大多数系统调用都是按照特定的顺序执行，分配内存、使用内存，然后释放内存。由于系统调用在执行过程中通常不涉及复杂的递归或长时间的内存保留，静态分析可以利用这些规律更准确地计算 s。 修改内核代码以适应静态分析\n“We are willing to change the kernel code to make it amenable to the reservation approach”\n为了适应内存保留方法并简化静态分析，开发者对内核代码进行了调整。特别是，他们避免了递归等复杂的内存管理模式，改写了一些函数，使代码结构更加适合分析工具的静态检测。 修改 Go 标准库：为了使静态分析工具更准确，Biscuit 还对 Go 的一些标准包进行了修改（如 time 和 fmt 包）。这些包可能涉及到复杂的内存分配模式或内存保留，修改它们可以让静态分析工具更容易理解这些包的内存行为，并正确估计内存使用量。 如何进行静态分析\nMAXLIVE 的静态分析流程大致如下：\n分析内存分配点：工具会扫描代码中的内存分配点（如变量声明、动态分配等），并跟踪这些内存分配的生命周期。 检测内存释放点：工具需要确定何时分配的内存不再使用，找到内存的释放点。 估算最大活跃内存：基于事件驱动的代码结构，工具能够估算每个系统调用在执行过程中同时活跃的最大内存量 s。 处理循环和递归：对于循环和递归，工具使用保守估计的方法，确保即使在复杂情况下，也能计算出一个安全的 s。 总的来说，MAXLIVE 是一种静态分析工具，用于计算每个系统调用执行过程中所需的最大内存保留量 s。它通过分析 Biscuit 内核代码和 Go 包，利用内核代码的事件驱动结构来简化分析过程。遇到循环、递归和后台任务等复杂情况时，工具采用保守估计。为了确保工具能够准确分析内存行为，开发者还修改了部分内核代码和 Go 标准包。这种分析方法确保了在系统调用执行时有足够的内存可用，避免了内存不足导致的系统调用失败。\nMAXLIVE 的基本操作原理 分析调用图（Call Graph）\n“MAXLIVE examines the call graph (using Go’s ssa and callgraph packages) to detect all allocations a system call may perform”\n调用图：调用图是一个描述程序中函数调用关系的图形结构。通过分析调用图，MAXLIVE 能够了解系统调用在执行过程中可能触发的所有函数调用，并在这些函数中检测内存分配行为。 Go 的 SSA 和 callgraph 包：MAXLIVE 使用 Go 的 SSA（静态单赋值，Static Single Assignment） 和 callgraph 包 来构建和分析系统调用的调用图。这帮助它检测出系统调用期间的所有潜在内存分配点。 逃逸和指针分析\n“It uses escape and pointer analysis (Go’s pointer package) to detect when an allocation does not “escape” above a certain point in the call graph”\n逃逸分析（escape analysis）和指针分析（pointer analysis）是静态分析的重要技术，MAXLIVE 使用 Go 的 pointer 包 来实现这些功能： 逃逸分析：用于确定内存分配是否会在调用图中的某个函数之外继续存在。如果分配的内存不会逃逸（即仅在局部函数中使用并在该函数返回时失效），那么可以认为内存在函数返回时已经死亡。 指针分析：用于追踪指针引用，确保分析可以精确地确定内存的使用情况。 逃逸的意义：如果某段内存分配不会逃逸出当前函数或调用点，MAXLIVE 就能够判断这块内存在函数返回时已经死亡，并不需要计入最大内存保留量 s。 特殊处理的内存分配类型\nMAXLIVE 对一些特定类型的内存分配进行了特殊处理：\na. go 关键字（创建 goroutine）\n“go (which creates a goroutine) is treated as an escaping allocation of the maximum kernel stack size”\ngoroutine 创建：在 Go 语言中，go 关键字用于启动一个新的 goroutine，这会创建一个独立的内核栈。MAXLIVE 将这种分配视为逃逸分配，因为新的 goroutine 会在不同的执行上下文中运行。 内核栈保留：当一个 goroutine 创建时，必须为其保留足够的内存，就像它是一个新的系统调用一样。因此，MAXLIVE 在分析 go 关键字时，会考虑它的最大栈空间大小并将其计入内存保留。 b. defer 关键字\n“defer is a non-escaping allocation, but is not represented by an object in the SSA so MAXLIVE specifically considers it an allocation”\ndefer：Go 中的 defer 关键字用于推迟执行某些操作，通常是在函数返回之前执行的清理操作。虽然 defer 不涉及逃逸分配（它的生命周期限定在函数内部），但它并不会在 SSA 中明确表示为对象。因此，MAXLIVE 需要特殊处理 defer，并将其视为一种分配行为。 c. maps 和 slices\n“Every insertion into a map or slice could double its allocated size; MAXLIVE generally doesn’t know the old size, so it cannot predict how much memory would be allocated”\nmaps 和 slices：在 Go 语言中，maps 和 slices 是动态数据结构，它们在需要扩展时可能会分配更多的内存。每次插入到 map 或 slice 时，分配的内存可能会翻倍。 问题：MAXLIVE 无法准确预测 map 和 slice 的初始大小或当前已分配的内存大小，因此它无法直接估计每次插入或扩展会消耗多少内存。 解决方案：为了解决这个问题，Biscuit 的代码中手动添加了 70 个注解，明确声明 maps 和 slices 的最大可能大小。通过这些注解，MAXLIVE 能够在静态分析时更精确地估算这些数据结构的内存消耗。 总的来说\nMAXLIVE 静态分析工具 通过分析调用图来检测系统调用期间的所有内存分配，并使用逃逸分析和指针分析来判断哪些分配在函数返回时会失效（不再存活）。 特殊处理：MAXLIVE 对某些内存分配行为进行了特殊处理，如 go 关键字启动的 goroutine、defer 操作，以及动态数据结构 maps 和 slices。对于后者，由于无法准确估算内存增长，Biscuit 的代码中添加了注解，帮助 MAXLIVE 估计最大可能的内存分配量。 这一静态分析工具的目的是为了准确计算系统调用需要保留的内存 s，从而确保系统调用能够在有限的内存中成功执行。 Biscuit 系统在应对 内存耗尽 问题时的局限性 内存耗尽与垃圾回收：Biscuit的设计要求垃圾收集器在内存几乎耗尽的情况下也能成功运行。然而，Go的垃圾收集器在运行时，可能需要额外分配内存，尤其是用于追踪未处理指针的工作栈（work stack）。这是垃圾回收的一个潜在问题：即使内存紧张，垃圾收集器仍可能需要分配内存来继续进行扫描。 解决方案的潜在改进：虽然目前Biscuit还没有实现解决这一问题的机制，但理论上它可以通过检测工作栈是否已满来应对这个问题。当工作栈满了时，可以回退到使用“标记位图”（mark bitmap）作为工作栈，通过扫描已经被标记的对象，并寻找那些包含未标记指针的对象。这种策略虽然可以确保垃圾收集完成，但效率会较低。好在这种情况预计比较少见，因为工作栈缓冲区可以事先以较低的成本进行预分配。在实验中，垃圾收集器最多只会为工作栈分配 0.8% 的堆内存。 内存碎片问题：Go的垃圾收集器不会移动对象（即不进行内存压缩），这意味着它不能减少内存碎片。因此，即便系统有足够的空闲内存，如果这些内存被分割成了太小的片段，仍然无法满足较大块的内存分配需求。 解决碎片问题的潜在方法：为了解决碎片问题，系统应该在每个系统调用期间为不同大小类别的对象计算其最大存活量（MAXLIVE），以确保在调用时能预留足够的内存。但目前的实现还没有做到这一点。 Biscuit 内核 的实现细节 内核的编程语言： Biscuit 内核几乎全部使用 Go 语言编写，有 27,583 行 Go 代码，1,546 行汇编代码，没有使用 C 语言。这个点强调了 Biscuit 的与众不同之处：大部分内核是用 Go 这种高级语言实现的，而不是通常的 C。 系统调用的支持： Biscuit 提供了 58 个系统调用（如图 4 所示），这些系统调用是操作系统与应用程序交互的重要接口。 它还具备一定的 POSIX 兼容性，这意味着它可以运行一些现有的服务器程序，比如 NGINX 和 Redis，这表明它有能力支持主流的网络服务应用程序。 设备驱动的实现： Biscuit 包含了针对 AHCI SATA 磁盘控制器 和 Intel 82599 系列的以太网控制器（比如 X540 10G 网卡）的驱动程序。 这两个驱动程序使用 DMA（直接内存访问） 来加速数据传输。驱动程序使用 Go 语言的 unsafe.Pointer 来访问设备寄存器和硬件定义的内存结构（如 DMA 描述符）。Go 的 atomic 包则用来控制访问顺序，以确保多线程环境下的内存一致性。 如果 Go 支持某种 内存屏障（memory fence），代码会更加简洁。内存屏障是一种用于保证内存访问顺序的机制，对于硬件寄存器的安全访问非常重要。 Go 中的 \u0026ldquo;unsafe\u0026rdquo; 使用： Biscuit 使用了 90 处 Go 中的 \u0026ldquo;unsafe\u0026rdquo; 例程（不包括 Go 运行时本身的使用）。这些 \u0026ldquo;unsafe\u0026rdquo; 操作通常是为了直接访问底层硬件，而这些操作无法通过 Go 的安全内存模型来完成。 这些不安全操作包括：解析和格式化网络数据包、在物理页号与指针之间进行转换、读写用户内存以及访问硬件寄存器等。这类操作在操作系统中是必须的，因为内核需要直接与硬件交互，而这些操作无法完全通过 Go 语言的安全机制来实现。 内存屏障 (Memory Fence) 内存屏障是一种用于在多核或多线程环境中保证内存访问顺序的机制。它确保某些特定的内存操作在内存屏障之前或之后的顺序是被严格遵守的。通常，在现代处理器中，出于性能优化的考虑，内存读写操作可以被乱序执行。但在某些场景下，乱序可能导致数据一致性问题，尤其是在多线程或多核程序中。因此，需要使用内存屏障来确保对内存的访问按照预期顺序发生。\n假设有两个线程 Thread A 和 Thread B，它们共享一个变量 x 和一个标志位 flag。Thread A 会先写入变量 x，然后设置标志位 flag，而 Thread B 会根据 flag 的值来判断是否读取 x。\nThread A:\n1 2 3 x = 42; // 写入数据 memory_fence(); // 内存屏障 flag = 1; // 设置标志位 Thread B:\n1 2 3 4 if (flag == 1) { // 确保读取的是最新的 x 值 printf(\u0026#34;%d\u0026#34;, x); } 在 Thread A 中，如果没有内存屏障，处理器可能会为了优化性能，先设置 flag 然后再写入 x，这就会导致 Thread B 在读取 x 时得不到最新的值。通过插入内存屏障，可以确保写入 x 的操作在设置 flag 之前执行，从而保证数据的一致性。\nDMA（直接内存访问，Direct Memory Access） DMA（Direct Memory Access） 是一种让设备直接与内存交互而无需经过 CPU 的技术。通常情况下，设备与内存之间的数据传输需要通过 CPU 进行调度，这样会占用大量 CPU 的时间。而使用 DMA，设备可以自己控制数据传输，CPU 只需启动传输，然后继续处理其他任务，等数据传输完成后再处理通知。\n假设有一个计算机系统，连接了一个硬盘驱动器和一个内存模块。如果使用传统方式，数据从硬盘读入内存需要经过 CPU，CPU 需要把每个字节的数据从硬盘拷贝到内存，这样 CPU 的资源会被大量占用。\n而如果采用 DMA，硬盘驱动器可以直接与内存通信，数据从硬盘被直接传输到内存，而不需要 CPU 逐字节进行拷贝。CPU 只需要在传输开始时配置 DMA 控制器，并在传输结束后收到通知。这样可以大大提高系统性能，因为 CPU 可以在传输数据时继续执行其他任务。\n例子中的流程：\nCPU 初始化 DMA 控制器，告诉它将硬盘上的数据块直接传输到内存的某个区域。 DMA 控制器直接从硬盘读取数据，并写入内存，CPU 不再干预。 传输完成后，DMA 控制器向 CPU 发送中断信号，通知 CPU 数据已写入内存。 这种方式极大提高了数据传输效率，特别适合大规模数据传输，比如磁盘到内存的拷贝，网络数据包的处理等。\nBiscuit 与 Go 运行时环境 的协同工作 对 Go 运行时的修改： 记录 goroutine 的内存分配：Biscuit 修改了 Go 运行时来记录每个 goroutine 的堆内存分配量（heap reservations）。这是为了监控各个 goroutine 在内核中的内存使用情况。 检测设备处理器 goroutine：Biscuit 的改动还允许检查是否有等待运行的设备处理 goroutine，以便及时处理设备请求。 增加默认栈大小：Biscuit 将 Go 运行时的默认栈大小从 2KB 增加到了 8KB，以避免在处理一些常见的系统调用时频繁触发栈扩展操作。这样可以提高系统调用的效率。 与 Go 运行时保持兼容的部分： Go 运行时在持有锁时不关闭中断：Go 运行时在持有锁或操作 goroutine 私有状态时不会关闭中断。这意味着内核中断不能直接与持有锁的代码同时操作，以避免潜在的死锁问题。 Biscuit 的解决方案：Biscuit 采取了一个较简单的策略：当处理设备中断时，中断处理器并不直接操作设备，而只是设置一个标志，表示应该唤醒对应的设备处理 goroutine。这样，实际的设备处理会由 goroutine 完成，避免了死锁的风险。 处理定时器中断和上下文切换： 定时器中断与 goroutine 上下文切换：Biscuit 的定时器中断处理器无法直接强制 goroutine 进行上下文切换（context switch），因为 Go 运行时可能正处于切换上下文的中间状态。这种情况下，如果强制切换可能会导致不一致或错误的状态。 使用 Go 的抢占机制：Biscuit 依赖 Go 编译器插入的抢占检查（pre-emption checks）来完成对内核 goroutine 的上下文切换。Go 编译器会在生成的代码中插入这些检查点，以确保 goroutine 可以被适时抢占。 用户态的上下文切换：对于从用户态发出的定时器中断，Biscuit 可以强制上下文切换，因为用户态与内核态的上下文切换是可以安全进行的。 Biscuit 通过修改 Go 运行时以跟踪内存分配和调整栈大小，确保内核 goroutine 的高效运行。为了避免中断与锁竞争导致的死锁，Biscuit 采用了一种间接的方式处理设备中断。对于上下文切换，它依赖 Go 的抢占机制来确保 goroutine 的正常调度。\nBiscuit 内核与 Go 运行时 的集成方式 Goroutine 调度和上下文切换：\n调度决策在运行时环境中：Biscuit 不直接控制 goroutine 的调度决策，因为这些决策是由 Go 运行时管理的。这意味着 Biscuit 继承了 Go 运行时的调度策略，而无法对其进行修改或优化。\n没有切换进程页表：由于 goroutine 调度和上下文切换是由 Go 运行时处理的，Biscuit 并不会在 goroutine 切换时切换每个进程的页表。因此，Biscuit 的系统调用代码不能直接使用用户空间地址。相反，Biscuit 需要显式地将用户的虚拟地址转换为物理地址，并检查相应的页权限。\n必要时切换页表：在从内核态切换到用户态之前，Biscuit 会在必要时切换页表，以确保内核和用户态之间的内存隔离。\n垃圾回收相关的改动：\n关闭垃圾收集器专用线程：为了减少由于垃圾回收导致的延迟，Biscuit 关闭了专用的垃圾收集器线程。这样做的目的是让应用线程与垃圾收集线程不再竞争 CPU 资源，避免性能下降。\n根标记提供分配信用：在垃圾回收的标记阶段，Biscuit 改进了根标记（root marking）操作，给予内存分配线程一定的“分配信用”，以免某个线程在执行标记时遇到过多的根标记工作，导致延迟。\n减少标记阶段大对象的分块大小：Biscuit 将大对象在标记过程中分割的块大小从 128kB 减小到 10kB。这可以让标记工作更平滑地进行，减少大对象标记时的开销。\n内核性能优化：\n使用大页映射内核代码：Biscuit 内核的文本段（代码部分）使用了**大页（large pages）**映射，这样可以减少 iTLB（指令翻译后备缓冲器）的缺失次数，从而提高指令读取的效率。\n使用每个 CPU 独立的网卡传输队列：Biscuit 对每个 CPU 使用独立的网卡（NIC）传输队列，从而提高网络传输的效率，避免多个 CPU 共享传输队列带来的竞争。\n无读锁的数据结构：在一些性能关键的代码中（例如目录缓存和 TCP 轮询），Biscuit 使用了无读锁的数据结构。无读锁的结构在并发读操作时性能更好，因为多个线程可以同时读取而不需要锁的开销。\nGo 对优化的影响：\nGo 并未妨碍优化：总体上，Biscuit 的开发者发现，虽然 Go 是一种高级语言，但它并没有显著妨碍内核性能优化的实现。通过合理的设计和技巧，Biscuit 仍然能够实现许多标准的内核优化。 Biscuit 内核在处理调度、垃圾回收以及性能优化时的策略依赖 Go 运行时管理调度和上下文切换，并在某些方面进行了调整以适应内核需求，如手动进行地址转换和页表切换。在垃圾回收方面，Biscuit 通过关闭专用线程和优化根标记来减少回收延迟。性能优化方面，Biscuit 实现了大页映射、无锁数据结构等标准优化措施，并且 Go 语言并未阻碍这些优化的实现。\n无读锁结构（lock-free read structure） 无读锁结构（lock-free read structure）是一种在并发编程中使用的数据结构，它允许多个线程或进程同时进行读取操作而不需要加锁。这种设计可以显著提高读取操作的并发性能，因为读操作之间不会互相阻塞或等待，从而减少了锁的开销。无读锁结构通常通过使用某种形式的原子操作或版本控制来确保数据一致性。\n在传统的读写锁设计中，读操作和写操作都会使用锁来保护共享数据的访问。一旦一个线程获取了读锁，其他线程可以同时获取读锁进行读取，但写操作则需要等待所有读操作完成。而无读锁结构通过设计，允许读操作在不加锁的情况下执行，通常通过以下几种机制来实现：\n原子操作：通过原子操作（如 compare-and-swap）来确保写操作不会影响读操作的一致性。 版本控制：读操作在读取数据时会检查一个版本号，如果写操作修改了数据，它会更新版本号。读操作在完成后检查是否版本号一致，从而判断数据是否被修改。 例子 1：无锁哈希表（Lock-free Hash Table）。在无锁哈希表中，多个线程可以同时读取数据，而不需要锁住整个哈希表。写操作（插入或删除键值对）可以通过原子操作完成，比如使用 CAS（Compare-And-Swap） 来更新哈希表中的指针。例如，当一个线程想要从无锁哈希表中读取数据时，它只需要找到哈希槽并读取其中的数据。如果在读的过程中有其他线程在写数据，写线程会通过原子操作来更新哈希表指针。读线程可以在读取后检查版本号，确保数据一致。\n例子 2：RCU (Read-Copy-Update)。RCU 是一种常见的无读锁技术，用于提高读操作的效率。它的工作方式是这样的：\n当进行写操作时，RCU 会创建数据的一个副本。修改不会直接应用到现有数据，而是应用到副本上。 读操作仍然读取原来的数据，不受写操作的影响。 写操作完成后，将新的副本替换为旧的数据，所有后续的读操作会看到新的数据。 RCU 的一个典型应用是 Linux 内核中的 目录缓存（dentry cache）。多个线程可以同时读取文件系统的目录信息，而不需要锁定整个缓存，写操作则通过版本控制机制来进行同步。\n例子 3：无锁队列（Lock-free Queue）。无锁队列是一种队列数据结构，它允许多个线程在无锁的情况下并发进行入队和出队操作。无锁队列通常通过原子操作来确保并发安全。最经典的例子是 Michael \u0026amp; Scott 无锁队列：\n入队操作：入队线程会通过 CAS 原子操作来更新队列的尾部指针，将新节点插入队列的末尾。 出队操作：出队线程也通过 CAS 操作更新队列的头部指针，从队列头部取出数据。 由于入队和出队是独立的、无锁的，多个线程可以同时执行这些操作，而不会相互阻塞。\n总而言之，无读锁结构允许多个线程在不需要加锁的情况下并发进行读取操作，从而提高了读取性能和系统的整体并发性。常见的实现方式有基于原子操作的无锁队列、无锁哈希表，以及使用版本控制机制的 RCU。通过这些机制，可以减少锁的争用，提高系统在高并发场景下的性能。\n什么是poll 在操作系统中，poll 是一种用于监控文件描述符（file descriptors）状态的系统调用，通常用于网络编程和 I/O 操作。它允许程序同时等待多个文件描述符的事件，如可读、可写或发生错误。\npoll 系统调用的主要作用是让程序可以同时监视多个 I/O 通道，而不需要为每个 I/O 通道分别等待。这在多路复用（multiplexing）I/O 中非常重要，特别是在处理并发网络连接时。程序员可以用 poll 来检查哪些文件描述符已经准备好进行读、写操作或是否有错误发生，然后对那些准备好的文件描述符进行处理。\npoll 通常用于网络服务器，特别是那些需要同时处理多个连接的服务器程序。例如，服务器会监听多个客户端的网络套接字，通过 poll 确定哪个套接字有数据可读，哪个套接字可以发送数据，或者哪个套接字发生了错误。其工作原理为：\n程序向操作系统传递一个文件描述符列表，每个文件描述符都关联了需要监听的事件（如可读、可写、错误等）。 poll 将阻塞进程，直到其中至少有一个文件描述符准备好处理（或指定的超时时间到达）。 一旦返回，程序可以根据 poll 返回的结果，处理对应的文件描述符。 假设你有多个客户端连接到服务器，服务器可以使用 poll 来检查哪些客户端有新的数据要读取：\n1 2 3 4 5 6 7 8 9 10 11 12 struct pollfd fds[2]; // 创建两个文件描述符的数组 fds[0].fd = socket_fd; // 监听的套接字文件描述符 fds[0].events = POLLIN; // 监听可读事件 fds[1].fd = another_fd; // 另一个文件描述符 fds[1].events = POLLOUT; // 监听可写事件 int ret = poll(fds, 2, timeout); // 调用 poll，等待事件 if (fds[0].revents \u0026amp; POLLIN) { // 如果 fds[0] 的文件描述符准备好读取数据 read(socket_fd, buffer, length); } poll 通过提供一种机制，让程序可以同时监视多个 I/O 通道的状态，而不会被阻塞在单个通道上。这对于处理高并发的网络应用程序至关重要。\nGo 语言的高级语言特性（HLL）对 Biscuit 内核性能的影响 性能成本分析： 这部分分析了 Go 的高级语言特性在 Biscuit 内核中的性能成本，并展示了三个应用的基准测试结果（见图 7）。 Tput（吞吐量）列：显示应用每秒处理的请求数量，表示系统处理任务的速度。 Kernel time（内核时间）列：显示应用在内核中执行的时间比例（而不是在用户空间）。结果表明，这些基准测试主要由内核活动主导，并且在测试中所有四个 CPU 核心都被 100% 占用。 内核堆内存和垃圾回收： 应用程序在运行时会在 Biscuit 内核堆中产生18 到 48 MB 的活动数据。 它们快速分配了大量的临时对象，导致每次基准测试运行中触发了数十次垃圾回收（GC）。 这些垃圾回收占用了系统1% 到 3% 的总 CPU 时间。 编译器生成的代码分析： Prologue cycles（函数前置代码）：这一项衡量由编译器生成的代码在每个函数开始时消耗的时间，用于检查栈是否需要扩展，以及垃圾收集器是否需要暂停整个系统。这部分代码是所有高级语言特性中最耗时的。 WB cycles（写屏障周期）：用于处理对象在并发垃圾回收期间被修改时，编译器生成的写屏障代码的执行时间。 运行时安全检查成本： Safety cycles（安全周期）：报告了用于运行时检查的成本，如 nil 指针检查、数组和切片边界检查、除零检查、以及动态类型转换检查。这些检查是由编译器自动生成的，贯穿整个代码。 通过一个工具分析这些检查在 Biscuit 二进制文件中的位置，并与 CPU 时间消耗交叉引用，以确定它们的成本。 内存分配时间： Alloc cycles（分配周期）：衡量 Go 内存分配器花费的时间，这包括检查自由列表以满足分配请求，但不包括并发垃圾回收的工作。虽然内存分配并不是高级语言独有的任务，但一些 C 语言编写的内核使用定制的分配器来优化分配过程。 结果总结： 基于图 7 的数据显示，函数前置代码（Prologue cycles） 是使用高级语言特性时最昂贵的部分。垃圾回收成本虽然明显，但并不是最大的性能瓶颈。 另一方面，随着活动数据量的增加，垃圾回收的成本会增长，这在 §8.6 进行了进一步讨论。同时，函数前置代码的开销似乎有可能通过优化进一步减少。 虽然Go 语言高级特性对 Biscuit 内核性能的影响，尤其是函数前置代码、垃圾回收、写屏障和安全检查的性能开销带来了一定的性能损耗，但其中大部分问题可以通过进一步优化来减轻。\n正课部分 C语言实现操作系统的优劣势 今天我们要讨论使用高级编程语言实现操作系统的优点和代价。今天课程的内容主要是我们之前写的一篇论文 ，论文作者包括了Robert和我，以及一个主要作者Cody Cutler，他是这门课程的一个TA。\n我通常不会讨论我们自己写的论文，但是因为这篇论文与课程内容相关，所以它在这。今天我们会用一些PPT，而不是在白板上讲解这节课。\n这篇论文的起因是这样一个问题：你应该用什么样的编程语言来实现操作系统内核？\n这个问题很多同学都问过，可能是因为你发现了在操作系统中有Bug，然后你会想，如果我使用一种其他的编程语言，或许我就不会有这些Bug了，所以这是一个经常出现的问题。\n虽然这也是操作系统社区里的一个热烈争论的问题，但是并没有很多事实来支撑这里的讨论。在课程结束的时候，我们其实也不能对这个问题有个干脆的回答。\n但是至少这篇论文贡献了一些数据使得你可以对应该使用什么样的编程语言来实现内核这个话题，有一些更深入的讨论。这是这篇论文的出发点。\n为了能回答上面的问题，我们使用了一个带自动内存管理的编程语言写了一个新的内核，所以现在内核带了Garbage Collector，你不用调用free来释放内存，这样就可以避免一类Bug。\nGC是高级编程语言通常都带有的属性，我们选择了一种带GC的编程语言，并且实现过程中我们遵循了传统的monolithic UNIX风格，这样我们才能做出公平的对比。\n实际上，从某些角度你可以认为我们创建的是类似于XV6的内核，但是拥有了更多的特性和更高的性能。你知道的，XV6中里存在各种O(n^2)算法和O(n)的查找，如果你想要写一个高性能的操作系统，你不能有这样的实现。\n以上就是论文的起因，以及我们创建Biscuit（也就是上面提到的操作系统）的原因，我们想要回答上面的问题，或者至少给出一些启发。\n今天这节课首先我要讨论一些通用的背景，之后我们会深入到Biscuit的细节中。\n现在很多操作系统都是用C实现的，你知道的，XV6是用C写的，一些更流行的运行在你的电脑、手机的操作系统，例如Windows，Linux，以及各种形式的BSD都是用C写的。\n为什么它们都是用C实现的呢？\n首先C提供了大量的控制能力，从我们的实验中你可以看到，C可以完全控制内存分配和释放 C语言几乎没有隐藏的代码，你几乎可以在阅读C代码的时候想象到对应的RISC-V机器指令是什么 通过C可以有直接内存访问能力，你可以读写PTE的bit位或者是设备的寄存器 使用C会有极少的依赖，因为你不需要一个大的程序运行时。你几乎可以直接在硬件上运行C程序。你们可以在XV6启动过程中看到这一点， 只通过几行汇编代码，你就可以运行C代码 以上就是C代码的优点，也是我们喜欢C语言的原因。但是C语言也有一些缺点。\n在过去几十年已经证明了，很难写出安全的C代码。\n这里存在各种各样的Bug，首先是最著名的buffer overrun，比如说数组越界，撑爆了Stack等等。其次是use-after-free bugs，你可能会释放一些仍然在使用的内存，之后其他人又修改了这部分内存。第三，当线程共享内存时，很难决定内存是否可以被释放。\n其中一些Bug在XV6中已经出现，其他的一些不太常见。因为XV6很少有动态内存分配，几乎所有的东西都是预分配的，所以共享内存Bug很少出现，但是buffer overrun和use-after-free bugs的确出现过。\nCVEs一个跟踪所有的安全漏洞的组织，如果你查看他们的网站，你可以发现，在2017年有40个Linux Bugs可以让攻击者完全接管机器。\n很明显，这些都是非常严重的Bugs，这些Bug是由buffer overrun和一些其他memory-safety bug引起。这就太糟糕了，如果你用C写代码，就很难能够完全正确运行。\n当然，我可以肯定你们在之前的实验中都见过了这些Bug，之前在课程论坛上的一些问题涉及了use-after-free Bug。特别是在copy-on-write lab中，这些问题出现了好几次。\n高级编程语言实现操作系统的优劣势 高级编程语言吸引人的一个原因是它提供了memory-safety，所以上一节中CVEs提到的所有Bugs，都将不再存在。\n要么当它们发生时程序运行时会检查数组是否越界，如果越界了就panic；要么高级编程语言不允许你写出引起Bug的代码，所以这些问题完全不可能出现。\n当然，高级编程语言还有一些其他的优点：\n首先是Type safety，类型安全 通过GC实现了自动的内存管理，所以free更容易了，你都不用去考虑它，GC会为你完成所有的内存释放工作 对并发更友好 有更好的抽象，接口和类等面向对象的语法使得你可以写出更加模块化的代码 高级编程语言有这么多优势，你不禁会想它有哪些缺点呢？为什么XV6或者Linux没有用Java，Golang，Python来写？\n这里的原因是高级编程语言通常有更差的性能。高级编程语言通常都有一些额外的代价，这被称为High Level Language Tax。\n比如说在索引一个数组元素时检查数据边界，比如说检查空指针，比如说类型转换。 除此之外，GC也不是没有代价的，需要花费一些时间来跟踪哪些对象可以被释放。 除了性能之外，高级编程语言与内核编程本身不兼容。\n比如说高级编程语言没有直接访问内存的能力，因为这从原则上违反了Type safety。 高级编程语言不能集成汇编语言，而在内核中的一些场景你总是需要一些汇编程序，比如说两个线程的context switching，或者系统启动 编程语言本身支持的并发与内核需要的并发并不一致，比如我们在调度线程的时候，一个线程会将锁传递给另一个线程。一些并发管理模式在用户程序中不太常见，但是在内核中会出现。 今天论文的目标是能够测量出高级编程语言的优劣势，并从safety，programmability和性能损失角度，探索使用高级编程语言而不是C语言实现内核的效果。\n当然，为了做到这一点，你需要在一个产品级的内核上做实验，而不是在XV6上。\nXV6现在是由C语言写的很慢的内核，如果你用Golang也写了个很慢的内核，这不能说明C还是Golang更快，这只能说明XV6很慢。所以，你会想要在一个为高性能而设计的内核中完成这里的测量。\n很奇怪之前并没有一个论文完成了这里的测量。有很多论文研究了在用户程序中高级编程语言的优劣势，但是你知道的，内核与用户程序还是很不一样的，比如内核中需要有更小心的内存管理，内核中的并发或许会略有不同。\n所以，现在我们想要在内核中而不是用户程序中完成分析，而我们并没有找到之前的任何论文真正做了这个工作。\n之前的确有很多内核是用高级编程语言写的，这里有很长的历史，甚至可以回溯到最早的计算机中。\n但是最近的一些基于高级编程语言的内核并不是为了评估High Level Language Tax，而是为了探索新的内核设计和新的内核架构，所以这些内核并没有在保持结构相同的同时，直接对比C语言内核。只有保持系统结构相同，你才可以真正的关注语言本身，而不是一些其他的问题。\n为什么没有很多论文做这样的对比测试？一个原因可能是这里的工作有点棘手。\n如果你想得到正确的结果，你需要与产品级别的C内核进行对比，例如Linux，Windows等等。同时，你也需要构建一个产品级别的内核。\n很明显，这对于一个小的团队来说很难，因为有许多许多的Linux开发人员日复一日做了许多许多的更新才创造了Linux，所以很难用高级编程语言实现同样的功能并构建同样的内核，所以我们这里会构建一个功能稍微少的系统内核。\n所以我们能做到的最好情况是：\n用高级编程语言构建内核 保留与Linux中最重要的部分对等的功能 优化性能使得其与Linux基本接近，即使这里的功能与Linux并不完全一致，但是我们至少可以将它们拉到一个范围内 最后我们就可以测量高级编程语言的优劣 当然，这种方法的风险在于我们构建的内核与Linux还是略有不同，它不会与Linux完全一样，所以在得出结论时需要非常小心。\n这就是为什么不能对论文提出的问题（注，也就是应该使用什么样的编程语言实现操作系统）给出一个十分清晰的答案的原因。尽管如此，我们还是可以期望更深入的了解这个问题，而不是完全不知道它的内容。\n以上就是论文的背景，以及为什么很少有人会做同样的工作的原因。\n高级编程语言选择 \u0026mdash; Golang 接下来我们看一下对比方法，图中左边是Biscuit，这是一个我们为了论文 专门用Golang写的内核，它以大概类似的方式提供了Linux中系统调用的子集。Biscuit和Linux的系统调用有相同的参数和相同的调用方式。\n并且我们在内核之上运行的是相同的应用程序，这里的应用程序是NGINX，这是一个web server，这里我们将相同的应用程序分别运行在Biscuit和Linux之上，应用程序会执行相同的系统调用，并传入完全相同的参数，Biscuit和Linux都会完成涉及系统调用的相同操作。\n之后，我们就可以研究高级编程语言内核和Linux之间的区别，并讨论优劣势是什么。以上就是对比方法的核心。\n因为Linux和Biscuit并不完全一样，它们会有一些差异，所以我们花费了大量的时间来使得这里的对比尽可能得公平。\n有很多同学可能会问，这里会使用什么样的高级编程语言呢？基于以下原因，我们选用了Golang。\n这是一个静态编译的编程语言，和Python不同，这里没有解释器。我们喜欢静态编译的语言的原因是编译语言性能通常更好，实际上Go编译器就非常好，所以基本上来说这是一种高性能编程语言。 另外，Golang被设计成适合系统编程，而内核就是一种系统编程所以Golang也符合这里的场景。例如： Golang非常容易调用汇编代码，或者其他的外部代码 Golang能很好地支持并发 Golang非常的灵活 另一个原因是Golang带有Garbage Collector。使用高级编程语言的一个优点就是你不需要管理内存，而GC是内存管理的核心。 在我们开始写论文的时候，Rust并不十分流行，并且也不是十分成熟和稳定。\n但是如果你现在再做相同的事情，你或许会想要用Rust来实现。因为Rust也是为系统编程而设计，它有一个小的运行时，它能生成好的代码。不过Rust相比Golang还有一个缺点，Rust认为高性能程序不能有GC，所以Rust不带GC。\n实际上Rust的类型系统以一种非常聪明且有趣的方式实现，所以GC对于Rust并不是必须的。\n这里涉及到一个有趣的问题：通过高级编程语言实现内核时，GC的代价到底有多少？而Rust通过不使用GC而跳过了这个问题。\n学生提问：如果我们这里使用Rust而不是Golang来实现高级编程语言内核，通过一定的优化有没有可能达到比C内核更高的性能？\nFrans教授：因为我们没有做过这样的实验，所以我就猜一下。我觉得不会有比C内核更高的性能，但是基本在同一个范围内。因为C是如此的底层，你可以假设你在Rust做的工作，都可以在C中完成。\n这里有一个问题，并且在这节课最后我们会再次回顾这个问题。\n我们想要使用高级编程语言内核的部分原因是为了避免一类特定的Bug，那么你可以问自己的一个问题的是，你们在实验中遇到的Bug，是否可以通过使用高级编程语言来避免？\n我肯定你可以回想起一些Bug，它们耗费了你很多的时间，很多精力，现在你可以问自己，如果实验中的XV6是使用某种高级编程语言实现的，你的生活会不会更轻松一些？你是否能有更多时间做一些其他的事情。\n让我们记住这个问题，并在这节课结束的时候再看这个问题。\nBiscuit 接下来我将对Biscuit稍作介绍，包括了Biscuit是如何工作的，以及在实现中遇到的问题。其中有些问题是预期内的，有些问题不在预期之内。\n就像Linux和XV6一样，Biscuit是经典的monolithic kernel。\n所以它也有用户空间和内核空间，用户空间程序可能是你的编译器gcc，或者论文中主要用到的webserver。\n这里用户空间程序主要用C实现，尽管原则上它可以是任何编程语言实现的，但是因为这里只是性能测试，我们这里统一选用的是C版本的应用程序。\n大部分用户程序都是多线程的，所以不像在XV6中每个用户程序只有一个线程，在Biscuit中支持用户空间的多线程。\n基本上，对于每个用户空间线程，都有一个对应的位于内核的内核线程，这些内核线程是用Golang实现的，在Golang里面被称为goroutine。你可以认为goroutine就是普通的线程，就像XV6内核里的线程一样。\n区别在于，XV6中线程是由内核实现的，而这里的goroutine是由Go runtime提供。\n所以Go runtime调度了goroutine，Go runtime支持sleep/wakeup/conditional variable和同步机制以及许多其他特性，所以这些特性可以直接使用而不需要Biscuit再实现一遍。\nBiscuit中的Go runtime直接运行在硬件上，稍后我将介绍更多这部分内容，但是你现在可以认为当机器启动之后，就会启动Go runtime。这里会稍微复杂，因为Go runtime通常是作为用户空间程序运行在用户空间，并且依赖内核提供服务，比如说为自己的heap向内核申请内存。\n所以Biscuit提供了一个中间层，使得即使Go runtime运行在裸机之上，它也认为自己运行在操作系统之上，这样才能让Go runtime启动起来。\nBiscuit内核本身与XV6非常相似，除了它更加的复杂，性能更高。它有虚拟内存系统可以实现mmap，有更高性能的文件系统，有一些设备驱动，比如磁盘驱动，以及网络协议栈。\n所以Biscuit比XV6更加完整，它有58个系统调用，而XV6只有大概18-19个系统调用；它有28000行代码，而XV6我认为只有少于10000行代码。所以Biscuit有更多的功能。\n学生提问：这里的接口与XV6类似对吧，所以进程需要存数据在寄存器中，进程也会调用ECALL。\nFrans教授：我稍后会再做介绍，但是这里完全相同。\n以上是Biscuit的特性，有些我已经提到过了。\n首先它支持多核CPU。Golang对于并发有很好的支持，所以Biscuit也支持多核CPU。类似的，XV6却只对多核CPU有有限的支持。所以在这里，我们相比XV6有更好的同步协调机制。 它支持用户空间多线程，而XV6并没有。 它有一个相比XV6更高性能的Journaled File System（注，Journaled就是指log，可以实现Crash Recovery）。如果你还记得EXT3论文，它与EXT3的Journaled File System有点类似。 它有在合理范围内较为复杂的虚拟内存系统，使用了VMAs并且可以支持mmap和各种功能。 它有一个完整的TCP/IP栈，可以与其他的服务器通过互联网连接在一起。 它还有两个高性能的驱动，一个是Intel的10Gb网卡，以及一个非常复杂的磁盘驱动AHCI，这比virtIO磁盘驱动要复杂的多。 Biscuit支持的用户程序中：\n每个用户程序都有属于自己的Page Table。 用户空间和内核空间的内存是由硬件隔离的，也就是通过PTE的User/Kernel bit来区分。 每个用户线程都有一个对应的内核线程，这样当用户线程执行系统调用时，程序会在对应的内核线程上运行。如果系统调用阻塞了，那么同一个用户地址空间的另一个线程会被内核调度起来。 如之前提到的，内核线程是由Go runtime提供的goroutine实现的。如果你曾经用Golang写过用户空间程序，其中你使用go关键字创建了一个goroutine，这个goroutine就是Biscuit内核用来实现内核线程的goroutine。 来看一下系统调用。就像刚刚的问题一样，这里的系统调用工作方式与XV6基本一致：\n用户线程将参数保存在寄存器中，通过一些小的库函数来使用系统调用接口。 之后用户线程执行SYSENTER。现在Biscuit运行在x86而不是RISC处理器上，所以进入到系统内核的指令与RISC-V上略有不同。 但是基本与RISC-V类似，控制权现在传给了内核线程。 最后内核线程执行系统调用，并通过SYSEXIT返回到用户空间。 所以这里基本与XV6一致，这里也会构建trapframe和其他所有的内容。\n学生提问：我认为Golang更希望你使用channel而不是锁，所以这里在实现的时候会通过channel取代之前需要锁的场景吗？\nFrans教授：这是个好问题，我会稍后看这个问题，接下来我们有几页PPT会介绍我们在Biscuit中使用了Golang的什么特性，但是我们并没有使用太多的channel，大部分时候我们用的就是锁和conditional variable。\n所以某种程度上来说Biscuit与XV6的代码很像，而并没有使用channel。我们在文件系统中尝试过使用channel，但是结果并不好，相应的性能很差，所以我们切换回与XV6或者Linux类似的同步机制。\n在实现Biscuit的时候有一些挑战：\n首先，我们需要让Go runtime运行在裸机之上。\n我们希望对于runtime不做任何修改或者尽可能少的修改，这样当Go发布了新的runtime，我们就可以直接使用。在我们开发Biscuit这几年，我们升级了Go runtime好几次，所以Go runtime直接运行在裸机之上是件好事。\n并且实际上也没有非常困难。Golang的设计都非常小心的不去依赖操作系统，因为Golang想要运行在多个操作系统之上，所以它并没有依赖太多的操作系统特性，我们只需要仿真所需要的特性。\n大部分这里的特性是为了让Go runtime能够运行起来，一旦启动之后，就不太需要这些特性了。\n我们需要安排goroutine去运行不同的应用程序。\n通常在Go程序中，只有一个应用程序，而这里我们要用goroutine去运行不同的用户应用程序，这些不同的用户应用程序需要使用不同的Page Table。\n这里困难的点在于，Biscuit并不控制调度器，因为我们使用的是未经修改过的Go runtime，我们使用的是Go runtime调度器，所以在调度器中我们没法切换Page Table。\nBiscuit采用与XV6类似的方式，它会在内核空间和用户空间之间切换时更新Page Table。所以当进入和退出内核时，我们会切换Page Table。\n这意味着像XV6一样，当你需要在用户空间和内核空间之间拷贝数据时，你需要使用copy-in和copy-out函数，这个函数在XV6中也有，它们基本上就是通过软件完成Page Table的翻译工作。\n另一个挑战就是设备驱动，Golang通常运行在用户空间，所以它并不能从硬件收到中断。\n但是现在我们在裸机上使用它，所以它现在会收到中断，比如说定时器中断，网卡中断，磁盘驱动中断等等，我们需要处理这些中断。然而在Golang里面并没有一个概念说是在持有锁的时候关闭中断，因为中断并不会出现在应用程序中，所以我们在实现设备驱动的时候要稍微小心。\n我们采取的措施是在设备驱动中不做任何事情，我们不会考虑锁，我们不会分配任何内存，我们唯一做的事情是向一个非中断程序发送一个标志，之后唤醒一个goroutine来处理中断。\n在那个goroutine中，你可以使用各种各样想要的Golang特性，因为它并没有运行在中断的context中，它只是运行在一个普通goroutine的context中。\n前三个挑战我们完全预料到了，我们知道在创造Biscuit的时候需要处理它们，而最难的一个挑战却不在我们的预料之中。这就是heap耗尽的问题。所以接下来我将讨论一下heap耗尽问题，它是什么，它怎么发生的，以及我们怎么解决的？\nHeap exhaustion 让我们讨论一下Heap耗尽问题，我不会像论文 一样深入讨论，但是至少会演示问题是什么。\n假设蓝色的矩形是内核，内核会有一个heap，它会从其中申请动态内存。\n在XV6中，我们并没有这样一个heap，我们在内核中没有内存分配器，所有内存都是静态分配的。但是任何其他的内核中，都会有heap，所以你在内核中可以调用malloc和free。可能通过heap分配的对象有socket对象，文件描述符对象和进程对象。\n所以，我们在XV6中静态分配的所有结构体，例如struct proc，struct fd，在正常的内核中都是动态分配的。所以当你打开一个新的文件描述符时，内核会通过heap分配一个文件描述符对象。\n这里的问题是，你可以运行很多个应用程序，它们会打开很多个文件描述符，拥有很多个socket，它们会逐渐填满heap。\n在某个时间点，heap会被填满，这时没有额外的空间可以用来分配一个新的对象。\n如果这时应用程序需要打开一个新的文件描述符，或者调用了fork使得内核想要在heap中分配一个新的proc结构体，heap中没有了空间。这时你该怎么办呢？\n这是一个不太常见的常见问题，但是如果你使劲用你的电脑，你或许会遇到所有内存都被使用了的情况，你的heap满了，并且没有进程调用free，因为它们都还在运行且想分配到更多的内存。\n所有的内核都会遇到这个问题，不管是C内核也好，Biscuit也好，任何内核都需要解决这个问题。\n之所以这个问题对于我们来说是个严重的问题，是因为在很多内核中，你可以对malloc返回错误，实际上，XV6就是这么做的。\n但是在Go runtime中，当你调用new来分配一个Go对象，并没有error condition，new总是可以成功。让我们来讨论一些可以解决这里问题的方法。\n第一种方法我们在XV6中见过。如果XV6不能找到一个空闲的block cache来保存disk block，它会直接panic。这明显不是一个理想的解决方案。这并不是一个实际的解决方案，所以我们称之为strawman。\n另一个strawman方法是，当你在申请一块新的内存时，你会调用alloc或者new来分配内存，你实际上可以在内存分配器中进行等待。这实际上也不是一个好的方案，原因是你可能会有死锁。\n假设内核有把大锁，当你调用malloc，因为没有空闲内存你会在内存分配器中等待，那么这时其他进程都不能运行了。因为当下一个进程想要释放一些内存时，但是因为死锁也不能释放。\n对于内核中有大锁的情况，这里明显有问题，但是即使你的锁很小，也很容易陷入到这种情况：在内存分配器中等待的进程持有了其他进程需要释放内存的锁，这就会导致死锁的问题。\n下一个strawman方法是，如果没有内存了就返回空指针，你检查如果是空指针就直接失败，这被称为bail out。\n但是bail out并不是那么直观，进程或许已经申请了一些内存，那么你需要删除它们，你或许做了一部分磁盘操作，比如说你在一个多步的文件系统操作中间，你只做了其中的一部分，你需要回退。所以实际中非常难做对。\n当研究这部分，并尝试解决这个问题，Linux使用了前面两种方法，但是两种方法都有问题。实际中，内核开发人员很难将这里弄清楚。\n如果你对这个问题和相关的讨论感兴趣，可以Google搜索“too small to fail ”，会有一篇小的文章讨论释放内存，在内存分配器中等待的复杂性。\n对于Biscuit来说，strawman 2解决方案不可能实施，因为new不会fail，它总是能成功。除此之外，这里的方案都不理想，所以我们需要有一种更好的方法。\nHeap exhaustion solution Biscuit的解决方案非常直观，当应用程序执行系统调用，例如read，fork时，在系统调用的最开始，跳转到内核之前，它会先调用reserve函数，reserve函数会保留足够的内存以运行系统调用。\n所以reserve会保留足够这个系统调用使用的空闲内存，以使得系统调用总是能成功。所以一旦系统调用被执行，且保留了足够的内存，那么它就可以一直运行而不会有内存耗尽和heap exhaustion的问题。\n如果reserve函数执行时没有足够的内存，那么程序会在这里等待。\n因为现在在系统调用的最开始，系统调用现在还没有持有任何的锁，也没有持有任何的资源，所以在这里等待完全没有问题，也不会有死锁的风险。当程序在等待的时候，内核可以撤回cache并尝试在heap增加空闲空间，比如说kill一个进程来迫使释放一些内存。\n一旦内存够用了，并且内核决定说是可以满足需要保留的内存，之后内核会让系统调用继续运行，然后执行系统调用需要的操作。在最后，当系统调用完成的时候，所有之前保留的内存都返回到池子中，这样后续的系统调用可以继续使用。\n这个方案中有一些很好的特性：\n在内核中没有检查。你不需要检查内存分配是否会失败，在我们的例子中这尤其得好，因为在Golang中内存分配不可能会失败。 这里没有error handling代码。 这里没有死锁的可能，因为你在最开始还没有持有锁的时候，就避免了程序继续执行。 当然，现在的问题是如何实现reserve函数，你如何计算运行一个系统调用会需要多少内存？\n你保留的内存数量是重要的，你可以为每个系统调用保留一半的内存或者一些其他夸张的内存数量。但是这意味着你限制了可以并发执行的系统调用的个数，所以你这里尽量精确地计算一个系统调用的内存边界。\n这里的解决方法是使用了高级编程语言的特性。Golang实际上非常容易做静态分析，Go runtime和Go生态里面有很多包可以用来分析代码，我们使用这些包来计算系统调用所需要的内存。\n所以你可以想象，如果你有一个read系统调用，我们可以通过系统调用的函数调用图查看比如函数f调用函数g调用函数h等等等等。我们可以做的是弄清楚这里调用的最大深度，对于最大的深度，计算这里每个函数需要的内存是多少。\n比如说函数f调用了new，因为这是一个高级编程语言，我们知道new的对象类型，所以我们可以计算对象的大小。我们将所有的new所需要的内存加起来，得到了一个总和S，这就是这个调用图（或者说系统调用）任何时间可能需要的最大内存。\n实际中并没有这么简单，会有点棘手。因为函数h可能会申请了一些内存，然后再回传给函数g。所以当h返回时，g会得到h申请的一些内存。这被称为escaping，内存从h函数escape到了函数g。\n存在一些标准算法来完成这里的escape分析，以决定哪些变量escape到了函数调用者。当发生escape时，任何由函数h申请的内存并且还在函数g中存活，我们需要将它加到函数g的内存计数中，最后加到S中。\n学生提问：某些函数会根据不同的工作负载申请不同量的内存，那么在计算函数消耗的内存时，会计算最差的情况吗？\nFrans教授：是的。这里的工具会计算最深函数调用时最大可能使用的内存量。所以它会计算出每个系统调用可能使用的最多内存，虽然实际中系统调用可能只会使用少的多的内存。\n但是保险起见，我们会为最坏情况做准备。一些系统调用内的for循环依赖于传给系统调用的参数，所以你不能静态地分析出内存边界是什么。所以在一些场景下，我们会标注代码并规定好这是这个循环最大循环次数，并根据这个数字计算内存总量S。\n类似的，如果有你有递归调用的函数，谁知道会递归多少次呢？或许也取决于一个动态变量或者系统调用的参数。实际中，我们在Biscuit中做了特殊处理以避免递归函数调用。所以最后，我们才可能完成这里的内存分析。\n所以，这里的内存分析不是没有代价的，也不是完全自动的。\n这花费了Cody（论文一作）好几天检查代码，检查所有的循环并标注代码。还有一些其他的Golang特有的问题需要处理，例如，向Slice添加元素可能会使内存使用量扩大一倍，所以我们也给Slice标注了最大的容量。\n但是所有这些工作都是可完成的，在花费了几天时间之后，使用这里的内存分析工具，你可以得到对于系统调用使用的最大内存量的合理评估。以上基本就是Biscuit如何解决heap exhaustion问题。\n学生提问：这里的静态内存分析工具，如果不是用来构建内核，它们通常会用来干嘛？\nFrans教授：Go编译器内部使用它来完成各种各样的优化，并分析得出最优的编译方式。这里正好编译器使用了一个包，我们也可以使用同样的包。在后面你还可以看到，我们还将它用于一些其他特性，有这么一个包非常的方便。\nEvaluation: HLL benefits Biscuit的实现与其他内核，例如XV6，非常相似，除了Biscuit比XV6性能要高的多。Biscuit采用了很多Linux内核的优化和聪明的设计：\n我们对于内核文本采用了大页，以避免TLB的代价。 我们有针对每个CPU的网卡队列，这样可以避免CPU核之间同步。 我们有RCU实现了不需要读锁的Directory Cache。 \u0026hellip;\u0026hellip; 通常为了高性能而做的优化，编程语言并不会成为阻碍。Golang并没有成为阻碍这些优化实现的因素。这些优化之前是在C和Linux中实现，我们现在只是在Golang中又实现它们。在实现这些优化时有很多的工作，但是这些工作与编程语言本身无关。\n今天论文 的出发点就是了解用高级编程语言实现操作系统的收益和代价。所以我们将分两部分来评估，首先是收益，其次是代价。\n有关高级编程语言，我们要回答三个问题：\n首先，我们有没有作弊？或许我们避免使用了所有Golang提供的高级编程语言中代价较高的功能。 其次，高级编程语言是否有简化Biscuit代码？ 最后，高级编程语言是否能阻止前面提到的内核漏洞？ 首先，我们有没有使用高级编程语言的特性？我们会对比一下Biscuit与其他两个大的Golang项目在使用语言特性上是否类似，这样我们才可以说我们的内核以类似的方式利用了相同的语言特性。\n这里我们使用了相同的静态分析工具来分析两个大的Golang项目，它们都有超过100万行代码，其中一个项目是Go runtime以及包含的所有包，另一个是一个叫做Moby的系统。\n之后我们画出了一些高级语言特性在每1000行代码中的使用量。图中X轴是语言特性：\nallocation对应于new maps就是hashtable slice是动态数组 channel是同步的工具，如你所见我们用的很少，Go runtine和Moby也用的很少 很明显我们最喜欢的特性就是函数返回多个值 我们使用了Closure（闭包） 我们稍微使用了defer 我们使用了Interface 使用了Type assertion来以一种类型安全的方式将一个类型转换成另一个类型 同时我们也import了很多包，Biscuit内核是由很多个包构建出来的，而不是一个大的单一的程序 如你所见，有些特性Biscuit用的比Go runtime和moby更少，有些特性Biscuit用的更多，这里没有很明显的区别。所以从这张图中可以得出的主要结论是：Biscuit使用了Golang提供的高级编程语言特性，而不是为了得到好的性能而避开使用它们。\n学生提问：你这里是怎么统计的？是不是使用了静态分析工具？\nFrans教授：是的，这里使用的就是静态分析工具。通过写一个小程序利用静态分析工具来查看这些项目的每一行代码，并记录对应的特性是什么，这样就能统计这些特性的使用数量。\n第二个问题有点主观，高级编程语言有没有简化Biscuit代码？笼统的说我认为有的，我这里会讨论一两个例子。\n使用Garbage allocation是极好的，你可以回想XV6，当你调用exit时，有大量的结构化数据需要被释放回给内核，这样后面的进程才能使用。\n如果使用Garbage Collector这里的工作着实容易，Garbage Collector会完成这里的所有工作，你基本不用做任何事情。如果你从地址空间申请了一段内存，对应这段内存的VMA会自动被GC释放，所以这里可以简化代码。\n如之前所说的，函数返回多个值对于代码风格很好。闭包很好，map也很好。XV6中很多地方通过线性扫描查找数据，但是如果你有map和hashtable作为可以直接使用的对象，那么你就不用线性扫描了。\n你可以直接使用map，runtime会高效地为你实现相应的功能。所以直观上的感受是，你可以得到更简单的代码。\n但是前面只是定性的评估，下面会介绍一些更具体的例子。当有大量的并发线程，且线程有共享的数据时，GC如何起作用的。\n这里有个最简单的例子。假设你申请了一些动态的对象，比如说buffer，你fork一个线程来处理这个buffer，原线程也会处理同一个buffer。\n当两个线程都完成了工作，buffer需要被释放，这样内存才可以被后面的内核代码使用。这在C语言里面有点难协调，因为你需要有某种方式来决定buffer不再被使用。\n如果你使用GC，那么就没什么好决定的，因为当两个线程都处理完buffer之后，没有线程会指向那个buffer。\nGC会从线程栈开始追踪，并且在任何线程栈中都找不到buffer，因此GC会在稍后某个时间释放内存。所以在一个带GC的编程语言中，你完全不必考虑这个问题。\n在C中你可以这样解决这个问题，为对象增加引用计数，引用计数需要被锁或者一些原子性操作保护，当引用计数到达0时，你可以释放内存。\n实际中锁加上引用计数代价稍微有点高。如果你想要高性能，并且并发可以扩展到CPU核数，这可能会是个瓶颈，我们在后面介绍RCU的时候会看这部分。\n所以，如果你想要高性能，好的并发能力，人们倾向于不给读数据加锁。\n在实际中，我们会使得读数据至少是不需要锁的，这样你就不需要付出额外的代价。\n上面是我们在Golang中的实现，我们有个get函数，它会读取并返回链表的头结点。这里就没有使用锁，而是使用了atomic_load，它会读取头结点，但是又不需要锁。\n后面的pop函数使用了锁。这种风格在Linux内核中非常常见，写数据需要加锁，读数据不用加锁。这里pop函数会从链表中弹出头结点，这样你就可以重用头结点对应的内存。\n在C中实现这种风格会有点困难，因为有可能当你释放头结点内存时，其他并发的线程正好读取到了头结点的指针。这样当你做完atomic_store，你不能释放指针内容，因为有可能有另一个线程的指针指向了这部分内容。如果你在这里释放了指针内容，你有可能会有use-after-free Bug。\n我们在这门课程的最后一节课会看到，Linux内核对这个问题有一种非常聪明的解决办法，被称为Read-Copy-Update或者是RCU。它的工作就是推迟释放内存，直到确定指针不再被使用，并且它有一种非常聪明的方案来决定什么时候可以安全释放内存。\n但是这个方案有各种各样的限制，程序员需要在RCU关键区域内遵守各种规则。比如说你不能在RCU关键区域sleep，也不能切换线程。\n所以尽管实际中Linux内核非常成功的使用了RCU，但是RCU还是有点容易出错，并且需要小心编程来使得它能正确工作。在带有GC的编程语言，例如Golang，这就不是问题了，因为GC会决定某个对象不再被使用，只有这时才释放它。\n所以现在对于编程人员来说没有限制了，所有的限制都被GC考虑了。这是一种带有GC的编程语言的明显优势。\n接下来看看CVEs Bugs，这在前面提到过（注，20.1）。\n我们手动的检查了所有的CVEs Bug，并尝试确定Golang是否修复了问题。\n第一行代表我们不能弄清楚这些Bug的结果是什么，它会怎么展现，我们知道如何修复这些问题，但是我们不能确定Golang是否能避免这些问题。 有很多逻辑Bug，可以认为Golang会有与C相同的Bug，所以结果是相同的 接下来是40个memory-safety Bugs，包括了use-after-free，double-free，out-of-bound。其中8个直接消失了，因为GC考虑了内存释放，32个会产生panic，比如说数组越界。当然panic并不好，因为内核会崩溃，但是或许要比直接的安全漏洞更好。所以在这40个Bug中，高级编程语言有帮到我们。 以上就是使用高级编程语言实现内核的优势，接下来讨论一些代价，也就是High Level Language Tax。\nEvaluation: HLL performance cost(1) 以上是6个问题，我应该不会全部介绍，因为我想在课程的最后留些时间来回顾我们在本节课开始时提出的问题。\n以上就是测试环境，Biscuit运行在裸机之上，所以我们的测试是在物理服务器而不是QEMU之上。我们使用了三个应用程序来做性能测试，它们分别是，Webserver，K/V store，Mail server benchmark。\n这三个应用程序都会给内核一定的压力，它们会执行系统调用，内核会做大量的工作。你可以看到，大部分CPU时间都在内核中。\n首先的问题是，Biscuit是否是一个工业质量的内核？我们将前面的三个应用程序分别运行在Linux和Biscuit上，并进行对比。\n在Linux中，我们会关闭所有Biscuit不提供的功能，比如Page Table隔离，repoline等等很多功能，这样的对比才会尽可能的公平。有些特性会比较难关闭，但是我们会尽量将它们关闭。\n之后我们会测试吞吐量，如你所见Biscuit总是会比Linux更慢，mailbench可能差10%，nginx和redis差10%到15%。这里的数字并不是绝对的，因为两个系统并不完全一样。但是可以看出两个系统基本在同一个范围内，而不是差个2倍或者10倍。\n接下来我们会分析代码，并找到高级编程语言额外的CPU cycle消耗。我们会找到：\n哪些CPU cycle是GC使用的， 哪些是函数调用的Prologue使用的。Golang会为函数调用做一些额外的工作来确保Stack足够大，这样就不会遇到Out-of-Stack的问题 Write barrier是GC用来跟踪不同空间的指针的方法 Safety cycles是用在数组边界检查，空指针检查上的CPU cycles 通过测试上面的应用程序，可以得到测量结果。\n3%的执行时间用在了GC cycles中，这里我稍后会介绍为什么这很少。同时这也可以说明GC是在运行的，我们并不是用了一大块内存而没有使用GC 令人奇怪的是，Prologue占有的CPU时间最多，这基本上跟我们用来检查kernel Stack或者goroutine Stack是否需要增加的方案有关，这里或许更容易降低一些 Write barrier使用的时间很少 2%-3%的CPU时间用在了Safety cycles中 这些数据都很好，High Level Language Tax并不是那么的大。\n当然GC的占比可能会更高，因为它完全取决于heap大小和存活对象的数量，GC会跟踪所有的存活对象，并决定哪些对象已经不被使用。\n如果有大量的存活对象，GC也需要跟踪更多的对象。所以这里的CPU时间完全与存活对象的数量相关。\n所以我们做了一些其他的实验。我们创建了大量的存活对象，大概有200万个vnode，可以认为这是200万个inode。然后修改heap的headroom，也就是GC可以使用的空闲内存数量，最后再测量GC的代价。\n上图就是测量结果，存活对象占了640MB内存，我们在不同内存大小下运行测试。第一次测试时，有320MB空闲内存，是存活对象内存的一半，这时Golang有非常严重的overhead，大概是34%，GC因为没有足够的headroom需要运行很多额外的程序。\n如果空闲内存是存活对象的2倍，那么GC的overhead就没有那么疯狂，只有9%。所以，为了保持GC的overhead在10%以内，物理内存大小需要是heap大小的三倍。\n学生提问：什么是write barrier？是设置权限吗？\nFrans教授：你还记得Lec17的内容吗？当GC在运行的时候，需要检查指针是否在from空间，如果在from空间你需要拷贝它到to空间。write barrier是非常类似的功能，它的想法是一样的，你需要检查指针看它是否在你需要运行GC的区域内。\n学生提问：当存活对象的内存大于空闲内存的时候，GC该怎么工作呢？\nFrans教授：你买一些内存，vnode会使用一些内存，然后还剩下320MB空闲内存。当应用程序申请更多内存时，首先会从空闲内存中申请，直到空闲内存也用光了。\n与此同时，GC也在运行。所以我们刚刚的测试中是在3个不同配置下运行，在最后一个配置中，空闲内存是存活对象占用内存的两倍。这意味着GC有大量的headroom来与应用程序并行的运行，如果有大量的headroom，GC的overhead就没那么高了，只有10%左右，而不是34%。在第一个配置中，总共是640+320MB内存，而不是只有320MB内存。\nEvaluation: HLL performance cost(2) 接下来稍微介绍GC pause。\nGo的GC是一个带有短暂pause的并发GC，它在一段非常短的时间内停止程序运行来执行write barrier，之后再恢复应用程序的运行，同时GC也会完成自己的工作。\nGo的GC也是递增的，就像我们在Lec17中介绍过的一样，每次调用new都会做一些GC的工作。所以每次GC做一些工作的时候，应用程序都会有一些延时，这就是代价。\n所以我们做了一些测试，我们找了个应用程序并测试了最大的pause时间。也就是由于GC可能导致应用程序最大的停止时间。\n最大的单个pause时间是115微秒，也就是在web server中，因为使用了TCP stack，TCP Connection table中很大一部分需要被标记（注，GC的一部分工作是标记对象），这花费了115微秒。\n一个HTTP请求最大的pause时间是582微秒，所以当一个请求走到一个机器，最多会有总共582微秒延时来执行这个请求。而超过100微秒的pause发生的非常非常少，只有少于0.3%。\n如果你尝试达成某种SLA，其中要求的最长请求处理时间很短，那么582微秒就很严重。\n但是如果你查看Google论文，The Tail at Scale ，其中介绍有关一个请求最长可以有多长处理时间，他们讨论的都是几毫秒或者几十毫秒这个量级。所以Biscuit拥有最大pause时间是582微秒还在预算之内，虽然不理想，但是也不会很夸张。\n这表明了，Golang的设计人员把GC实现的太好了。并且我们在做Biscuit项目的时候发现，每次我们升级Go runtime，新的runtime都会带一个更好的GC，相应的GC pause时间也会变得更小。\n之前在Linux和Biscuit之间的对比并不真正的公平，因为Biscuit和Linux实现的是不同的功能。\n所以我们做了一个额外的测试，我们写了两个完全相同的内核，一个用C实现，另一个用Golang实现。这两个内核实现了完全相同的东西，并且我们会查看汇编代码以检查区别在哪。可能会有一些区别，因为Golang会做一些安全检查，但是对于基本功能来说，汇编代码是一样的。\n以上是有关测试的一部分，通过pipe来回传输一个字节。我们查看内核中有关将一个字节从pipe的一端传到另一端的代码。Go里面是1.2K行代码，C里面是1.8K行代码。\n这里没有内存分配和GC，所以这里只有语言上的差异。我们还查看了两种实现语言中花费最多时间的10个地方，这样我们才能确保两种语言实现的代码尽可能的接近。\n之后我们查看了每秒可以完成的操作数，如你可见Golang要慢15%。如果你查看Golang的Prologue和safety-check，这些指令是C代码所没有的，这些指令占了16%，这与更慢的处理速度匹配的上。\n所以这里的主要结论是Golang是更慢，但并不是非常夸张的慢，Golang还是非常有竞争力的。并且这与我们早些时候做的Biscuit和Linux对比结果一致。\n第6个问题跳过。\nShould one use HLL for a new kernel? 最后我想讨论我们在最开始问过的一个问题，你应该在一个新内核中使用高级编程语言吗?\n与其直接回答这个问题，我在这页有一些我们的结论和一些考虑。或许你们该回退一步，并问自己，你们更喜欢哪种方式？\n你们是喜欢像在实验中用C写XV6，还是喜欢使用类似Golang的高级编程语言。更具体的说，你们更想避免哪类Bug？或许在这节课的过程中想想你们遇到过什么Bug？我想听听你们的体验，你们是怎么想的？切换到高级编程语言会不会改变你们的体验？\n一些学生介绍自己的体验，有说C好的，有说C不好的\n当然，我们不会将XV6改成Golang或者任何高级编程语言。具体原因刚刚一些同学已经提到了，Golang还是隐藏了太多细节，这门课的意义在于理解系统调用接口到CPU之间的所有内容。\n举个例子，Golang隐藏了线程，我们并不想隐藏线程，我们想要向你解释线程是如何实现的。所以接下几年，这门课程还是会使用C语言。\n但是如果你要实现一个新的内核，并且目标不是教育你的学生有关内核的知识，目标是写一个安全的高性能内核。你可以从我们的研究中得出一些结论：\n如果性能真的至关重要，比如说你不能牺牲15%的性能，那么你应该使用C。 如果你想最小化内存使用，你也应该使用C。 如果安全更加重要，那么应该选择高级编程语言。 或许在很多场景下，性能不是那么重要，那么使用高级编程语言实现内核是非常合理的选择。 Cody、Robert和我在实现这个项目的过程中学到的一件事情是，任何一种编程语言就是编程语言，你可以用它来实现内核，实现应用程序，它并不会阻止你做什么事情。\n学生提问：我很好奇你们是怎么实现的Biscuit，你们直接在硬件上运行的Go runtime，具体是怎么启动的？\nFrans教授：这里有一层中间层设置好了足够的硬件资源，这样当Go runtime为heap请求内存时，我们就可以响应。这是Go runtime依赖的一个主要内容。\n学生提问：我知道你们实现了一些Go runtime会调用的接口，因为你们现在自己在实现内核，所以没有现成的接口可以使用。你们是全用汇编实现的这些接口吗？还是说有些还是用Golang实现，然后只在必要的时候用汇编？\nFrans教授：这就是Biscuit中1500行汇编代码的原因，它会准备好一切并运行Go runtime。有一些我们可以用C来实现，但是我们不想这么做，我们不想使用任何C代码，所以我们用汇编来实现。并且很多场景也要求用汇编，因为这些场景位于启动程序。\n我们的确写了一些Go代码运行在程序启动的最开始，这些Go代码要非常小心，并且不做内存分配。我们尽可能的用Golang实现了，我需要查看代码才能具体回答你的问题，你也可以查看git repo。\n学生提问：我有个不相关的问题，Golang是怎么实现的goroutine，使得它可以运行成百上千个goroutine，因为你不可能运行成百上千个线程，对吧？\nFrans教授：运行线程的主要问题是需要分配Stack，而Go runtime会递增的申请Stack，并在goroutine运行时动态的增加Stack。\n这就是Prologue代码的作用。当你执行函数调用时，如果没有足够的Stack空间，Go runtime会动态的增加Stack。而在线程实现中，申请线程空间会是一种更重的方法，举个例子在Linux中，对应的内核线程也会被创建。\n学生提问：goroutine的调度是完全在用户空间完成的吗？\nFrans教授：大部分都在用户空间完成。Go runtime会申请m个内核线程，在这之上才实现的的Go routine。所有的Go routine会共享这些内核线程。人们也通过C/C++实现了类似的东西。\n学生提问：C是一个编译型语言，所以它可以直接变成汇编或者机器语言，它可以直接运行在CPU上，所以对于XV6来说就不用加中间层代码。但是我理解Golang也是一种编译型语言，所以它也会变成汇编语言，那么为什么还要中间层（位于机器和Go runtime之间）？XV6有这样的中间层吗？为什么有一些事情不能直接编译后运行在CPU上？\nFrans教授：好问题。Go runtime提供了各种你在XV6中运行C时所没有的功能。\nGo runtime提供了线程，提供了调度器，提供了hashtable，提供了GC。举个例子，为了支持GC，需要一个heap来申请内存，通常是向底层的操作系统来申请内存作为heap。这里说的中间层Go runtime需要用来完成工作的相应功能（比如说响应内存申请）。\n学生提问：我们不能直接将runtime编译到机器代码吗？\nFrans教授：Runtime会被编译到机器码，但是当你运行Go代码时，有一部分程序是要提前运行的，这部分程序需要在那。\n即使C也有一个小的runtime，比如printf就是C runtime的中间层的一部分，或者字符串处理也是C runtime的一部分，它们也会被编译。C runtime有一些函数，但是这个runtime是如此之小，不像Go runtime需要支持许多Go程序所依赖的功能。\n学生提问：看起来这里的中间层像是一个mini的系统层，它执行了一些底层的系统功能。\nFrans教授：是的，或许一种理解中间层的方法是，XV6也有一个非常非常小的中间层。当它启动的时候，它做的第一件事情是分配一些Stack这样你才能调用C的main函数。\n你可以认为这一小段代码是针对XV6的中间层。一旦你执行了这些指令，你就在C代码中了，然后一切都能愉快的运行。Go runtime的中间层稍微要大一些，因为有一些功能需要被设置好，之后Go runtime才能愉快的运行。\n","date":"2024-10-05T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08120kernels-and-hll/bg_hu_1d1dc1ec5093caca.jpg","permalink":"https://echudet.github.io/p/mit6s08120kernels-and-hll/","title":"MIT6S081：20Kernels and HLL"},{"content":"Lecture 19 Virtual Machines 课前预习 论文速览 论文链接 这篇论文《Dune: Safe User-level Access to Privileged CPU Features》通过以下几个部分展开。\n在摘要 (Abstract)部分，提到Dune系统为应用程序提供安全、直接的硬件特权访问（如页表、特权模式等），并展示了其在64位x86 Linux系统上的实现。文中介绍了Dune在沙盒、权限分离以及垃圾回收器中的应用，展示了其性能优势。\n在引言 (Introduction)部分，介绍了许多应用程序需要访问“内核级”硬件功能的原因，但这些功能通常限制在内核空间。传统的解决方法（如修改内核或使用虚拟机）存在种种弊端，而Dune通过虚拟化硬件为用户程序提供了安全访问硬件功能的新方式。\n在虚拟化和硬件支持 (Virtualization and Hardware)部分，详细描述了Intel的VT-x扩展如何支持虚拟化，包括VMX root和VMX non-root模式。Dune利用这些硬件特性为用户程序提供对特权硬件（如异常处理、虚拟内存、特权模式等）的访问。\n在Dune的内核支持 (Kernel Support for Dune)部分，讨论了Dune核心模块的架构、威胁模型及其与传统虚拟机管理程序（VMM）的区别。Dune通过一个轻量级内核模块来管理虚拟化硬件，并使用EPT（扩展页表）来确保内存隔离，同时提供对内存管理、硬件访问及系统调用的支持。\n在用户模式环境 (User-mode Environment)部分，介绍了Dune提供的用户模式库libDune，帮助用户程序管理特权硬件功能，包括页表管理、异常处理等。讨论了如何从普通进程过渡到Dune模式，及其对现有Linux程序的影响。\n在应用 (Applications)部分，提到Dune的三个应用场景：沙盒、权限分离、垃圾回收器。在每个场景下，Dune简化了实现并提供了显著的性能提升。例如，在沙盒应用中，Dune能够有效隔离不可信代码，同时保证较低的性能开销。\n在 评估 (Evaluation)部分，通过对系统调用、页错误处理、页表访问等操作的基准测试，量化了Dune的性能开销。并对其在不同应用场景中的加速效果进行了分析，显示了Dune在内存密集型应用中的优势。\n在对硬件的反思 (Reflections on Hardware)部分，讨论了在开发Dune过程中遇到的硬件限制，提出了一些硬件改进建议，尤其是在EPT和VMX过渡开销方面。这部分对硬件开发者更有意义。\n在 相关工作 (Related Work)部分，介绍了Dune与其它操作系统设计（如Exokernel、SPIN、Wedge等）的关系，并探讨了Dune相较于其他虚拟化和安全系统的独特优势。\n在结论 (Conclusion)部分，总结了Dune为用户程序提供访问特权硬件的新方法，提升了性能并简化了实现。作者还提出Dune的潜在扩展方向，如在不同硬件架构上的应用。\n阅读重点：\n摘要部分可以快速了解论文的整体内容。 引言和虚拟化和硬件支持部分能够帮助理解Dune的核心设计动机和硬件背景，适合第一次接触Dune的读者。 对于实际应用和性能评估，应用和评估部分是阅读的重点，尤其是对系统性能和应用场景感兴趣的读者。 虚拟化硬件 虚拟化硬件是一种通过硬件支持实现系统虚拟化的技术，它允许多个操作系统或用户进程共享同一台物理机器的硬件资源，同时确保每个操作系统或进程的独立性和安全性。这种技术通常通过 CPU 和内存的扩展来支持虚拟化操作，从而使虚拟机（Virtual Machine, VM）可以像独立物理机器一样运行。英特尔的 VT-x 和 AMD 的 SVM（Secure Virtual Machine）是两种主流的虚拟化硬件技术。\n虚拟化的早期实现主要通过软件来模拟硬件（如仿真 CPU 指令和内存访问），这种方式的性能开销较大。现代 CPU 引入了硬件虚拟化支持，例如 Intel 的 VT-x 和 AMD 的 SVM，使虚拟化操作更加高效。虚拟化硬件使得大部分虚拟机的操作能够直接在硬件上执行，从而减少了虚拟机管理程序（VMM）或操作系统内核的干预，这提高了虚拟化的性能并降低了开销。\n虚拟化硬件（如 Intel VT-x）通过将 CPU 划分为两种模式：\nVMX Root 模式：通常用于运行虚拟机管理程序（VMM），类似于操作系统内核的特权模式。 VMX Non-root 模式：限制了 CPU 的操作权限，主要用于运行虚拟机中的客体操作系统（Guest OS）。 硬件通过 VM entry 和 VM exit 来管理不同模式的切换。例如，当 VMM 发出 VMLAUNCH 指令时，CPU 进入 VMX Non-root 模式，运行虚拟机。当虚拟机需要 VMM 处理一些操作（如系统调用或页表管理），硬件会触发 VM exit，将控制权返回给 VMM。这种机制使得虚拟机可以高效地执行大部分指令，而无需 VMM 的干预。\n在论文《Dune: Safe User-level Access to Privileged CPU Features》中，Dune 系统利用虚拟化硬件（Intel VT-x）为用户进程提供对特权硬件功能（如页表、异常处理、特权模式等）的访问。Dune 中的用户进程运行在 VMX Non-root 模式下，这使得它们可以直接访问 CPU 的特权级操作，而不需要内核直接参与。这种机制不仅提高了系统性能，还保持了安全隔离。\n基于 xv6 和 Linux 的示例：\n在 xv6 中：xv6 是一个简单的教学操作系统，所有用户进程运行在用户模式下，只有内核才能运行在特权模式（ring 0）下。如果要通过虚拟化硬件将特权模式的一些功能暴露给用户程序，xv6 将需要借助类似 VT-x 这样的虚拟化技术。通过将用户进程放入 VMX Non-root 模式，xv6 可以在不破坏其进程隔离模型的前提下，让用户进程访问页表或执行硬件异常处理。 在 Linux 中：Linux 的设计更加复杂，虚拟化硬件可以通过虚拟机（如 KVM）让多个 Linux 内核在同一台物理机上运行。虚拟机使用硬件虚拟化技术，通过 VMM 来管理内存和 CPU 资源。在 Dune 的设计中，它直接通过虚拟化硬件将某些内核特权功能（如页表管理、系统调用拦截）暴露给用户进程，从而避免了复杂的内核修改。这对于高性能应用（如垃圾回收、权限隔离等）具有极大的好处。 虚拟化本身并不是新技术，最早可追溯到 1960 年代的 IBM 大型机。然而，现代硬件虚拟化（如 Intel VT-x 和 AMD SVM）是 2000 年代后期发展起来的技术，专为提高虚拟机性能而设计。\n虚拟化硬件技术是指虚拟机的指令大部分直接在真实硬件上执行，但在受控的环境中，不能直接影响到主机系统的资源和数据。这种技术通过使用虚拟化硬件（如 Intel VT-x），将虚拟机的执行限制在隔离的虚拟机监控程序（VMM）内，保证虚拟机对主机系统的干扰最小化。\n虚拟化硬件通过多种机制保证安全性，防止虚拟机中的恶意代码（如病毒）攻击主机系统的真实数据：\n隔离机制：虚拟化硬件通过扩展页表（Extended Page Table, EPT）等技术，确保虚拟机只能访问被分配给它的内存区域，而不能直接访问主机系统的物理内存。即使在虚拟机中执行恶意指令，硬件会通过 EPT 确保这些指令不能访问到不属于虚拟机的地址空间。 VM Exit：虚拟机运行时，如果尝试执行某些特权操作（如系统调用），CPU 会触发 VM Exit，将控制权转交给虚拟机监控程序（VMM）。这样，VMM 可以拦截并检查这些特权操作，确保安全性。 硬件虚拟化的防护：Intel 的 VT-x 和 AMD 的 SVM（Secure Virtual Machine）都提供了特定的硬件支持，确保虚拟机与物理机之间的安全隔离。任何尝试逃逸虚拟机环境的操作都会被硬件机制检测并阻止。 扩展页表（EPT）和地址空间隔离：虚拟化硬件通过 EPT 使虚拟机的物理地址与真实物理地址隔离。即便是恶意软件试图操作内存，也只能操作虚拟机自己的地址空间，而不能访问主机系统的数据。 因此，主机系统的内存数据是“锁定”的，虚拟机无法直接接触到。如果有恶意代码试图窃取数据或发起攻击，硬件将中止操作并返回给 VMM。\nQEMU 是一种广泛使用的开源虚拟机仿真器和虚拟化工具，它支持多种虚拟化方式，包括全系统仿真（full system emulation）**和**硬件辅助虚拟化（hardware-assisted virtualization）。当 QEMU 与 KVM（Kernel-based Virtual Machine）结合使用时，虚拟机的执行速度大幅提升，因为 KVM 可以利用硬件虚拟化特性（如 Intel VT-x 或 AMD SVM）将虚拟机的指令直接运行在物理 CPU 上。\n在 MIT 6.828 课程中使用的 xv6 操作系统是在 QEMU 中运行的。QEMU 在这个环境中既可以作为一个纯仿真器（即不依赖硬件虚拟化），也可以在支持的主机上与 KVM 一起工作，利用硬件虚拟化功能。这取决于你如何配置 QEMU。通常在课程中的 xv6 是在纯软件仿真环境中运行的，但如果启用了 KVM，则 QEMU 会利用硬件虚拟化功能加速虚拟机的运行。\n的确，安全攻击一直是虚拟化技术的一个潜在问题，特别是通过虚拟机逃逸（VM Escape）攻击主机系统的风险。虚拟化硬件通过严格的权限管理和隔离机制防止这种攻击。尽管硬件虚拟化技术在设计上已经考虑了安全问题，但历史上也出现过一些硬件漏洞（如 Intel 的 Meltdown 和 Spectre），这使得虚拟化系统需要通过额外的软件补丁和配置来增强安全性。\n总结：\n虚拟化硬件通过页表隔离和硬件中断等机制确保虚拟机不能干涉主机系统的真实数据。 QEMU 支持硬件虚拟化，可以利用 KVM 加速虚拟机的执行。 虽然硬件虚拟化设计上保证了安全性，但历史上也出现过一些需要防范的硬件漏洞。 特权硬件 在 Dune 系统中，特权硬件功能是指那些通常只有操作系统内核才有权限访问和控制的 CPU 功能。这些功能包括：\n异常处理（Exceptions）：处理 CPU 中断和异常（如页面错误、除零等）。 虚拟内存（Virtual Memory）：直接访问和管理页表，控制虚拟地址到物理地址的映射。 权限模式（Privilege Modes）：包括 CPU 中的权限环（如 ring 0 和 ring 3），用于区分内核态和用户态的操作权限。 其他硬件功能：如控制寄存器、TLB（转换后备缓冲区）等，用于内存管理和性能优化。 与特权硬件功能不同，不特权的硬件功能是用户程序可以直接访问的 CPU 功能。这些功能通常限于普通计算操作和基本的内存访问，例如：\n算术运算：加减乘除等 CPU 指令。 基本内存访问：访问分配给该进程的用户空间内存。 I/O 操作：通过操作系统的系统调用接口进行文件读写和网络操作等。 这些功能是受限的，用户态程序无法直接访问内核态所需的特权硬件资源，如页表管理和系统调用向量。\nDune 系统的设计初衷是通过虚拟化硬件技术（Intel VT-x）为用户进程提供对特权硬件功能的直接访问，目的是让用户程序能够更高效地进行某些操作，而不需要频繁地切换到内核态。这带来了几个显著的好处：\n提高性能：用户程序可以直接处理诸如页面错误等异常，减少了传统的系统调用和上下文切换的开销。例如，Dune 能够将页面错误的处理速度提升四倍以上。 灵活的内存管理：Dune 允许用户程序直接操作页表，控制地址翻译和访问权限。这对于实现自定义的垃圾回收、数据压缩等内存密集型操作非常有用。 权限分离和沙盒：通过暴露权限模式（如 ring 0 和 ring 3），Dune 可以有效地隔离不可信代码，提供类似操作系统内核的安全防护机制，但不需要进行复杂的内核修改【5†source】。 直接将特权交给用户进程会带来巨大的安全风险，比如：\n攻击面扩大：恶意程序可能会利用这些特权功能执行恶意操作，例如篡改内核数据或干扰其他进程。 系统不稳定：一旦用户进程获得了特权功能，可能会误用，导致系统崩溃或不稳定。 Dune 的设计是通过虚拟化硬件（Intel VT-x）提供了一种受控的方式，让用户进程能够安全访问特权硬件，同时避免直接干预到主机系统。这种机制主要通过以下几个方面确保安全：\n隔离机制：通过虚拟化模式，用户进程运行在 VMX non-root 模式下，主机操作系统运行在 VMX root 模式。即使用户进程具有特权硬件访问权，它们依旧在一个受限制的虚拟化环境中，无法直接访问主机系统的资源。 VM Exit 机制：每当用户进程尝试进行高风险的操作（如访问特定的硬件资源），硬件会触发 VM Exit，返回控制权给虚拟机管理程序（VMM），确保操作的合法性。 Dune和传统虚拟机管理程序（VMM）的差异 VMM和Dune的主要不同点：\nVMM Dune 环境模拟 传统的虚拟机管理程序模拟一个完整的机器环境，支持运行一个完整的客操作系统。这种模拟需要支持各种物理硬件接口，并且通常通过复杂的方法来提供虚拟硬件，通过这种方式，可以运行未修改过的操作系统。 Dune暴露的是进程环境，而非机器环境。因此，Dune不能支持一个完整的客操作系统，但由于只需处理进程环境，Dune更加轻量和灵活。 Hypercalls Hypercalls用于支持半虚拟化技术，这种技术要求对客操作系统进行修改，以便使用一些更有效率且更容易虚拟化的接口。例如，VMM可能提供一个hypercall来注册虚拟网络设备的中断处理程序。 Hypercalls机制调用的是正常的Linux系统调用。例如，Dune中的hypercall允许通过调用标准的操作系统服务，例如在TCP socket上进行读取操作。 硬件接口的模拟 为了支持未修改的客操作系统，VMM需要模拟各种物理硬件接口（例如虚拟显卡）。 只提供可以直接访问的硬件功能，不能直接访问的情况下，会退回到操作系统上。例如，对于显卡接口，Dune直接使用正常的操作系统显示服务。 状态保存与恢复 必须保存和恢复所有必要的状态以支持一个完整的客操作系统。 由于暴露的只是进程级别的硬件接口，Dune限制了客体与主机状态之间的差异，从而减少了执行VM entries和VM exits的开销。 地址空间 为每个虚拟机提供一个独立的地址空间，模拟平面的物理内存。 EPT（扩展页表）反映的是进程地址空间，这样内存布局可以是稀疏的，而且当两个进程映射同一个内存段时，内存可以一致地共享。 Hypercalls hypercalls的目的是让客操作系统调用虚拟机管理程序，以获取某些特定的虚拟化服务 hypercalls直接调用Linux系统调用，这大大简化了虚拟化层的设计，因为它不需要提供专门的虚拟化服务接口，直接利用操作系统已有的服务即可 状态保存 VMM需要保存和恢复完整的机器状态，以便可以中断和重新启动操作系统及其运行的应用程序。 Dune不需要保存大量的状态，是因为它只是提供一个进程环境，而该进程环境和主机操作系统的环境非常接近。Dune进程始终在主机操作系统中运行，其状态保存和恢复主要由操作系统本身管理，而不是由虚拟化层管理。 硬件模拟 VMM必须模拟完整的硬件层，以支持未修改的客操作系统，这包括CPU、内存、设备等。 Dune只模拟了硬件和操作系统之间的那一层，对于那些无法直接访问的硬件功能，直接调用主机操作系统的服务即可。 关于Hypercalls：\n大多数VMM采用Hypercall来支持半虚拟化。在这种情况下，客操作系统被修改以通过Hypercall与VMM通信。这些Hypercall是专门的虚拟化接口，通常由VMM来处理，处理时往往涉及复杂的步骤，因为VMM需要模拟硬件。 假设我们有一个虚拟网络设备。客操作系统需要注册一个中断处理程序来处理虚拟设备的中断。在传统VMM中，客操作系统通过Hypercall告诉VMM：“我需要注册一个中断处理程序。” VMM接收到这个Hypercall后，会更新其内部的数据结构，并确保当虚拟设备产生中断时，中断处理程序将由客操作系统正确处理。\n在Dune中，Hypercall直接调用主机操作系统（比如Linux）的系统调用。这意味着Dune不需要维护复杂的虚拟化接口，借助操作系统已有的机制即可实现功能。 同样是虚拟网络设备，在Dune中，进程不直接与Dune层通信以注册中断处理程序，而是通过标准的Linux系统调用来完成该操作。例如，进程可能使用read系统调用从一个TCP socket读取数据。实际上，这种调用通过Hypercall直接由Linux内核来处理，而不需要Dune进行复杂的虚拟化处理。\n关于状态保存：\n在传统的VMM中，每个虚拟机（包括其运行的客操作系统和应用程序）都有独立的计算状态，这包括CPU寄存器、内存状态、I/O设备状态等。当发生VM entry（进入虚拟机）或VM exit（退出虚拟机）时，VMM必须保存和恢复所有这些状态，以确保虚拟机的运行不受干扰。这是一项复杂且耗时的任务，因为状态信息非常多。 假设虚拟机正在执行某个操作，中途发生了VM exit（可能是由于某个中断）。在这种情况下，VMM必须保存一些寄存器、内存和I/O设备的状态，以便下一次虚拟机恢复运行时，能够无缝地继续之前的操作。\nDune不需要保存和恢复大量状态，部分原因是它处理的是进程层面而非机器层面的状态。进程的状态由操作系统（比如Linux）管理，包括上下文切换等操作。因为：\nDune进程是主机操作系统的一部分，从操作系统的角度来看，它是一个普通进程。 当发生切换时，Linux已经处理了大部分的状态保存和恢复工作，Dune只需极少的额外处理。 假设一个Dune进程需要执行IO操作。当进程被调度时，Linux内核会自动保存当前进程的状态（如寄存器、内存页表等），并恢复下一个进程的状态。这与标准的进程上下文切换相同，不需要Dune进行额外的状态管理。\n关于硬件模拟：\n传统的VMM需要完整地模拟物理硬件层，以支持未修改的客操作系统。这意味着VMM必须提供虚拟CPU、虚拟内存管理单元、虚拟设备（如网络、显卡等）等。\n对于虚拟显卡，VMM需要模拟显卡的各种寄存器和状态，让客操作系统认为它在与一个真实的显卡打交道。这涉及到高复杂性的模拟和处理。\nDune暴露的是硬件和操作系统之间的一层接口。它不完全模拟硬件，只是提供直接访问一些硬件功能的接口，而对于无法直接访问的功能，依赖于主机操作系统。\n在图形显示方面，Dune不会提供虚拟显卡，而是让进程使用操作系统的显示服务（例如通过Unix域套接字和共享内存访问X服务器）。这样，图形显示的复杂性被主机操作系统处理，Dune不需要额外的处理。\n总结来说，Dune尝试通过暴露进程级的虚拟环境来简化虚拟化的复杂性，这样做不仅使其更加轻量和灵活，而且能够充分利用主机操作系统已提供的服务来实现功能。\nDune 技术 Dune技术确实不同于传统虚拟机的虚拟化，它提供的不是完整的虚拟机，而是一种更轻量级的、针对线程级别的虚拟化，但是它仍然利用了虚拟化硬件的能力，这就是为什么你在论文中看到虚拟机相关的术语，如“VM entry”和“VM exit”。\nDune的核心思想是利用硬件虚拟化技术（如Intel VT-x）为用户态的进程或线程提供对底层硬件资源（如内存、CPU寄存器等）的更直接访问，而无需引入完整的虚拟机（即没有必要模拟整个操作系统）。它让用户态的程序在某种程度上获得与内核态程序类似的硬件访问权限，同时通过虚拟化技术保持对系统安全的控制。\n简单来说，Dune并没有像传统的虚拟机那样完全模拟一台机器，而是借助硬件虚拟化特性，让用户进程可以更高效地访问一些低级的硬件功能，通常只需一部分特权。因此，Dune更像是线程级别的虚拟化，但使用了虚拟机技术来实现这些特权访问。虚拟机与线程的区别:\n虚拟机（VM）：通常模拟的是整个物理机器，包括CPU、内存、存储、网络接口等完整的硬件设备。虚拟机运行的是一个完整的操作系统，并且这个操作系统可以管理多个线程或进程。因此，虚拟机不仅仅是一个线程，它包含了一个完整的执行环境。 线程：线程是操作系统中更轻量级的执行单位，它是进程的一部分，多个线程共享同一个进程的内存空间。每个线程只表示一个执行路径，而没有独立的硬件环境。Dune技术关注的是如何给线程提供某些底层硬件的直接访问权限。 Dune并不创建完整的虚拟机，但它使用了虚拟化硬件中的一些概念，比如：\nVMX Root 和 Non-Root模式：这些概念本质上属于硬件虚拟化管理，用来控制哪些操作系统级别的操作可以被虚拟化进程访问。Dune利用这套机制，让用户态进程进入一种“受控”的环境，能够直接访问某些硬件资源（比如页表操作），但同时保持一定的隔离。 VM entry/exit：尽管Dune不是传统虚拟机，但它通过类似虚拟机的方式进行状态切换。当用户态线程需要使用特权操作时，可能会触发类似“VM exit”的动作，切换到内核态或者让VMM（虚拟机监控器）进行处理。 在Dune的上下文中，虚拟机不是指一个完整的虚拟操作系统环境，而是指Dune利用虚拟化硬件为线程创建的一种“受控环境”，让用户态进程可以安全地使用某些特权指令。\n在传统的虚拟机系统中，**虚拟机（VM）**不是单一的线程，而是一个独立的计算环境，通常运行在虚拟机监控器（VMM）之上，它可以包含多个线程和进程。但是在Dune这样的技术中，虚拟机的概念被简化了，它更像是给某个用户线程提供了一种增强的硬件访问权限。从这个角度看，虚拟机可以被视为一个特权线程的运行环境，但它仍然与完整的虚拟机概念不同。\n总而言之\nDune提供的虚拟化是线程级别的，让用户态线程能够利用硬件虚拟化访问底层资源。 虽然Dune使用了虚拟化硬件的概念（如VMX模式和VM exit等），但它并不创建完整的虚拟机，而是通过类似虚拟机的机制来管理线程对硬件的访问。 虚拟机和线程的区别在于虚拟机模拟的是完整的操作系统环境，而线程只是单一的执行路径。在Dune中，虚拟机的作用类似于为用户态线程提供特权硬件访问的受控环境。 Dune的系统调用拦截 在Dune系统中，用户进程执行系统调用的方式确实与传统操作系统有所不同。这种设计主要是为了提供灵活性和安全性，特别是为了实现系统调用插装（interposition）和防止不可信代码直接访问内核。对于这一点，Dune系统采取了一些特殊的机制来处理系统调用。\n在传统的操作系统中，系统调用是通过陷入内核（trap to the kernel）来执行的。也就是说，当用户进程执行系统调用时，它会从用户态切换到内核态，并由内核处理这个系统调用。然而，在Dune系统中，系统调用会通过一个特殊的机制——系统调用插装（interposition）——来处理，这使得系统调用可以在用户空间被拦截和处理。\nDune系统利用了虚拟化技术，使用户态代码能够有更多的控制权，但同时确保系统安全性。具体来说，当用户进程执行系统调用时，可以采取以下几个步骤：\n系统调用捕获： 当一个Dune用户进程执行系统调用时，这个调用会被虚拟化系统（如虚拟机监控器或Dune层）捕获。这并不是直接陷入内核，而是通过Dune层进行拦截。 自定义处理： Dune层会检查该系统调用，并执行自定义处理逻辑。这个处理逻辑是用户定义的，可以用来： 审计和记录系统调用行为。 替换或修改系统调用的功能。 防止不可信代码执行某些潜在危险的系统调用。 条件性传递到内核： 根据自定义处理的结果，Dune层可以决定是否将系统调用传递给内核。如果需要，系统调用会以传统方式传给内核处理。如果不需要，Dune层可以直接返回结果，阻止系统调用到达内核。 用户进程在Dune系统中并不会直接包括内核的系统调用代码，但是它会通过Dune层的拦截机制进行处理。以下是其中几个可能的机制：\n基于虚拟化的拦截： Dune利用虚拟化硬件（如Intel VT-x）实现系统调用的拦截和处理。这样即使在用户态，进程也能执行一些类似于内核态的操作，但这些操作是在受控的环境下完成的。 共享地址空间： 为了实现更好的性能和灵活性，Dune可能会在用户态与内核态之间共享某些数据结构（例如页表）。这样，Dune可以避免频繁的上下文切换。 中间层代码： Dune可能包括一些用户态的库或运行时（runtime），这些库或运行时会拦截系统调用并将其传递给Dune层进行处理。Dune层为这些库提供了一套API，用户态进程通过这些API与Dune进行交互。 总而言之\nDune系统中的用户进程并不会直接包含内核的系统调用代码。相反，当用户进程执行系统调用时，系统调用会被Dune层捕获和处理。 系统调用捕获机制允许Dune进行系统调用插装、修改或阻止，从而增加系统的安全性和灵活性。 系统调用最终的处理可以通过自定义逻辑决定是否传递给内核，提供了对系统调用行为的细粒度控制。 总的来说，Dune系统通过虚拟化技术提供了一种新颖的、灵活的系统调用捕获和处理机制，使得用户进程可以在不陷入内核的情况下高效且安全地执行系统调用。这大大扩展了用户进程在用户态下的能力，同时通过严格的控制防止了安全问题。\nDune用户态执行指令 Dune确实可以被视为一种增强的缓存和拦截机制，它允许某些特定类型的指令或系统调用在用户态下直接执行，而无需陷入内核。这种机制通过虚拟化技术赋予用户态进程额外的权限，从而实现更高效的操作。\nDune如何在用户态执行指令。Dune系统依赖于硬件虚拟化技术（如Intel VT-x）以及一层特殊的中间层，这层中间层给予用户态一些改进后能够安全执行的权限。以下是Dune系统如何在用户态执行指令的几个核心机制：\n虚拟化技术的使用。Dune利用虚拟化技术，在用户态程序和底层硬件之间插入一层，用来管理和控制特权指令的执行。在Intel VT-x的支持下，Dune可以使用户态程序进入一种称为**VMX非根模式（VMX Non-Root Mode）**的状态，这种状态下可以执行原本需要内核态权限的特权操作。\n特权指令的捕获和处理。当用户态进程执行特权指令时，例如修改页表或执行特权系统调用，Dune层会通过虚拟机控制结构（VMCS）进行捕获。具体步骤如下：\n指令捕获：Dune会预先配置哪些指令或事件需要捕获（如页表访问、某些特殊的CPU指令等）。\n检查与执行：捕获后，Dune会检查这些指令，如果可以在用户态执行就直接执行，否则会传递给内核进行处理。\n回传结果：执行结果或状态更新会通过Dune层传回给用户进程。\n安全性和隔离措施。尽管Dune赋予了用户态进程更多的权限，但它也采取了一些安全措施来确保系统的整体安全性：\n内存隔离：通过虚拟化技术，如扩展页表（EPT），Dune确保进程只能访问它被授权的内存区域。\n特权管理：利用硬件虚拟化特性，Dune限制了用户态程序对内核态资源的直接访问，防止其对系统造成不安全影响。\nDune赋予的权限\n增强的用户态权限。通常情况下，用户态的权限是非常有限的，但通过Dune，用户态进程能够访问和管理一些本来只有内核态才能使用的资源和指令。这是通过虚拟化技术，使得用户态进程可以在安全的前提下执行部分特权操作。\n特权模式运行。在Dune环境下，用户态进程运行在一种特定模式下，这种模式被称作VMX非根模式，它允许：\n页面表管理：用户态进程可以直接修改页表，管理自己的虚拟地址空间，而无需每次都陷入内核。\n特权指令执行：某些以前需要切换到内核才能执行的指令可以直接在用户态执行，例如某种寄存器操作。\n执行优化：通过减少内核和用户态之间的上下文切换，Dune能够显著提高系统调用的性能。\n总结\nDune的设计目的是增强用户态程序的执行权限，但同时保持系统的安全性和隔离性。\n通过虚拟化技术（如Intel VT-x），Dune让用户态进程在一种受控的环境下运行，有些特权指令可以直接在用户态执行，而不用陷入内核。\n特权管理和隔离确保即使赋予用户态更多权限，系统仍然是安全的，不会因为用户进程的错误或恶意操作而受到破坏。\n总的来说，Dune的关键在于利用虚拟化硬件提供的一种隔离机制，使用户态进程能够安全、高效地执行一些原本需要内核态处理的操作，从而提高系统的整体性能。\nDune模式和页表 当用户进程进入Dune模式时，它确实会涉及到某些与页表相关的操作，但这些操作与传统的用户态到内核态的切换有所不同。以下是详细的解释：\n用户进程进入Dune模式和页表相关的操作\n进入Dune模式。当用户进程进入Dune模式，它并不会像传统的系统调用那样完全切换到内核态，而是进入一种特定的虚拟化模式（VMX Non-Root Mode）。在这种模式下，用户进程仍然运行在用户态，但它有更多的权限可以直接访问某些硬件资源和执行特权操作。\n页表管理\nEPT（扩展页表）：Dune模式下的存储管理通常依赖于扩展页表（Extended Page Table，EPT）。EPT是硬件虚拟化的一部分，它允许虚拟机监控器（VMM）管理内存映射，而不需要操作系统频繁切换页表。通过EPT，Dune可以为用户进程提供一种“伪内核态”的权限，让其直接管理和访问自己的内存空间，而不必每次都陷入内核。\n直接页表修改：Dune模式下，用户进程可以直接修改自己的页表，这是通过虚拟化硬件的支持实现的。在这种模式下，用户进程可以安全地进行页表操作，而不需要每次都进行上下文切换。\n用户进程进入内核态的页表切换\n用户态到内核态的切换。在传统的操作系统架构中，当用户进程需要进入内核态（例如执行系统调用），通常会涉及到一些上下文切换，这包括页表的切换。\n页表切换：当用户进程进入内核态时，操作系统可能会切换到一个不同的页表。这是因为内核态和用户态可能有不同的地址空间，需要通过不同的页表来管理。\n上下文切换：这包括保存当前用户态的CPU寄存器状态、切换到内核态的寄存器状态、更新栈指针等。然后内核会根据系统调用的参数和当前状态执行相应的内核逻辑。\n为什么需要页表切换\n地址空间隔离：用户态进程和内核态拥有不同的地址空间，使用不同的页表会提供更好的隔离和安全性，防止用户进程直接访问内核内存区域。\n权限管理：内核态需要访问所有系统资源，而用户态的访问权限较为有限。通过页表切换，内核可以获得全面的内存访问权限。\nDune与传统内核态的区别\n权限管理\n传统内核态：权限管理严格，用户态和内核态通过页表进行严格隔离，任何特权操作都需要通过陷入内核完成。\nDune模式：通过硬件支持的虚拟化技术，Dune在提供类似内核态权限的同时减少了上下文切换的开销，提升了操作效率。\n虚拟化硬件支持\n传统内核态：依赖于操作系统和CPU架构的基本特性进行上下文切换和权限隔离。\nDune模式：使用虚拟化硬件（如Intel VT-x）的高级特性，通过扩展页表（EPT）等机制实现更高效的权限管理和访问控制。\n总结\n用户进程进入Dune模式时，虽然会涉及到一些与页表相关的操作，但这些操作通过虚拟化硬件进行管理，使得切换过程更加高效。\n传统的用户态到内核态的切换通常需要页表切换和上下文切换，以保证地址空间的隔离和权限管理。\nDune模式通过虚拟化硬件提供了一种高效的替代方案，使用户进程在保留安全性的同时能够直接执行某些特权指令，减少了上下文切换的开销。\nEPT format incompatibility问题 Dune 解决 EPT（扩展页表）和标准 x86 页表格式不兼容的问题的方式如下。以下是详细步骤、每步的主题和执行地点：\n初始化空的 EPT（扩展页表）\n主题：EPT 初始配置 执行地点：Dune 模块初始化阶段 在 Dune 中，首先创建一个空的 EPT，表示此时并没有任何地址转换信息可用。这是因为 EPT 的格式与标准的 x86 页表格式不同，不能直接用于内核页表。\n在访问缺失的 EPT 条目时触发 EPT Fault\n主题：VM exit 处理 执行地点：EPT Fault 处理程序 每当访问缺失的 EPT 条目时，会触发一个 EPT Fault（这是一种 VM exit）。此时处理程序会捕获这个 Fault。\n向内核查询进程内存映射\n主题：查询内存映射 执行地点：EPT Fault 处理程序内部 当 EPT Fault 发生时，Dune 的处理程序会查询 Linux 内核，获取当前被访问的虚拟地址的内存映射和权限信息。此时需要向内核页面故障处理程序查询该地址的地址转换和权限。\n手动更新 EPT 以反映内核页表的映射\n主题：更新 EPT 执行地点：EPT Fault 处理程序内部 根据从内核查询到的地址转换信息，Dune 的处理程序会手动创建一个新的 EPT 条目。这个新条目反映了目标地址的正确物理地址和权限。该条目随后会**入到 EPT 中，以便处理故障的内存访问请求。\n处理地址区间的卸载（unmap）\n主题：地址区间管理 执行地点：MMU 通知链中的处理程序 系统运行过程中，有些地址区间可能会被卸载（unmap）。Dune 通过挂接到 MMU 通知链（这与 KVM 的做法类似）来处理这些事件。\n当某个地址被卸载（unmapped）时，Dune 接收到一个事件通知。\nDune 然后会逐出（evict）受影响的 EPT 条目，并且在适当的 Linux 页面结构中设置脏位（dirty bits）。\n支持页面访问信息和脏状态\n主题：页面状态维护 执行地点：Dune 模块中的 MMU 通知链处理程序中 为了支持内核的换页（swapping）和写回磁盘操作，内核需要页面的访问信息和脏状态。Dune 通过在 MMU 通知链中获取这些信息并适时地更新 EPT 及相应的页表条目来实现这一点。例如，当发现一个页面需要写回磁盘时，会将其在内核页表中的脏位标记为已设置。\n整体流程总结\n初始化：创建空的 EPT。 EPT Fault 处理：捕获 EPT Fault。 查询内核映射：向内核查询当前地址的映射关系。 更新 EPT：根据内核信息手动更新 EPT 条目。 处理 Unmap：使用 MMU 通知链处理地址区间的卸载。 页面状态维护：通过 MMU 通知链确保页面访问和脏状态信息的正确性。 这些步骤确保了 Dune 能够在不直接共享页表的情况下，尽可能反映进程在标准 Linux 操作系统下的地址空间。这一过程克服了 EPT 和标准 x86 页表格式不兼容的问题，并且在处理地址转换和权限时保持高效和灵活。\nEPT Fault是VM Exit VM Exit 是指从虚拟机模式退出到虚拟机管理程序（VMM）或主机操作系统的过程。当虚拟机遇到某些特定的事件或情况时，处理无法继续在虚拟机模式下进行，需要切换到VMM模式来处理，这时候会发生VM Exit。\nEPT（扩展页表）Fault 是由于虚拟机访问一个未被映射到物理内存的虚拟地址时发生的。当这样的情况发生时，虚拟机不能自行处理这个访问请求，需要主机的VMM处理，比如更新页表、加载数据等。这就需要退出到VMM模式，即触发一个VM Exit。\n为了更详细地理解过程，我们可以假设有一个运行在Dune环境下的进程，该进程尝试访问一个尚未映射的虚拟地址。假设前提为：\nDune进程在使用虚拟地址0xCAFEBABE进行读操作。 内核页表中关于此地址的信息尚未被加载到EPT中。 具体步骤\n初始化空的EPT 主题：EPT初始配置 地点：Dune模块初始化阶段 操作：Dune开始时创建一个空的EPT，表示尚无地址转换信息可用。 触发一个EPT Fault和VM Exit 主题：VM exit处理 地点：EPT Fault处理程序 操作：Dune进程尝试访问地址0xCAFEBABE。由于EPT中没有此地址的映射，发生了EPT Fault，导致一个VM Exit。 查询内核映射 主题：查询内存映射 地点：EPT Fault处理程序内部 操作：VM Exit引发Dune的Fault处理程序通过页表查询内核，找到0xCAFEBABE对应的物理地址和权限。假设内核返回此地址映射到物理地址0x12345000，并且权限为读（R），写（W），执行（X）。 手动更新EPT 主题：更新EPT 地点：EPT Fault处理程序内部 操作：根据内核信息，Dune在EPT中创建一个新的条目，将虚拟地址0xCAFEBABE映射到物理地址0x12345000，并设置适当的权限（RWX）。 Dune进程继续执行 主题：继续进程执行 地点：Dune进程 操作：EPT更新后，Dune进程能够继续执行对地址0xCAFEBABE的访问请求，且这次不会触发VM Exit。 处理地址区间的卸载\n假设后来某些原因导致地址区间0xCAFEBABE需要被卸载：\n地址区间unmap事件 主题：地址区间管理 地点：MMU通知链的处理程序 操作：Dune挂接到内核的MMU通知链，接收到地址0xCAFEBABE被unmap的通知。 清除EPT条目并更新内核页表 主题：页面状态维护 地点：Dune模块中的MMU通知链处理程序中 操作：Dune清除受影响的EPT条目，并在Linux页表中相应设置脏位（如果需要）。 Dune通过捕获VM Exit和EPT Fault来处理内存映射，并在必要时更新EPT条目。相对于传统的VMM，它并不完整模拟硬件环境，而是利用内核提供的接口和通知机制（例如MMU通知链）来实现内存管理。这使得Dune更加轻量和高效，能够在尽可能少的开销下提供虚拟化支持。\nTSC（时间戳计数器） TSC（Time Stamp Counter）是在CPU上的一个64位寄存器，用于记录自处理器重启以来经过的时钟周期数。程序可以使用rdtsc指令读取TSC的值来进行高精度的时间测量。由于TSC是与处理器的实际运行时钟直接相关的计数器，读取它不会引入额外的延时，因此非常适合性能分析等场景。然而，在虚拟化环境中，如果VMM对时间进行干预，TSC值可能会变得不太可靠，这就是为什么大多数VMM选择虚拟化TSC，以确保客操作系统获得一致的时间读数。\n例如，在高性能计算或需要精确时间测量的应用环境中，直接访问TSC可以提供非常高的时间测量精度。然而，在虚拟化环境中，如果VMM对TSC时间进行干预，时间测量的精度和一致性可能会受到影响。\nDune处理信号（signals） Dune在处理信号（signals）时，使用了一种与传统方法不同的机制。\n对于硬件页面故障（page faults），Dune完全利用硬件机制，基本上取代了软件信号（如SIGSEGV）的角色。 而对于其他类型的信号（如SIGINT），Dune模块通过向进程注入伪硬件中断（fake hardware interrupts）来处理这些信号。这种机制不仅高效，而且在处理特权模式时也能正确组合。 Dune在处理信号（signals）的具体解释：\n硬件页面故障和SIGSEGV 硬件页面故障：当进程访问没有映射的内存页时，硬件会产生一个页面故障。这种机制本身已经可以处理很多情况，比如尝试访问未授权内存、非法内存访问等。 SIGSEGV替代：Dune利用硬件页面故障来处理SIGSEGV（Segmentation Fault，段错误）的情况，不再需要额外的软件信号处理，因为硬件页面故障已经涵盖了这些角色。 其他信号（如SIGINT）的处理 注入伪硬件中断：对于其他类型的信号（如SIGINT），Dune会通过向进程内注入伪硬件中断来处理。当进程收到SIGINT信号时，Dune模块不直接处理信号，而是伪造一个硬件中断，这样进程在下次执行时会被中断处理器打断。 高效性：这种方式高效，因为硬件中断的处理机制通常比软件信号处理更直接，并且利用现有的硬件支持。 特权模式的正确组合 特权模式：特权模式指的是处理器的运行级别，通常包括用户态（ring 3）和内核态（ring 0）。 自动切换特权模式：例如，一个用户进程在运行某些不可信的代码时，可以被置于更低的特权级别（ring 3）。当发生硬件中断时，处理器会自动从ring 3切换到ring 0，确保中断处理和信号处理的安全性。 安全性的增强：这种机制确保即使在处理不可信代码时，硬件会自动进行特权级别的切换，从而保障系统的整体安全性。 假设一个用户进程正在运行，并且某些部分的代码由于不可信而在特权级别ring 3运行。在运行过程中：\n进程访问了一个未映射的内存地址，产生了页面故障。Dune利用硬件页面故障来处理这个访问，并不触发SIGSEGV软件信号。 用户按下Ctrl+C，发送SIGINT信号以中断进程。Dune模块接收到这个信号后，注入一个伪硬件中断到进程中。 由于硬件中断，处理器会自动从ring 3切换到ring 0安全地处理这个中断，并确保系统的安全性。 这种方法不仅高效，而且在不同特权级别之间正确切换，增强了系统的安全性。\nVPID的实现与工作原理 虚拟处理器标识符（VPIDs）提高了系统效率，特别是减少了TLB（翻译后备缓冲区）失效的开销。通过为每个Dune进程分配一个唯一的TLB标签，VPIDs使得在进行hypercalls和上下文切换时，不再需要频繁地刷新TLB。\nTLB是一个高速缓存，用于存储虚拟地址到物理地址的映射。它的主要作用是加速虚拟内存的地址转换。如果TLB中的条目发生变化（例如由于上下文切换到另一个进程），则需要进行TLB失效操作，以确保地址转换正确。\n每次TLB失效都意味着需要重新加载虚拟地址到物理地址的映射，这会带来显著的性能开销。尤其是当上下文切换频繁时，TLB失效会对系统性能产生较大影响。\nVPID（Virtual Processor Identifier）是在处理器虚拟化扩展中引入的一项技术，用于减少或消除TLB失效操作。\n为每个Dune进程分配唯一的TLB标签： 传统做法：没有VPID时，每次上下文切换必须使TLB失效，以确保新进程不会使用旧进程的地址映射。 VPID做法：有了VPID，每个Dune进程都有一个唯一的标识符，这个标识符也存在于TLB条目中。 TLB条目的标签： TLB条目：每个TLB条目不仅包含虚拟地址和物理地址的映射，还会包含对应的VPID。 有效性检查：在上下文切换时，处理器可以根据VPID快速判断当前TLB条目是否对新进程仍然有效。如果VPID与当前进程的标识符不匹配，说明该条目属于其他进程，应忽略。 如何具体实现VPID呢？\n分配VPID： 初始化：每个Dune进程启动时，Dune模块会从处理器的VPID池中分配一个唯一的VPID。 使用VPID： TLB条目包含VPID：当一个Dune进程运行时，处理器在生成TLB条目时会包含该进程的VPID。 上下文切换：上下文切换到另一个进程时，系统切换VPID而不必立即失效所有的TLB条目。 判断有效性： TLB查找：在进行地址转换时，处理器不仅检查虚拟地址，还会检查条目中的VPID是否与当前进程的VPID匹配。 有效TLB命中：如果匹配，说明当前TLB条目有效，可以直接使用，避免了重新加载地址映射。 假设有两个Dune进程A和B，分别分配了VPID 1和VPID 2。\n进程A运行 处理器在TLB中存储了若干条目，VPID为1，这些条目映射A进程的地址。 切换到进程B 处理器收到上下文切换请求，将当前的VPID设置为2，但不失效TLB条目。 TLB操作 处理器在进行地址转换时，如果发现TLB条目中的VPID不等于2，则忽略这些条目。 只有VPID等于2的条目（即进程B的映射）才会被使用。 通过这种方式，Dune在进行hypercalls和上下文切换时，不再需要频繁地使TLB失效，从而大大提高了系统的效率。VPID的使用确保了每个进程的TLB条目是独立的，进程间互不干扰。\nVMCALL与SYSCALL SYSCALL是x86架构中的一个指令，用于从用户态快速切换到内核态执行系统调用。它的主要用途是调用操作系统提供的服务，例如文件操作、进程管理等。这种切换通常伴随着上下文切换和权限级别的变化。\nVMCALL是虚拟化扩展中的一条指令，用于从虚拟机内部向虚拟机管理程序（VMM）发出请求。这种请求通常用于管理虚拟机资源或者操作VMM提供的虚拟化服务。\n为什么VMCALL比SYSCALL更快？在解释用VMCALL代替SYSCALL时，涉及一些虚拟化机制的细节：\n拦截处理与开销： SYSCALL：传统的SYSCALL指令需要从用户态切到内核态，这涉及到特权级别的转换和上下文保存，可能会带来一定的开销。 VMCALL：在虚拟化环境中，VMCALL用于虚拟机和虚拟机管理程序之间的通信。Dune可以利用这个机制来快速处理特权操作，而不需要进行多余的特权级别转换。 直接操作与简化路径： SYSCALL路径：SYSCALL路径可能涉及到复杂的系统调用处理逻辑，以及潜在的上下文切换和寄存器保存/恢复操作。 VMCALL路径：VMCALL路径可以更直接，因为它已经被设计用于虚拟环境中的高效通讯，大部分情况下不需要进行复杂的操作。 Dune的特殊优化： Dune环境：在Dune中，用户进程可以直接使用一些特权硬件功能（通过Dune提供的接口），利用VMCALL可以避免SYSCALL路径的复杂性。 libDune中的优化：修改后的libc通过VMCALL指令直接与Dune进行交互，可以减少系统调用的开销，从而获得性能提升。 假设有一个用户程序需要调用一个系统服务，比如获取当前时间：\n传统的SYSCALL路径： 用户程序发出SYSCALL指令。 处理器切换到内核态，保存当前用户态上下文。 内核处理系统调用并返回结果。 处理器切换回用户态，恢复上下文。 改用VMCALL路径（在Dune中）： 用户程序发出VMCALL指令。 处理器直接交由Dune处理，可能跳过一些特权级别转换和复杂的系统调用路径逻辑。 Dune通过已经设置好的接口直接处理请求，提高执行效率。 正是由于这些优化，使得在虚拟化环境中，使用VMCALL指令相比于传统的SYSCALL指令，可以获得轻微的性能提升。这样，Dune可以更加高效地实现其特权操作，充分利用虚拟化环境下的硬件性能。\n进程切换到Dune模式的启动流程 将一个进程切换到Dune模式的启动流程。这类似于启动一个操作系统（OS）。具体步骤如下：\n创建有效的页表 要求：在切换到Dune模式之前，必须提供一个有效的页表。 原因：简单的身份映射（identity mapping，虚拟地址直接映射到相同的物理地址）是不够的，因为需要考虑到EPT（扩展页表）的压缩布局。这个压缩布局意味着虚拟地址空间在EPT中的表现方式可能有所不同，需要一个特定的页表来匹配这种布局。 调用Dune入口ioctl 过程：一旦页表创建完成，通过传递页表根（page table root）作为参数来调用Dune entry ioctl。这个系统调用是用来通知Dune模块开始切换模式。 操作：Dune模块接收到ioctl调用后，会切换进程到Dune模式，并开始执行代码，使用提供的页表根作为初始的%CR3寄存器值。 libDune进一步配置环境 配置特权寄存器：libDune会配置一些特权寄存器来建立一个合理的操作环境。 加载GDT：全局描述符表（GDT）被加载，用于提供基本的平面分段。 加载IDT：中断描述符表（IDT）被加载，用于捕获硬件异常。 设置TSS中的独立栈：任务状态段（TSS）中设置了一个独立的栈，用于处理双重故障（double faults）。 配置GS段基址：配置GS段的基址，以便轻松访问每个线程的数据。 为什么不能直接映射页表？简单的身份映射无法满足Dune需求的原因有以下几点：\nEPT的压缩布局： 在Dune环境中，EPT所使用的物理内存布局可能不同于传统的直接映射。这种压缩布局允许Dune更高效地管理内存，但也意味着需要特定的页表来正确映射这些地址。 地址空间一致性： 尽管目标是让进程在转换前后的地址保持一致，但需要确保这些地址能够在EPT的压缩布局中有效映射。直接身份映射可能无法在压缩布局中找到合适的位置。 在切换到Dune模式和配置操作环境期间，以下寄存器会发生变化：\n%CR3寄存器： 这个寄存器包含页表根地址，切换到Dune模式后，%CR3会被设置为提供的页表根。 特权寄存器： GDT（全局描述符表基址寄存器）：加载一个新的GDT来提供基本平面分段。 IDT（中断描述符表基址寄存器）：加载一个新的IDT来捕获硬件异常。 TSS（任务状态段）： 设置一个独立的栈用于处理双重故障。如果发生双重故障，处理器会使用TSS中的栈。 段寄存器： GS段寄存器：配置GS段的基址，以便访问线程本地存储（TLS）数据。这个访问是为了让每个线程能够拥有独立的数据区，减少线程之间的干扰。 将进程切入Dune模式需要考虑到许多细节，以确保环境的正确性和高效性。从提供合适的页表到配置各种特权寄存器，每一步都至关重要。这种复杂的启动过程确保了Dune能够提供一个强大且高效的虚拟化环境。\n传递页表根（page table root）作为参数在将进程切换到Dune模式中的作用非常重要，这是因为页表根决定了整个虚拟地址空间的映射方式。以下是更详细的解释：\n页表根通常指页表的顶层指针，它在 x86 架构中存储在 %CR3 寄存器中。这个指针指向一个多级页表的起点，页表用于管理虚拟内存地址到物理内存地址的映射。页表根的主要作用为：\n初始化内存映射环境： 作用：页表根指向了多级页表的起点，通过这些页表，系统可以将虚拟地址转换为物理地址。这是虚拟内存管理的核心。 重要性：在切换到Dune模式时，传递页表根作为参数可以确保虚拟地址能够正确映射到物理地址，从而使进程能够继续无缝运行。 保证地址空间一致性： 作用：传递页表根使得Dune可以使用调用方提供的内存映射，从而保证进程在切换到Dune模式前后的地址空间保持一致。 重要性：地址空间的一致性对于进程的正常运行至关重要。否则，进程可能会在切换后找不到所需的内存，导致崩溃或异常行为。 支持EPT压缩布局： 作用：Dune利用EPT（扩展页表）对内存进行管理，这可能涉及到内存的压缩布局。传递特定的页表根可以确保内存映射符合Dune的需求。 重要性：简单身份映射无法适应EPT的压缩布局，必须使用特定的页表根来匹配这些布局，以确保虚拟地址在物理内存中的合理分布。 具体过程\n创建页表： 在将进程切换到Dune模式之前，系统或libDune会生成并设置一个新的页表，这个页表按照Dune环境的需求进行映射和配置。 传递页表根： 当调用Dune entry ioctl时，新创建的页表根作为参数传递给Dune模块。 更新%CR3寄存器： Dune模块接收到页表根后，会将其设置为%CR3寄存器的值。这样，处理器在进行地址转换时，就会根据这个新的页表来进行虚拟地址到物理地址的映射。 环境配置： 之后，libDune会继续配置其他特权寄存器和必要的系统资源，例如加载GDT、IDT，设置TSS栈和GS段基址等，以完成虚拟化环境的初始化。 假设我们有一个进程，其虚拟内存地址需要映射到特定的物理地址：\n原始页表：原来的页表根指向某个起始地址P，包含对虚拟地址V1到物理地址P1的映射，以及V2到P2的映射。 新页表：在切换到Dune模式之前，生成一个新页表根指向地址Q，这个新页表符合Dune的EPT布局，并且依然包含对V1到P1，V2到P2的映射。 当进程切换到Dune模式并传递新页表根之后：\n%CR3寄存器会被更新为新页表根Q。 处理器将使用新页表来进行地址转换，确保虚拟地址V1和V2依然能够正确地映射到P1和P2，从而确保进程的正常运行。 传递页表根作为参数在切换到Dune模式中至关重要，因为它确保了虚拟地址到物理地址的正确映射，保证地址空间一致性，并适应Dune的EPT布局需求。这个步骤有效地初始化了内存管理环境，使得系统能够顺利过渡到虚拟化状态。\n为什么直接使用旧的页表根并不可行，以及为什么在Dune模式下需要新页表根。关键差异和原因：\nEPT和标准页表的格式差异\n标准页表：这是操作系统使用的页表，用于管理虚拟地址到物理地址的映射。\nEPT（扩展页表）：这是硬件虚拟化中的一个关键组件，尤其在Intel VT-x技术中使用。EPT具有不同的格式和特点，不完全兼容常规的页表格式。\nEPT压缩布局\n地址空间压缩：Dune模式可能对某些虚拟地址进行压缩布局，以适应硬件虚拟化需求。如果直接使用旧的页表根，无法利用这些压缩布局的优化，可能会导致内存映射无效或效率低下。\n专用页表结构：为了支持这些优化，Dune需要一个特别设计的页表结构，这些结构与常规页表可能完全不同。\n内存保护和隔离\n安全性：Dune需要确保进程在虚拟化和非虚拟化模式之间的切换过程中，内存保护和隔离的一致性。采用新页表根可以防止在转换过程中出现安全漏洞或内存访问违规。\n特权寄存器设置：不同模式下，特权寄存器的配置方式不同，直接使用旧页表根可能无法正确初始化这些寄存器。\n通过实际操作的解析，理解为什么需要新页表根会更为直观。\n初始化新页表根： 生成新页表：在进入Dune模式之前，系统会生成一个符合Dune需求的页表，它考虑到EPT的特殊布局和格式。\n设置映射：根据旧页表的信息，将必要的虚拟地址到物理地址的映射配置到新页表中，但格式和布局可能不同。\n调用初始化IOCTL： 传递新页表根：通过IOCTL调用，把新页表根传递给Dune模块。\n更新%CR3寄存器：Dune模块将新页表根设置为%CR3寄存器的值，使处理器在进入Dune模式后使用新页表进行地址转换。\n配置其他特权资源： 加载GDT、IDT：在初始化过程中，libDune会加载全局描述符表（GDT）和中断描述符表（IDT），这些操作依赖于新页表的正确配置。\n设置TSS栈和GS段基址：这些配置同样需要依赖新的页表布局，以确保处理器可以正确访问所需的内存区域。\n假设我们有以下内存布局需求：\n虚拟地址V1映射到物理地址P1。 虚拟地址V2映射到物理地址P2。 原始页表的情形：\n页表根P：指向的页表中，V1 -\u0026gt; P1，V2 -\u0026gt; P2。 新页表的情形（Dune的需求下）：\n页表根Q：由于EPT的格式和压缩布局要求，这个新页表的结构可能如下：\n新的页表具有压缩特性，在同样完成V1 -\u0026gt; P1，V2 -\u0026gt; P2映射的同时，其方式更加优化和特定化，适应Dune环境。\n尽管旧页表和新页表在内容上可能看起来相似（例如映射相同的虚拟地址到物理地址），但它们的格式、结构和布局可能存在显著差异，这些差异对于支持Dune模式下的高效虚拟化至关重要。直接使用旧的页表根无法满足这些需求，因此必须提供一个新的页表根来确保Dune模式下的正确性和性能。\nsandbox防御ELF header 攻击 ELF文件是Linux系统的常见可执行文件格式，包含了程序的头部（header），该头部定义了二进制文件的各种结构和加载方式。不良设计或恶意构造的ELF头部可能会试图利用ELF加载器中的漏洞，从而实现代码注入或其他攻击。\n为了防御ELF头部攻击，Dune的沙箱采用了双重ELF加载器策略：\n初始ELF加载器（Minimal ELF Loader）： 角色：负责加载沙箱运行时环境。 实现：这个加载器是libDune的一部分，只支持静态二进制文件的加载。 安全性：由于只支持静态二进制文件，其复杂度和攻击面都较小，减少了被恶意ELF头部利用的风险。 二次ELF加载器（ld-linux.so）： 角色：在沙箱内加载不可信的二进制文件。 实现：使用Linux中的ld-linux.so，它是一个经过广泛使用和信任的ELF加载器。 安全性：ld-linux.so已经是Linux系统中不可或缺且受到信任的组件，经过严格的安全审查和优化。 双重ELF加载器策略防御的详细流程如下\n加载沙箱运行时环境： 步骤：内核启动沙箱进程，并通过libDune中的初始ELF加载器载入一个独立的、最小化的沙箱运行时环境。 复杂度：因为这个加载器只支持静态二进制文件，功能简单，减少了被攻击的可能性。 加载二次ELF加载器： 步骤：沙箱运行时环境使用初始ELF加载器，把ld-linux.so（作为二次ELF加载器的一部分）加载到一个不可信的环境中。 信任：使用ld-linux.so是因为它是Linux系统的一部分，经过广泛使用和安全验证。 加载不可信二进制文件： 步骤：ld-linux.so运行在沙箱内的用户态中，从不可信的环境中加载和执行不可信的二进制文件。 防御：因为ld-linux.so在用户态（ring 3）运行，即使不可信的二进制文件包含恶意的ELF头部，其攻击范围也仅限于用户态，无法直接影响内核或沙箱的运行时环境。这限制了其破坏力。 Dune沙箱通过采用双重ELF加载器机制，有效地将潜在的ELF头部攻击风险最小化。初始ELF加载器简单且只有有限的功能，使其难以成为攻击目标。而二次ELF加载器ld-linux.so在用户态加载不可信二进制文件，即使有攻击发生，也局限在用户态范围内，避免影响沙箱的核心安全性。\nELF头部攻击的实现和原理\nELF（Executable and Linkable Format）文件是Linux和其他类Unix操作系统中常见的可执行文件格式。ELF头部包含了关于文件布局和加载信息的元数据，例如程序头表、节头表、入口点地址等。恶意构造的ELF头部可以尝试利用ELF加载器中的漏洞，从而实现代码注入或其他攻击。\nELF头部攻击通常利用以下几个方面：\n格式解析漏洞： 描述：ELF加载器在解析头部信息时出现的漏洞。例如，缓冲区溢出、整数溢出、未做边界检查等。 影响：攻击者可以通过构造特别的头部数据，使加载器在解析时发生溢出或崩溃，从而执行任意代码或读取不应访问的内存区域。 高级加载器功能滥用： 描述：ELF格式支持复杂的加载和链接功能，如动态链接、内存映射、重定位等。攻击者可以利用这些功能，通过精心设计的头部信息，实现加载器预期之外的行为。 影响：例如，构造特定的段表和节表，使加载器跳到恶意代码的地址处执行。 以下是几个可能的ELF头部攻击实现方法：\n缓冲区溢出： 原理：通过构造异常长的字段值，诱使加载器在处理这些字段时发生缓冲区溢出。常见的目标字段包括节名字、节表项、程序头表项等。 示例：将段表或节表的大小字段设置得超大，加载器试图读取或处理超大数据量时发生溢出，导致恶意代码被执行。 整数溢出： 原理：利用加载器在处理ELF头部时的整数溢出漏洞。通过设置特定的字段值，诱使加载器在计算内存位置或大小时发生溢出，从而覆盖关键数据。 示例：将某个偏移字段设置为接近整数上限，使加载器在加法操作时发生溢出，导致访问不该访问的内存地址。 段表和节表操控： 原理：通过操纵段表和节表中的条目，使加载器加载和执行恶意代码段。 示例：将PT_LOAD段的虚拟地址设为指向恶意代码的位置，加载器在加载段时会将恶意代码映射到进程地址空间并执行。 重定位表攻击： 原理：ELF文件中包含重定位表，用于动态重定位符号地址。攻击者可以构造恶意重定位条目，导致加载器跳转到恶意代码。 示例：在重定位表中添加条目，使特定的符号地址被重定向到攻击者控制的内存区域。 为了防止上述攻击，常见的防御措施包括：\n硬化加载器： 边界检查：认真检查每个字段的边界，不允许超出预期范围。 格式验证：严格遵循ELF规范，验证每个部分的格式和一致性。 运行时防护： 地址空间布局随机化（ASLR）：随机化内存地址布局，使攻击者难以预测内存位置。 栈保护（Stack Canaries）：在栈变量前后添加特定的哨兵值，防止栈溢出攻击。 最小化加载器功能： 静态二进制支持：如Dune中的初始ELF加载器，仅支持静态二进制文件，减少复杂功能的使用。 双层加载策略：分离初始加载器和高级加载器，减少初始加载器的攻击面，并在不可信环境中运行高级加载器，如ld-linux.so。 以下是一个简单的ELF头部溢出攻击示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ccodepreviewrun// 伪代码示例，实际情况可能更复杂 // 攻击者构造的恶意ELF头部 struct elf_header { // 正常的ELF头部 char e_ident[16]; // ELF标识 uint16_t e_type; // 文件类型 uint16_t e_machine; // 机器类型 uint32_t e_version; // 版本 uint64_t e_entry; // 入口点 uint64_t e_phoff; // 程序头偏移 uint64_t e_shoff; // 节头偏移 uint32_t e_flags; // 标志 uint16_t e_ehsize; // ELF头大小 uint16_t e_phentsize; // 程序头项大小 uint16_t e_phnum; // 程序头项数目 uint16_t e_shentsize; // 节头项大小 uint16_t e_shnum; // 节头项数目 uint16_t e_shstrndx; // 字符串节索引 // 溢出部分 char overflow[1024]; // 超长字段，企图溢出缓冲区 uint64_t malicious_code; // 恶意代码地点 }; 攻击者可以通过修改上述字段，例如设置虚拟地址段为指向malicious_code部分，使加载器在解析阶段受溢出影响，将控制权转移到恶意代码。\nELF头部攻击通过构造恶意头部数据，利用加载器中的漏洞，实现代码注入或其他恶意操作。通过严谨的加载器设计和运行时保护机制，可以有效防御这类攻击。Dune采用双重ELF加载器策略，通过最小化初始加载器功能和使用可信的高级加载器，大大降低了头部攻击的风险。\n分离初始加载器和高级加载器的概念\n初始加载器（Minimal ELF Loader）\n功能有限：只支持加载静态（static）二进制文件，具有最低限度的功能。 目标：主要用于加载高级加载器，而不直接加载不可信的二进制文件。 典型例子：libDune中的最小ELF加载器。 高级加载器（Secondary ELF Loader）\n功能全面：可以处理动态链接（dynamic linking）、重定位（relocation）等复杂操作。 目标：在不可信环境中加载和执行不可信的二进制文件。 典型例子：Linux中常用的ld-linux.so。 分离加载策略的流程\n初始加载器加载沙箱运行时环境： 加载并初始化一个非常精简和受信任的沙箱运行时环境，以便在安全和受限的环境中执行后续操作。 初始加载器加载高级加载器： 使用初始加载器将高级加载器（如ld-linux.so）加载到沙箱的用户态环境中。此时，初始加载器的责任就完成了。 高级加载器加载不可信的二进制文件： 在沙箱的用户态环境中，高级加载器开始加载和执行不可信的二进制文件。\n即使不可信的二进制文件包含恶意头部，高级加载器运行在用户态，攻击面较小，只能影响沙箱内部而不影响整个系统。\n设想以下场景：一个不可信的二进制文件untrusted_app需要在Dune沙箱中运行。\n初始化和配置 创建沙箱：内核启动一个新的进程，进入Dune模式，创建一个隔离的沙箱运行时环境。\n初始加载器：libDune的最小ELF加载器被加载到沙箱中。\n1 2 3 4 5 6 7 8 9 // 初始加载器的简化伪代码 void initial_loader(const char* secondary_loader_path) { // 加载静态二进制文件 load_static_binary(\u0026#34;/path/to/sandbox_runtime\u0026#34;); // 加载二级加载器 load_static_binary(secondary_loader_path); // 跳转执行二级加载器 jump_to_entry_point(secondary_loader_path); } 初始加载器加载高级加载器 路径：假设高级加载器的路径是/lib64/ld-linux-x86-64.so.2。\n加载操作：初始加载器将ld-linux.so加载到沙箱中，并跳转到它的入口点。\n1 2 3 4 5 6 7 8 9 10 11 // 继续初始加载器的伪代码 void initial_loader(const char* secondary_loader_path) { // 加载静态二进制文件 load_static_binary(\u0026#34;/path/to/sandbox_runtime\u0026#34;); // 加载二级加载器 load_static_binary(secondary_loader_path); // 跳转执行二级加载器 jump_to_entry_point(secondary_loader_path); } initial_loader(\u0026#34;/lib64/ld-linux-x86-64.so.2\u0026#34;); 高级加载器加载不可信二进制文件 启动路径：在ld-linux.so加载并初始化后，它开始处理不可信的二进制文件untrusted_app。\n内存映射：ld-linux.so处理ELF头部，并映射不可信二进制文件中的各个段。\n执行：完成加载后，ld-linux.so将控制权转交给untrusted_app的入口点，开始执行不可信代码。\n1 2 # 使用ld-linux.so加载并执行不可信二进制文件 /lib64/ld-linux-x86-64.so.2 /path/to/untrusted_app 安全保障 用户态执行：整个加载和执行过程在沙箱的用户态（ring 3）进行，限制了不可信代码的权限和影响范围。\n高级加载器的安全性：由于ld-linux.so是经过大量使用和安全审查的组件，其安全性较高，减少了被恶意头部利用的可能。\n总结\n通过分离初始加载器和高级加载器的加载过程，Dune能够有效地抵御ELF头部攻击：\n初始加载器功能有限，只负责加载高级加载器，并且设计简单，减少攻击面。 高级加载器运行在用户态，并且经过充分验证和使用，在加载不可信二进制文件时，能最大限度地减少安全风险。 这种分离策略不仅提高了加载过程的安全性，还确保了即使不可信代码包含恶意头部，攻击的范围和影响也被限制在沙箱环境内。\n初始加载器确实在限定和设置高级加载器的运行环境方面起到了关键作用。如果没有初始加载器，直接加载高级加载器可能会带来一系列问题和安全隐患。让我们详细探讨这种分离策略的设计理由和必要性。\n初始加载器的主要作用\n环境初始化： 安全的执行环境：初始加载器负责创建一个受限制的、安全的执行环境，确保不可信代码在隔离的沙箱中运行。加载高级加载器之前，初始化必要的安全机制。 基础设施设置：例如，配置页表、加载全局描述符表（GDT）、中断描述符表（IDT），以及设置任务状态段（TSS）中的栈等。这些操作需要在特权模式下（ring 0）完成，是高级加载器无法直接处理的。 功能简化，减少攻击面： 只加载静态二进制：初始加载器只支持加载功能受限的静态二进制文件，功能简单，减少了潜在漏洞的数量和复杂度。 有限的攻击面：由于功能的简化，初始加载器的代码攻击面相对较小，不容易被恶意利用。 上下文切换和执行控制： 受控地启动高级加载器：初始加载器负责安全地加载和启动高级加载器，在确保环境安全的前提下，转移执行控制权。 配置用户态环境：在加载高级加载器之前，初始加载器可以设置用户态的特定配置，确保高级加载器在一个预期和受控的环境中运行。 如果没有初始加载器的后果\n缺乏安全保障： 直接暴露高级加载器：如果直接加载高级加载器（如ld-linux.so），任何环境初始化和安全配置都无法保证，加载过程可能会暴露在潜在的安全威胁中。 特权操作：高级加载器一般在用户态运行，缺乏执行特权操作的能力，如设置页表和配置特权寄存器，这些操作对于创建安全的执行环境至关重要。 复杂度增加： 处理复杂性：直接暴露高级加载器需要它处理非常复杂的环境初始化和安全设置，增加了代码复杂性，也增加了潜在漏洞的数量。 安全审计难度：代码复杂性和多样化功能使得对加载器进行全面安全审计变得更加困难，增加了被攻击的风险。 加载路径不受控制： 潜在恶意干扰：未经初始加载器预先配置和限制的加载路径可能被恶意代码干扰，导致加载不可信的或未签名的代码片段。 通过对加载流程的具体示例，我们可以更好地理解初始加载器的作用：\n步骤1：内核启动并加载初始加载器\n进入Dune模式：内核启动一个新的进程，并将其切入Dune模式，初始化隔离环境。 加载最小ELF加载器：内核通过libDune加载一个功能受限的、只支持静态二进制的初始加载器。 1 2 3 4 5 6 7 // 伪代码示例：内核启动的部分 void kernel_startup() { // 切换到Dune模式，初始化安全环境 enter_dune_mode(); // 加载并执行初始加载器 load_initial_loader(\u0026#34;/path/to/minimal_loader\u0026#34;); } 步骤2：初始加载器加载和设置高级加载器\n加载高级加载器：初始化基本环境后，初始加载器将ld-linux.so加载到受限的用户态环境中。 1 2 3 4 5 6 7 8 9 10 // 初始加载器的伪代码 void initial_loader(const char* secondary_loader_path) { // 设置基本的安全环境，如页表、GDT、IDT、TSS等 setup_basic_environment(); // 加载高级加载器 load_static_binary(secondary_loader_path); // 转移控制权到高级加载器 jump_to_entry_point(secondary_loader_path); } initial_loader(\u0026#34;/lib64/ld-linux-x86-64.so.2\u0026#34;); 步骤3：高级加载器加载不可信二进制文件\n用户态执行：在安全的用户态环境中，高级加载器ld-linux.so负责加载不可信的二进制文件，并执行它们。 防御机制生效：即使不可信二进制文件包含恶意ELF头部，攻击面仅限于用户态，无法影响内核或特权环境。 1 2 # 使用高级加载器加载并执行不可信二进制文件 /lib64/ld-linux-x86-64.so.2 /path/to/untrusted_app 通过以上解释和示例可以看到，初始加载器在设置和限制高级加载器的运行环境方面起到了关键作用。没有初始加载器的附加保护，高级加载器的环境将无法控制，容易受到恶意代码的攻击和干扰。初始加载器确保了安全的环境初始化和基本配置，从而为高级加载器提供了一个安全和受控的执行环境。这种分离策略不仅提高了加载过程的安全性，还限制了恶意代码的攻击范围，确保整个系统的稳定和安全。\nMachine Abstraction “使用虚拟化硬件提供进程抽象，而不是机器抽象”，实际上在描述虚拟化技术的不同层次和方式。为了更好地理解其中的概念，我们需要先弄清楚“进程抽象（process abstraction）”和“机器抽象（machine abstraction）”的含义。\nMachine Abstraction（机器抽象）\n“机器抽象”指的是通过虚拟化技术，将一台物理计算机抽象成多个虚拟机，每个虚拟机看起来像是一台独立的物理计算机。这种虚拟化是以“机器”为核心的，每个虚拟机都有独立的硬件资源（如CPU、内存、存储等），并运行一个完整的操作系统。用户或应用程序看到的是一个完整的虚拟机器。\n这种虚拟化技术的典型例子是虚拟机管理程序（Hypervisor），比如VMware、Xen或者KVM。每个虚拟机都有自己的操作系统，并且能运行不同的应用程序，就像使用一台真实的物理机器一样。\nProcess Abstraction（进程抽象）\n“进程抽象”则不同，它更关注虚拟化的粒度。在这种情况下，虚拟化提供的是进程级别的隔离，而不是虚拟一整台机器。它不再是模拟出一整台虚拟的计算机，而是提供一个隔离的、用于运行单个进程或应用的环境。这种进程级别的虚拟化通常比机器抽象更加轻量。\n**容器（Containers）**就是一种常见的进程级别的虚拟化方式，比如Docker、Kubernetes等。与虚拟机不同，容器不需要一个完整的操作系统实例，而是共享主机的内核资源，同时每个容器运行自己的应用程序或进程。这样可以大大提高资源利用率，并减少开销，因为不需要为每个应用程序运行一个完整的操作系统。\n解释\n这句话的意思是，文章所指的虚拟化技术是以“进程抽象”为核心，而不是传统的“机器抽象”。也就是说，虚拟化硬件并不是为每个应用创建一个独立的虚拟机，而是为每个应用或进程提供一个隔离的、虚拟化的运行环境。这样的虚拟化方式更加轻量化，更接近容器化技术，比如Docker或其他类似的技术。\n总结来说，虚拟机技术（machine abstraction）模拟的是一整台虚拟机器，而进程级别的虚拟化（process abstraction）则是在操作系统的级别进行隔离，为单个应用提供环境。\n虚拟化硬件可以被配置成避免为虚拟机保存和恢复某些硬件状态 当你运行一个虚拟机时，底层硬件（CPU、内存、I/O设备等）必须模拟或管理整个虚拟机的运行状态。这些状态包括：\nCPU寄存器：比如通用寄存器、指令指针、栈指针等。 内存状态：虚拟机当前使用的内存页面。 I/O设备状态：磁盘、网络接口等外设的状态。 其他硬件资源的状态：如中断控制器状态等。 每当虚拟机从一个任务切换到另一个任务时，或者从“主机”系统切换到“客户”虚拟机时，虚拟化系统必须保存这些硬件状态，并在需要时进行恢复。这被称为上下文切换（context switch），这是非常重要且资源密集的工作，尤其在处理虚拟机时，因为虚拟机模拟了整台计算机的硬件，所以每次切换时要保存和恢复的状态较多。\n保存和恢复完整的硬件状态代价很高。每次切换虚拟机时，虚拟化管理程序（如Hypervisor）都需要保存虚拟机的所有硬件状态，并在切换回来时重新恢复。这会导致以下几个问题：\n性能开销大：保存和恢复所有的硬件状态需要大量的时间和计算资源，尤其是在虚拟机数量较多时，频繁的上下文切换会导致系统性能下降。 复杂性增加：完整的硬件状态管理非常复杂，特别是在涉及多个硬件设备和资源时。 因此，避免或减少这种状态保存和恢复可以显著提高虚拟化的效率，特别是在处理较轻量级的虚拟化工作负载时，比如容器化进程（process abstraction），它们并不需要管理完整的硬件状态。\n要避免保存和恢复虚拟机所需的多个硬件状态，虚拟化硬件和管理程序可以通过以下方式进行优化：\n更轻量的虚拟化层次：像容器这样的进程级虚拟化不需要模拟整个操作系统和硬件，而是直接使用主机操作系统的内核，因此不需要保存和恢复大量的硬件状态。相比虚拟机，容器技术只需要隔离进程和网络等基本资源，从而减少了硬件状态的依赖。 硬件辅助虚拟化：现代CPU架构（如Intel的VT-x和AMD的AMD-V）提供了硬件辅助虚拟化技术，允许某些硬件状态直接在CPU层面管理，减少了虚拟化管理程序需要手动保存和恢复的状态。这样做可以减少上下文切换的负担，提升性能。 减少硬件仿真：通过简化虚拟化环境中的硬件仿真（如只提供虚拟机必要的部分硬件接口），虚拟化软件可以避免保存不必要的硬件状态，专注于管理核心资源（如CPU和内存），避免设备和外设等不常用资源的状态切换。 这句话的核心意思是：通过优化虚拟化硬件配置和管理方式，可以减少或避免对某些不必要硬件状态的保存和恢复。这种优化方式尤其适用于较轻量的虚拟化场景，比如容器，而不适用于完整虚拟机的模拟。通过减少状态保存和恢复的工作量，虚拟化系统可以显著提高性能，尤其是在频繁的上下文切换中。\nVMM 这些段落讨论了虚拟化技术中的几个关键概念，特别是VMX（Intel的虚拟机扩展）如何管理虚拟机和虚拟机管理程序（VMM）的交互。让我逐一解释并总结这些概念。\nVMM（Virtual Machine Monitor，虚拟机监控器）是虚拟化技术的核心部分，有时也称为Hypervisor。它的主要任务是管理虚拟机的执行，确保每个虚拟机能够独立运行，同时控制虚拟机与物理硬件的交互。\nVMM的作用：\n管理虚拟机的生命周期（创建、运行、暂停、销毁）。 隔离和保护每个虚拟机的资源，防止它们互相干扰。 控制虚拟机对硬件的访问，确保虚拟机不会直接操作底层硬件。 VMX Root Mode vs VMX Non-Root Mode\nVMX Root Mode：这是VMM运行的模式。虚拟化硬件在这个模式下给VMM完全控制权，可以管理虚拟机和底层物理硬件。VMM在VMX root mode下运行，并在需要管理虚拟机时切换到VMX non-root mode。 VMX Non-Root Mode：这是虚拟机（或Guest）运行的模式。在这个模式下，虚拟机以一种“受控”的方式运行，它并不直接访问硬件，而是通过VMM的控制。虚拟机的一些操作可能会触发VM exit，将控制权交还给VMM。 因此，VMX Root Mode 是VMM的运行模式，而VMX Non-Root Mode 是虚拟机的运行模式。\n这些模式与进程的关系\n在虚拟化环境中，虚拟机（或“guest”）其实相当于一个运行在VMM控制下的“操作系统”。这不同于普通的用户进程。普通的用户进程是运行在操作系统上的程序，而虚拟机则是运行在虚拟机管理程序上的操作系统。因此，它的虚拟化级别更深，与进程并不是同一个概念。 Guest 通常指的是虚拟机，而不是用户进程。在虚拟化环境中，guest 可能是运行在虚拟机中的一个完整的操作系统或多个进程。 VM entry 和 VM exit\nVM entry：当VMM执行 VMLAUNCH 或 VMRESUME 指令时，硬件会自动进行VM entry，这意味着CPU从VMX root mode切换到VMX non-root mode，开始运行虚拟机（guest）。 VM exit：当需要VMM采取某种操作时（比如虚拟机执行了某些敏感指令），硬件会触发VM exit，将CPU从VMX non-root mode切换回VMX root mode，并跳转到VMM的入口点。 在这些切换过程中，硬件自动保存和恢复大部分的架构状态，这些状态被存储在内存中的数据结构中，称为VM控制结构（VMCS）。\nVMCS 和硬件状态\nVMCS（Virtual Machine Control Structure）是一个特殊的数据结构，存储了虚拟机和VMM之间上下文切换时的状态信息。它包括：\n虚拟机的架构状态（如CPU寄存器等）。 各种配置参数，这些参数让VMM能够控制虚拟机的执行行为。 通过配置VMCS，VMM可以控制哪些硬件暴露给虚拟机。例如，VMM可以配置让虚拟机执行 HLT 指令时触发VM exit，也可以允许虚拟机直接暂停CPU。\n虚拟内存和EPT（Extended Page Table）\n虚拟内存是虚拟化过程中最复杂的硬件特性之一。一个简单的解决方案是让虚拟机直接访问页表根寄存器（%CR3），这会给虚拟机完全的控制权，允许它配置页表来访问任意物理内存地址，甚至包括VMM自身的内存，这显然是不安全的。\n为了解决这个问题，VT-x 提供了扩展页表（EPT）机制，它增加了一个额外的地址翻译层。EPT由VMM管理，能够确保虚拟机在访问虚拟内存时仍然受到内存隔离的约束，防止其访问不该访问的物理内存。AMD的SVM（Secure Virtual Machine）技术有类似的机制，叫做嵌套页表（NPT。\n总结\nVMM 是虚拟机监控器，管理虚拟机与物理硬件的交互。 VMX Root Mode 是VMM运行的模式，VMX Non-Root Mode 是虚拟机运行的模式。 VMCS 是存储虚拟机状态和配置的结构，允许VMM控制虚拟机的执行。 虚拟内存管理 通过硬件辅助技术（如EPT/NPT）实现，确保虚拟机不能访问不该访问的物理内存。 VM entry和VM exit 在Dune系统中，VM entry和VM exit是两个重要的过程，用来管理用户态进程和虚拟化层之间的权限切换。以下是它们分别在什么情况下执行以及一些具体的示例。\nVM Entry：进入Dune模式\nVM Entry是指从传统的用户态或内核态“进入”到Dune提供的虚拟化环境（VMX Non-Root Mode）。在这个过程中，系统会保存当前的状态并切换到Dune模式，使得用户进程可以利用Dune提供的增强权限执行特权操作。\nVM Entry通常在用户进程需要利用Dune的增强权限操作时执行。以下是一些典型的例子：\n进程启动： 当一个用户进程启动并被设置为运行在Dune模式下时，Dune系统会执行VM Entry，进入虚拟环境。此时，用户态进程获取到更多的硬件访问权限，如页表管理和特权指令的执行权限。\n示例：启动一个需要直接管理内存页面的高性能计算应用，该应用在启动时经过Dune配置可以直接操作页表而无需频繁陷入内核。\n特定特权操作： 某些操作需要Dune模式下的权限才能执行，如直接访问硬件寄存器或执行特别的系统调用。此时，需要通过VM Entry进入Dune模式。\n示例：一个用户态网络堆栈需要直接访问网络设备的寄存器以实现高效的数据包处理。为了实现这一点，在启动网络堆栈时执行VM Entry。\nVM Exit：退出Dune模式\nVM Exit是指从Dune模式“退出”到普通用户态或内核态。在这个过程中，虚拟化层会捕获某些特权操作或事件，将控制权交还给Dune的管理层或内核进行处理。\nVM Exit通常在用户进程执行了某些特权操作或事件，这些操作不能直接在Dune模式下完成或需要更高权限的内核处理时触发。以下是一些典型的例子：\n敏感指令执行： 当在Dune模式下执行某些受控的特权指令时，这些指令会触发VM Exit，将控制权交还给Dune的管理层或内核。\n示例：一个用户进程试图修改某些受保护的寄存器或执行不允许的特权指令时，硬件会触发VM Exit，将控制权交回给Dune管理层处理。\n系统事件： 某些系统事件如中断、异常或I/O操作可能无法完全在用户态处理，需要内核介入，此时会触发VM Exit。\n示例：处理I/O操作或设备中断时，由于这些操作需要内核的直接参与处理，它们会触发VM Exit，并将控制权交还给内核。\n请求Dune管理层处理： 用户进程可以显式请求退出Dune模式并交给Dune管理层处理某些操作。\n示例：用户进程通过执行特殊指令（如VMCALL）主动请求退出Dune模式以便执行某些需要完整内核权限的操作。\n总结\nVM Entry是在用户进程进入Dune模式以利用虚拟化提供的增强特权时执行的，例如进程启动时需要直接操作硬件或管理内存页面。 VM Exit是在用户进程执行特权操作或系统事件时，Dune模式需要交还控制权给管理层或内核进行处理时执行的，例如敏感指令执行、系统中断或I/O操作。 通过这些机制，Dune系统能够在保证安全性的前提下大幅提升用户态进程执行特权操作的效率，同时确保必要时能够安全地退出到内核或进行更多权限的操作。\nVM Exit，将控制权交还给内核 VM Exit 是指从虚拟化环境（在 Dune 系统里，即 VMX Non-Root Mode）退出，将控制权交还给虚拟机监控器（VMM）或内核。这一过程涉及一系列复杂的状态切换和资源管理操作，以确保系统能正确处理和恢复。以下是这一过程中具体的变化和步骤：\nVMX Non-Root Mode 到 VMX Root Mode 的切换\n保存虚拟机状态。当触发 VM Exit 时，硬件会自动保存当前虚拟机（Guest）的状态到 VMCS（Virtual Machine Control Structure）。这些状态信息包括：\nCPU 寄存器状态：通用寄存器、程序计数器、栈指针、状态寄存器等。\n控制寄存器和系统寄存器：CR0, CR3, CR4, EFER 等。\n访存相关寄存器：如 base address 寄存器。\n恢复 VMM 或内核的状态。随后，硬件会恢复 VM Entry 前的寄存器和状态，包括：\nVMM 或内核的 CPU 状态：恢复至 VM Entry 时保存的状态。\n指令指针和栈指针：指向处理 VM Exit 的位置。\nVM Exit 的原因和处理逻辑\n识别 VM Exit 原因。硬件会在 VMCS 中记录导致 VM Exit 的具体原因，VMM 或内核可以通过以下信息确定处理逻辑：\n退出原因码（Exit Reason Code）：如 CPUID 指令、执行外部中断、I/O 读写等。\n退出指令信息：包括导致 VM Exit 的指令地址、指令执行上下文等。\n权限管理和安全检查。在恢复 VMM 或内核状态后，VMM 或内核会对 VM Exit 事件做一系列检查和权限管理。确保虚拟机未执行未授权的操作或尝试访问受保护的资源：\n系统调用检查：确认系统调用是否合法。\nI/O 操作检查：确保不会扰乱底层硬件或操作系统状态。\n内存访问控制：通过 EPT 和页表确保虚拟机只能访问允许的地址空间。\n上下文切换和状态恢复\n上下文切换。VMM 或内核需要进行上下文切换，以便处理虚拟机引发的事件。通常包括：\n内核栈切换：切换到内核的栈空间。\n控制寄存器切换：切换控制寄存器以适应内核态或VMM的执行环境。\n内存管理单元（MMU）配置：恢复内核或 VMM 的页表状态，以便正确管理内存访问。\n状态恢复。在处理完 VM Exit 事件后，VMM 或内核可能需要重新进入虚拟机，继续其执行。这需要调用 VM Entry 过程：\n恢复虚拟机状态：从 VMCS 恢复虚拟机的寄存器和控制寄存器状态。\n更新控制寄存器：CR0, CR3, CR4, EFER 等需要更新为虚拟机状态。\n重新加载内存页面映射：确保 EPT 和页表正确映射虚拟机的地址空间。\n事件处理和恢复执行\n事件处理。具体的 VM Exit 事件可能涉及各种不同的处理逻辑，以下是一些常见的情况：\n外部中断处理：内核处理硬件中断，可能涉及调度其他任务或更新系统状态。\nI/O 操作：VMM 或内核处理与存储、网络、显示等设备的交互。\n敏感指令执行：VMM 可能模拟并处理虚拟机不能直接执行的特权指令。\n恢复执行。完成对 VM Exit 事件的处理后，VMM 或内核可能选择回到虚拟机继续其执行。此时需要重新执行 VM Entry：\n恢复虚拟机状态：重新加载 VMCS 中保存的虚拟机状态。\n重新进入虚拟机执行：执行 VMLAUNCH 或 VMRESUME 指令，进入 VMX Non-Root Mode，继续虚拟机程序的执行。\n在 Dune 系统以及其他基于硬件虚拟化的系统中，VM Exit 将控制权交还给内核或 VMM 触发一系列复杂的状态保存和恢复操作。这些操作包括保存虚拟机状态、识别和处理 VM Exit 的原因、上下文切换和权限管理、以及在处理完事件后重新进入虚拟机执行。通过这些机制，Dune 可以在用户态下高效执行特权操作，同时在需要时安全地交还控制权给内核或 VMM，以确保系统的稳定和安全。\n权限分离（Privilege Separation） 权限分离（Privilege Separation）是一种安全技术，通过将应用程序或系统的不同部分分离开来，并为不同部分分配不同的权限，以尽量减少安全漏洞的影响。如果某一部分被攻击或被利用，只会影响该部分，而不会波及整个系统。例如，一个具有高权限的进程可以分离出一个低权限的进程来处理不信任的数据，这样即使低权限的进程被攻破，高权限的进程依然安全。\nSthread 是一种在 Wedge 系统中引入的轻量级进程，设计用于提供类似于操作系统进程的隔离，同时具有类似于线程的高性能。其主要用途包括：\n隔离风险代码：将可能存在风险的代码运行在 sthread 中，以减少因漏洞攻击而带来的影响。 用户请求隔离：在如网络服务器这类应用中，每个用户请求可以在一个单独的 sthread 中运行，保证不同用户间的隔离，防止一个用户的攻击影响其他用户。 Sthread 的回收利用确实有点类似于线程池的思想。线程池技术通过预先创建一组线程，并在需要时重复使用这些线程，从而避免频繁创建和销毁线程的开销。Sthread 回收利用的原理是：\n在第一次创建 sthread 时，进行检查点保存。 当 sthread 完成任务并退出时，将其恢复到初始状态，这样在下次需要创建 sthread 时，可以直接从这个恢复的状态开始，而无需重新分配资源和初始化。 这样做减少了 sthread 创建和销毁的成本提高了系统的整体性能。 sthread 回收时的脏位 \u0026ldquo;脏位\u0026rdquo;（dirty bit）是一种用于管理内存页面状态的机制，但需要明确的是，脏位本身不会直接进行内存恢复，而是作为一个标记来帮助实现恢复过程。以下是具体的工作原理：\n脏位的作用：在内存管理中，脏位用来标识一个内存页面是否被修改过。每当一个页面被写操作修改，硬件会将该页面的脏位设为1，表示该页面已被修改。 ssthread的回收：在ssthread回收利用过程中，我们需要将ssthread的内存恢复到其初始状态，以便重新利用。这时，脏位的状态可以帮助识别哪些页面在ssthread运行期间被修改过。 如何实现内存恢复： 初始化检查点：在ssthread第一次创建时，保存其完整的干净状态，包括所有内存页面的内容。 运行期间标记修改：通过硬件的脏位机制，实时监控哪些页面在ssthread的执行过程中被修改。 恢复过程：当ssthread完成任务并准备回收时，检查所有被标记为“脏”的页面，然后将这些页面恢复到初始检查点保存的状态。 通过这种方法，我们不需要每次都全面检查和恢复所有内存页面，仅需关注那些实际发生修改的页面，从而大大提高了恢复效率。\n总结一下，脏位是一个标记，告诉系统哪些页面被修改了。它本身并不会实际“恢复”内存，而是在恢复过程中起到导引作用，使系统知道哪些页面需要恢复，大大优化了内存恢复的操作效率。\n影子页表（Shadow Page Table） 影子页表（Shadow Page Table）在计算机系统特别是虚拟化环境中扮演了关键角色。它主要应用于虚拟机管理程序（Hypervisor）或者虚拟机监控器（VMM）中，用于有效地管理虚拟机的内存以及确保访客操作系统和主机操作系统之间的内存隔离与映射的效率。其地位：\n中介角色：在虚拟化环境中，Shadow Page Table充当虚拟机的虚拟页表和宿主机的物理页表之间的中介。虚拟机运行时，每次访问内存时，都会先查找虚拟页面，然后由影子页表将其映射到宿主机的物理页面。 高效的内存管理：通过维护影子页表，虚拟机管理程序能够有效管理和隔离不同虚拟机的内存空间，确保各虚拟机互不干扰。 性能优化：由于影子页表可以批量管理页表更新和TLB刷新，它能显著提高虚拟机内存操作的性能，减少因频繁的页表更新导致的性能开销。 影子页表由一系列的数据结构构成，这些数据结构用于保存虚拟内存地址到物理内存地址的映射信息。具体来说，影子页表通常包含以下部分：\n页目录和页表页：包含页目录（Page Directory）和页表页（Page Table Entry）结构，用于存储虚拟到物理地址映射。 TLB管理：涉及管理和刷新TLB（Translation Lookaside Buffer），确保在影子页表更新时，TLB中的缓存条目也能得到及时更新。 访问控制信息：包含与访问权限和保护策略相关的信息，以确保安全性和内存隔离。 假设有一个虚拟化环境，其中运行一个虚拟机，该虚拟机需要访问虚拟内存地址0xABCD1234，这个访问过程涉及以下步骤：\n虚拟机访问：虚拟机尝试访问其虚拟地址空间中的地址0xABCD1234。 初步映射：虚拟机内有自己的页表，假设虚拟机页表指出虚拟地址0xABCD1234映射到虚拟机的“物理”地址0x101234。 影子页表查找：虚拟机管理程序查找影子页表，假设影子页表指示虚拟机的“物理”地址0x101234实际上映射到宿主机的物理地址0x12345678。 物理访问：最后，虚拟机管理程序访问宿主机的物理内存地址0x12345678，从而完成虚拟地址到物理地址的转换。 在这个过程中，影子页表起到了关键的桥接作用，不仅确保了虚拟机对内存的访问安全和隔离，同时也确保了内存操作的高效性。\n影子页表在虚拟内存管理中扮演了重要的角色，提供高效、安全和隔离的内存访问手段。它有效地桥接虚拟地址和物理地址，使得虚拟机能够像访问自己的物理内存一样访问宿主机内存，从而实现无缝的内存虚拟化。\n正课部分 Why Virtual Machine 今天讨论的话题是虚拟机。今天的内容包含三个部分:\n第一个部分是Trap and Emulate，这部分会介绍如何在RISC-V或者QEMU上构建属于自己的Virtual Machine Monitor（注，有些场合也称为Hypervisor）。 第二部分会描述最近在硬件上对于虚拟化的支持。 最后是讨论一下今天的论文 ，它使用了第二部分中硬件上的支持。 首先什么是虚拟机？你可以认为这是对于计算机的一种模拟，这种模拟足够能运行一个操作系统。\nQEMU可以认为是虚拟机的一个例子（注，QEMU应该是属于VMM/Hypervisor）。\n在架构的最底层，位于硬件之上存在一个Virtual Machine Monitor（VMM），它取代了标准的操作系统内核。\nVMM的工作是模拟多个计算机用来运行Guest操作系统。\nVMM往上一层，如果对比一个操作系统的架构应该是用户空间，但是现在是叫做Guest空间。\n所以在今天的架构图里面，上面是Guest空间，下面是Host空间（注，也就是上面运行Guest操作系统，下面运行VMM）。\n在Guest空间，会有一个或者多个Guest操作系统内核，或许其中一个是Linux kernel。\n这里的Linux kernel会觉得自己就是个普通的内核，并在自己之上还运行一堆用户进程，例如VI，C Compiler。\n我们或许还有另一个Guest运行了Windows操作系统，同时也包含了Windows用户进程。\n所以，在Host空间运行的是VMM，在Guest空间运行的是普通的操作系统。\n除此之外，在Guest空间又可以分为Guest Supervisor Mode，也就是Guest操作系统内核运行的模式，和Guest User Mode。\nVMM的主要目的是提供对计算机的模拟，这样你可以不做修改就启动普通的Linux，普通的Windows系统，并运行在虚拟机内，并且不用担心任何奇怪的事情发生。\n所以，VMM必须要能够完全按照实际硬件的行为来模拟Guest Supervisor Mode和Guest User Mode，尽管实际上不可能完全一样，我们之后会讨论VMM对于这两种模式的模拟。\n那么人们为什么会想要使用虚拟机呢？实际中有很多原因使得人们会在一个计算机上运行多个相互独立的操作系统。\n在一个大公司里面，你需要大量的服务，例如DNS，Firewall等等，但是每个服务并没有使用太多的资源，所以单独为这些服务购买物理机器有点浪费，但是将这些低强度的服务以虚拟机的形式运行在一个物理机上可以节省时间和资金。\n虚拟机在云计算中使用的也非常广泛。\n云厂商，例如AWS，不想直接出借物理服务器给用户，因为这很难管理。它们想向用户出借的是可以随意确定不同规格的服务器。\n或许有两个用户在一台物理服务器上，但是他们并没有太使用计算机，这样AWS可以继续向同一个物理服务器上加入第三或者第四个用户。\n这样可以不使用额外的资金而获得更高的收益。所以，虚拟机提供了额外的灵活性，这里借助的技术是：将操作系统内核从之前的内核空间上移至用户空间，并在操作系统内核之下增加新的一层（注，也就是虚拟机的内核是运行在宿主机的用户空间，虚拟机的内核通过新增加的一层VMM来对接底层硬件）以提供这里的灵活性。\n还有一些其他的原因会使得人们使用虚拟机。\n第一个是开发内核，这就是为什么我们在课程中一直使用QEMU。\n能够在虚拟环境而不是一个真实的计算机运行XV6，使得这门课程对于你们和我们来说都要方便的多。\n同时对于调试也更容易，因为相比在物理计算机上运行XV6，在QEMU提供的虚拟机环境中运行可以更容易的提供gdb的访问权限。\n最后一个人们使用虚拟机的原因是，通过新增的VMM提供的抽象可以实现更多的功能。\n例如，你可以为整个操作系统和其中的用户进程做一个快照，并在磁盘中保存下来。\n稍后再恢复快照，并将操作系统和其中的用户进程恢复成做快照时的状态。这可以增加运行的可靠性，或者用来调试，或者用来拷贝虚拟机的镜像并运行多次。\n除此之外，还可以将一个Guest操作系统迁移到另一个计算机上。\n如果你在一个物理计算机上运行了一个Guest操作系统，现在需要关闭并替换该物理计算机，你可以在不干扰虚拟机运行的前提下，将它迁移到另一个物理计算机，这样你就可以安全的关闭第一个物理计算机。\n以上就是人们喜欢使用虚拟机的原因。虚拟机实际上应用的非常非常广泛，并且它也有着很长的历史。虚拟机最早出现在1960年代，经过了一段时间的开发才变得非常流行且易用。\n对于这们课程来说，我们之所以要学习虚拟机是因为VMM提供了对于操作系统的一种不同视角。\n在操作系统的架构中，内核之上提供的封装单元（注，视频中说的是container，但是container还有容器的意思，所以这里说成是封装单元）是我们熟悉的进程，内核管理的是多个用户进程。\n而在VMM的架构中，VMM之上提供的封装单元是对计算机的模拟。VMM的架构使得我们可以从另一个角度重新审视我们讨论过的内容，例如内存分配，线程调度等等，这或许可以给我们一些新的思路并带回到传统的操作系统内核中。\n所以，在虚拟机场景下，大部分的开发设计研究工作，从传统的内核移到了VMM。某种程度上来说，传统操作系统内核的内容下移了一层到了VMM。\n今天课程的第一部分我将会讨论如何实现我们自己的虚拟机。\n这里假设我们要模拟的是RISC-V，并运行针对RISC-V设计的操作系统，例如XV6。\n我们的目的是让运行在Guest中的代码完全不能区分自己是运行在一个虚拟机还是物理机中，因为我们希望能在虚拟机中运行任何操作系统，甚至是你没有听说过的操作系统，这意味着对于任何操作系统的行为包括使用硬件的方式，虚拟机都必须提供提供对于硬件的完全相同的模拟，这样任何在真实硬件上能工作的代码，也同样能在虚拟机中工作。\n除了不希望Guest能够发现自己是否运行在虚拟机中，我们也不希望Guest可以从虚拟机中逃逸。很多时候人们使用虚拟机是因为它为不被信任的软件甚至对于不被信任的操作系统提供了严格的隔离。\n假设你是Amazon，并且你出售云服务，通常是你的客户提供了运行在虚拟机内的操作系统和应用程序，所以有可能你的客户运行的不是普通的Linux而是一个特殊的修改过的Linux，并且会试图突破虚拟机的限制来访问其他用户的虚拟机或者访问Amazon用来实现虚拟机隔离的VMM。\n所以Guest不能从虚拟机中逃逸还挺重要的。Guest可以通过VMM使用内存，但是不能使用不属于自己的内存。类似的，Guest也不应该在没有权限的时候访问存储设备或者网卡。\n所以这里我们会想要非常严格的隔离。\n虚拟机在很多方面比普通的Linux进程提供了更加严格的隔离。Linux进程经常可以相互交互，它们可以杀掉别的进程，它们可以读写相同的文件，或者通过pipe进行通信。\n但是在一个普通的虚拟机中，所有这些都不被允许。运行在同一个计算机上的不同虚拟机，彼此之间是通过VMM完全隔离的。\n所以出于安全性考虑人们喜欢使用虚拟机，这是一种可以运行未被信任软件的方式，同时又不用担心bug和恶意攻击。\n前面已经指出了虚拟机的目标是提供一种对于物理服务器的完全准确的模拟。但是实际中出于性能的考虑，这个目标很难达到。\n你将会看到运行在Guest中的Linux与VMM之间会相互交互，所以实际中Linux可以发现自己是否运行在VMM之上。\n出于效率的考虑，在VMM允许的前提下，Linux某些时候知道自己正在与VMM交互，以获得对于设备的高速访问权限。\n但这是一种被仔细控制的例外，实现虚拟机的大致策略还是完全准确的模拟物理服务器。\nTrap-and-Emulate \u0026mdash; Trap 我们该如何构建我们自己的VMM呢？\n一种实现方式是完全通过软件来实现，你可以想象写一个类似QEMU的软件，这个软件读取包含了XV6内核指令的文件，查看每一条指令并模拟RISC-V的状态，这里的状态包括了通过软件模拟32个寄存器。你的软件读取每条指令，确定指令类型，再将指令应用到通过软件模拟的32个寄存器和控制寄存器中。\n实际中有的方案就是这么做的，虽然说考虑到细节还需要做很多工作，但是这种方案从概念上来说很简单直观。\n但是纯软件解析的虚拟机方案应用的并不广泛，因为它们很慢。\n如果你按照这种方式实现虚拟机，那么Guest应用程序的运行速度将远低于运行在硬件上，因为你的VMM在解析每一条Guest指令的时候，都可能要转换成几十条实际的机器指令，所以这个方案中的Guest的运行速度比一个真实的计算机要慢几个数量级。\n在云计算中，这种实现方式非常不实用。所以人们并不会通过软件解析来在生产环境中构建虚拟机。\n相应的，一种广泛使用的策略是在真实的CPU上运行Guest指令。\n所以如果我们要在VMM之上运行XV6，我们需要先将XV6的指令加载到内存中，之后再跳转到XV6的第一条指令，这样你的计算机硬件就能直接运行XV6的指令。当然，这要求你的计算机拥有XV6期望的处理器（注，也就是RISC-V）。\n但是实际中你又不能直接这么做，因为当你的Guest操作系统执行了一个privileged指令（注，也就是在普通操作系统中只能在kernel mode中执行的指令，详见3.4）之后，就会出现问题。\n现在我们在虚拟机里面运行了操作系统内核，而内核会执行需要privileged权限指令，比如说加载一个新的Page Table到RISC-V的SATP寄存器中，而这时就会出现问题。\n前面说过，我们将Guest kernel按照一个Linux中的普通用户进程来运行，所以Guest kernel现在运行在User mode，而在User mode加载SATP寄存器是个非法的操作，这会导致我们的程序（注，也就是虚拟机）crash。\n但是如果我们蠢到将Guest kernel运行在宿主机的Supervisor mode（注，也就是kernel mode），那么我们的Guest kernel不仅能够修改真实的Page Table，同时也可以从虚拟机中逃逸，因为它现在可以控制PTE（Page Table Entry）的内容，并且读写任意的内存内容。所以我们不能直接简单的在真实的CPU上运行Guest kernel。\n相应的，这里会使用一些技巧。\n首先将Guest kernel运行在宿主机的User mode，这是最基本的策略。\n这意味着，当我们自己写了一个VMM，然后通过VMM启动了一个XV6系统，VMM会将XV6的kernel指令加载到内存的某处，再设置好合适的Page Table使得XV6看起来自己的内存是从地址0开始向高地址走。\n之后VMM会使用trap或者sret指令（注，详见6.8）来跳转到位于User mode的Guest操作系统的第一条指令，这样不论拥有多少条指令，Guest操作系统就可以一直执行下去。\n一旦Guest操作系统需要使用privileged指令，因为它当前运行在User mode而不是Supervisor mode，会使得它触发trap并走回到我们的VMM中（注，在一个正常操作系统中，如果在User mode执行privileged指令，会通过trap走到内核，但是现在VMM替代了内核），之后我们就可以获得控制权。\n所以当Guest操作系统尝试修改SATP寄存器，RISC-V处理器会通过trap走回到我们的VMM中，之后我们的VMM就可以获得控制权。\n并且我们的VMM也可以查看是什么指令引起的trap，并做适当的处理。这里核心的点在于Guest操作系统并没有实际的设置SATP寄存器。\n学生提问：VMM改如何截获Guest操作系统的指令？它应该要设置好一个trap handler对吧，但这不是一个拥有privileged权限的进程才能做的事情吗？而VMM又是个宿主机上的用户程序，是吧？\nRobert教授：我这里假设VMM运行在Supervisor mode。所以在这里的图中，VMM就是宿主机的kernel。\n这里我们不是启动类似Linux的操作系统，而是启动VMM（注，类似VMware的ESXi）。VMM以privileged权限运行，并拥有硬件的完整控制权限，这样我们就可以在VMM里面设置各种硬件寄存器。有一些VMM就是这么运行的，你在硬件上启动它们，并且只有VMM运行在Supervisor mode。\n实际上还有很多很多其他的虚拟机方案，比如说在硬件上启动Linux，之后要么Linux自带一个VMM，要么通过可加载的内核模块将VMM加载至Linux内核中，这样VMM可以在Linux内核中以Supervisor mode运行。今天我们要讨论的论文就是采用后者。\n这里主要的点在于，我们自己写的可信赖的VMM运行在Supervisor mode，而我们将不可信赖的Guest kernel运行在User mode，通过一系列的处理使得Guest kernel看起来好像自己是运行在Supervisor mode。\n在RISC-V上，如果在User mode尝试运行任何一个需要Supervisor权限的指令都会触发trap。\n这里需要Supervisor权限的指令并不包括与Page Table相关的指令，我们稍后会介绍相关的内容。\n所以每当Guest操作系统尝试执行类似于读取SCAUSE寄存器，读写STVEC寄存器，都会触发一个trap，并走到VMM，之后我们就可以获得控制权。\nTrap-and-Emulate \u0026mdash; Emulate VMM会为每一个Guest维护一套虚拟状态信息。所以VMM里面会维护虚拟的STVEC寄存器，虚拟的SEPC寄存器以及其他所有的privileged寄存器。\n当Guest操作系统运行指令需要读取某个privileged寄存器时，首先会通过trap走到VMM，因为在用户空间读取privileged寄存器是非法的。之后VMM会检查这条指令并发现这是一个比如说读取SEPC寄存器的指令，之后VMM会模拟这条指令，并将自己维护的虚拟SEPC寄存器，拷贝到trapframe的用户寄存器中。\n（注，有关trapframe详见Lec06，这里假设Guest操作系统通过类似“sread a0, sepc”的指令想要将spec读取到用户寄存器a0）\n之后，VMM会将trapframe中保存的用户寄存器拷贝回真正的用户寄存器，通过sret指令，使得Guest从trap中返回。\n这时，用户寄存器a0里面保存的就是SEPC寄存器的值了，之后Guest操作系统会继续执行指令。最终，Guest读到了VMM替自己保管的虚拟SEPC寄存器。\n学生提问：VMM是怎么区分不同的Guest？\nRobert教授：VMM会为每个Guest保存一份虚拟状态信息，然后它就像XV6知道是哪个进程一样，VMM也知道是哪个Guest通过trap走到VMM的。XV6有一个针对每个CPU的变量表明当前运行的是哪个进程，类似的VMM也有一个针对每个CPU的变量表明当前是哪个虚拟机在运行，进而查看对应的虚拟状态信息。\n学生提问：VMM可以给一个Guest分配多个CPU核吗？\nRobert教授：稍微复杂点的VMM都可以实现。\n学生提问：在实际的硬件中会有对应寄存器，那么为什么我们不直接使用硬件中的寄存器，而是使用虚拟的寄存器？\nRobert教授：这里的原因是，VMM需要使用真实的寄存器。\n举个例子，想象一下SCAUSE寄存器，当Guest操作系统尝试做任何privileged操作时（注，也就是读写privileged寄存器），会发生trap。\n硬件会将硬件中真实的SCAUSE寄存器设置成引起trap的原因，这里的原因是因为权限不够。\n但是假设Guest操作系统只是从Guest用户进程执行了一个系统调用，Guest操作系统需要看到SCAUSE的值是系统调用。\n也就是说Guest操作系统在自己的trap handler中处理来自Guest用户进程的系统调用时，需要SCAUSE的值表明是系统调用。\n而实际的SCAUSE寄存器的值却表明是因为指令违反了privilege规则才走到的trap。通常情况下，VMM需要看到真实寄存器的值，而Guest操作系统需要能看到符合自己视角的寄存器的值。\n（注，在Guest操作系统中，可能有两种情况会触发trap，一种是Guest用户空间进程的系统调用，也就是正常操作系统中正常的trap流程，另一种是Guest内核空间读取privileged寄存器时，因为Guest内核空间实际上也是在宿主机的用户空间，导致这是个非法操作并触发trap。Robert这边举的例子的流程应该是这样，Guest用户进程执行系统调用，在这一个瞬间SCAUSE寄存器的值是ECALL，也就是8，详见6.6。但是稍后在Guest系统内核的trap handler中需要读取SCAUSE的值，以确定在Guest中引起trap的原因，但是这就触发了第二种trap，SCAUSE的值会变成Illegal Access。我们不能让Guest系统内核看到这个值，所以VMM这里将它变成ECALL并返回。）\n在这种虚拟机的实现中，Guest整个运行在用户空间，任何时候它想要执行需要privilege权限的指令时，会通过trap走到VMM，VMM可以模拟这些指令。\n这种实现风格叫做Trap and Emulate。\n你可以完全通过软件实现这种VMM，也就是说你可以只通过修改软件就将XV6变成一个可以运行在RISC-V上的VMM，然后再在之上运行XV6虚拟机。当然，与常规的XV6一样，VMM需要运行在Supervisor mode。\n所有以S开头的寄存器，也就是所有的Supervisor控制寄存器都必须保存在虚拟状态信息中。\n同时还有一些信息并不能直接通过这些控制寄存器体现，但是又必须保存在这个虚拟状态信息中。其中一个信息就是mode。\nVMM需要知道虚拟机是运行在Guest user mode还是Guest Supervisor mode。\n例如，Guest中的用户代码尝试执行privileged指令，比如读取SCAUSE寄存器，这也会导致trap并走到VMM。但是这种情况下VMM不应该模拟指令并返回，因为这并不是一个User mode中的合法指令。所以VMM需要跟踪Guest当前是运行在User mode还是Supervisor mode，所以在虚拟状态信息里面也会保存mode。\nVMM怎么知道Guest当前的mode呢？\n当Guest从Supervisor mode返回到User mode时会执行sret指令，而sret指令又是一个privileged指令，所以会通过trap走到VMM，进而VMM可以看到Guest正在执行sret指令，并将自己维护的mode从Supervisor变到User。\n虚拟状态信息中保存的另外一个信息是hartid，它代表了CPU核的编号。\n即使通过privileged指令，也不能直接获取这个信息，VMM需要跟踪当前模拟的是哪个CPU。\n实际中，在不同类型的CPU上实现Trap and Emulate虚拟机会有不同的难度。\n不过RISC-V特别适合实现Trap and Emulate虚拟机，因为RISC-V的设计人员在设计指令集的时候就考虑了Trap and Emulate虚拟机的需求。\n举个例子，设计人员确保了每个在Supervisor mode下才能执行的privileged指令，如果在User mode执行都会触发trap。你可以通过这种机制来确保VMM针对Guest中的每个privileged指令，都能看到一个trap。\n学生提问：Guest操作系统内核中会实际运行任何东西吗？还是说它总是会通过trap走到VMM？\nRobert教授：如果你只是执行一个ADD指令，这条指令会直接在硬件上以硬件速度执行。如果你执行一个普通的函数调用，代码的执行也没有任何特殊的地方。所有User代码中合法的指令，以及内核代码中的non-priviledged指令，都是直接以全速在硬件上执行。\n学生提问：在Guest操作系统中是不是也有类似的User mode和Kernel mode？\nRobert教授：有的。Guest操作系统就是一个未被修改的普通操作系统，所以我们在Guest中运行的就是Linux内核或者XV6内核。\n而XV6内核知道自己运行在Supervisor mode，从代码的角度来说，内核代码会认为自己运行在Supervisor mode，并执行各种privileged指令，并期望这些指令能工作。\n当Guest操作系统执行sret指令时，它也知道自己将要进入到User空间。不过在宿主机上，Guest操作系统是运行在User mode，VMM也确保了这里能正常工作。\n但是从Guest角度来说，自己的内核看起来像是运行在Supervisor mode，自己的用户程序看起来像是运行在User mode。\n所以，当Guest执行sret指令从Supervisor mode进入到User mode，因为sret是privileged指令，会通过trap进入到VMM。\nVMM会更新虚拟状态信息中的mode为User mode，尽管当前的真实mode还是Supervisor mode，因为我们还在执行VMM中的代码。\n在VMM从trap中返回之前，VMM会将真实的SEPC寄存器设置成自己保存在虚拟状态信息中的虚拟SEPC寄存器。\n因为当VMM使用自己的sret指令返回到Guest时，它需要将真实的程序计数器设置成Guest操作系统想要的程序计数器值（注，因为稍后Guest代码会在硬件上执行，因此依赖硬件上的程序计数器）。\n所以在一个非常短的时间内，真实的SEPC寄存器与虚拟的SEPC寄存器值是一样的。\n同时，当VMM返回到虚拟机时，还需要切换Page table，这个我们稍后会介绍。\nGuest中的用户代码，如果是普通的指令，就直接在硬件上执行。\n当Guest中的用户代码需要执行系统调用时，会通过执行ECALL指令（注，详见6.3，6.4）触发trap，而这个trap会走到VMM中（注，因为ECALL也是个privileged指令）。\nVMM可以发现当前在虚拟状态信息中记录的mode是User mode，并且发现当前执行的指令是ECALL，之后VMM会更新虚拟状态信息以模拟一个真实的系统调用的trap状态。\n比如说，它将设置虚拟的SEPC为ECALL指令所在的程序地址（注，执行sret指令时，会将程序计数器的值设置为SEPC寄存器的值。这样，当Guest执行sret指令时，可以从虚拟的SEPC中读到正确的值）；\n将虚拟的mode更新成Supervisor；\n将虚拟的SCAUSE设置为系统调用；\n将真实的SEPC设置成虚拟的STVEC寄存器；\n注，STVEC保存的是trap函数的地址，将真实的SEPC设置成STVEC这样当VMM执行sret指令返回到Guest时，可以返回到Guest的trap handler。Guest执行系统调用以为自己通过trap走到了Guest内核，但是实际上却走到了VMM，这时VMM需要做一些处理，让Guest以及之后Guest的所有privileged指令都看起来好像是Guest真的走到了Guest内核\n之后调用sret指令跳转到Guest操作系统的trap handler，也就是STVEC指向的地址。\nTrap-and-Emulate \u0026mdash; Page Table 有关Trap and Emulate的实现还有两个重要的部分，一个是Page Table，另一个是外部设备。\nPage Table包含了两个部分，第一个部分是Guest操作系统在很多时候会修改SATP寄存器（注，SATP寄存器是物理内存中包含了Page Table的地址，详见4.3），当然这会变成一个trap走到VMM，之后VMM可以接管。\n但是我们不想让VMM只是简单的替Guest设置真实的SATP寄存器，因为这样的话Guest就可以访问任意的内存地址，而不只是VMM分配给它的内存地址，所以我们不能让Guest操作系统简单的设置SATP寄存器。\n但是我们的确又需要为SATP寄存器做点什么，因为我们需要让Guest操作系统觉得Page Table被更新了。\n此外，当Guest上的软件运行了load或者store指令时，或者获取程序指令来执行时，我们需要数据或者指令来自于内存的正确位置，也就是Guest操作系统认为其PTE指向的内存位置。\n所以当Guest设置SATP寄存器时，真实的过程是，我们不能直接使用Guest操作系统的Page Table，VMM会生成一个新的Page Table来模拟Guest操作系统想要的Page Table。\n所以现在的Page Table翻译过程略微有点不一样，首先是Guest kernel包含了Page Table，但是这里是将Guest中的虚拟内存地址（注，下图中gva）映射到了Guest的物理内存地址（注，下图中gpa）。\nGuest物理地址是VMM分配给Guest的地址空间，例如32GB。并且VMM会告诉Guest这段内存地址从0开始，并一直上涨到32GB。\n但是在真实硬件上，这部分内存并不是连续的。所以我们不能直接使用Guest物理地址，因为它们不对应真实的物理内存地址。\n相应的，VMM会为每个虚拟机维护一个映射表，将Guest物理内存地址映射到真实的物理内存地址，我们称之为主机物理内存地址（注，上图中的hpa）。\n这个映射表与Page Table类似，对于每个VMM分配给Guest的Guest物理内存Page，都有一条记录表明真实的物理内存Page是什么。\n当Guest向SATP寄存器写了一个新的Page Table时，在对应的trap handler中，VMM会创建一个Shadow Page Table，Shadow Page Table的地址将会是VMM向真实SATP寄存器写入的值。\nShadow Page Table由上面两个Page Table组合而成，所以它将gva映射到了hpa。Shadow Page Table是这么构建的：\n从Guest Page Table中取出每一条记录，查看gpa。 使用VMM中的映射关系，将gpa翻译成hpa。 再将gva和hpa存放于Shadow Page Table。 在创建完之后，VMM会将Shadow Page Table设置到真实的SATP寄存器中，再返回到Guest内核中。\n注，这样的效果是，Guest里面看到的Page Table就是一个正常的Page Table，而Guest通过SATP寄存器指向的Page Table，将虚拟内存地址翻译得到的又是真实的物理内存地址\n所以，Guest kernel认为自己使用的是一个正常的Page Table，但是实际的硬件使用的是Shadow Page Table。\n这种方式可以阻止Guest从被允许使用的内存中逃逸。Shadow Page Table只能包含VMM分配给虚拟机的主机物理内存地址。\nGuest不能向Page Table写入任何VMM未分配给Guest的内存地址。这是VMM实现隔离的一个关键部分。\n学生提问：如果Guest操作系统想要为一个进程创建一个新的Page Table，会发生什么呢？\nRobert教授：Guest会完全按照Linux或者XV6的行为来执行。首先是格式化Page Table Entries以构造一个Page Table。\n之后执行指令将Page Table的地址写入到SATP寄存器，这就是Guest操作系统的行为。\n但是它又不能设置实际的SATP寄存器，因为这是一个privileged操作，所以设置SATP寄存器会触发trap并走到VMM。\nVMM会查看trap对应的指令，并发现Guest要尝试设置SATP寄存器，之后VMM会创建一个新的Shadow Page Table。\nVMM会查看Guest尝试要设置的Page Table的每一条记录，通过gpa-\u0026gt;hpa的映射关系，将gva和hpa的对应关系翻译出来。\n如果Guest尝试使用一个不被允许的物理地址，VMM会生成一个真实的Page Fault。之后VMM会将Shadow Page Table设置到真实的SATP寄存器中，并返回到Guest中。\nShadow Page Table是实现VMM时一个比较麻烦的地方。\n除了设置SATP寄存器，Guest操作系统还有另一种方式可以与Page Table进行交互。\nXV6有时候会直接修改属于自己的Page Table Entry，或者读取PTE中的dirty bit。如果你读了RISC-V的文档，你可以发现在RISC-V上，如果软件更改了PTE，RISC-V不会做任何事情。\n如果你修改了PTE，RISC-V并不承诺可以立即观察到对于PTE的修改，在修改那一瞬间，你完全是不知道PTE被修改了。\n注，这里主要对比的是privileged指令，因为如果在用户空间执行了privileged指令，会立刻触发trap，而这里修改PTE不会有任何的额外的动作\n相应的，文档是这么说的，如果你修改PTE并且希望MMU可以看到这个改动，你需要执行sfence.vma指令，这个指令会使得硬件注意到你对Page Table的修改。\n所以如果你要自己写一个VMM，你在RISC-V上的VMM会完全忽略Guest对于PTE的修改，但是你知道Guest在修改完PTE之后将会执行sfence.vma指令，并且这是一个privileged指令，因为它以s开头，所以这条指令会通过trap走到VMM，VMM就可以知道sfence.vma指令被执行了。\n之后VMM会重新扫描Guest的当前Page Table，查找更新了的Page Table Entry。\n如果修改合法的话，VMM会将修改体现在Shadow Page Table中，并执行真实的sfence.vma指令来使得真实的硬件注意到Shadow Page Table的改动。最后再会返回到Guest操作系统中。\n学生提问：所以MMU只使用了一个Page Table，也就是Shadow Page Table，对吧？这里并没有使用EPT（Extended Page Table），对吧？\nRobert教授：这里还没有EPT。\n学生提问：所以Guest认为它自己有一个Page Table，也就是gva-\u0026gt;gpa，但是这里并没有做任何的翻译工作。VMM通过两个映射关系构建了属于自己的Page Table。\nRobert教授：是的。这里澄清一下，EPT是一种非常不一样的虚拟机实现方式，并且需要硬件的支持。我们这里假设除了对privileged指令触发trap以外，不需要使用任何特殊的硬件支持来构建一个虚拟机。\n学生提问：这里会弄乱direct mapping吗？\nRobert教授：这里不会有direct map。Guest会认为自己有一个direct mapping，但这只是在虚拟的世界里的一个direct mapping，在真实的机器上这不是direct mapping。但是这没有关系，因为我们这里欺骗了Guest使得看起来像是direct mapping。\n学生提问：我们刚刚说过性能的损耗，如果我们使用VMM，对于这里的trap机制看起来也会有大量的性能损耗。\nRobert教授：是的。如果你的操作系统执行了大量的privileged指令，那么你也会有大量的trap，这会对性能有大的损耗。\n这里的损耗是现代硬件增加对虚拟机支持的动机。今天要讨论的论文使用的就是现在硬件对于虚拟机的支持，Intel和AMD在硬件上支持更加有效的trap，或者说对于虚拟机方案，会有少得多的trap。\n所以是的，性能很重要。但是上面介绍的方案，人们也使用了很多年，它能工作并且也很成功，尽管它会慢的多，但是还没有慢到让人们讨厌的程度，人们其实很喜欢这个方案。\nTrap-and-Emulate \u0026mdash; Devices 接下来我们来看Trap and Emulate的最后一个部分，也就是虚拟机的外部设备。\n外部设备是指，一个普通的操作系统期望能有一个磁盘用来存储文件系统，或者是期望有一个网卡，甚至对于XV6来说期望有一个UART设备来与console交互，或者期望有一张声卡，一个显卡，键盘鼠标等等各种各样的东西。所以我们我们的虚拟机方案，需要能够至少使得Guest认为所有它需要的外部设备是存在的。\n这里人们通常会使用三种策略。\n第一种是，模拟一些需要用到的并且使用非常广泛的设备，例如磁盘。\n也就是说，Guest并不是拥有一个真正的磁盘设备，只是VMM使得与Guest交互的磁盘看起来好像真的存在一样。\n这里的实现方式是，Guest操作系统仍然会像与真实硬件设备交互一样，通过Memory Map控制寄存器与设备进行交互。\n通常来说，操作系统会假设硬件已经将自己的控制寄存器映射到了内核地址空间的某个地址上。\n在VMM中不会映射这些内存地址对应的Page，相应的会将这些Page设置成无效。这样当Guest操作系统尝试使用UART或者其他硬件时，一访问这些地址就会通过trap走到VMM。\nVMM查看指令并发现Guest正在尝试在UART发送字符或者从磁盘中读取数据。VMM中会对磁盘或者串口设备有一些模拟，通过这些模拟，VMM知道如何响应Guest的指令，之后再恢复Guest的执行。\n这就是我们之前基于QEMU介绍XV6时，QEMU实现UART的方式。\n在之前的介绍中，并没有UART硬件的存在，但是QEMU模拟了一个UART来使得XV6正常工作。\n这是一种常见的实现方式，但是这种方式可能会非常的低效，因为每一次Guest与外设硬件的交互，都会触发一个trap。但是对于一些低速场景，这种方式工作的较好。如果你的目标就是能启动操作系统并使得它们完全不知道自己运行在虚拟机上，你只能使用这种策略。\n在现代的世界中，操作系统在最底层是知道自己运行在虚拟机之上的。\n所以第二种策略是提供虚拟设备，而不是模拟一个真实的设备。\n通过在VMM中构建特殊的设备接口，可以使得Guest中的设备驱动与VMM内支持的设备进行高效交互。\n现在的Guest设备驱动中可能没有Memory Mapped寄存器了，但是相应的在内存中会有一个命令队列，Guest操作系统将读写设备的命令写到队列中。\n在XV6中也使用了一个这种方式的设备驱动，在XV6的virtio_disk.c文件中，你可以看到一个设备驱动尝试与QEMU实现的虚拟磁盘设备交互。\n在这个驱动里面要么只使用了很少的，要么没有使用Memory Mapped寄存器，所以它基本不依赖trap，相应的它在内存中格式化了一个命令队列。\n之后QEMU会从内存中读取这些命令，但是并不会将它们应用到磁盘中，而是将它们应用到一个文件，对于XV6来说就是fs.image。\n这种方式比直接模拟硬件设备性能要更高，因为你可以在VMM中设计设备接口使得并不需要太多的trap。\n第三个策略是对于真实设备的pass-through，这里典型的例子就是网卡。\n现代的网卡具备硬件的支持，可以与VMM运行的多个Guest操作系统交互。\n你可以配置你的网卡，使得它表现的就像多个独立的子网卡，每个Guest操作系统拥有其中一个子网卡。\n经过VMM的配置，Guest操作系统可以直接与它在网卡上那一部分子网卡进行交互，并且效率非常的高。所以这是现代的高性能方法。\n在这种方式中，Guest操作系统驱动可以知道它们正在与这种特别的网卡交互。\n以上就是实现外部设备的各种策略。\n我认为在实现一个VMM时，主要的困难就在于构建外部设备和设备驱动，并使得它们能正确的与Guest操作系统配合工作。这里或许是实现VMM的主要工作，尤其是当你使用第一种策略时。\n学生提问：我并没有太理解策略一emulation和策略二virtual device的区别。\nRobert教授：它们是类似的。可以这么想，如果你启动了一个完全不知道虚拟机的操作系统，它或许包含了很多磁盘驱动，但是所有的驱动都是为真实硬件提供的。\n如果你想要在虚拟机中启动这样一个操作系统，你需要选择其中一种真实的硬件，并且以一种非常准确的方式来模拟该硬件。\n这种方式并没有问题，只是大部分情况下硬件接口并没有考虑Trap and Emulate VMM下的性能。所以真实的设备驱动需要你频繁的读写它的控制寄存器，而VMM需要为每一次写控制寄存器都获取控制权，因为它需要模拟真实的硬件。\n这意味着每一次写控制寄存器都会触发一次trap走到VMM，并消耗数百个CPU cycles。所以策略一非常的慢且低效。\n策略二并没有卑微地模仿真实的设备，某些设计人员提出了一种设备驱动，这种设备驱动并不对接任何真实的硬件设备，而是只对接由VMM实现的虚拟设备。\n这种驱动设计的并不需要很多trap，并且这种驱动与对应的虚拟设备是解耦的，并不需要立即的交互。\n从功能层面上来说，使用策略一的话，你可以启动任何操作系统，使用策略二的话，如果你想要使用虚拟设备，你只能启动知道虚拟设备的操作系统。\n实际中，策略二是一种标准，并且很多虚拟机的实现方案都能提供。虽然我们并没有在除了QEMU以外的其他场景测试过，XV6中的virtio_disk.c稍作修改或许也可以在其他虚拟机方案上运行。\n学生提问：所以对于每一种主板，取决于不同的磁盘，编译XV6都需要不同的磁盘驱动，是吗？\nRobert教授：是的。我认为或许你可以买到支持virtio_disk驱动的真实硬件，但是大部分的磁盘硬件还不支持这个驱动，这时你需要为真实的硬件实现一种新的驱动。\n策略一：Emulation（设备模拟）\nEmulation 的核心思想是：让虚拟机（Guest）以为它正在与一个真实的物理设备交互，即使实际情况是虚拟机管理器（VMM）在模拟这个设备。\n工作原理：\nGuest 操作系统中的设备驱动程序是为真实硬件编写的，因此它期望设备具备物理设备的接口，比如内存映射的寄存器、IO 端口等。 VMM 需要模拟一个真实的硬件设备，包括这些设备的寄存器、控制信号、状态信息等。当 Guest 操作系统访问这些设备时，它会使用与真实设备相同的方式（例如读写寄存器、内存映射地址）进行交互。 VMM 拦截这些访问，并通过软件模拟真实设备的行为。比如，Guest 操作系统写入磁盘设备的控制寄存器，VMM 会模拟相应的寄存器操作，并将数据写入一个虚拟磁盘文件。 设备交互：\n由于 VMM 拦截 Guest 操作系统对外部设备的访问，每次交互都会引发trap，从而将控制权交还给 VMM，这一过程通常很耗时，因为每个寄存器的读写都需要切换到 VMM 中进行模拟。 性能问题：\n由于每次设备交互都需要 trap 进入 VMM 执行，trap 和 emulation 的频率很高，特别是在对设备进行大量小块数据读写时，性能会受到显著影响。这种方式适用于那些低速或不常被访问的设备（如 UART），但对于高速设备（如磁盘）效率较低。 总结：\nEmulation 适用于那些不需要虚拟化支持的操作系统，因为它们完全不需要知道自己运行在虚拟机中。通过模拟一个真实的硬件设备，VMM 可以让任何 Guest 操作系统与设备交互，几乎不需要修改操作系统的设备驱动。 缺点是每次设备交互都需要频繁 trap，性能较差。 策略二：Virtual Devices（虚拟设备）\n虚拟设备的关键思想是：直接为虚拟机提供一组虚拟化的设备接口，而不是模拟真实硬件设备。操作系统中的设备驱动程序是专门为虚拟设备编写的，而不是为真实硬件设计的。\n工作原理：\n在这种模式下，Guest 操作系统和 VMM 之间的设备交互方式是专门为虚拟环境设计的，跳过了模拟真实硬件的开销。 虚拟设备与 Guest 操作系统之间的交互不再依赖传统的硬件控制寄存器，而是通过共享内存或命令队列来交换数据。例如，Guest 操作系统将操作命令写入内存中的命令队列，VMM 直接读取该队列并执行相应的操作（如读写虚拟磁盘）。 设备交互：\n由于不再需要模拟真实硬件的寄存器和控制信号，虚拟设备不依赖 trap。这种方法允许 Guest 操作系统通过高速、低延迟的方式与 VMM 中的虚拟设备交互。例如，Guest 直接将命令写入共享内存，而 VMM 通过轮询或通知机制来处理这些命令。 VMM 可以设计更高效的交互机制，使 Guest 操作系统与虚拟设备之间的通信更为流畅，减少了频繁 trap 带来的性能开销。 性能优势：\n由于虚拟设备的设计不需要频繁的 trap，交互效率显著提高，尤其是对于需要快速读写的大量数据的设备（如虚拟磁盘、网卡等）。 这种方法通过优化内存交互，减少了 VMM 和 Guest 之间的上下文切换，从而提升了性能。 总结：\nVirtual Devices 是专为虚拟化环境设计的设备接口，只有知道虚拟化存在的操作系统才能使用它。因为虚拟设备不再试图模拟真实硬件，而是使用完全不同的接口，因此需要 Guest 操作系统的设备驱动支持这种虚拟设备。 优点是性能较好，因为减少了 trap 的频率，交互更加高效。缺点是只能在支持虚拟设备的操作系统上运行（比如支持 virtio 的操作系统）。 策略一与策略二的主要区别\n交互方式： 策略一模拟真实的硬件设备，Guest 操作系统按照与物理设备相同的方式进行设备交互，通过 trap 触发 VMM 进行处理。 策略二直接提供虚拟设备接口，Guest 操作系统通过优化的方式（如命令队列）与 VMM 进行交互，减少了 trap 频率。 性能： 策略一频繁的 trap 操作导致性能较差，尤其在处理高速设备时效率低下。 策略二通过虚拟设备设计，减少了 trap 的需求，性能显著提高。 操作系统的适配： 策略一对 Guest 操作系统没有特别要求，它可以运行任何不知晓虚拟化存在的操作系统，因为 VMM 模拟了物理设备。 策略二则需要 Guest 操作系统具备与虚拟设备兼容的驱动程序，比如 virtio 驱动。它不能伪装成真实设备，因此只有知道这些虚拟设备的操作系统才能正确与其交互。 为什么虚拟设备不能伪装成真实设备？\n虚拟设备和真实设备之间的差异在于它们的接口方式。虚拟设备并没有尝试去完全模拟物理设备的寄存器和操作方式，而是设计了更加高效的交互机制。如果虚拟设备伪装成真实设备，等于是走回了策略一的老路，需要模拟寄存器、控制信号等，增加了 trap 的频率，导致性能下降。因此，虚拟设备选择不伪装成真实设备，而是要求操作系统通过新的接口与它们交互，从而提升性能。\n总结：\n策略一：Emulation 模拟真实硬件，可以支持不知晓虚拟化的操作系统，但性能较差。 策略二：Virtual Devices 通过新的接口设计提供更高效的交互，但需要操作系统支持虚拟设备驱动。 策略一：设备模拟（Emulation）\n假设你有一个运行在虚拟机上的操作系统，这个操作系统（Guest）认为它有一个物理磁盘。Guest 操作系统上的磁盘驱动程序是为传统的硬件磁盘设计的，它会按照与物理磁盘交互的方式来访问磁盘。例如，操作系统会通过访问特定的寄存器来向磁盘发出读写命令。寄存器通常是内存映射的，操作系统通过向这些寄存器写入地址或指令来控制磁盘。\n在设备模拟模式下（策略一），VMM 会模拟一个硬件磁盘的行为。当 Guest 操作系统试图通过这些寄存器向磁盘发出命令时，由于这些寄存器并不实际存在，VMM 会捕获这些访问（触发trap），然后通过软件来模拟磁盘设备的行为。VMM 在后台可能将 Guest 系统的读写请求映射到一个虚拟磁盘文件中（例如 QEMU 使用的 .qcow2 或 .img 文件）。所以，从 Guest 的角度看，它好像在和一个真实的磁盘设备交互，但实际上是 VMM 模拟了整个交互过程。\n例子：\nGuest 操作系统向磁盘寄存器写入“读取块 1000”指令。 这个指令触发了一个trap，进入 VMM。 VMM 读取了这条指令，模拟磁盘的行为，并从虚拟磁盘文件中提取块 1000 的数据。 VMM 返回数据，并恢复 Guest 的执行。 这个过程每次需要访问寄存器都需要经过 VMM，性能较低，因为每次访问都涉及大量的 CPU 上下文切换。\n策略二：虚拟设备（Virtual Devices）\n在策略二中，磁盘设备的交互方式被重新设计，Guest 操作系统并不再使用标准的硬件寄存器来访问磁盘，而是使用虚拟化的磁盘接口。最常见的虚拟设备方案是Virtio，它是为虚拟机设计的高效设备接口。\n使用虚拟设备时，Guest 操作系统需要使用支持 Virtio 的磁盘驱动程序。这种驱动不再通过寄存器控制磁盘，而是通过共享内存中的命令队列来传递命令。VMM 和 Guest 共享一个内存区域，Guest 操作系统将所有的磁盘操作指令（例如读、写命令）写入到这个内存队列中，VMM 直接从内存中读取这些命令并处理它们。\n例子：\nGuest 操作系统通过 Virtio 磁盘驱动向共享内存的命令队列中写入“读取块 1000”指令。 VMM 监控这个队列，发现有新的读取请求。 VMM 直接从虚拟磁盘文件中提取块 1000 的数据，并将数据写回到共享内存中的结果缓冲区。 Guest 操作系统从共享内存中读取数据。 由于没有频繁的 trap，VMM 不需要每次都进入内核模式来模拟设备行为，这大幅提升了性能。\n具体场景中的区别\n假设你在虚拟化环境中运行一个高性能数据库服务器，这个服务器频繁地向磁盘读写数据：\n使用设备模拟（策略一）：每次数据库服务器向磁盘发起写操作，VMM 都会模拟磁盘寄存器的访问。由于每次都需要 trap，数据库的性能会受到很大影响，尤其是磁盘 IO 操作较为频繁时，频繁的上下文切换会消耗大量 CPU 资源。 使用虚拟设备（策略二）：Virtio 磁盘驱动通过高效的内存共享方式将读写命令发送给 VMM，VMM 直接从命令队列中读取指令，执行完命令后直接将数据写入内存。这种方式减少了大量的 trap 操作，大大提升了数据库的 IO 性能。 硬件对虚拟机的支持 接下来我将讨论硬件对于虚拟机的支持，这里特指的就是Intel的VT-x。为什么Intel和其他的硬件厂商会为虚拟机提供直接的硬件支持呢？\n首先虚拟机应用的非常广泛，硬件厂商的大量客户都在使用虚拟机\n其次，我们刚刚描述的Trap and Emulate虚拟机方案中，经常会涉及到大量高成本的trap，所以这种方案性能并不特别好。\n第三个原因或许就没那么有趣了。\nRISC-V非常适合Trap and Emulate虚拟机方案，但是Intel的x86处理器的一些具体实现使得它可以支持虚拟化，但是又没那么容易。\n所以Intel也有动力来修复这里的问题，因为它的很多客户想要在x86上运行VMM。\n这里硬件上的支持，是为了让人们能够更容易地构建运行更快的虚拟机。它已经存在了10年左右了，并且现在在构建虚拟机时使用的非常非常广泛。\n在Trap and Emulate方案中，VMM会为每个Guest在软件中保存一份虚拟状态信息，而现在，这些虚拟状态信息会保存在硬件中。\n这样Guest中的软件可以直接执行privileged指令来修改保存在硬件中的虚拟寄存器，而不是通过trap走到VMM来修改VMM中保存在软件中的虚拟寄存器。\n所以这里的目标是Guest可以在不触发trap的前提下，执行privileged指令。\n我们还是有一个VMM在内核空间，并且Guest运行在用户空间。\n当我们使用这种新的硬件支持的方案时，我们的VMM会使用真实的控制寄存器，而当VMM通知硬件切换到Guest mode时，硬件里还会有一套完全独立，专门为Guest mode下使用的虚拟控制寄存器。\n在Guest mode下可以直接读写控制寄存器，但是读写的是寄存器保存在硬件中的拷贝，而不是真实的寄存器。\n硬件会对Guest操作系统的行为做一些额外的操作，以确保Guest不会滥用这些寄存器并从虚拟机中逃逸。\n在这种硬件支持的虚拟机方案中，存在一些技术术语，至少Intel是这么叫的，Guest mode被称为non-root mode，Host mode中会使用真实的寄存器，被称为root mode。\n所以，硬件中保存的寄存器的拷贝，或者叫做虚拟寄存器是为了在non-root mode下使用，真实寄存器是为了在root mode下使用。\n现在，当我们运行在Guest kernel时，可以在不触发任何trap的前提下执行任何privileged指令。\n比如说如果想读写STVEC寄存器，硬件允许我们直接读写STVEC寄存器的non-root拷贝。\n这样，privileged指令可以全速运行，而不用通过trap走到VMM。这对于需要触发大量trap的代码，可以运行的快得多。\n现在当VMM想要创建一个新的虚拟机时，VMM需要配置硬件。\n在VMM的内存中，通过一个结构体与VT-x硬件进行交互。这个结构体称为VMCS（注，Intel的术语，全称是Virtual Machine Control Structure）。\n当VMM要创建一个新的虚拟机时，它会先在内存中创建这样一个结构体，并填入一些配置信息和所有寄存器的初始值，之后VMM会告诉VT-x硬件说我想要运行一个新的虚拟机，并且虚拟机的初始状态存在于VMCS中。\nIntel通过一些新增的指令来实现这里的交互。\nVMLAUNCH，这条指令会创建一个新的虚拟机。你可以将一个VMCS结构体的地址作为参数传给这条指令，再开始运行Guest kernel。 VMRESUME。在某些时候，Guest kernel会通过trap走到VMM，然后需要VMM中需要通过执行VMRESUME指令恢复代码运行至Guest kernel。 VMCALL，这条新指令在non-root模式下使用，它会使得代码从non-root mode中退出，并通过trap走到VMM。 通过硬件的支持，Guest现在可以在不触发trap的前提下，直接执行普通的privileged指令。\n但是还是有一些原因需要让代码执行从Guest进入到VMM中，其中一个原因是调用VMCALL指令，另一个原因是设备中断，例如定时器中断会使得代码执行从non-root模式通过trap走到VMM。\n所以通常情况下设备驱动还是会使得Guest通过trap走回到VMM。这表示着Guest操作系统不能持续占有CPU，每一次触发定时器中断，VMM都会获取控制权。\n如果有多个Guest同时运行，它们可以通过定时器中断来分时共享CPU（注，类似于线程通过定时器中断分时共享CPU一样）。\nVT-x机制中的另外一大部分是对于Page Table的支持。\n当我们在Guest中运行操作系统时，我们仍然需要使用Page Table。\n首先Guest kernel还是需要属于自己的Page Table，并且会想要能够加载CR3寄存器，这是Intel中类似于SATP的寄存器。VT-x使得Guest可以加载任何想要的值到CR3寄存器，进而设置Page Table。\n而硬件也会执行Guest的这些指令，这很好，因为现在Guest kernel可以在不用通过trap走到VMM再来加载Page Table。\n但是我们也不能让Guest任意的修改它的Page Table，因为如果这样的话，Guest就可以读写任意的内存地址。\n所以VT-x的方案中，还存在另一个重要的寄存器：EPT（Extended Page Table）。EPT会指向一个Page Table。当VMM启动一个Guest kernel时，VMM会为Guest kernel设置好EPT，并告诉硬件这个EPT是为了即将运行的虚拟机准备的。\n之后，当计算机上的MMU在翻译Guest的虚拟内存地址时，它会先根据Guest设置好的Page Table，将Guest虚拟地址（gva）翻译到Guest 物理地址（gha）。\n之后再通过EPT，将Guest物理地址（gha）翻译成主机物理地址（hpa）。硬件会为每一个Guest的每一个内存地址都自动完成这里的两次翻译。EPT使得VMM可以控制Guest可以使用哪些内存地址。\nGuest可以非常高效的设置任何想要的Page Table，因为它现在可以直接执行privileged指令。但是Guest能够使用的内存地址仍然被EPT所限制，而EPT由VMM所配置，所以Guest只能使用VMM允许其使用的物理内存Page（注，EPT类似于19.4中的Shadow Page Table）。\n学生提问：我对于硬件中保存的虚拟寄存器有问题，如果你有两个CPU核，然后你想要运行两个虚拟机，你会得到多少虚拟寄存器？\nRobert教授：每一个CPU核都有一套独立的VT-x硬件。\n所以每一个CPU核都有属于自己的32个通用寄存器，属于自己的真实的控制寄存器，属于自己的用在Guest mode下的虚拟控制寄存器，属于自己的EPT，所以你可以在两个CPU核上运行两个不同的虚拟机，它们不会共用任何寄存器，每个CPU核都有属于自己的寄存器。\n学生提问：那也需要一个新的VMM吗？\nRobert教授：VMM可以像一个普通的操作系统一样。XV6可以支持多个进程，并且为每个进程配备一个proc结构体。\n而我们的VMM也会为每个虚拟机配备一个vm结构体，用来跟踪Guest的信息。\n并且，如我之前所说的，如果你只有一个CPU核，但是有3个Guest，可以通过定时器中断结合VMM在3个Guest之间切换。\nDune: Safe User-level Access to Privileged CPU Features 今天要讨论的论文 利用了上一节介绍的硬件对于虚拟机的支持，但是却将其用作其他的用途，这是这篇论文的有趣之处，它利用了这种完全是为了虚拟机而设计的硬件，但是却用来做一些与虚拟机完全无关的事情。\n从一个全局的视角来看这篇论文的内容，它想要实现的是普通的进程。\n所以现在我们的场景是在一个Linux而不是VMM中，但是我们又用到了硬件中的VT-x。\n我们将会在Linux中加载Dune可加载模块，所以Dune作为kernel的一部分运行在Supervisor mode（注，又或者叫做kernel mode），除此之外，内核的大部分还是原本的Linux。\n因为这里运行的是Linux进程，所以我们期望Dune可以支持进程，以及包括系统调用在内的各种Linux进程可以做的事情。\n不过现在我们想要使用VT-x硬件来使得普通的Linux进程可以做一些额外的事情。\nDune会运行一些进程，或者说允许一个进程切换到Dune模式，这意味着，之前的进程只是被Page Table保护和隔离，现在这个进程完全被VT-x机制隔离开了。\n现在进程有了一套完整的虚拟控制寄存器，例如CR3寄存器，并且这些进程可以运行在non-root Supervisor mode，所以它可以在VT-x管理的虚拟状态信息上直接执行所有的privileged指令。\n基于上面的描述，Dune管理的进程可以通过属于自己的CR3寄存器，设置好属于自己的Page Table。\n当然Dune也会控制属于这个进程的EPT，EPT会被设置的只包含这个进程相关的内存Page。\n所以进程可以向CR3寄存器写入任意的Page Table地址，但是因为MMU会在翻译完正常的Page Table之后再将地址送到EPT去翻译，所以进程不能从分配给它的内存中逃逸。\n所以进程并不能修改其他进程或者kernel的内存，它只是有了一种更灵活的设置自己内存的方式。\nDune管理的进程也可以拥有Guest Supervisor mode和Guest User mode，就像一个小的虚拟机一样，并且可以保护运行在Supervisor mode的代码，不受运行在User mode的代码影响。\n论文中提到了可以基于Dune做的两件事情：\n首先，Dune能够在硬件层面支持进程同时拥有Guest Supervisor mode和Guest User mode，这样进程可以在自己的User mode中运行未被信任的插件代码。\n这里的主进程或许是一个网页浏览器，你可以为浏览器下载并运行各种各样的插件，或许是一个新的视频解码器，一个新的广告拦截插件等等。\n但是我们并不能完全信任这个插件，所以我们希望能够在权限受控的前提下运行它。虽然一个普通的Linux也可以达到这个目的，但是会比较麻烦。\n通过Dune，我们可以在Guest User mode下运行插件，同时让网页浏览器运行在进程的Guest Supervisor mode下。因为现在可以修改CR3寄存器，所以可以为Guest User mode配置一个不同的Page Table。\n这样，即使插件是恶意的，进程也可以安全的运行这里的未被信任的插件代码，因为插件代码现在不能任意的读写主浏览器的内存，只能访问网页浏览器指定的某些内存Page。\n进程的Guest User代码可能会执行系统调用，但是这些系统调用会通过trap走到进程的Guest Supervisor mode，而不是Linux内核，所以这里的插件代码或许会认为自己调用了fork/read/write等系统调用，但是实际上这里尝试运行的系统调用通过trap走到了进程对应的网页浏览器，而网页浏览器可以做任意的事情，它可以选择执行或者不执行系统调用。\n所以现在网页浏览器对于插件代码有了完全的控制能力。\n公平的说，这里提到的隔离效果可以通过Linux中一些非常不一样的技术来实现，但是Dune通过使用VT-x硬件，为你可以提供一个特别优雅且有效的实现方式。\n进程可以做的另一个事情是：通过Dune，进程的垃圾回收（Garbage Collect，GC）变得更快了。\n在这个场景中，没有了Guest Supervisor mode和Guest User mode。\n假设我们在运行任意一种带有GC的编程语言，比如说Java或者Python。\nGC可能会很慢，并且本身有着非常非常多的技术可以使得GC变快。许多GC都会扫描并找到位于内存中仍然在使用的对象，扫描会从寄存器中保存的对象指针开始，依次找到所有正在使用对象的所有指针。\n如果在扫描之后没能找到某个对象，那说明这个对象不被任何指针引用，那么它就可以被释放了。\n许多GC会同时在主程序的一个线程中运行，所以GC会从寄存器中保存的指针开始，根据指针之间的树或者图的关系，扫描一个个的对象。\n但是因为GC与程序本身是并行的在运行，所以程序可能会修改GC已经扫描过的对象，这很糟糕，因为这样的话，GC在扫描完成之后确定的要释放和不能释放的对象清单可能就不再正确了。\nDune使用了Page Table Entry中的一个特性来帮助GC检测这样的修改。\nDune管理的进程首先会设置好由VT-x提供的虚拟CR3寄存器，指向属于自己的Page Table，其中的PTE都是有效的。\n每一条PTE的dirty位，表明对于对应的Page存在写操作。所以如果程序在GC的过程中修改了某些对象，那么对应PTE的dirty位会被设置为1。\n当GC查找完所有的对象之后，它会查看所有PTE中的dirty位，找到包含了可能修改过的对象的内存Page，然后再重新扫描这些对象。\n实际中，获取PTE dirty位的过程在普通的Linux中既困难又慢，我甚至都不确定Linux是否支持这个操作，在一些其他操作系统中你可以通过系统调用来查询PTE的dirty位。\n但是如果你使用Dune和 VT-x，进程可以很快的使用普通的load和store指令获取PTE，进而获取dirty位。所以这里，Dune使得某些需要频繁触发GC的程序明显变得更快。\n学生提问：如果Guest User mode中的插件程序想要运行自己的GC会怎样？\nRobert教授：现在我们使用了Dune，并且有一个进程是被Dune管理的。\n这个进程通过VT-x实现了Supervisor mode和User mode，我们在User mode运行了一个插件，并且插件也是由带GC的编程语言写的，所以它有属于自己的Page Table，并且其中的PTE也包含了dirty位。\n但是刚刚说的GC加速在这不能工作，因为Dune会将插件运行在Guest User mode，而就像普通的User mode一样，Guest User mode不允许使用CR3寄存器。\n所以在Guest User mode，我们不能快速的访问PTE的dirty位。只有在Guest Supervisor mode，才能通过CR3寄存器访问Page Table。所以，并不能同时使用以上Dune提供的两种功能。\n学生提问：如果某人基于Dune写了个浏览器，那么对于不支持Dune的计算机来说就很难使用这样的浏览器，对吗？就像很难直接让Chrome使用Dune，因为不是所有的计算机都有这个内核模块。\nRobert教授：首先，这里提到的内容需要运行在支持VT-x的计算机上，也就是说底层的计算机必须支持VT-x，所以需要VT-x来运行Dune。\n其次Dune需要被加载来运行浏览器以利用前面说到的特性。\n所以是的，你需要将所有的东西都设置好。并且Dune是一个研究项目，它的目标是使得人们去思考可以部署在真实世界，并且有足够的价值的一些东西。\n就像Linux一样，Linux有成千上万个功能，如果某人决定将Dune添加到Linux中作为一个标准功能，那么我们就可以依赖这个功能，并且Chrome也可以直接用它了。\n学生提问：所以从整体来看，这里就像是创建了一个VM，但是实际上运行的又是一个进程？\nRobert教授：你可以这么描述。这里主要是对进程进行抽象，但是这里没有用Page Table硬件来时先进程间的隔离（注，其实也使用了，但是主要不依赖Page Table硬件），这里使用的是CPU上的硬件来支持进程，这里说的CPU上的硬件就是VT-x，它包含了一些额外的功能，例如设置好属于进程的Page Table。\n学生提问：论文里提到了，如果Dune管理的一个进程fork了，那就会变成一个不被Dune管理的进程，这不会是一个安全漏洞吗？\n比如说你通过Dune运行了一个进程，并且认为它现在是安全的。但是fork之后的进程因为不被管理所以可能会逃逸。\nRobert教授：Dune管理的进程的Guest Supervisor mode中，不存在安全的问题。这部分代码已经拥有了相应的权限，通过fork也不会获得更多的权限。\n但是另一方面，Dune的Guest User mode代码中，我们有未被信任的代码，如果让它在没有Dune管理的情况下运行会有一定的风险。\n所以这部分代码不能fork，如果它尝试执行fork系统调用，会通过trap走到进程的Guest Supervisor mode。\n假设进程的Guest Supervisor mode部分代码写的非常的小心，并且不会被欺骗，那么它不会执行fork，所以这时fork不能工作。\n如果Supervisor mode的代码允许fork，它会调用Linux的fork系统调用，并得到一个fork进程包含了与原进程有相同的内存镜像，所以我们在新进程中包含可能是恶意的插件代码。\n如果新进程没有意识到Dune已经被关闭了，那么原来的Supervisor mode中的privileged指令会是非法的。\n所以我们需要假设Dune管理的进程里面的Supervisor mode部分代码能够足够的小心且足够的聪明，来阻止User mode中的插件代码执行fork。\n学生：被Dune管理的进程拥有Supervisor mode并没有不安全，因为它实际上是non-root mode下的Supervisor mode，就像是Guest操作系统中的Supervisor mode一样，你可以让它做任何事情，因为VT-x的存在，进程就像是一个虚拟机一样，并不能伤害到真正的操作系统。\nRobert教授：是的，进程不能逃逸出来，因为存在EPT，而EPT会限制进程的地址空间。\n学生提问：在VT-x的方案中，当我们访问Page Table时，因为我们需要通过EPT进行第二层翻译，将Guest物理内存地址翻译到Host物理内存地址，这样从Page Table返回的延时是不是增加了？\nRobert教授：这里可能会花费更多的时间让硬件MMU来翻译内存地址。\n在最坏的情况下，比如在RISC-V中，会有多层Page Table，MMU需要一层一层的去查找PTE，x86下同样也有多层Page Table，所以在x86中首先会查找主Page Table，如果要访问更多的内存地址，每一次内存地址的访问都需要再次走到EPT，而EPT也是一个多层的Page Table。\n所以我并不知道最差情况下需要访问Page Table多少次才能完成翻译，但是很明显在VT-x下会比普通情况下差得多。不过实际中会有cache所以通常不会走到最坏的情况。\n学生提问：今天的虚拟机还是普遍会更慢吗？如果是的话，AWS是怎么工作的，因为看起来还挺快的，并且工作的也很好。\nRobert教授：我认为他们使用了硬件上的VT-x支持，并且使用了我们讨论过的一些功能，这样使得AWS虚拟机比较快，或者并不比真实的计算机慢多少。\n学生提问：我对于Trap and Emulate中的Shadow Page Table有个问题，每次都会创建Shadow Page Table吗？难道不能记住上次的Shadow Page Table吗？\nRobert教授：VMM需要创建新的Shadow Page Table以供真实的硬件使用。当然在很多时候都可以增加缓存，对于一个聪明的VMM，它可以注意到Guest更新了一个PTE，VMM可以做相应的有限的工作来更新Shadow Page Table。\n如果机器是在多个虚拟机上分时复用的，VMM会为还在运行的虚拟机保存Shadow Page Table，这样这些虚拟机可以在恢复时直接重用。\n学生提问：这难道不是意味着VMM为每个虚拟机中的每个进程都保存了Shadow Page Table的拷贝？\nRobert教授：是的，虚拟机里面有很多很多个Page Table，所以维护Shadow Page Table需要大量的工作。\n而类似于VT-x的硬件支持使得这部分工作更加的容易了，因为EPT表示你不用构建Shadow Page Table了。\n学生提问：我有个问题有关GC的，如果有dirty位的话需要重新扫描对象，那么有没有可能会无限扫描？\nRobert教授：是的，这有个问题，如果一直有对象在更新，扫描能正常结束吗？\n实际中，GC会先扫描一次，之后它会冻结除了GC线程以外的其他线程，所以这期间不可能再发生任何其他的变更。之后GC才会查看所有PTE的dirty位，但是因为其他所有线程都冻结了，所以不可能会有更多的dirty位了，所以GC查看了所有的dirty位，之后结束GC会结束扫描并创建需要释放对象的列表，最后再恢复所有之前冻结的线程的执行。GC是一个复杂的流程，Dune的论文中并没有足够的篇幅讨论它。\n","date":"2024-10-04T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08119virtual-machines/bg_hu_f4ae0fb7dbe2799.jpg","permalink":"https://echudet.github.io/p/mit6s08119virtual-machines/","title":"MIT6S081：19Virtual Machines  "},{"content":"Lecture 18 OS organization 课前预习 论文速览 论文链接 论文的架构大致分为以下几个部分，每个部分的内容总结如下：\n引言 (Introduction)\n本节介绍了微内核（µ-kernel）的背景，特别是第一代微内核系统因其性能低下和灵活性不足而受到批评。随后引入了L4，这是一种第二代精简的微内核，旨在通过减少内核中的功能来提高效率。作者的目标是通过实验表明基于微内核的系统可以在实际中使用，并且具有良好的性能。文章对比了L4与Linux以及MkLinux（基于Mach微内核的Linux版本），通过基准测试来衡量性能差异。\n相关工作 (Related Work)\n本节介绍了与微内核系统相关的研究历史，并对已有的实验和实现进行总结，包括对Mach、Chorus等微内核的描述。作者指出，过去的大量评估工作集中在操作系统部分功能的加速上，而对完整Unix系统实现的全面评估较少。文章的贡献在于探讨如何在第二代微内核上移植现有的单体操作系统，并评估其性能。\nL4基本概念 (L4 Essentials)\nL4微内核的设计围绕线程和地址空间展开。它支持跨地址空间的通信（IPC），这是L4的核心机制之一。本节详细介绍了L4中的递归地址空间构建、页管理、I/O端口的映射以及硬件中断的处理方式。L4使用用户级的调页器来处理页错误，实现了灵活的内存管理策略。此外，还讨论了L4在不同硬件架构（如Pentium、Alpha、MIPS）上的移植情况，强调了L4的跨平台特性。\nL4上的Linux (Linux on Top of L4)\n本节描述了将Linux移植到L4上的过程。L4Linux是作为一个用户级单服务器运行在L4微内核之上，且保持与Linux的二进制兼容。作者详细解释了系统调用的实现、设备驱动程序的处理、进程的调度等。此外，文章讨论了L4Linux设计中的一个错误（dual-space），并解释了它如何影响性能。最终，通过更简化的设计（single-space），实现了更高效的L4Linux实现。\n性能评估 (Performance Evaluation)\n本节通过微基准测试（如getpid系统调用）、宏基准测试（如Linux内核编译）和AIM基准测试来评估L4Linux的性能。结果表明，L4Linux的性能仅比原生Linux差5%-10%，而且明显优于基于Mach的MkLinux。性能评估表明，底层微内核的效率对整个系统性能有显著影响。\n扩展性评估 (Extensibility Evaluation)\n本节探讨了通过µ-kernel扩展系统功能的可能性。通过几个实验（如管道通信、内存管理操作）展示了在µ-kernel上进行系统扩展的潜力，尤其是在不影响已有Unix功能的前提下引入新的功能，如实时内存管理系统。实验表明，通过µ-kernel实现的扩展性不仅能提升性能，还能支持多种不同应用场景的高效共存。\n替代的基本概念 (Alternative Basic Concepts)\n本节讨论了是否可以通过引入更底层的机制（如保护性控制传输PCT）来进一步提高µ-kernel的性能。作者分析了PCT与IPC的性能差异，并认为虽然PCT理论上应该更快，但经过优化的IPC在很多实际场景中表现得更好。此外，文章还探讨了其他改进µ-kernel性能的方法，如引入\u0026quot;grafting\u0026quot;模型。\n结论 (Conclusion)\n文章总结了L4微内核在性能和扩展性方面的优势，表明µ-kernel可以在性能接近原生操作系统的情况下，支持更灵活和高效的系统扩展。同时，文章指出未来的工作应继续探索µ-kernel在不同平台和应用场景中的适用性。\n基于线程和地址空间的核心设计思想 L4微内核的核心设计思想是基于线程和地址空间，并通过跨地址空间通信（IPC）机制实现线程间的高效消息传递。同时，L4微内核允许用户级的服务器递归地构建地址空间，并通过这种方式实现灵活的内存管理。让我具体说明这个过程的实现。\n跨地址空间通信（IPC）\nIPC（Inter-Process Communication）是L4微内核中最基本、最核心的机制之一，用于支持线程间在不同地址空间中的通信。每个线程都在特定的地址空间中执行，而当需要进行线程间通信时，L4微内核提供了一种安全且高效的方式来进行同步消息传递。\nIPC的基本步骤\n发送方线程通过IPC将消息发送到接收方线程所在的地址空间中。这是同步的，即发送方会等待接收方的响应。 IPC消息可以携带多种信息，除了普通的消息数据外，还可以包含地址空间中的页面映射信息，允许在不同地址空间之间共享内存页。 通过这种方式，L4可以实现远程过程调用（RPC）以及受控的线程迁移。 递归地址空间构建\nL4微内核通过支持递归地址空间构建来实现内存的灵活管理。最初的地址空间（即σ0σ0）代表了整个物理内存，其他地址空间都是在此基础上由用户级服务器逐步构建的。\n地址空间构建的基本过程\n映射与解除映射：地址空间的构建是通过映射（mapping）、授予（granting）和解除映射（unmapping）三种操作来实现的。L4中使用了一种叫做“柔性页”（flexpages）的概念，这些页的大小可以从一个物理页（通常是4KB）扩展到整个地址空间（几GB）。 映射（Mapping）：用户态的内存管理程序可以将自己的某些页面映射到另一个地址空间，从而使该页面可以在多个地址空间中访问。 授予（Granting）：拥有某个地址空间的线程可以将页面的完全控制权授予另一个地址空间，从而允许接收方也能够修改或进一步映射该页面。 解除映射（Unmapping）：拥有页面的地址空间可以随时解除该页面的映射，取消其他地址空间对该页面的访问权限。 递归地址空间的安全性：这些操作是安全的，因为它们是基于虚拟页面的，而不是直接作用于物理内存。也就是说，线程只能映射或解除映射它已经拥有的页面。这样，L4确保了跨地址空间的操作不会破坏系统的安全性。 页面错误处理与用户级分页器\n当线程访问了尚未映射的页面时，会发生页面错误。在L4微内核中，内核会通过IPC将该页面错误传递给与该线程相关联的分页器（Pager）。分页器通常是一个用户态的内存管理程序，负责处理页面错误并决定如何分配内存。\n当页面错误发生时，L4微内核会向相应的分页器发送消息，通知其处理该页面错误。分页器可以通过映射新页面来解决错误，也可以选择拒绝该请求。\n由于分页器的实现是完全由用户定义的，L4支持灵活的内存管理策略。这意味着，系统可以根据需要实现不同的内存管理模型，如实时内存管理或分层缓存管理。\n输入/输出端口与硬件中断的处理\n除了内存页面，L4还将I/O端口视为地址空间的一部分，使得它们可以像内存页一样进行映射和解除映射。L4通过将硬件中断转换为消息的形式，实现了中断处理的用户级实现。当硬件中断发生时，微内核会将中断转化为一条消息，发送给相应的线程，这为用户级设备驱动程序的实现提供了基础。\nL4微内核通过跨地址空间通信（IPC）实现线程间的高效消息传递，而递归地址空间的构建则为灵活的内存管理提供了强大的支持。用户级分页器和灵活的映射/解除映射机制允许L4实现多种内存管理策略，同时确保系统的安全性和效率。\nL4的二级缓存 在计算机系统中，缓存（Cache）是一种快速存取的存储器，用于存储频繁使用的数据，以提高系统性能。通常，缓存分为多级，例如一级缓存（L1 Cache）和二级缓存（L2 Cache），它们的主要区别在于速度、容量以及与处理器的距离。\n一级缓存（L1 Cache） 二级缓存（L2 Cache） 位置 一级缓存通常内置于处理器的核心内，每个核心都有自己的一级缓存。 二级缓存位于处理器与内存之间，可能是每个核心独立拥有，也可能是多个核心共享的。 速度 一级缓存是处理器最快的缓存，因为它最接近处理器核心，访问延迟非常低。 二级缓存比一级缓存稍慢，但仍比内存快得多。 容量 一级缓存容量较小，一般为几十KB。它主要用于存储最频繁使用的指令和数据。 二级缓存的容量通常比一级缓存大得多，一般为几百KB到几MB。它用于缓存一级缓存未命中的数据或指令。 分区 一级缓存通常分为两个部分：一个用于存储指令（指令缓存，L1I），一个用于存储数据（数据缓存，L1D）。 当处理器在一级缓存中找不到所需数据时，会去二级缓存中查找。如果二级缓存也没有命中，才会访问内存。 二级缓存的作用\n二级缓存通过缓存较大数据块，减轻了处理器访问主存（RAM）的频率，因此可以显著提高处理器的执行效率。虽然二级缓存的访问速度不及一级缓存，但因为其容量较大，能够存储更多的数据，减少了缓存未命中的次数，从而进一步提高性能。\n一级缓存与二级缓存的区别\n访问速度：一级缓存比二级缓存快，但二级缓存仍然远远快于主存。 容量：一级缓存容量较小，而二级缓存容量更大，能够存储更多的数据。 位置：一级缓存紧靠处理器核心，二级缓存则稍远，通常在核心外或是多核心共享。 结合 xv6 缓存实现\n在 xv6 操作系统中，缓存的一个典型实现是缓冲区缓存（Buffer Cache），它通过在内存中缓存磁盘块的数据，来减少磁盘 I/O 操作的开销。xv6 中的缓存机制与 L1 和 L2 缓存有一些相似之处：\n缓存层级：虽然 xv6 中的缓冲区缓存不是严格意义上的 L1 或 L2 缓存，但它在存储体系中起到缓存的作用，用于临时存储从磁盘读取的数据。类似于 L1 和 L2 缓存，缓冲区缓存通过缓存经常访问的数据块，减少了对慢速磁盘的直接访问。 缓存命中与未命中：在缓冲区缓存中，当进程请求一个数据块时，系统首先检查该块是否已经在缓冲区缓存中。这类似于处理器首先在 L1 或 L2 缓存中查找数据。如果命中，则直接返回缓存中的数据，避免了磁盘 I/O；如果未命中，则需要从磁盘读取数据块并将其存入缓存，以备后续访问。 性能优化：缓冲区缓存的设计目标之一就是通过减少对磁盘的频繁访问来提高性能，这与硬件缓存设计中的目标相似。L1 和 L2 缓存的主要目的也是通过减少对主存的访问来提高处理器的执行效率。 在L4上的二级缓存管理\n论文中的二级缓存管理指的是控制二级缓存的分配，以提高实时应用的最坏情况执行时间。具体而言，通过在L4的用户级实现中，分配和隔离部分二级缓存的页面，减少了任务之间的缓存干扰。这个机制类似于xv6中的缓冲区缓存系统，只不过在L4中，这种缓存隔离更为灵活，能够动态分配缓存资源给实时任务，确保它们不被其他任务干扰，最终提升系统的可预测性。\n总的来说，一级缓存与二级缓存的主要区别在于速度、容量和作用范围，而这与xv6中的缓冲区缓存在系统性能优化中起到的作用有相似之处。通过缓存热数据并减少慢速存储访问次数，缓存系统能够显著提升系统整体性能。\nLower-level Communication Primitive 低级通信原语Lower-level Communication Primitive，顾名思义，是一种与操作系统或硬件紧密相关的基础通信机制。它比传统的高层抽象（如 L4 中的 IPC，进程间通信）更贴近硬件的运行方式。这些低级原语直接操作硬件或硬件相关的机制，理论上能够比高层抽象提供更高的性能和更少的延迟。一个例子是 Exokernel 中的 protected control transfer（PCT，保护性控制传输），它允许在不同地址空间之间进行快速的控制转移，类似于硬件级别的中断或函数调用。\nL4 中的 IPC 是一种高层抽象，用于在不同地址空间的线程之间进行通信。它是一种同步的消息传递机制，带有内核级别的安全保护，同时允许实现更复杂的通信模式，如远程过程调用（RPC）。虽然 IPC 是一种非常强大的通信方式，但由于它是一种高层抽象，可能在一些极端情况下性能不如更直接的低级通信原语。\n在论文中的实验中，研究人员想通过在 Alpha 处理器上实现一种类似于 Exokernel 的保护性控制传输（PCT）的低级通信原语，来测试这种更底层的机制是否能够在性能上超越 L4 的 IPC。虽然理论上低级通信原语应该更快，因为它省去了高层抽象的开销，但实验结果表明，通过优化，L4 的 IPC 仅比 PCT 慢了几个 CPU 周期。因此，在很多实际应用场景中，L4 IPC 的性能表现已经非常接近低级原语。\n结论：L4 的 IPC 和低级通信原语不是同一层级的机制。L4 的 IPC 属于操作系统的高层抽象，提供了更丰富的功能和安全性，而低级通信原语则直接面向硬件，属于更基础的系统层级。L4 IPC 的优势在于它能够通过提供同步消息传递和安全的跨地址空间通信来实现复杂的操作系统功能，而低级通信原语则更适合于对性能要求极高且不需要高层抽象的特殊场景。\nL4中的异常（Exceptions）与xv6中的陷阱（Trap）的区别 在L4中，异常和陷阱与线程的执行是同步的，也就是说，异常会在引发它的线程的执行过程中同步触发。在Pentium处理器上，L4将处理器的异常处理机制分配给每个线程。当某个线程发生异常时，处理器会将指令指针和标志寄存器的值压入该线程的用户态栈，然后调用该线程的用户态异常或陷阱处理程序。L4的内核会简单地将这些异常镜像到用户态，允许用户态处理这些异常。\n与此相对，xv6中的trap机制是操作系统内核捕捉处理器异常、系统调用和中断的机制。不同于L4的同步异常处理，xv6中的陷阱是由内核来处理的。每当发生一个trap，处理器会跳转到内核的trap处理程序（通常是由硬件触发的中断或系统调用），内核会根据陷阱类型采取不同的动作（例如，系统调用会进入对应的系统调用处理函数，硬件中断会跳转到设备的中断处理程序等）。xv6不会把这些异常直接反射到用户空间，而是由内核直接管理这些异常。\n主要区别在于，L4将异常处理推送给用户态线程，而xv6中，所有的trap（包括异常和系统调用）都由内核处理。L4的设计强调最小化内核功能，将更多的处理移交给用户态，这也是微内核设计的核心思想。\n在L4的Pentium实现中，“small address spaces”（小地址空间）是一种优化机制。它利用了Pentium处理器的段机制，通过共享所有页表物理上对齐的小地址空间（范围在4 MB到512 MB），模拟标签化的TLB，用于在小地址空间之间进行上下文切换时避免不必要的TLB刷新。这个机制能够在不影响系统功能的情况下提高性能，尤其是在频繁上下文切换时有效。\n当线程访问数据超出了当前小地址空间的范围时，内核会自动将其切换回正常的3 GB地址空间模型。每个任务中的线程可能会使用不同的地址空间模式，即一些线程使用正常的大地址空间，另一些则使用相应的小地址空间。这种机制仅用于优化，目标是减少TLB刷新和提升IPC性能。\n总结一下：\nL4中的exception处理直接交由用户态线程处理，而xv6则由内核捕捉并处理异常和系统调用。 small address spaces是L4中的一种性能优化，通过在小地址空间中模拟TLB标签化来减少TLB刷新，提高系统效率，尤其是在频繁上下文切换场景下有显著作用。 L4中的Small Address Space是如何通过模拟标签化TLB减少TLB刷新？ 在L4的Pentium实现中，small address spaces（小地址空间）是一种专门的优化机制，旨在减少TLB（Translation Lookaside Buffer，翻译后援缓冲区）的刷新频率，进而提高性能。TLB的作用是缓存虚拟地址到物理地址的映射，以加速虚拟内存的访问。当发生上下文切换时，如果不同的进程使用不同的虚拟地址空间，TLB必须被刷新，以防止一个进程的虚拟地址被误用于另一个进程。这种TLB刷新的过程会引入性能开销。\n标签化TLB的概念 标签化TLB（tagged TLB）是一种通过为不同的地址空间分配标签来避免频繁刷新TLB的机制。每个进程的地址空间都有一个唯一的标签，TLB可以根据这些标签区分不同的虚拟地址空间。当进程切换时，TLB不会被完全刷新，而是根据标签来区分哪些条目属于当前进程。因此，即使发生了上下文切换，只要进程的标签匹配，TLB中的条目依然有效，减少了重建TLB的开销。\n在Pentium架构上，L4使用段机制来模拟标签化TLB。当进程使用的地址空间较小时（比如4MB到512MB），L4能够通过共享所有页表并使用Pentium的段机制来保护这些地址空间，避免了频繁的TLB刷新。这一过程模拟了标签化TLB的效果：\n共享页表：当一个线程的地址空间很小时，所有这些小地址空间可以共享同一个物理页表，这意味着在不同进程或线程之间切换时，页表保持不变，从而减少了对TLB的刷新需求。 使用段机制保护：Pentium的段机制允许L4通过段寄存器来限制线程对其内存的访问范围。当地址空间较小时，L4可以通过将这些小地址空间映射到特定段中，并使用段机制保护这些地址空间。这样，当线程在这些小地址空间之间切换时，不需要完全刷新TLB，因为段机制提供了额外的保护。 上下文切换中的优化：当上下文切换发生在使用小地址空间的线程之间时，由于页表是共享的且通过段机制进行保护，内核无需刷新TLB。这种模拟的标签化TLB机制显著减少了上下文切换中的TLB刷新开销，提高了系统性能。 通过共享页表并使用段机制保护小地址空间，L4避免了每次上下文切换时的TLB刷新。通常情况下，上下文切换会导致TLB被清空，因为不同进程有不同的地址空间，而TLB中的虚拟地址到物理地址的映射可能会失效。通过使用段机制和共享页表，L4能够确保不同小地址空间的线程可以无缝切换而不刷新TLB，这就大大减少了上下文切换的开销，特别是在频繁发生上下文切换的环境中。\n总结：小地址空间的优化在L4中通过共享页表和段机制来模拟标签化TLB，减少了上下文切换时的TLB刷新，提升了系统性能。\nPALcode 在L4的Alpha实现中，PALcode（Privileged Architecture Library Code）是Digital公司设计的一段特殊固件，它在Alpha处理器上运行，用于执行特权级别的操作。这段代码帮助操作系统与底层硬件进行交互，通常被用来执行一些与硬件直接相关的操作，如寄存器的设置、中断处理、上下文切换等。\nL4微内核在Alpha处理器上的实现是完全重写了Digital原有的PALcode，目的是优化L4在Alpha平台上的性能。PALcode负责处理一些时间紧迫的操作（例如：上下文切换、TLB操作等），这些操作会通过手动调优来减少时间消耗，并且直接在PALcode中执行。而对于那些需要较长时间且可以被中断的操作，这些操作会先进入PALcode，然后切换到内核模式，剩余部分使用标准的机器指令执行。\nPALcode是一种低级别的固件代码，通常用来处理与硬件相关的关键操作，确保这些操作能够在不干扰内核复杂性的情况下快速执行。L4选择将一些紧急的操作在PALcode中完成，是为了最大限度地减少操作开销，而对于复杂的、可以被中断的操作，则会交给内核进一步处理，这样可以在不牺牲灵活性的同时实现性能优化。\n紧急操作（如上下文切换等）通常与处理器的硬件机制紧密相关，PALcode可以直接控制这些硬件机制，从而快速完成操作，而不需要经过内核的复杂调度系统。因此，紧急操作在PALcode中执行，确保快速响应。\n而对于长期操作（例如可以被中断的操作），则在PALcode中启动后，切换到内核来处理剩余部分。这样做的好处是利用PALcode的高效处理硬件操作的能力，同时借助内核的丰富功能来处理复杂的操作逻辑。这样分层处理可以在确保系统灵活性的同时优化关键路径上的性能。\nPALcode是处理硬件相关操作的一个固件层，在L4/Alpha的实现中，它主要负责高效地完成紧急操作，减少时间开销。而对于可以被中断的长期操作，则由PALcode进入内核模式，利用内核的丰富功能完成操作。\nL4Linux中的三类系统调用接口 在L4Linux中，用户进程无法直接访问Linux内核，而是通过L4微内核的IPC机制与Linux服务器（作为用户态任务运行的Linux内核）进行通信。这三类系统调用接口是为了让用户进程能够向Linux服务器发起系统调用请求的不同方式。\n下面详细说明这三种接口，并配合示例帮助理解。\n修改后的标准共享库，使用L4 IPC调用Linux服务器\n这种方式修改了Linux标准的共享库函数，例如libc库中的系统调用函数。每当用户进程调用这些库函数时，这些函数不再直接执行传统的Linux系统调用，而是通过L4的IPC机制与运行在用户态的Linux服务器通信。\n例子：假设用户进程调用write()系统调用，将数据写入文件。在L4Linux中：\nwrite()函数调用不再直接调用Linux内核的系统调用入口。 相反，经过修改的libc库中的write()函数会打包该请求，然后通过L4 IPC将请求发送给Linux服务器。 Linux服务器收到这个IPC请求后，模拟传统的Linux内核来执行相应的系统调用，处理写文件操作。 最后，Linux服务器通过IPC将执行结果返回给用户进程。 这一机制允许用户进程无缝地与Linux服务器通信，而不需要内核态的系统调用机制。\n修改后的静态库版本\n与共享库类似，静态库版本是为了那些不使用共享库的程序提供的。静态库直接链接到应用程序中，应用程序在编译时将这些修改过的系统调用代码包含在可执行文件中。\n这种方式特别适合那些在编译时就打包所有依赖库的程序，它的工作方式与共享库类似，但在性能上可能有稍微的优势，因为不需要运行时动态加载库。\n例子：假设用户进程调用open()系统调用打开一个文件：\n在编译时，用户进程链接的是一个修改过的静态库版本，该版本中的open()函数通过L4 IPC发送请求给Linux服务器。 Linux服务器处理这个请求，模拟原始的Linux系统调用，执行文件打开操作。 结果通过IPC返回给用户进程。 与共享库不同的是，静态库在编译时已经确定，运行时不需要再加载外部库文件，减少了运行时的开销。\n用户级异常处理程序模拟原生的系统调用陷阱指令\n这是一种更底层的机制，允许通过模拟传统的系统调用陷阱（Trap）指令来实现系统调用。通常在传统的Linux中，系统调用是通过陷阱指令（如int 0x80或syscall）触发内核模式的系统调用入口的。L4Linux中，通过用户态异常处理程序来捕获这些系统调用陷阱，并将它们转换为L4 IPC请求发送给Linux服务器。\n例子：假设用户进程调用fork()，在传统的Linux中，这通常是通过一条系统调用陷阱指令进入内核的：\n用户进程发出一个陷阱指令，例如int 0x80，原本这会导致处理器切换到内核模式并调用内核中的fork()系统调用处理程序。 在L4Linux中，L4的用户级异常处理程序捕获了这个陷阱指令，而不会直接进入内核模式。 异常处理程序通过L4 IPC将这个系统调用请求发送给Linux服务器。 Linux服务器接收该请求，执行fork()操作，并将结果通过IPC返回给用户进程。 这种方式模拟了传统系统调用的行为，使得不需要修改应用程序本身，只需用户态的异常处理程序来捕获并转换陷阱指令。\n总结与对比\n修改后的标准共享库：通过修改libc等系统调用库，使用L4 IPC直接与Linux服务器通信。这种方法简单且高效，适合大多数应用场景。 修改后的静态库版本：与共享库方式相似，但在编译时链接到可执行文件中。它适合那些不依赖共享库的应用程序，能够在运行时减少一些开销。 用户级异常处理程序：更底层的方式，通过捕获用户态的系统调用陷阱，模拟内核行为。这种方法使得系统调用与传统Linux兼容性最好，但可能在性能上略低于其他两种方式，因为它需要捕获和处理异常。 这三种方式各有其使用场景，结合使用能够满足不同的需求。\narchitecture 在这段文字中，\u0026ldquo;architecture-dependent\u0026rdquo; 和 \u0026ldquo;architecture-independent\u0026rdquo; 中的 \u0026ldquo;architecture\u0026rdquo; 主要是指 硬件体系结构，特别是不同的 CPU 架构（如 x86、Alpha、SPARC 等）。换句话说，\u0026ldquo;architecture\u0026rdquo; 指的是不同的硬件平台，它不仅包括 CPU 结构，还可能涉及内存管理、I/O 设备等与特定硬件直接相关的功能。\nArchitecture-dependent 部分：是指那些与特定硬件（如 CPU、内存、I/O 等）密切相关的内核功能。这些部分需要根据不同硬件平台进行移植和调整，比如中断处理、设备驱动、地址空间管理等低级操作。这些功能完全封装了底层的硬件架构，确保 Linux 能够在不同的硬件平台上运行。这是 Linux 需要修改的地方，尤其当要移植到新平台时。 Architecture-independent 部分：是指那些与硬件架构无关的通用功能，比如进程管理、资源管理、文件系统、网络子系统等。无论是 x86 还是其他架构，这些部分都可以在不同平台之间保持不变，不需要特别修改。它们占据了 Linux 内核代码的大部分（约 98%），因此对于内核移植来说，绝大部分代码是通用的。 所以，\u0026ldquo;architecture\u0026rdquo; 主要指的是 CPU 及相关硬件架构，而不是 Linux 内核本身。不同的硬件平台可能需要调整内核的 architecture-dependent 部分，但大多数通用功能都属于 architecture-independent 部分，不需要变动。\nLinux as server 虽然 µ-kernel 和 Linux 都是内核，但它们的作用和架构是不同的。\nµ-kernel（微内核）：是一种内核设计方法，旨在将操作系统的核心功能（如进程管理、内存管理、中断处理等）精简到最低限度，只保留一些基础服务，其他功能则放在用户态中实现。微内核的目标是模块化、灵活性和可移植性，它本身只负责一些基本任务（比如进程切换、消息传递等）。 Linux：是一个传统的宏内核（monolithic kernel），意味着它的内核包括了大量的功能（如文件系统、设备驱动、网络协议等），这些功能都是运行在内核态的。 尽管 µ-kernel 和 Linux 都是内核，但它们并不是互相排斥的。Linux 可以被移植到 µ-kernel 上的原因在于将 Linux 作为一个用户态进程运行在 µ-kernel 上，而不是直接替换或拆解 Linux 的内核部分。\n具体来说，移植过程如下：\nL4 µ-kernel 提供底层支持：L4 作为一个微内核，提供基本的内核功能（如进程调度、内存管理、消息传递等），这些功能足够支持上层应用的运行。 Linux 运行在用户态：Linux 本身被设计为一个用户态的服务，运行在 µ-kernel 提供的框架之上。它本身仍然提供文件系统、网络等高级功能，只是这些功能从传统的内核态移到了用户态。 单服务器模型：通过单一的 Linux 服务器进程，Linux 的所有用户进程在 µ-kernel 上得到服务。这种单服务器的模型意味着，所有 Linux 用户进程通过该服务器进程进行系统调用和操作。 因此，Linux 并没有被拆解成各个小部分融入 µ-kernel，而是整体上作为一个用户态的服务来运行。这种方法不仅保持了二进制兼容性（即任何 Linux 软件仍然可以运行），还检验了在这种微内核架构下，能否不通过针对 µ-kernel 的深度优化就实现良好的性能。\n这种架构的好处在于，通过 µ-kernel 提供底层支持，Linux 可以保持其高层功能模块的一致性，而不需要针对每种硬件做太多的修改。\n可以这么理解：在移植到 µ-kernel 上时，Linux 原来内核中与底层硬件直接交互的部分会被替代或重新封装，因为这些功能（如进程调度、内存管理、消息传递等）已经由 L4 µ-kernel 提供了。也就是说，Linux 内核中与硬件紧密耦合的底层部分会被 µ-kernel 接管，从而让 Linux 可以运行在 µ-kernel 之上，成为一个用户态的服务。更具体地讲：\n底层内核功能被 µ-kernel 接管：原本由 Linux 内核直接负责的功能，例如进程调度、内存管理、消息传递机制等，这些底层硬件交互的操作将由 µ-kernel 负责处理。所以在 µ-kernel 的架构下，Linux 内核中的这些部分被剥离掉或者不再直接发挥作用。 Linux 的上层功能保留：Linux 内核中仍然保留了很多操作系统需要的高层功能模块（如文件系统、网络协议栈、设备驱动接口等），这些功能继续提供服务，只是它们运行在用户态，而不是传统内核态。这个被移植后的 Linux 内核整体上看起来更像是一个“单服务器”，提供操作系统的一些高级功能。 ABI 和系统调用保持不变：虽然 Linux 在 µ-kernel 上运行，但是它的应用程序二进制接口（ABI）保持不变，用户态进程通过类似传统 Linux 的系统调用接口与该“单服务器”交互。这意味着，从用户的角度来看，程序依然像是在一个传统的 Linux 系统上运行。 所以，Linux 原来内核中的部分确实被替换为 µ-kernel 提供的功能，但并不是全部被剔除。Linux 的核心功能模块被保留下来，并在用户态作为一个整体服务运行，而 µ-kernel 负责最底层的系统功能。这样，Linux 就从一个完整的操作系统内核变成了一个运行在 µ-kernel 上的用户态服务。\n可以把它想象成 Linux 成为一个服务器，我们通过与 µ-kernel 交互来间接获得 Linux 这台“服务器”提供的服务。更具体来说：\nµ-kernel 是底层管理者：µ-kernel 负责最底层的硬件交互、进程调度、内存管理和消息传递等功能。这些功能是操作系统核心中的基础部分。 Linux 作为单服务器进程：在这种架构下，Linux 被看作是一个单一的服务器进程，它提供文件系统、网络、设备驱动等高层操作系统服务。所有的用户进程不再直接与硬件交互，而是通过 µ-kernel 进行中转。 交互方式：用户进程首先通过系统调用或消息传递与 µ-kernel 交互，µ-kernel 再将请求转发给运行在用户态的“Linux 服务器”进程。这台“服务器”处理高级功能的请求，如文件系统操作、网络通信等，并将结果返回给 µ-kernel，最终传递回用户进程。 这种设计不仅保持了 Linux 高层服务的功能性，还允许 µ-kernel 作为底层的调度器和资源管理器，提供了模块化和更灵活的系统架构。\n总的来说，µ-kernel 是操作系统的核心支柱，Linux 作为其中的一个重要服务提供操作系统的大部分功能，类似于用户通过 µ-kernel 访问“Linux 服务器”来获取服务。\ntop half 和 bottom half 在 Linux 中，中断处理分为 top half 和 bottom half，它们协同工作来处理硬件中断，保证系统高效运行。\nTop Half（中断上半部） Bottom Half（中断下半部） 快速处理高优先级任务：当硬件中断发生时，top half 立即响应。它的目的是快速处理与中断相关的最关键部分，确保硬件能够尽快恢复正常工作。 延迟处理非关键任务：由于 top half 只负责处理最关键的部分，剩下的一些非实时性或耗时较长的任务会被推迟到 bottom half 来执行。 典型任务：包括确认中断、读取硬件状态、清除中断标志，以及做一些非常简单且关键的工作。 典型任务：可能包括数据包处理、调度更多任务、进行较复杂的硬件状态处理等。总之，任何不需要立即执行、但仍然需要对中断进行完整处理的工作都会放在 bottom half。 优先执行，优先级最高：top half 是在中断发生时首先被调用的。它是以高优先级执行的，因为它直接由硬件中断触发，且需要快速完成以避免影响系统的实时性。 在 top half 完成后执行：bottom half 会在所有相关的 top half 完成后被调用，确保硬件中断的快速处理。 不可抢占：top half 不会被其他 top half 或 bottom half 中断。它以最高优先级执行，其他中断会被屏蔽（在 SMP 系统中，一般是局部屏蔽），直到该 top half 处理完成。 低优先级：bottom half 的优先级通常低于 top half，但高于正常的用户态进程。它可以被其他 top half 中断，但不会被其他 bottom half 中断。 执行流程与关系\n硬件中断触发：硬件中断（如网络包到达、键盘输入等）会触发 µ-kernel 产生一个消息，该消息被映射到一个中断事件。 top half 执行：与该中断源相关联的 top half 处理程序（thread）会被触发。此时，top half 会以最高优先级运行，快速处理最关键的中断逻辑。 记录状态，推迟非关键任务：top half 在完成了对中断的快速处理后，会将剩余的任务（非关键任务）记录下来，通常是放入一个待处理队列。 bottom half 执行：当所有 top half 都完成后，专门的 bottom half 处理线程 会被触发，开始处理那些被推迟的非关键任务。此时，系统不再受硬件中断的影响，因此 bottom half 可以更从容地执行较为复杂的逻辑。 优先级关系：为了确保系统响应及时，top half 具有最高优先级，其次是 bottom half，最后才是普通的用户进程和 Linux 服务器进程。 总结\ntop half 和 bottom half 的关系：top half 负责快速处理中断，确保最关键的工作能够立刻完成。而 bottom half 则延迟处理耗时或非关键的任务，确保系统高效运行。 执行顺序：硬件中断 → top half 执行 → bottom half 执行 → 继续用户进程或 Linux 服务器进程。 这样划分的目的在于，最小化系统停滞的时间，即确保中断处理不会长时间阻塞其他系统操作，同时又能及时完成对硬件中断的响应。\nL4 的 copyin 和 copyout 机制 L4 的 copyin 和 copyout 机制看起来与传统 Linux 内核的做法相似，但实际上有一些关键区别，导致了性能差异。为什么 L4 的方法在这种情况下更有效率：\n在传统的 Linux/x86 中，用户态地址空间总是映射到内核态地址空间中。这样，copyin 和 copyout 操作只需要通过简单的内存拷贝来完成，地址转换是由硬件自动处理的。这种方法的优点是拷贝操作本身非常简单，不需要在软件层面进行额外的地址转换，因为硬件已经将用户地址映射到内核空间了。\n在 L4Linux 中，copyin 和 copyout 采用的是物理地址拷贝。这意味着在每次拷贝操作时，L4Linux 需要通过服务器内部的逻辑页表手动解析用户态的虚拟地址，并将其转换为“物理地址”（实际上是指在 L4Linux 服务器的地址空间中对应的地址）。完成地址转换后，再进行基于物理地址的内存拷贝。\n表面上看，这两种方法的区别似乎不大，都是在用户态和内核态之间进行数据拷贝，但实际的性能影响来源于 虚拟地址空间的管理方式 和 硬件开销：\n硬件开销的不同：在传统的 Linux/x86 中，虽然地址映射是由硬件自动处理的，但每次进行内核态和用户态的切换时，硬件需要处理大量的上下文切换，这包括重新加载页表等操作。在 L4 这样高度模块化的系统中，这些上下文切换的开销更为显著，导致传统方法在 L4 环境下表现不佳。 L4 的页表管理方式更轻量：L4 采用了一种更加轻量级的虚拟内存管理机制，它避免了频繁的硬件上下文切换。通过在软件层面手动解析地址（即通过逻辑页表将虚拟地址映射为物理地址），L4Linux 可以减少硬件参与的地址转换操作。这种方式虽然看似增加了软件上的处理负担，但却避免了硬件在上下文切换上的高开销，从而在实际应用中提高了整体性能。 内核态和用户态的隔离：L4 微内核架构中的内核态和用户态有更加严格的隔离，这种隔离在提高安全性和模块化的同时，也使得传统的全局地址空间映射在性能上不再适用。L4Linux 的物理地址拷贝机制更加符合 µ-kernel 的架构特点，在模块化的设计下能够更有效地处理数据交换。 L4 的 copyin 和 copyout 机制通过在软件中手动解析虚拟地址，从而减少了硬件上下文切换的开销。虽然传统 Linux/x86 的方法在硬件上显得更简单，但它在 L4 的 µ-kernel 架构下表现不佳，因为上下文切换的硬件开销过大。而 L4 通过物理地址拷贝的方式能够更好地适应这种高度模块化的微内核架构，因此效率更高。\n用户态和内核态的转换\nL4 在 copyin 和 copyout 时：L4 的 copyin 和 copyout 操作不需要进行传统的用户态和内核态的转换，因为这些操作通常是在 L4Linux 服务器进程中完成的。L4 采用的是用户态服务（比如 L4Linux 作为一个单服务器进程运行在用户态），而 µ-kernel 本身并不直接参与大量的用户态和内核态转换。\n什么时候 L4 需要用户态和内核态转换：\n在 L4 中，用户态和内核态的转换主要发生在 L4 微内核提供的基本系统服务中，例如进程调度、消息传递等。L4 作为一个微内核，其职责是提供最低限度的核心功能，而 Linux 服务器和其他用户态服务都运行在用户态。 L4 仅在处理用户进程请求 µ-kernel 服务时，或者需要执行内核态任务时，才需要执行用户态和内核态的转换。相比传统 Linux 内核，L4 这种结构减少了内核态和用户态切换的次数，从而提升性能。 页表和 TLB（Translation Lookaside Buffer）\nL4 的页表：在 L4Linux 的环境下，L4 微内核依然依赖页表机制进行地址翻译，但它的设计更加轻量化，可能不会直接使用传统的硬件页表机制。对于 copyin 和 copyout 操作，L4Linux 使用了 逻辑页表 来手动管理虚拟地址到物理地址的映射，而不依赖硬件来进行地址转换。\nTLB 和硬件地址转换：L4 微内核和 L4Linux 并没有完全放弃 TLB 和硬件地址转换功能，特别是在处理用户态进程的时候。页表和 TLB 仍然是操作系统的一部分，但由于 L4Linux 作为一个用户态服务运行在 µ-kernel 上，L4Linux 服务器在与 µ-kernel 交互时可能不需要频繁依赖硬件的 TLB 机制。在某些情况下，L4Linux 通过软件的逻辑页表完成了大部分的地址转换，这减少了对硬件 TLB 的依赖。\nL4Linux 是否完全不需要硬件页表和 TLB？\n并不是完全不需要：L4Linux 仍然需要 TLB 和硬件页表转换来支持 用户态进程的地址空间管理，特别是涉及到进程的虚拟内存管理时。L4 的轻量级设计和 L4Linux 的逻辑页表只是减少了对硬件地址转换的依赖，并不是完全不使用这些功能。\n当 L4Linux 服务器需要处理大量的进程时，TLB 和硬件地址翻译依然是不可或缺的，特别是在提高多进程或多线程系统的性能方面。\n总结\nL4 不依赖用户态和内核态切换进行 copyin 和 copyout，这些操作通常发生在用户态的 L4Linux 服务器中。\nL4 仅在处理核心服务时需要用户态和内核态转换，而大部分操作在用户态下完成。\n逻辑页表用于 L4Linux 服务器中的地址映射操作，这减少了对硬件 TLB 和页表机制的依赖，但 L4 及其用户态进程仍然需要硬件支持的页表和 TLB 进行地址翻译。\nTLB（Translation Lookaside Buffer）是一个缓存，专门用来加速虚拟地址到物理地址的转换。它并不是直接存储整个页表，而是存储一部分最近使用的虚拟地址到物理地址的映射。现在来详细解释为什么更换页表的时候需要flush TLB。\nTLB 与页表的关系 TLB 并不包含完整的页表，它只是对一部分页表内容的缓存。每次 CPU 需要进行地址转换（将虚拟地址转换为物理地址）时，首先会在 TLB 中查找。如果找到了虚拟地址的映射，这就是“TLB 命中”，CPU 直接使用 TLB 中的映射来访问物理内存，速度很快。如果没有命中，则会发生“TLB 未命中”，这时 CPU 就需要去页表中查找相应的虚拟地址映射并加载到 TLB 中，这个过程相对较慢。\n为什么更换页表需要 flush TLB 当操作系统切换进程或者更改页表时，意味着虚拟地址到物理地址的映射发生了改变。此时，TLB 里面缓存的映射可能已经过时或不再有效。如果我们继续使用这些过期的映射，CPU 可能会把错误的物理地址当作有效的进行访问，导致数据访问错误甚至系统崩溃。\n因此，每当页表发生变化时，需要将 TLB 中旧的条目清除掉，这个操作称为“flush TLB”。这确保了后续的地址转换操作不会使用到已经无效的虚拟地址到物理地址的映射。\nflush TLB 的具体作用 flush TLB 的目的并不是把页表的内容“装载”进 TLB，相反，它是为了清除 TLB 中的无效条目。更换页表之后，TLB 重新开始缓存新的页表条目。在新的地址映射被频繁访问时，它们会逐渐被加载进 TLB，但 TLB 并不会一下子缓存整个页表。\n总结 TLB 是页表的一种缓存，它只保存一部分页表条目。 当页表发生变化时，TLB 中的缓存可能会失效，继续使用这些失效的映射会导致错误的地址转换。 flush TLB 是为了清除旧的映射，保证 CPU 在新的映射生效时不会使用错误的地址映射。 所以，更换页表的时候，flush TLB 不是把页表条目装载进 TLB，而是为了防止使用无效的映射，确保地址转换的正确性。\nEmulation Library 在 L4Linux 的上下文中，“emulation library” 通常指的是一个用于支持系统调用或信号处理的库。它模拟了传统 Linux 系统调用或行为，以便在 L4 这种不同内核架构下运行标准的 Linux 应用程序。它可以让 Linux 应用程序以为它们在一个完整的 Linux 环境中运行，虽然底层实际上是 L4 微内核。\n修改 Signal Thread 和 Emulation Library 只会影响自身线程 在 L4Linux 中，每个 Linux 用户进程都会拥有一个单独的 Signal Thread（信号处理线程）和相关的 Emulation Library。这些组件只与该用户进程共享同一个地址空间，因此它们彼此间的修改不会对其他进程造成影响。具体原因如下：\n地址空间隔离：在 L4 中，信号线程和 emulation library 仅在与主线程共享的地址空间中运行。任何对这些线程或库的修改都不会超出这个特定的地址空间。因此，修改它们可能会影响这个进程的正常运行，但不会影响其他进程或系统的整体稳定性。 进程的自我损害：由于这些组件都在用户进程自己的地址空间内，只有该进程本身能够访问并修改它们。这样，即使进程修改了这些组件，只会影响到它自己，不会对其他进程造成干扰。 主线程进入 Linux Server 线程 这里的“主线程进入 Linux Server 线程”的意思并不是指主线程被暂停，然后由 Linux Server 线程取代它的执行。实际上，它指的是 主线程进入了 Linux 的处理模式，即从用户态切换到内核态，准备响应信号并进入 L4Linux 系统服务的处理流程。具体过程如下：\n信号到达：Linux Server 发送信号给信号处理线程（Signal Thread）。 信号处理线程修改主线程的状态：信号线程通过调整主线程的堆栈指针和指令指针，使其进入 Linux 内核态。这一动作相当于让主线程进入内核的信号处理逻辑。 主线程执行信号处理：主线程从修改后的堆栈和指令指针开始，进入 Linux 的信号处理流程，这个过程被称为“进入 Linux”，是从用户态到内核态的一个过渡，而不是完全暂停或切换成其他线程。 完成信号处理后恢复执行：主线程在完成信号处理后恢复正常执行（即用户态执行）。 因此，“进入” 指的是主线程进入了一个内核态的上下文处理过程，而不是实际进入 Linux Server 线程。Linux Server 本身还是在独立的线程中运行的，主线程只是通过被信号通知进入了一个特定的信号处理状态。\n“进入 Linux”是否指的是单个线程对应的 Linux server 状态的改变，实际上 L4 内核并没有真正改变用户态和内核态的概念。你的理解基本是对的。以下是对这种机制的进一步解释：\nL4 微内核的用户态和内核态\n在传统的 Linux 系统中，用户态（user mode）和内核态（kernel mode）是操作系统的两个运行模式。在用户态，程序的权限有限，不能直接访问硬件或内核资源；而在内核态，操作系统具有完全的控制权，可以执行特权指令和访问所有系统资源。\n但在 L4 微内核 架构中，内核（µ-kernel）的角色非常小，它只处理非常底层的任务，如进程调度、消息传递和内存管理等。L4 的设计减少了用户态与内核态之间的频繁切换，因为大多数操作系统的功能（例如文件系统、驱动程序等）是在用户态的服务中完成的（例如 L4Linux 作为用户态服务运行）。因此，L4 系统中内核态和用户态的区分相对弱化了很多，只有在极少数情况下才需要用户态和内核态的切换。\n\u0026ldquo;进入 Linux\u0026rdquo; 的过程\n在 L4Linux 中，当一个用户进程需要进入内核态来执行特权操作（例如处理信号），它实际上是从 L4Linux 服务器接收处理指令。在这个过程中：\nL4 内核本身不会进行用户态和内核态的切换。L4 内核保持运行在自己的模式中，只负责处理消息传递和基本的调度等操作。 \u0026ldquo;进入 Linux\u0026rdquo; 意味着用户进程（比如主线程）开始进入 L4Linux 服务器进程内部的信号处理逻辑（或者其他内核态相关的操作），这个操作由 L4Linux 服务器来管理，而不是 L4 微内核本身。 简而言之：\n在传统 Linux 中，\u0026ldquo;进入内核态\u0026rdquo; 是一个涉及特权级别变化的硬件操作。 而在 L4Linux 上，这更多是一个用户态服务逻辑的转换，意味着主线程开始处理本应该在传统 Linux 内核态执行的功能，但 L4 内核本身并不会进行实际的用户态到内核态的切换。 结论：“进程进入 Linux”确实主要是指 L4Linux 服务器进程内部状态的改变，而 L4 微内核本身并不改变用户态和内核态的划分。L4 系统的设计本质上就是通过减少这种切换来提升性能和简化操作系统的架构。\n两个执行路径 在L4Linux环境中，主线程会有一张页表，这张页表主要用于管理用户进程的地址空间，并映射该进程可以访问的虚拟地址到实际的物理地址。这意味着：\n主线程可以在自己的地址空间中执行它有权限访问的用户态代码。这个用户态代码是直接通过页表进行映射的，主线程可以直接在它的上下文中运行这些代码。 但是，这不包括Linux内核级别的代码，因为在L4Linux架构下，Linux本身被封装为一个用户态进程运行在L4微内核之上。也就是说，L4Linux服务器（即Linux内核）不会直接通过页表暴露给每个用户进程。主线程仅可以访问其自身的用户态代码，而非Linux内核的代码。 因此，主线程的页表不会直接映射到Linux服务器的代码空间中，它主要用于运行用户态代码，而无法直接执行与内核态相关的代码。\n在L4Linux架构中，主线程要执行需要Linux内核权限的代码（例如系统调用、信号处理等）时，通常会通过 IPC（进程间通信） 与 L4Linux服务器 进行交互。这是L4系统中非常重要的机制：\n当主线程需要进行系统调用或处理信号时，它会将这些请求封装成 IPC消息，并发送给L4Linux服务器（Linux内核进程）。 L4Linux服务器 接收到这些消息后，会在它自己的上下文中处理这些请求（即执行Linux内核中的相应代码）。 处理完成后，L4Linux服务器会通过IPC回复主线程，从而让主线程继续执行。 这意味着，主线程并不会直接执行Linux服务器的代码，而是通过IPC来请求Linux服务器的服务。L4微内核通过这种方式实现了用户态与L4Linux服务器（内核态）之间的隔离，减少了用户态与内核态之间的频繁切换，提升了系统的性能和安全性。\n总结：\n主线程通过页表直接执行的代码 仅限于用户态代码，无法直接执行L4Linux服务器的内核态代码。 主线程与L4Linux服务器的交互主要依赖IPC，当主线程需要系统级操作时，它通过发送消息给L4Linux服务器，由服务器代为执行内核级的代码。 这种架构设计使得L4Linux能够高效地在L4微内核上运行，同时确保了系统的模块化和隔离性。\nCoroutine（协程） 协程（Coroutine）是一种更轻量的并发执行单元，它与线程类似，但有一些关键的不同。协程通常运行在用户空间，由程序手动调度，而线程则通常由操作系统调度。\n协程的特点：\n手动切换：协程是通过程序中的某些机制显式切换的，而不是像线程那样由操作系统隐式调度。当一个协程执行到一定位置时，它可以主动让出控制权，并且保存执行状态，以便稍后恢复执行。\n非抢占式调度：协程不像线程或进程那样抢占系统资源，它们可以在任务之间主动切换，而不会**作系统强制中断。这减少了上下文切换的开销。\n协程 vs 线程 vs 进程\n进程 线程 协程 进程是操作系统分配资源和独立运行的最基本单元。每个进程拥有独立的内存空间，并且进程之间不能直接共享内存。 线程是进程内部的执行单元，一个进程可以包含多个线程。线程共享进程的内存空间，但每个线程都有自己独立的执行栈。 协程比线程更轻量。它们在同一线程的上下文中运行，并且由程序来调度而不是操作系统。它们共享同一个线程的栈帧，切换时只需保存和恢复少量的状态。 进程的上下文切换通常比较昂贵，因为涉及到虚拟内存空间的切换。 线程的上下文切换比进程轻量一些，但仍然涉及到保存和恢复CPU寄存器、堆栈等信息。 协程的上下文切换开销非常低，因为它不涉及操作系统的调度机制。 协程的例子\n异步编程：现代编程语言（如 Python 中的 async/await）常常使用协程来实现异步编程。协程允许程序在等待 I/O 操作（如网络请求、文件读写）时不阻塞线程，而是切换到其他任务继续执行。例子:\n1 2 3 async def fetch_data(): await some_io_operation() # 模拟 I/O 操作，协程让出控制权，等待操作完成 print(\u0026#34;Data fetched\u0026#34;) 游戏开发中的协程：在游戏开发中，协程常常用于处理时间敏感的任务。例如，Unity 引擎中的协程可以用于控制动画的播放或延迟事件的执行。例子：\n1 2 3 4 IEnumerator WaitAndPrint() { yield return new WaitForSeconds(2); // 延迟2秒 print(\u0026#34;2 seconds later\u0026#34;); } 总结：协程是一种轻量级的并发机制，允许程序在不依赖操作系统的调度的情况下管理多任务执行，减少了上下文切换的开销。相比线程和进程，协程更加高效，尤其在I/O密集型和异步任务处理中表现良好。\nL4Linux 的调度策略 支持“将 schedule() 函数的调用延迟到进程的时间片耗尽”的机制，主要在于 L4Linux 的调度策略，以及 µ-kernel 的轻量级结构和协程机制。以下是原因和实现方式：\n延迟 schedule() 调用的机制\n减少协程切换：L4Linux 中每个系统调用完成后，如果没有需要立即处理的内核任务或其他协程的切换请求，系统并不会立即调用 schedule() 来切换到另一个任务，而是让当前用户线程继续执行。这种做法的好处是减少了不必要的调度器调用，从而最小化了协程的上下文切换，减少了开销。\n基于时间片调度：系统设置了一个固定的时间片，每个线程在时间片到期之前可以连续执行任务。L4Linux 延迟调用 schedule()，意味着只在当前用户线程的时间片耗尽后，才会强制进行调度。这种调度策略与传统 Linux 类似，减少了不必要的调度器调用，提升了性能。\n支持用户进程在时间片内进行多次系统调用\n系统调用的执行不导致阻塞：L4Linux 的设计允许用户进程在其时间片内可以进行多个系统调用，而不必等待内核中的其他任务或协程切换。这是因为在时间片内系统不会立即调用 schedule() 切换到其他任务，而是尽可能让当前进程继续运行。除非有更高优先级的任务请求处理，比如中断，否则会尽量避免打断当前进程的执行。\n避免频繁上下文切换：系统调用完成后，L4Linux 并不会立即切换到其他任务，而是恢复用户进程的执行状态。这种方式避免了传统系统中每次系统调用可能触发上下文切换的情况，用户进程可以连续执行多个系统调用，直到时间片用尽才进行调度切换。这个特性减少了切换带来的性能损耗，提高了系统调用密集型任务的执行效率。\n具体执行方式：\n在系统调用完成后，如果没有内核中断或其他需要立即处理的任务，L4Linux 服务器会检查 reschedule 标志。若该标志没有被设置，系统不会立即调用 schedule()，而是恢复用户线程的执行。这种行为让用户进程在当前时间片内尽可能长地持续执行，且允许多个系统调用连续完成。 这种方式允许系统在一个时间片内完成多个系统调用，而不会在每次调用后立即进入调度切换。这提高了系统的整体吞吐量，因为多个系统调用可以在单个时间片内无缝地执行而不被中断。 总结：通过延迟 schedule() 调用至时间片耗尽，L4Linux 可以减少协程切换的频率，从而提高系统调用密集型任务的执行效率。这种方法在用户进程时间片内允许多个系统调用连续执行而不被调度器打断，从而避免了频繁的上下文切换，达到了提升性能的效果。\n系统调用完成后延迟 schedule() 的这种方法在现代操作系统中看似是一种常见的优化，但实际上，它的出色之处在于减少不必要的调度器调用，特别是在处理多任务并发时可以有效提升性能。让我们仔细看一下这个方法的背景以及为什么它能够带来改进。\n传统系统中的调度行为\n在传统的操作系统（如早期版本的 Linux）中，系统调用完成后，调度器可能会立即进行上下文切换，这取决于当时的系统状态和调度策略。例如：\n每次系统调用完成后，内核会评估是否有其他任务需要更高优先级的处理。如果有，调度器会立即切换到另一个进程或线程。 这种方式可能会导致频繁的上下文切换，尤其是在系统调用密集的场景下。上下文切换虽然使系统可以多任务运行，但过于频繁的切换会带来较高的开销（如保存和恢复寄存器状态、页表等），降低整体性能。 现代操作系统的优化策略\n现代操作系统中，特别是在高性能系统设计中，通过延迟 schedule() 调用，只在时间片用尽时进行调度，这是为了避免每次系统调用后立即进行上下文切换的开销。这个优化之处包括：\n减少上下文切换：传统方法可能会在每个系统调用后立即触发调度，而现代系统通过延迟 schedule() 调用，允许用户进程在其时间片内连续执行多个系统调用，而不会被过早中断。这减少了每个系统调用之后的上下文切换次数，进而减少了开销。 提高系统吞吐量：在 I/O 密集型任务或需要频繁系统调用的场景下，延迟调度能够使用户进程在单个时间片内完成更多工作（如多个系统调用）。这提高了系统的整体吞吐量，因为 CPU 花费在上下文切换上的时间被最小化，更多时间被用于实际工作。 L4Linux 的创新\nL4Linux 的这种方式虽然与现代操作系统相似，但它的设计针对微内核环境进行了优化：\n协程和调度结合：L4Linux 将系统调用后的调度延迟到时间片耗尽，结合了协程的轻量级特性，减少了频繁的上下文切换。由于协程相比传统线程切换开销更小，整个系统的调度开销进一步降低。 微内核架构的需求：在 L4 微内核上，许多操作（如系统调用）通过 IPC 完成，L4Linux 通过延迟调度器调用可以减少跨用户空间和内核空间的频繁切换，适合 µ-kernel 的高性能需求。 早期操作系统的情况\n在一些早期的操作系统或某些特定调度策略下，系统调用后立即触发调度确实可能发生，这在多任务环境下会引发频繁的上下文切换，导致性能瓶颈。例如：\n如果有大量进程同时发出系统调用，每个系统调用完成后，操作系统可能会认为需要重新调度来确保公平性，导致频繁的切换。 尤其在早期的 Linux 版本中，调度器的调度策略没有进行足够的优化，会导致这种问题。 总结：L4Linux 中延迟 schedule() 调用的策略虽然看起来像是现代操作系统的一种标准优化，但它的出色之处在于适应了 L4 微内核的架构，特别是减少了协程切换和上下文切换的频率，提高了系统的整体效率。在某些早期的操作系统中，系统调用后立即调度的方式曾是常见的，但这种方式的高开销在现代系统中逐渐被优化设计所取代。\n在所有地址空间中代码和数据的虚拟分配相同会最大化独立应用程序之间的 TLB 冲突 TLB 是一种高速缓存，用来加速虚拟地址到物理地址的转换。每次内存访问时，处理器首先查找 TLB，如果找到匹配的虚拟地址条目，就可以快速获取对应的物理地址，避免慢速的页表查找。\n在操作系统中，不同应用程序通常有各自的虚拟地址空间。但是，如果所有应用程序的代码和数据在虚拟地址空间中的分配位置相同（也就是说，所有程序的代码和数据都映射到相同的虚拟地址区间），会导致这些独立应用程序在 TLB 中的条目彼此冲突。\n假设我们有两个独立的应用程序 A 和 B：\n应用 A 的代码和数据被映射到虚拟地址 0x1000-0x2000。 应用 B 的代码和数据也被映射到相同的虚拟地址 0x1000-0x2000。 这时，两个应用程序在运行时，虽然它们的物理地址是不同的，但因为它们的虚拟地址相同，TLB 在缓存这些地址映射时只会保留最后一次的映射。当应用 A 在运行时，TLB 缓存了 A 的虚拟地址到物理地址的映射。然而，当应用 B 开始运行时，它的虚拟地址 0x1000 也会被写入 TLB，覆盖了 A 的映射。随后，当 A 再次运行时，发现 TLB 中的映射已经被 B 覆盖，就会导致TLB 未命中，系统不得不重新查找页表来更新 TLB。\n这种频繁的 TLB 未命中会导致性能下降，因为每次发生这种情况时，处理器都要执行耗时的页表查找。\n当所有应用程序的代码和数据在虚拟地址空间中的分配完全相同时，每个应用程序在 TLB 中的映射都会覆盖前一个应用程序的映射。由于 TLB 条目有限，不同应用程序之间频繁切换时，会频繁刷新和重新加载 TLB，从而最大化冲突。这种现象在同时运行多个进程或频繁上下文切换的场景中尤为显著，特别是在时间片较短或上下文切换频繁的情况下。\n为了解决这个问题，可以通过为不同应用程序构建不同的虚拟地址布局，避免所有应用程序使用相同的虚拟地址。例如，L4Linux 允许仿真库和信号线程被映射到应用程序附近的低地址空间，而不是总是使用高地址空间。通过这样的方法，可以减少不同进程之间的 TLB 冲突，提升性能。\n执行时间比较大的过程的排名 在描述的过程中，涉及多个系统操作的开销。根据常见情况和上述分析，以下是这些操作按通常的时间花费从大到小的大致排名：\nTLB 未命中导致的页表查找\n花费原因：每次 TLB 未命中（TLB miss），处理器都必须进行页表查找，这涉及内存访问，并且需要多级页表的遍历。这一过程相对耗时，特别是在频繁上下文切换或不同应用程序共享虚拟地址布局时，频繁发生未命中会带来显著开销。\n相对开销：时间消耗较高。\n频繁的上下文切换\n花费原因：每次上下文切换，系统都需要保存当前进程的寄存器状态，并恢复目标进程的状态。这一过程涉及 CPU 寄存器状态和内存管理数据的操作。当上下文切换频繁且时间片较短时，累积的时间开销较大。\n相对开销：高，但略低于 TLB 未命中。\nTLB 刷新（TLB Flush）\n花费原因：TLB 刷新会清除当前 TLB 的所有条目，导致后续访问都可能需要重新建立 TLB 工作集，带来额外的页表查找和 TLB 加载。与 TLB 未命中相似，这种操作的开销取决于 TLB 的大小和具体的页表配置。\n相对开销：通常低于频繁的 TLB 未命中，但在切换回原地址空间时依然显著。\n协程切换\n花费原因：协程切换涉及的状态信息较少，因为它们通常是在用户空间操作，且不涉及到系统级寄存器和内存管理数据的切换。协程切换一般较轻量，但如果切换过于频繁，仍会带来一定开销。\n相对开销：中等偏低。比线程切换低，但如果协程频繁上下文切换，开销会增加。\n系统调用开销\n花费原因：系统调用从用户态切换到内核态，会有一定的开销。系统调用的开销通常与其具体操作内容相关。例如 I/O 操作比普通的内存操作要耗时。\n相对开销：一般较小，但系统调用密集时，累计的开销会显著。\n与 Linux 服务器的 IPC 消息传递\n花费原因：在 L4Linux 中，许多内核操作是通过 IPC 传递给 L4Linux 服务器。虽然 IPC 是轻量级的，尤其是在 µ-kernel 系统中被优化，但如果过于频繁，仍然会增加时间开销。\n相对开销：低，但高频调用情况下也可能累积显著时间。\n综上，TLB 未命中和频繁的上下文切换通常是系统开销较大的过程，特别是在多任务环境下。相对而言，协程切换和系统调用在 µ-kernel 系统下的开销较低，但仍需视具体情况和调用频率而定。\nL4用户空间和内核空间 用户空间（User Space）：\n包含内容： 用户空间主要用于用户态程序的执行和数据存储。每个进程都在用户空间中运行，并且它们相互隔离。 包含应用程序代码、用户程序的数据、堆（heap）、栈（stack）、共享库等。 用户空间的大小取决于操作系统设计和硬件架构。通常在 x86 系统上，用户空间分配 2GB 或 3GB。 访问权限： 用户空间的进程只能访问自己所属的内存地址，不能直接访问内核空间的数据或代码。这是为了安全性和稳定性，防止用户进程意外或恶意修改系统关键数据。 内核空间（Kernel Space）：\n包含内容： 内核空间是操作系统内核运行的区域，负责系统级的操作，比如硬件管理、进程调度、内存管理、网络通信等。 包含内核代码、内核数据结构（比如页表）、设备驱动、系统调用接口、内核栈等。 在 x86 架构下，通常为内核空间分配 1GB 或 2GB 的地址空间。 访问权限： 只有运行在内核态（例如内核或驱动程序）的代码才能访问内核空间。用户态的进程如果想要与内核交互，必须通过系统调用接口，而不能直接操作内核空间。 用户空间与内核空间的关系：\n用户空间和内核空间是通过虚拟内存管理体系联系在一起的。每个进程都有自己的虚拟地址空间，并且通过内核管理的页表将这些虚拟地址转换为物理地址。 页表是一个重要的数据结构，它为每个进程维护虚拟地址到物理地址的映射表。内核管理这些页表，通过它们实现用户态进程与内核态之间的内存隔离，同时允许在适当的时候（如系统调用时）进行安全的内存共享或数据交换。 页表的作用： 页表不仅帮助用户进程管理自己的虚拟内存，还帮助系统在需要时切换到内核空间。例如，用户进程发起系统调用时，系统通过页表和内存管理单元（MMU）在用户空间和内核空间之间进行地址转换，使得进程可以通过内核接口访问硬件资源或系统服务。 页表的存在确保了用户空间和内核空间在物理上是分离的，但在逻辑上它们通过页表联合起来，使得进程在需要时可以受控地与内核交互。 Linux 内核和 L4 微内核的空间区别 原生 Linux 内核的地址空间管理\n单一内核空间和用户空间：在原生 Linux 中，内核直接运行在系统的最高权限级别上（通常叫做 ring 0）。它能够直接管理整个 4GB 的虚拟地址空间（对于 32 位系统），并在地址空间中分配了 用户空间（通常为 3GB） 和 内核空间（通常为 1GB）。\n内核访问用户空间的便利性：Linux 的这种结构使得内核能够直接访问用户空间，因为这两个部分共享一个统一的地址空间。在同一个地址空间中，Linux 内核可以通过硬件支持的地址转换机制直接访问用户数据，像是直接在用户空间和内核空间之间拷贝数据。\nMMU 和 TLB 的作用：Linux 利用内存管理单元（MMU）和转换后备缓冲区（TLB）来进行地址翻译和管理，允许内核在访问用户空间时借助硬件来处理大部分的内存管理，减少了开发人员在内存管理方面的复杂性。\nL4 微内核的设计特点\nL4 微内核架构：L4 是一个微内核，设计理念是将操作系统中许多复杂的功能从内核中移除，转而将这些功能放在用户态进程中处理。因此，L4 微内核只处理一些非常基础的功能，比如进程调度、地址空间管理、消息传递等，而像 Linux 这样的操作系统内核（即 L4Linux）则运行在用户态之上。\nL4Linux 的限制：L4Linux 并不直接运行在最高权限级别上，而是作为 L4 微内核的一个用户态服务器。因此：\n地址空间隔离：在 L4 中，每个进程有自己的独立地址空间，L4Linux 作为一个用户态服务器，也有自己的地址空间。这使得它与 Linux 用户进程之间的内存并不共享，无法像原生 Linux 那样轻松地访问用户空间。 无法直接访问用户空间：由于 L4Linux 运行在用户态，它无法直接访问其他用户进程的内存。因此，L4Linux 试图模仿原生 Linux 的内存管理方式，即将用户空间映射到自己的地址空间中，这样的设计在 L4 上会因为地址空间限制和切换开销过大而变得不现实。\n技术限制和性能问题\n内存映射的限制：在原生 Linux 中，内核和用户空间共享一个地址空间，这使得内核能够轻松访问每个用户进程的内存。然而，在 L4 的设计中，由于每个用户进程和 L4Linux 服务器都有独立的地址空间，这样导致无法将多个用户进程的内存同时映射到 L4Linux 的地址空间中。L4Linux 服务器的地址空间只有 3GB，而多个用户进程的内存加起来显然会超出这个范围。\nTLB 刷新和性能开销：L4 中的每个地址空间都是独立的，L4Linux 服务器与用户进程的切换会涉及到 TLB（转换后备缓冲区）的刷新。这意味着每次上下文切换（如系统调用）时，整个 TLB 工作集都需要重新加载，导致性能损失严重。而原生 Linux 内核不需要频繁进行这种刷新，因此访问用户空间更加高效。\n为什么原生 Linux 能做到，而 L4Linux 做不到\n特权级和内存访问权限：原生 Linux 内核运行在 ring 0（最高权限级别），可以控制整个虚拟地址空间，因此可以访问用户进程的内存。而 L4Linux 是一个运行在用户态（较低权限级别）的服务器，不能直接操控系统的内存管理单元（MMU），所以无法像原生 Linux 那样自由访问用户进程的地址空间。\n架构设计目标不同\n原生 Linux 强调的是性能和集成，它将用户空间和内核空间放在一个统一的地址空间中，从而允许快速的数据交换。 L4 微内核 强调的是模块化和隔离性，设计目的是将复杂性降低到内核之外，实现更小的信任计算基础（TCB），这也导致了 L4Linux 在地址空间管理上受到了一定的限制。 举例来描述原生 Linux 和 L4 微内核在虚拟地址空间管理方面的区别，并解释为什么 L4Linux 的地址空间管理会遇到困难。\n原生 Linux 内核中的虚拟地址空间管理\n假设我们有一个应用程序（比如 myapp），它正在运行在 Linux 操作系统上。Linux 会给 myapp 分配一个 虚拟地址空间。在 32 位系统上，这个虚拟地址空间通常是 4GB。\n地址空间分布： 3GB 给用户空间（通常为地址 0x00000000 到 0xbfffffff）。 1GB 给内核空间（通常为地址 0xc0000000 到 0xffffffff）。 共享地址空间： 在原生 Linux 中，每个进程都有它自己的用户空间部分，但所有进程的内核空间部分是相同的。也就是说，当某个进程运行系统调用进入内核时，它可以访问内核部分的地址空间，而内核可以看到该进程的用户地址空间。 例如：如果 myapp 调用了一个系统调用（比如 read()），内核可以直接访问 myapp 的用户空间中数据，通过内存拷贝或者硬件支持的地址翻译，将数据从用户空间直接传输到内核空间，反之亦然。因为它们共享同一个统一的 4GB 地址空间，内核可以直接通过指针引用访问这些数据。 TLB 与地址空间切换： 在 Linux 中，虚拟地址到物理地址的转换由 TLB（Translation Lookaside Buffer） 缓存来加速。 TLB 缓存虚拟地址到物理地址的映射，因此每次访问相同的内存地址时不必重复查找页表。 内核和用户空间共享 TLB，因此当系统在用户空间和内核空间之间切换时，不需要大规模刷新 TLB，保证了访问速度。 关键点：为什么这样有效\n内核直接访问用户空间数据：由于用户空间和内核空间共享一个虚拟地址空间，Linux 内核在进行用户数据的操作时，可以直接使用硬件进行地址转换。 TLB 刷新减少：切换用户空间和内核空间时不需要频繁刷新 TLB，从而减少了性能开销。 L4 微内核中的虚拟地址空间管理\nL4 微内核采取了一种 模块化与隔离 的架构，目的是为了提高安全性和稳定性。\n假设我们有同样的应用程序 myapp，但这次它运行在 L4 微内核上的 L4Linux 中。\nL4 微内核的架构： L4 微内核本身只处理基础功能，如进程调度和通信。Linux 运行于 L4Linux 服务器中，而 L4Linux 服务器本身是一个 用户态进程，受到和普通应用程序类似的权限限制。 L4 微内核中的每个用户进程（包括 L4Linux 和应用程序）都有自己独立的地址空间。这意味着，myapp 和 L4Linux 服务器并不共享同一个地址空间。 地址空间的隔离： 在 L4 微内核上，每个进程，包括 L4Linux 服务器，都有自己的独立地址空间，例如： L4Linux 服务器：有自己的 3GB 用户态地址空间。 应用程序 myapp：也有自己的独立 3GB 用户态地址空间。 因为 L4Linux 运行在用户态中，它的权限不允许它直接访问其他应用程序的地址空间（例如 myapp 的空间）。 用户空间和内核空间的访问： L4Linux 的目标是模拟原生 Linux 的功能，但它没有内核态的权限。因此，myapp 和 L4Linux 必须通过 IPC（进程间通信） 来传递数据。这与原生 Linux 内核中内核直接访问用户空间的数据方式不同，增加了数据拷贝和通信的开销。 L4Linux 试图实现类似原生 Linux 的共享用户空间和内核空间的方法，但由于每个地址空间是独立的，它无法直接访问应用程序的数据，必须通过复杂的映射或者复制方式，这就引发了管理上的问题。 TLB 切换与性能问题：\nTLB 的问题：在 L4 微内核上，由于 L4Linux 和应用程序地址空间是独立的，每次 L4Linux 服务器调用（例如在处理系统调用时），系统都需要切换地址空间，这会导致TLB 必须完全刷新\n每次地址空间切换后，原先缓存的虚拟地址到物理地址的映射就无效了，因此必须重新建立，这显著降低了性能。 例如，当 myapp 发起一个系统调用进入 L4Linux 时，由于它们不共享地址空间，TLB 中的映射无效，需要重新加载，增加了大量的性能开销。\n关键点：为什么不如原生 Linux 高效\n独立地址空间的隔离性：L4 微内核的设计初衷是提高安全性和模块化，保持进程间的强隔离性，但这导致 L4Linux 服务器不能像原生 Linux 那样自由地直接访问用户进程的数据。 TLB 刷新频繁：L4Linux 和用户进程之间的频繁切换导致 TLB 被频繁刷新，系统不得不在每次系统调用或进程切换时重新加载 TLB，性能因此严重受损。 总结与对比\n原生 Linux： 共享地址空间：内核和用户空间共享同一个虚拟地址空间，内核直接访问用户空间的内存。 TLB 切换少：因为内核和用户空间共享，切换时无需刷新 TLB，访问非常高效。 L4 微内核： 地址空间隔离：L4Linux 和用户应用程序处于独立的地址空间中，为了安全和稳定，但导致内存管理复杂。 TLB 刷新频繁：每次 L4Linux 和用户进程切换时需要刷新 TLB，影响性能。 IPC 增加开销：因为 L4Linux 没有内核态权限，数据传递需要通过 IPC，进一步增加了数据拷贝的开销。 因此，在原生 Linux 中，由于内核和用户空间共享同一个地址空间，访问更快且管理简单。而在 L4 微内核架构下，L4Linux 运行于用户态、进程间隔离严格，导致无法像原生 Linux 那样自由高效地管理和共享地址空间，这就是为什么在 L4 上无法简单复用 Linux 的虚拟地址管理方式的原因。\nL4中的共享库 L4中的共享库主要通过动态链接的方式，将共享的代码库（如标准C库 libc.so）与应用程序进程关联在一起，促进跨进程调用的优化和性能提升。以下是L4环境下共享库的工作流程和每个环节所需的时间：\n库的初始化与加载 每个应用进程都会从Linux服务器请求将共享库加载到高地址区域。这样，应用进程能够共享这些库的实例，减少内存占用。 初始化通常通过一个页错误（page fault）请求向Linux服务器进行，服务器会映射适当的库页到应用程序的地址空间。 系统调用的重定向 为实现系统调用，L4使用了远程过程调用（Remote Procedure Calls，RPC）机制，即通过L4的进程间通信（IPC）来完成用户进程与Linux服务器之间的系统调用。 共有三种系统调用接口可用： 修改版的共享库（如 libc.so），直接调用L4的IPC原语与Linux服务器通信。这种方式速度较快。 修改后的静态库（如 libc.a），通常用于少量非动态链接的程序。 用户级异常处理机制（\u0026ldquo;trampoline\u0026rdquo;），模拟本地系统调用。这种方式提供了较好的二进制兼容性，但性能较低。 信号处理 信号的传递在L4中通过一个额外的信号线程实现。这个线程接收来自Linux服务器的消息，通知应用进程进入信号处理流程。 为保障系统的隔离性，L4限制了不同地址空间线程间的直接操作，因此信号处理只能在同一地址空间中实现。 数据交换与地址映射 服务器内部管理虚拟地址与物理地址的映射，每当有数据交换需求时（如 copyin/copyout 操作），服务器解析逻辑页表，将虚拟地址转换为实际的物理地址，并执行数据复制操作。 各部分所需时间\n系统调用延迟：与共享库 libc.so 相关的系统调用大约需要3.95微秒，使用trampoline机制的调用则需要5.66微秒的调用则需要5.66微秒。 信号传递延迟：通常依赖L4 IPC机制完成，虽然具体时间未明确指出，但其效率受限于系统的优先级调度策略和信号线程的调度频率。 数据复制延迟：使用物理地址解析的 copyin/copyout 操作在L4环境下因减少了TLB刷新，效率约为原本的两倍B刷新，效率约为原本的两倍。 共享库 libc.so与trampoline机制的差异 共享库 libc.so 与 trampoline 机制的调用存在时间差异，尽管它们都使用了 IPC 通信，这是因为两者在实现上的差别主要体现在调用路径的复杂性和处理系统调用的方式上。\n共享库 libc.so 的调用路径\n在 L4 系统中，使用 libc.so 的共享库可以更直接地通过修改后的系统调用接口与 Linux 服务器进行通信。这个方式的调用路径较短，因为共享库中的系统调用直接使用 L4 的 IPC 原语，与服务器进行高效的进程间通信。整个过程基本是直接发起 IPC 请求，然后由服务器处理请求并返回结果。因此，这种机制可以更好地利用 L4 IPC 的高效性，减少了不必要的上下文切换和额外的指令处理，导致更快的调用时间。\n共享库调用所需的时间（如 getpid 系统调用大约需要 3.95 微秒）得益于这种更简化的通信路径和直接调用 IPC 原语的机制。\nTrampoline 机制的调用路径\n相比之下，trampoline 机制更复杂，它主要是为了提供二进制兼容性。trampoline 的实现方式是在用户空间模拟系统调用陷阱（trap），通过异常处理机制（exception handler）捕获系统调用，并在捕获到之后再将系统调用转发给 Linux 服务器。这意味着系统调用的处理需要先触发一次用户空间的异常处理，再进行 IPC 通信，然后再返回结果。这增加了一个额外的步骤（捕获和处理异常），也使得路径更长。\n正是由于多了异常捕获和处理这一步，trampoline 机制的调用比直接使用 libc.so 调用要慢。getpid 使用 trampoline 机制时大约需要 5.66 微秒，这比共享库调用多出约 43% 的时间。\n结论\n尽管两者都基于 IPC 通信，trampoline 机制由于引入了额外的异常处理步骤，导致其调用时间较长。而共享库 libc.so 的直接 IPC 调用路径较短，因此性能更高。在需要二进制兼容性时，trampoline 是必要的，但在性能上则不如直接修改后的共享库调用高效。\ntrampoline机制的二进制兼容性 二进制兼容性（binary compatibility）指的是一个操作系统或软件能够运行为其他环境编译的程序或二进制文件，而不需要重新编译。在L4系统中的 trampoline 机制，旨在提供与传统Linux系统调用的二进制兼容性。这意味着，即使程序是为普通Linux编译的二进制文件，也可以在L4上执行，而无需对这些程序进行修改或重新编译。\n虽然共享库 libc.so 和 trampoline 机制都涉及系统调用，并且都依赖于L4的IPC（进程间通信）机制，但二者之间的实现方式不同，导致共享库调用时不需要进行与trampoline机制相同的上下文切换。\nlibc.so 调用：在使用 libc.so 的情况下，系统调用是通过经过修改的库来直接与Linux服务器通信。共享库调用的过程中，尽管也需要进行从用户空间到内核空间的切换（因为系统调用本质上都是如此），但是，L4的IPC机制使得这类上下文切换更加高效。此外，由于共享库调用使用了较为直接的IPC原语，减少了不必要的额外步骤，从而减少了切换和处理时间。 trampoline机制：trampoline机制为了提供二进制兼容性，使用了一种模拟的系统调用陷阱。首先，它通过用户空间的异常处理机制捕捉系统调用，再将该系统调用转发到Linux服务器。这种方式增加了上下文切换的复杂性，除了从用户空间到内核空间的切换，还包括捕获和处理异常的额外步骤，这些都使得性能不如直接使用 libc.so 调用。 在简化的调用路径下，libc.so 调用的性能相对更好，因为它可以避免一些复杂的上下文切换和额外的处理步骤【5†source】。\n因此，虽然两者都涉及IPC通信，但共享库的调用更加高效，不需要像trampoline机制那样频繁的上下文切换和额外的异常处理步骤。\nIPC和RPC IPC RPC 作用 IPC用于在同一计算机上不同进程之间共享数据或发送消息。其主要目的是协调进程之间的操作，确保数据一致性和完整性。 RPC允许程序调用位于不同机器上的函数，就像调用本地函数一样。其主要目的是简化分布式系统中的通信，使程序员无需关心底层网络通信细节。 例子 在操作系统中，一个音频播放器（进程A）需要和音频解码器（进程B）通信。进程A可以使用共享内存（IPC的一种）来将数据传送给进程B，进而完成音频解码。 一个客户端应用需要从远程服务器获取用户信息。客户端使用RPC调用服务器上的函数，从而获取数据，而不必直接处理网络传输。 范围 IPC通常用于同一台计算机上的进程之间。 RPC用于不同计算机上的进程之间。 实现复杂性 IPC相对简单，只需在一个系统内管理。 RPC需要处理网络协议、数据序列化和反序列化等问题。 性能 IPC因数据传输距离短，通常具有较低的延迟。 RPC需要通过网络，可能有较高的延迟。 故障处理 IPC主要处理进程间的同步和数据一致性。 RPC还需考虑网络故障、节点不可用等问题。 相同点 通信目的：两者都用于进程之间的信息交换。 抽象层次：提供了一种更高层次的抽象，使得程序员无需直接操作底层通信机制。 总结：IPC和RPC帮助程序在进程间或跨网络协同工作，分别适用于本地和分布式系统环境，都提供了显著提高开发效率的抽象能力。\nblock IPC和RPC 阻塞IPC 阻塞RPC 进程挂起：当一个进程进行阻塞IPC调用时，该进程会挂起，直到通信完成。这会导致该进程在等待响应期间无法执行其他任务。 网络通信延迟：阻塞RPC涉及网络通信，任何网络延迟或故障都会使调用挂起的时间更长，影响系统响应速度。 系统资源利用率下降：如果多个进程处于阻塞状态而等待IPC完成，系统资源（如CPU、内存）利用率会下降，因为这些进程无法继续处理其他任务。 进程挂起：发起RPC调用的进程会挂起直到收到远程服务器的响应，导致本地进程无法继续进行其他操作。 延迟增加：阻塞通信会导致系统整体延迟增加，尤其是在需要快速响应的实时系统中，这可能会影响系统性能。 分布式系统复杂度增加：在分布式系统中，网络问题、服务器负载问题等都可能导致RPC调用的阻塞，从而影响整个系统的性能和稳定性。 优先级逆转问题：当高优先级进程被阻塞等待低优先级进程的响应时，可能会导致优先级逆转，从而影响系统的响应时间。 资源等待：像阻塞IPC一样，阻塞RPC可能导致多个进程或线程在等待资源时挂起，从而降低系统的并行处理能力。 总结：在微内核L4上，阻塞IPC和阻塞RPC都会导致进程挂起，影响系统响应时间和资源利用率。虽然阻塞机制可以简化编程模型，但需要在高性能和实时系统中谨慎使用，可以考虑异步通信方式来减少阻塞的影响，例如：\n使用异步IPC/RPC，实现非阻塞调用，让进程继续执行其他任务。 利用多线程或多进程架构，将通信和计算分离，减轻阻塞对系统的影响。 通过超时机制避免长时间的阻塞等待，从而提高系统的鲁棒性。 了解这些影响后，设计和优化系统时，可以根据需求选择合适的同步或异步通信模型，以平衡性能和编程的复杂度。\nL4的缓存 实时系统的缓存分区： 缓存分区可以显著减少实时任务受其他任务干扰的影响，从而优化最坏情况下的执行时间。 实验中，通过分配专用的缓存页面，最坏情况下的执行时间从96.1毫秒减少到24.9毫秒，显示了缓存分区对提高实时性能的有效性。 尽管这种方法会影响分时系统的性能，但它使得系统资源在实时和分时任务之间进行动态分配成为可能。 μ-kernel的优势： μ-kernel通过RPC和用户级分页器提供了改进Unix兼容性功能的机制，还可以实现非Unix兼容功能，且性能优越。 实时内存管理案例表明，μ-kernel架构可以有效支持不同范式的系统共存，并且能够提供高性能。 总体而言，这部分强调了μ-kernel体系结构在内存管理和任务调度方面的优势，尤其是在需要严格时序保证的实时系统中。通过将资源进行精细化控制和分配，μ-kernel能够显著提升系统的实时性能，同时保证在分时任务中的合理表现。\n在一个实验中，一个64×64矩阵乘法被周期性地中断，以产生最大化缓存冲突的合成负载。在不受干扰的情况下，矩阵乘法需要10.9毫秒。每100微秒（μs）中断一次，最坏情况下的执行时间为96.1毫秒，减慢了8.85倍。\n在缓存分区的情况下，分页器从总共64个二级缓存页面中专门为矩阵乘法分配了3个页面。这既不能避免主缓存的干扰，也不能避免矩阵乘法的数据工作集（64 KB）的二级缓存未命中。然而，通过避免与其他任务的二级缓存干扰，最坏情况下的执行时间减少到24.9毫秒，减慢了仅2.29倍。从实时角度来看，分区后的矩阵乘法比未分区的快近4倍。\n为实时系统分配资源会降低分时性能。然而，描述的技术使系统资源在实时和分时系统之间进行定制的动态分区成为可能。\nco-location 在计算机体系结构和操作系统领域，\u0026ldquo;co-location\u0026rdquo;（共置或同址）通常指的是将相关的或彼此需要频繁交互的代码或资源放置在相同的物理位置（如相同的内存或存储单元）或运行在相同的上下文或地址空间内，以减少上下文切换和通信开销。其目标是提高系统性能和效率。\n在操作系统设计中，co-location 指的是将某些代码模块、服务或资源放在一起，以减少它们之间的通信延迟和开销。例如：\n内核与用户态代码： 在某些设计中，用户态的操作系统服务可能被“共置”到内核态中，以避免频繁的用户态和内核态之间的切换。例如，某些系统扩展（如驱动程序或高频操作）可以直接被放置在内核中执行，减少了内核态和用户态之间的上下文切换。 进程和线程： 在多核系统上，频繁交互的线程可以被分配到相同的处理器核心，以减少它们之间的通信延迟，这也是一种共置策略。 在论文中提到的 \u0026ldquo;co-location\u0026rdquo; 技术，指的是将扩展或服务代码直接共置在内核中，以减少用户态和内核态之间的切换，并减少地址空间切换。这些技术的目标是提高系统性能，但也带来了新的挑战，例如需要完全信任这些共置的代码或者需要对其进行沙箱隔离。\n以下是文中的具体描述：\n简单共置技术：将共置代码直接信任，避免了复杂的安全检查。 更复杂的技术：使用隔离技术，限制内核扩展可能造成的潜在损害。 举例系统包括：\nVino 使用了复杂但昂贵的事务机制。 Exokernel 提供了从可解释的中间代码到运行时检查的多种机制，支持应用程序特定的处理程序。 SPIN 是一种高效且复杂的共置技术，它通过尽可能多地使用编译时的沙箱技术，而避免了额外的运行时沙箱开销（除了下标检查）。 文中还提到，共置技术如果过于简单，可能无法显著提高性能，例如“共置”的 MkLinux 相较于用户态的 L4Linux 表现更差。\n总结：co-location 在操作系统设计中通过将相互依赖的代码或资源放置在相同位置或地址空间，减少了通信和切换开销，提高了系统性能。然而，这需要在安全和信任之间找到平衡，以避免潜在的安全风险。\n扩展代码嵌接到操作系统内核 嵌接的优势： 将嵌接代码下载到内核模式运行，可以减少用户态和内核态的切换及地址空间的切换，潜在提高性能。 安全性和信任： 内核模式下的嵌接代码要么需要完全信任，要么需要进行沙箱隔离，以防范潜在的安全风险。 现有技术： 简单的共址技术仅是信任共置代码，但效果并不一定好，如共置的MkLinux表现比用户模式的L4Linux差。 Vino和Exokernel引入了更加复杂的处理机制，从事务机制到应用程序特定的处理程序。 SPIN是一种利用编译时沙箱技术的高效嵌接方法，避免了额外的运行时沙箱开销。 性能比较： 虽然L4在用户模式下的基准测试时间比SPIN快了2到4.7倍，但由于硬件平台的不一致性，这种比较需要谨慎。 性能接近显示了两者的实现大致相当。 尚未解决的问题： 嵌接代码下载到内核中是否在性能上优于微内核方法，仍然是一个开放的问题。 总结而言，虽然将扩展代码嵌接到内核中可能提高性能，但需要解决信任和安全问题。不同方法各有优劣，尚不完全明确哪一种方法能在所有情况下表现最佳。\nProtected Control Transfers (PCT) 定义：PCT是一种类似于硬件中断的机制，在微内核系统中用于实现跨地址空间的过程调用。使用PCT，不带参数的过程可以通过被调用者定义的调用门实现。\n作用：\n实现跨地址空间的快速调用。 比IPC更基础，提供快捷的控制转移。 PCT与IPC的区别\nPCT IPC 抽象层次 低级，类似硬件的操作，主要用于跨地址空间的直接控制转移。 较高级的机制，用于进程间发送和接收消息。 性能 通常更快，因为它不涉及消息传递和同步，仅进行控制转移。 稍慢，因为会进行消息传递、同步、和堆栈分配等额外操作。 功能 不涉及数据传输，主要是过程调用和控制转移。 支持数据传输和线程间的复杂通信。 资源需求 没有额外的内核资源需求，每个调用只需要用户级堆栈。 需要分配目标线程的系统资源，支持更复杂的通信场景。 总结：PCT是一种简化的、快速的控制转移方式，适用于需要低开销、快速切换的场景。而IPC作为更高级的通信机制，提供了丰富的功能用于线程间的数据交换和同步。选择使用哪种机制，通常取决于具体应用场景的需求。\n下图是一个通信过程中消耗的操作时间\n操作 PCT IPC 备注 进入PAL模式 5 5 打开帧 7 7 设置堆栈帧以允许多个中断、TLB未命中和简化线程切换 发送/接收 - 0.5 确定操作 检查接收者有效性 2 2 检查无主要传输 - 0.5 接收者接受？ - 1 我们可以进行传输吗 设置我的接收超时 - 1 保存接收参数 - 2 执行接收 验证排队状态 - 1 设置唤醒排队无效，如果超时NEVER 上下文切换 10 10 切换地址空间编号 内核线程切换 - 6 设置调用者ID 2 - 保存调用者ID到PCT返回 查找被调用者条目 2 - 被调用者的PCT条目地址 关闭帧 7 7 退出PAL模式 2 2 总计 38 45 二者的例子说明↓\nPCT 示例\n场景：操作系统中断处理。在操作系统中，中断处理程序需要快速响应硬件信号，例如键盘输入或网络数据包。这种处理需要快速切换到一个特定的代码段执行，然后迅速返回。\nPCT作用：\n快速切换：通过PCT，可以直接跳转到中断处理程序，而无需携带参数或进行额外的数据传输。 高效执行：减少上下文切换开销，快速响应硬件事件。 区别：\nPCT在这里用于快速控制流切换，而无需数据传递。 IPC 示例\n场景：客户端与服务器通信。假设有一个图像处理服务器，客户端应用程序需要将图像数据发送给服务器进行处理，并接收结果。\nIPC作用：\n数据传输：客户端通过IPC将图像数据发送给服务器。 结果接收：服务器处理完后，通过IPC将处理结果返回给客户端。 区别：\nIPC在这里用于数据传输和消息交换，实现复杂的客户-服务器模型。 总结\nPCT适用于需要快速控制流切换的场景，无需参数传递。 IPC用于进程之间的数据和消息传递，支持更复杂的通信和同步需求。 正课部分 Monolithic kernel 今天主要会讨论微内核（Mircro kernel）。\n为什么人们会研究微内核？微内核是人们在思考操作系统内核应该做什么事情的过程中设计出来的。\n比如说XV6是一个Unix系统，它按照Unix风格提供了各种功能，并提供了Unix系统调用。\n实现一个Unix系统就是我们设计XV6的目标。\n但是一个完全值得思考的问题是，一个操作系统内核本身应该具备什么功能？或许XV6和Linux具备的功能并不是一个完美操作系统应该有的功能，又或许是呢。\n我们现在正在讨论一个变化莫测的问题，内核可以看做是一种程序员的开发平台，而我们知道不同的程序员对于他们喜欢的开发平台有着非常不同的主观喜好，所以我们不能期望这个问题有一个完美的答案。\n但是我们可以仍然可以期望从思考这个问题的过程中学到一些东西，并且尝试想一下答案可能是什么。\n传统实现方式\n首先，让我说明一下操作系统的传统实现方式以及应该具备的功能。我个人将Linux，Unix，XV6称为用传统方式实现的操作系统。\n另一个形容这些操作系统的词是monolithic。\nmonolithic的意思是指操作系统内核是一个完成了各种事情的大的程序。\n实际上，这也反应了人们觉得内核应该具备什么样的功能。类似于Linux的典型操作系统内核提供了功能强大的抽象。\n它们选择提供例如文件系统这样一个极其复杂的组件，并且将文件，目录，文件描述符作为文件系统的接口，而不是直接将磁盘硬件作为接口暴露给应用程序。\nmonolithic kernel通常拥有例如文件系统这样强大的抽象概念，这比提供一些简单的抽象有着巨大的优势。\n其中一个好处是，这些高度抽象的接口通常是可移植的，你可以在各种各样的存储上实现文件和目录，你可以使用文件和目录而不用担心它们是运行在什么牌子的磁盘，什么类型的存储之上，或许是SSD，或许是HDD，或许是NFS，但是因为文件系统接口是高度抽象的，所以它们都拥有相同的接口。\n所以这里的一个好处是可以获取可移植性。你可以在不修改应用程序的前提下，将其运行在各种各样的硬件之上。\n另一个例子是，Linux/Unix提供地址空间的抽象而不是直接访问MMU硬件的权限。\n这不仅可以提供可移植性，并且也可以向应用程序隐藏复杂性。所以操作系统具备强大抽象的另一个好处是，它们可以向应用程序隐藏复杂性。\n举个例子，XV6提供的文件描述符非常简单，你只需要对文件描述符调用read/write就可以，但是在XV6内核中是非常复杂的代码来实现读写磁盘上的文件系统。这对于程序员是极好的，但是内核却因此变得又大又复杂。\n这里的强大的抽象还可以帮助管理共享资源。\n例如我们将内存管理委托给了内核，内核会跟踪哪些内存是空闲的。\n类似的，内核还会跟踪磁盘的哪个部分是空闲的，磁盘的哪个部分正在被使用，这样应用程序就不用考虑这些问题，所以这可以帮助简化应用程序。\n同时也可以提供健壮性和安全性，因为如果允许应用程序决定磁盘的某个位置是否是空闲的，那么应用程序或许可以使用一个已经被其他应用程序使用的磁盘位置。\n所以，内核管理硬件资源可以提供资源共享能力和安全性。\n但是同样的，这也使得内核变得更大。内核提供的这些诱人的抽象能力，使得内核包含了很多的复杂性，进而导致内核很大且复杂。\n有关monolithic kernel的另一个好处是，因为所有这些功能都在一个程序里面，所有的内核子系统，例如文件系统，内存分配，调度器，虚拟内存系统都是集成在一个巨大的程序中的一个部分，这意味着它们可以访问彼此的数据结构，进而使得依赖多个子系统的工具更容易实现。\n举个例子，exec系统调用依赖文件系统，因为它要从磁盘中读取二进制文件并加载到内存中，同时它也依赖内存分配和虚拟内存系统，因为它需要设置好新的进程的地址空间，但是它的实现是相对简单的。\n在XV6或者Linux中做到这些完全没问题，因为这些操作系统已经在内核程序中包含了文件系统和虚拟内存系统。\n但是如果严格分隔了文件系统和虚拟内存系统，那么实现类似exec的系统调用将会难得多。\n在一个monolithic操作系统中，因为本身就是一个大的程序，实现起来会容易的多。\n另一个使得类似于XV6或者Linux的操作系统中可以更简单实现软件的原因是，内核的所有代码都以完整的硬件权限在运行。\n举个例子，整个XV6都运行在Supervisor mode，这意味着你可以读写任意内存地址，并且所有的内核代码都以最大的权限在运行。Linux操作系统也是这样。\n所以，monolithic kernel这种设计策略对于内核开发人员来说非常方便，并且也更容易构建更强大的抽象，进而对于应用程序开发人员来说也更容易。\n然而，对于传统的monolithic kernel，也有一些缺点。这也是之所以会出现其他内核架构，比如说微内核的原因。\n所以这里的问题是，为什么不在所有的场合使用monolithic kernel呢？\n第一个原因是它们大且复杂。\n取决于你怎么计算，Linux总是有数十万到数百万行代码。Linux的一部分可以查看Linux的另一个部分的数据，的确使得编程更加容易，但是同样也使得内部代码有大量的交互和依赖。\n有的时候查看并弄明白Linux代码会有点挑战。任何时候你有了一个大的程序，尤其它们还具有复杂的结构，你都会有Bug，操作系统内核也不例外。\n在这些年，内核有过各种各样的Bug，包括了安全性相关的Bug。所以这是一个令人烦恼的关系图，如果你使用了大的内核，你不可避免的会遇到Bug和安全漏洞。\n另一个人们不喜欢monolithic kernel的原因是，随着时间的推移，它们倾向于发展成拥有所有的功能。\nLinux应用在各种场合中，从移动电话到桌面工作站，从笔记本电脑到平板电脑，从服务器到路由器。\nLinux可以支持这么多设备是极好的，但是这也使得Linux非常的通用，所以Linux支持了很多很多不同的东西。\n而任何一个应用程序，例如我的web server程序不太会需要用到Linux中非常复杂的声卡支持。\n所以，Linux中包含了大量的内容使得它很通用，这很好，但是另一方面，通用就意味着慢。\n对于各种不同的场景都能支持，或许就不能对某些特定场景进行优化。\n当你尝试快速运行一些程序时，如果程序只做一两件事情是极好的，因为这样你就可以专注在优化一两个代码路径上。\n但是如果你的程序想要做上千件事情，优化会更加难。\nLinux并不一定慢，但是你或许会想，它真的在所有场景下都达到了最快速度吗？如果你去看Linux或者XV6中的任何模块，你或许会想它们真的有必要去做所有的事情吗？\n例如，你从一个进程向一个Pipe写一个字节传输到另一个进程，即使在XV6这样一个简单的内核中，都有大量的指令需要被执行。\n这里有buffering，locking，或许在Pipe的读写中有sleep/wakeup，或许有线程调度导致context switching，对于从一个进程移动一个字节到另一个进程来说，这里有大量的内容或许并不是必须的。\n对于monolithic kernel来说，另一个潜在的问题是，因为它们是如此之大，它们会削弱一些复杂的抽象能力。\n在内核中会有大量的设计考虑，应用程序需要遵守这些设计并与之共存。\n反之，在一个理想世界中，应用程序或许可以做更多的决定。\n举个例子，在Unix中，你可以wait子进程，比如说你fork出来的子进程，但是你不能wait其他进程（注，详见13.7），或许你会想要wait孙子进程或者一个不相关的进程，但是这是不可能的。\n或许你会想要更改其他进程的地址空间，比如说替其它受你控制的进程调用mmap，但是这也不可能。mmap只能修改你自己的地址空间，但是不能修改其他进程的地址空间。\n或许你是个数据库，你在磁盘上有B树索引，你或许知道很多快速展开B树的方法，但是当你读写文件系统中的文件时，文件系统并不知道你正在读写一个B树，以及如何更快的在磁盘上展开B树。\n所以如果你是个数据库的话，你或许很高兴文件系统可以任你摆布，但是文件系统并不会按照你想要的方式工作。以上就是内核中需要考虑的设计。\nmonolithic kernel的最后一个问题是，可扩展性（Extensibility）。\n应用程序或许想要实时更改内核，比如说向内核下载代码并更改内核的工作方式，这样数据库或许就可以更改数据在磁盘上的分布方式。\n至少在10年前，monolithic kernel没有任何功能可以支持这里的Extensibility，你只能使用内核提供的能力。\n以上就是人们认为的monolithic kernel的问题。\n这些问题使得人们去思考操作系统的其他架构。这里有很多不同的想法，其中一些想法非常的激进，我们今天只会讨论其中的一种，也是现在非常流行的一种，这就是微内核，Micro kernel。\nMicro kernel 有关微内核的很多思想可以回溯到计算机的发展历史中。\n微内核从1980年代中后期开始就是一个非常热门的研究课题，它是指一种通用的方法或者概念，它并不特指任何特定的产品。\n有很多人遵循微内核的设计思想并构建了操作系统，但是这些项目中的每一个具体的操作系统都与另一个非常不一样。\n微内核的核心就是实现了IPC（Inter-Process Communication）以及线程和任务的tiny kernel。\n所以微内核只提供了进程抽象和通过IPC进程间通信的方式，除此之外别无他物。\n任何你想要做的事情，例如文件系统，你都会通过一个用户空间进程来实现，完全不会在内核中实现。\n画个图来展示一下，整个计算机还是分为两层，下面是kernel，上面是用户空间。\n在用户空间或许还是会有各种各样常见的程序，例如VI，CC，桌面系统。\n除此之外，在用户空间还会有文件系统以及知道如何与磁盘交互的磁盘驱动，或许我们还会有一个知道如何进行TCP通信的网络协议栈，或许还有一个可以实现酷炫虚拟内存技巧的虚拟内存系统。\n当文本编辑器VI需要读取一个文件时，它需要与文件系统进行交互，所以它通过IPC会发送一条消息到文件系统进程。\n文件系统进程中包含了所有的文件系统代码，它知道文件，目录的信息。\n文件系统进程需要与磁盘交互，所以它会发送另一个IPC到磁盘驱动程序。\n磁盘驱动程序再与磁盘硬件进行交互，之后磁盘驱动会返回一个磁盘块给文件系统。之后文件系统再将VI请求的数据通过IPC返回给VI。\n这里需要注意的关键信息是，在内核中唯一需要做的是支持进程/任务/线程，以及支持IPC来作为消息的传递途径，除此之外，内核不用做任何事情。\n内核中没有任何文件系统，没有任何设备驱动，没有网络协议栈，所有这些东西以普通用户进程在运行。\n所以这提供给你一种非常小的内核，以及相对少的代码去优化，你可以优化IPC，除此之外也没有别的东西了。\n上图就是我们在这节课剩下的时间要讨论的内容。\n在今天仍然有使用微内核的场景。实际上，今天论文 要讨论的L4微内核就有很多很多的运行实例，它用在很多手机中用来控制手机的射频，在最近的iphone中的一个旁路处理器中，也使用了L4微内核来隐藏加密密钥。\n所以在一些微型嵌入式系统中，微内核能够胜出，比如说在一些专门为某种功能设计的计算机中，你需要使用某种操作系统，而你又不需要Linux带来的复杂性。\n微内核中的用户进程通过IPC通信，这在很多操作系统都存在。\n例如我现在运行的macOS，它就是一个普通的monolithic kernel，它也很好的支持用户进程通过IPC进行通信。\n所以用户进程通过内核内的IPC相互通信，这是一个成功的思想并且被广泛采用。\n以上就是微内核的基本架构，我接下来会讨论一下这个架构的优势。\nWhy micro kernel? 人们构建微内核的动机是什么？\n其中一个动机你可能不常看到，这就是审美（注，原话是sense of aesthetic）。\n我认为很多人都觉得像Linux内核这样大的复杂的程序并不十分优雅。我们肯定可以构建一些小得多且专注得多的设计，而不是这样一个巨大的拥有各种随机特性的集合体。\n所以从审美角度来说，我们必然可以做的比大的内核更好。\n其他还有一些更具体且可量化的动机。\n更小的内核或许会更加的安全。只有几行代码的话，Bug也不会太多，其他人利用Bug来破坏安全性的可能也就更小。\n在特殊场景下，你需要证明一个操作系统是正确的，没有Bug，并且只做了它应该做的事情。\n现实中至少有一种经过验证是安全的微内核系统：seL4。这是L4微内核的多个衍生项目之一。\n人们知道怎么去验证中小型程序，但是不知道怎么验证巨大的程序。微内核通常都很小，这是它能够被证明是安全的一个关键因素。\n小内核的另一个优势是，少量代码的程序比巨大的程序更容易被优化。\n小内核可能会运行的更快，你不用为很多用不上的功能付出代价。\n相比monolithic kernel，微内核几乎不会做任何事情，所以你不用为你不使用的很多功能付出代价。\n使用小内核的另一个原因是，小内核或许自带了少得多的设计限制，进而使得应用程序的设计限制也更少。\n这样给应用程序提供了更多的灵活性，使得应用程序可以自己做出设计决定。\n以上都不是微内核必须要支持的特性，只是说人们期望通过使用微内核可以得到的特性。\n另一个微内核吸引人的原因是，有很多我们习惯了位于内核的功能和函数，现在都运行在用户空间。\n这种将内核拆分，并在用户空间的不同部分运行，比如说在用户空间运行文件系统服务，可以使得代码更模块化。\n用户空间代码通常会比内核更容易被修改，调整和替换，所以它更容易被定制化。\n将操作系统放在用户空间，或许可以使得它更加的健壮。\n如果内核出错了，通常你需要panic并重启，因为如果内核有Bug，并且会随机更改数据，那就不能信任内核了。\n然而，如果你将内核运行成一些用户空间的服务，其中一个出现故障，比如说除以0，索引了一个野指针，或许只有这一个服务会崩溃，操作系统的剩余部分还是完好的，这样你可以只重启那一个服务。\n所以，将操作系统的功能移到用户进程可以使得系统更加健壮。\n这对于驱动来说尤其明显，内核中大部分Bug都在硬件驱动中，如果我们能将设备驱动从内核中移出的话，那么内核中可能会有少的多的Bug和Crash。\n最后一个优势是，你可以在微内核上模拟或者运行多个操作系统。\n所以尽管微内核几乎不做任何事情，你还是可以在它之上运行一个Unix系统之类的，或许还可以在同一个机器上运行超过一个操作系统。\n今天的论文主要就是描述如何在微内核之上将Linux作为服务来运行。\n以上就是人们在微内核这条道路上期望得到的一些好处。\n当然，这里也有一些挑战。\n如果你想要设计属于你自己的微内核，其中一个挑战是你会想要你的微内核的系统调用接口尽可能的简单，因为使用微内核的出发点就是内核很小。\n那么什么才是有用的系统调用的最小集？这一点并不十分明确。\n所以这里我们要确定最少的系统调用API，你需要这些系统调用API尽可能的少，但是你又需要基于这些API构建一些非常复杂的功能，因为即使内核没有做太多工作，你最终还是要运行程序。\n或许你想要在微内核之上运行Unix，你需要能执行类似fork，mmap的工作。\n所以底层的系统调用在简单的同时，需要能够足够强大以支持人们需要做的各种事情，比如说exec，fork，copy-on-write fork，mmap file。\n但是内核又完全不知道文件和文件系统。\n所以现在系统调用需要支持exec，而内核又不知道文件。\n微内核或许会非常简单，但是我们仍然需要开发一些用户空间服务来实现操作系统的其他部分。\n最后，微内核的设计需要进程间通过IPC有大量的通信。\n所以有很大的需求使得IPC能够足够的快。我们会好奇，IPC可以足够的快来使得微内核足够有竞争力吗？\n有关性能，不仅与IPC的速度相关。\n通常来说，monolithic kernel可以获得更好的性能，是因为它里面的文件系统代码和与虚拟内存代码可以直接交互，它们开心的位于一个巨大的程序中。\n但是如果你需要将这些模块都拆分开成为不同的服务，那么在集成的时候就有更少的机会可以优化，这或许会影响性能。\n以上就是许多微内核项目要面对的用户期望和潜在的挑战。\nL4 micro kernel 今天要讨论的论文 ，有许多有关L4微内核的内容。这是今天论文作者开发和使用的一种微内核。\nL4必然不是最早的微内核，但是从1980年代开始，它是最早一批可以工作的微内核之一，并且它非常能展现微内核是如何工作的。\n在许多年里面它一直都有活跃的开发和演进。如果你查看Wikipedia，L4有15-20个变种，有一些从1980年代开始开发的项目现在还存在。\n接下来我将从我的理解向你们解释L4在今天的论文发表的时候是如何工作的。\n首先，L4是微内核，它只有7个系统调用，虽然其中有一些稍微有点复杂，但是它还是只有7个系统调用。\n然而现在的Linux，我上次数了下有大概350个系统调用。甚至XV6这个极其简单的内核，也有21个系统调用。\n从这个指标来看，L4更加简单。\n其次，L4并不大，论文发表的时候，它只有13000行代码，这并不多。\nXV6的代码更少，我认为XV6内核只有6000-7000行代码，所以作为内核XV6非常的简单。\nL4也没有复杂太多，它只有Linux代码的几十分之一，所以它非常的小。\n第三，它只包含几个非常基础的抽象。\n它在内部有一个叫做Task或者地址空间的概念，这或多或少的对应了Uinx内的进程概念。\nTask包含了一些内存，地址从0开始，并且可以像进程一样执行指令。\n区别于XV6的是，每个Task可以有多个线程，L4会调度每个Task内的多个线程的执行。这样设计的原因是，可以非常方便地用线程来作为组织程序结构的工具。\n我不知道在论文发表的时候，L4是否支持了多处理器，或许它包含了在多个处理器上运行同一个程序的能力。\n所以L4内核知道Task，知道线程，也知道地址空间，这样你就可以告诉L4如何映射地址空间内的内存Page。\n另一个L4知道的事情是IPC。\n每一个线程都有一个标识符，其中一个线程可以说，我想要向拥有这个标识符的另一个线程发送几个字节。\n这里的Task，线程，地址空间，IPC是L4唯一有的抽象。\n我不确定是否能列出所有的系统调用，这里涉及到的系统调用有：\nThreadcreate系统调用，你提供一个地址空间ID并要求创建一个新的线程。\n如果地址空间或者Task不存在，系统调用会创建一个新的Task。所以这个系统调用即可以创建线程，又可以创建Task。\nSend/Recv IPC系统调用。\nMapping系统调动可以映射内存Page到当前Task或者其他Task的地址空间中。\n你可以要求L4来改变当前Task的地址空间和Page Table，如果你有足够的权限，你也可以要求L4改变其他Task的地址空间。这实际上是通过IPC完成的，你会发送一个特殊的IPC消息到目标线程，内核可以识别这个IPC消息，并会修改目标线程的地址空间。\n如果你创建一个先的线程，新线程最开始没有任何内存。所以如果你想创建一个线程，你先调用Threadcreate系统调用来创建新的线程，新的Task和地址空间。\n然后你创建一个特殊 IPC，将你自己内存中的一部分，其中包含了指令和数据，映射到新的Task的地址空间中。\n之后你再发送一个特殊的Start IPC消息到这个新的Task，其中包含了你期望新的Task开始执行程序的程序计数器和Stack Pointer。\n之后新的Task会在你设置好的内存中，从你要求的程序计数器位置开始执行。\n虽然我不知道具体是怎么实现的，但是Privileged Task可以将硬件控制寄存器映射到自己的地址空间中。\n所以L4并不知道例如磁盘或者网卡的设备信息，但是实现了设备驱动的用户空间软件可以直接访问设备硬件。\n你可以设置L4将任何一个设备的中断转换成IPC消息。\n这样，运行设备驱动的Task不仅可以读写了设备，并且也可以设置L4将特定设备的中断通过IPC消息发送给自己。\n最后，一个Task可以设置L4内核通知自己有关另一个Task的Page Fault。\n所以如果一个Task发生了Page Fault，L4会将Page Fault转换成一个IPC消息，并发送给另一个指定的Pager Task。\n每一个Task都有个与之关联的Pager Task用来处理自己相关的Page Fault。这就是关联到Page Fault的方法，通过它可以实现类似copy-on-write fork或者lazy allocation。\n以上就是内核的内容，L4里面不包含其他的功能，没有文件系统，没有fork/exec系统调用，除了这里非常简单的IPC之外，没有其他例如pipe的通信机制，没有设备驱动，没有网络的支持等等。\n任何其他你想要的功能，你需要以用户空间进程的方式提供。\nL4能提供的一件事情是完成线程间切换。\nL4会完成线程调度和context switch，来让多个线程共用一个CPU。\n它实现的方式你会觉得非常熟悉，L4会为每个Task保存寄存器，当它执行一个线程时，它会跳到用户空间，切换到那个线程对应Task的Page Table，之后那个线程会在用户空间执行一会。之后或许会有一个定时器中断，定时器是L4知道的一个设备，定时器中断会使代码执行返回到L4内核，L4会保存线程的用户寄存器，然后在一个类似于XV6的线程调度循环中，选择一个Task来运行。\n通过将这个Task之前保存的寄存器恢复出来，切换Page Table，就可以跳转到Task中再运行一会，直到再发生另一个定时中断，或者当前Task出让了CPU。\n所以我认为L4或许还有一个yield系统调用。\n在这种情况下Task可以等待接收一个IPC消息，这时代码会跳转回L4内核，L4内核会保存寄存器，并切换到一个新的Task。所以L4中有关线程切换的部分你们会非常熟悉。\n我之前提到过这个概念，Pager。\n如果一个进程触发了Page Fault，通过trap走到了内核，内核会将Page Fault转换成IPC消息并发送到指定的Pager Task，并告诉Pager Task是哪个线程的哪个地址触发了Page Fault。\n在Pager Task中，如果它实现了lazy allocation，那么它会负责从L4分配一些内存，向触发Page Fault的Task发送一个特殊的IPC，来恢复程序的运行。\n所以Pager Task实现了XV6或者Linux在Page Fault Handler中实现的所有功能。\n如果你想的话，你可以在Pager Task中实现copy-on-write fork或者memory mapped files，Pager Task可以实现基于Page Fault的各种技巧。\n这是类似L4的微内核相比传统的内核，对于用户程序要灵活的多的众多例子之一。\n如果Linux并没有copy-on-write fork，并且你想要有这个功能，你不可能在不修改内核的前提下完成这个功能。\nLinux中没有办法写一些可移植的用户空间代码来实现copy-on-write fork。这样描述可能并不完全正确，但是一定要这么做的话会很复杂。\n然而，在L4里面，这就相对简单了。\nL4就好像是完全设计成让你去写用户空间代码来获取Page Fault，并实现copy-on-write fork。所有这些都可以在用户空间完成，而不用弄乱内核。\n学生提问：能说明一下Task和线程之间的区别吗？\nRobert教授：可以。一个Task就像XV6的一个进程一样，它有一些内存，一个地址空间，你可以在其中运行用户代码。\n如果你在XV6中有一个进程，它只能包含一个线程。\n但是在现代的操作系统和L4中，在一个进程，一个地址空间中，可以有多个线程。\n如果你有多个CPU核，那么多个CPU核可以同时运行一个Task。\n每个线程在Task的地址空间中都有一个设置好的Stack，这意味着你可以写一个程序，并通过并行运行在多个CPU核上得到性能的提升，其中的每个线程都运行在不同的CPU核上。\n所以你们可以看到，这里的设计非常依赖IPC，因为如果你想与你的文件系统交互，文件系统想要与设备驱动交互，你都需要来回发送IPC消息。\n对于每个系统调用，每个Page Fault，每个设备中断，都会有反复的IPC消息。所以IPC系统需要非常快。\nImproving IPC by Kernel Design 接下来我们讨论微内核里面一个非常重要的问题：IPC的速度。\n首先让我展示一个非常简单，但是也非常慢的设计。这个设计基于Unix Pipe。\n我之所以介绍这种方法，是因为一些早期的微内核以一种类似的方式实现的IPC，而这种方式实际上很慢。\n假设我们有两个进程，P1和P2，P1想要给P2发送消息。\n这里该怎么工作呢？一种可能是使用send系统调用，传入你想将消息发送到的线程的ID，以及你想发送消息的指针。\n这个系统调用会跳到内核中，假设我们是基于XV6的pipe来实现，那么这里会有一个缓存。\n或许P2正在做一些其他的事情，并没有准备好处理P1的消息，所以消息会被先送到内核的缓存中。\n所以当你调用send系统调用，它会将你的消息追加到一个缓存中等待P2来接收它。\n在实际中，几乎很少情况你会只想要发送一个消息，你几乎总是想要能再得到一个回复。\n所以P1在调用完send系统调用之后，会立即调用recv来获取回复。\n但是现在让我们先假设我们发送的就是单向的IPC消息，send会将你的消息追加到位于内核的缓存中，我们需要从用户空间将消息逐字节地拷贝到内核的缓存中。\n之后再返回，这样P1可以做一些其他的事情，或许是做好准备去接受回复消息。\n过了一会，P2可以接收消息了，它会调用recv系统调用，这个系统调用会返回发送消息线程的ID，并将消息从内核拷贝到P2的内存中。\n所以这里会从内核缓存中取出最前的消息，并拷贝到P2的内存中，之后再返回。\n这种方式被称为异步传输，因为P1发完消息之后，只是向缓存队列中追加了一条消息，并没有做任何等待就返回了。\n同时这样的系统这也被称作是buffered system，因为在发送消息时，内核将每条消息都拷贝到了内部的缓存中，之后当接收消息时，又从buffer中将消息拷贝到了目标线程。\n所以这种方法是异步buffered。\n如果P1要完成一次完整的消息发送和接收，那么可以假设有两个buffer，一个用来发送消息，一个用来接收消息。\nP1会先调用send，send返回之后。之后P1会立即调用recv，recv会等待接收消息的buffer出现数据，所以P1会出让CPU。\n在一个单CPU的系统中，只有当P1出让了CPU，P2才可以运行。\n论文中的讨论是基于单CPU系统，所以P1先执行，之后P1不再执行，出让CPU并等待回复消息。\n这时，P2才会被调度，之后P2调用recv，拷贝消息。\n之后P2自己再调用send将回复消息追加到buffer，之后P2的send系统调用返回。\n假设在某个时间，或许因为定时器中断触发导致P2出让CPU，这时P1可以恢复运行，内核发现在接收消息buffer有了一条消息，会返回到用户空间的P1进程。\n这意味着在这个慢的设计中，为了让消息能够发送和回复，将要包含：\n4个系统调用，两个send，两个recv 对应8次用户空间内核空间之间的切换，而每一次切换明显都会很慢 在recv的时候，需要通过sleep来等待数据出现 并且需要至少一次线程调度和context switching来从P1切换到P2 每一次用户空间和内核空间之间的切换和context switching都很费时，因为每次切换，都需要切换Page Table，进而清空TLB，也就是虚拟内存的查找缓存，这些操作很费时。\n所以这是一种非常慢的实现方式，它包含了大量的用户空间和内核空间之间的切换、消息的拷贝、缓存的分配等等。\n在微内核架构中，进程间通信（IPC） 的实现通常依赖内核的支持，因此当一个线程执行发送操作时，它确实会进入内核态。这涉及多个操作，包括上下文切换、内存访问和消息的传输。\n线程进入内核会导致上下文切换吗？\n是的，进入内核通常会导致上下文切换。\n在微内核的 IPC 实现中，线程在发送消息时需要从用户态进入内核态，至少有以下两个层面的上下文切换：\n用户态到内核态的切换：当用户线程执行系统调用（例如 send）时，CPU 会切换到内核态。这种切换虽然涉及一些保存和恢复寄存器的操作，但不一定是完整的进程上下文切换。它主要是用户态到内核态的权限转换。 进程间上下文切换：如果 IPC 涉及多个线程或者进程间通信，当发送消息的线程完成操作后，CPU 可能会切换到接收消息的线程或进程，这种情况下是完整的上下文切换。这时，需要保存和恢复所有的寄存器、页表信息、堆栈指针等。这是一个相对昂贵的操作。 进入内核会改变页表吗？\n不一定需要切换页表，但可能会发生。\n同一进程的线程间切换：如果线程属于同一个进程，页表通常不会发生变化。内核只需要完成权限转换（从用户态切换到内核态）而不改变虚拟内存的布局。 跨进程切换：如果线程属于不同的进程，那么切换到内核态可能会导致页表的改变。每个进程拥有自己独立的虚拟地址空间，因此当内核从一个进程切换到另一个进程时，内核必须加载接收方进程的页表，以确保接收方进程能正确访问自己的内存。 放入 buffer 的消息从哪里来？\n消息的来源是发送方线程的用户空间。一般情况下，发送方进程的线程在调用 IPC 时，会将消息从自己的用户空间缓冲区复制到内核提供的 buffer 中。 例如：\n线程 A 通过 send 调用向线程 B 发送一条消息。 线程 A 的消息存储在用户态的内存区域（如用户的堆内存）。 当执行系统调用时，内核将这条消息从线程 A 的用户空间复制到内核的 buffer 中。 之后，内核会将消息传递给线程 B，可能通过共享内存、消息队列等方式。 举例说明微内核 IPC 的操作流程\n假设有两个线程 A 和 B，分别属于不同的进程。A 想要发送一条消息给 B，流程可能如下：\n线程 A 调用 send 函数： A 从用户态进入内核态，执行 send 系统调用。 此时发生了用户态到内核态的上下文切换。 内核处理消息传递： 内核从线程 A 的用户空间中读取消息数据，将其复制到内核 buffer 中。 如果线程 B 当前正在等待这条消息，内核可能会准备唤醒 B，发生线程调度。 上下文切换： 如果线程 B 属于另一个进程，则需要加载线程 B 所在进程的页表，保证线程 B 可以正确访问自己的虚拟内存空间。 进行完整的上下文切换，保存 A 的上下文（寄存器、堆栈指针、页表等）并恢复线程 B 的上下文。 线程 B 处理消息： 线程 B 被唤醒，从内核 buffer 中读取消息，并复制到自己的用户空间。 线程 B 处理完消息后，可能返回用户态继续执行。 总结\n线程进入内核时，通常会发生上下文切换，至少会有用户态到内核态的切换，可能还会有完整的进程上下文切换。\n页表的切换取决于是否发生进程切换。同一进程的线程切换不需要更换页表，但不同进程之间的切换则需要更换页表。\n消息通常是从发送方进程的用户空间复制到内核 buffer，再传递给接收方。\n这种机制带来了 IPC 中的性能开销，特别是上下文切换和数据拷贝，这是早期微内核性能瓶颈的原因之一。\n尤其在系统设计中，安全性和性能经常需要权衡。在只有一个 CPU 的情况下，线程从用户态进入内核态，确实意味着线程的权限发生了变化，从较低的用户权限提升到较高的内核权限。这种权限切换使得线程可以访问更多的系统资源，比如硬件设备、内核内存等。这当然带来了潜在的安全风险，但操作系统设计中有多种机制来应对这些安全问题。\n进入内核态：权限切换 当线程在用户态运行时，操作系统的硬件保护机制会限制它只能访问用户空间内存和执行特定的非特权指令。如果线程需要执行特权操作（比如与硬件交互或访问内核资源），它必须通过系统调用来请求内核完成这些操作。\n在系统调用中，CPU 会切换到内核态，线程的权限位（通常是 CPU 的控制寄存器或状态寄存器中的标志位）也会相应地改变。这时，线程能够执行特权指令，访问内核内存和设备。但这个切换过程不是随意的，操作系统会通过以下几种方式来防止线程滥用这种高权限：\n防止安全问题的机制 （1）受限的系统调用接口\n用户态线程不能直接访问内核态资源，它只能通过特定的系统调用接口来请求内核执行操作。这些系统调用是经过严格定义的，并受到内核的权限控制。例如：\n用户态线程不能直接读写内核内存。 系统调用会进行参数校验，确保用户态线程传递的参数合法、合理。比如，如果用户传递了无效的地址或者非法的文件描述符，内核会返回错误。 这意味着，即使用户线程进入了内核态，它只能在内核规定的范围内行动，无法随意执行其他有害操作。\n（2）进程隔离与页表保护\n每个用户进程都有自己独立的虚拟内存空间，页表的设置确保了用户进程无法直接访问其他进程的内存或内核的内存。在进入内核态时，虽然权限提升了，但内核会严格管理线程能够访问的内存区域。这一保护措施确保即使线程进入了内核态，它也不能随意访问或修改系统内核的数据。\n（3）中断处理与内核模式的控制流\n当线程从用户态进入内核态时，系统会有一整套严格的控制流来保证安全：\n系统调用入口：系统调用是通过中断（或陷阱指令）进入内核的，内核中有专门的系统调用处理程序，它会接管线程的执行，并确保线程只能在安全的范围内运行。 中断处理器：系统调用或者硬件中断会触发硬件控制流，跳转到内核指定的安全位置，并保存当前用户态的执行上下文，防止恶意线程篡改这些数据。 （4）内核空间与用户空间分离\n操作系统将内核空间和用户空间进行隔离。用户空间的代码和数据处于较低的内存区域，而内核空间则处于较高的内存区域。内核态执行的代码是经过严格审查和保护的，用户态代码不能直接修改内核空间的数据。\n通过硬件和软件的结合，线程即使在进入内核态后也无法访问用户态之外的资源，除非通过安全的系统调用接口，这样防止了线程“乘机运行有害代码”。\n特权切换的安全性保障 虽然线程进入内核意味着获得了更高的权限，但这并不意味着用户态的恶意线程能够自由破坏系统。这是因为：\nCPU硬件支持的分级权限：现代 CPU 使用分级权限环（privilege rings），通常操作系统使用Ring 0（内核态）**和**Ring 3（用户态）。进入内核态的切换是通过特定的硬件中断或者陷阱指令实现的，并且内核会在处理结束后立即返回用户态。 系统调用检查：内核会对每个系统调用进行严格的参数验证和安全检查，防止用户进程通过恶意传递参数来破坏系统。错误或非法的调用会被拒绝，通常内核会返回错误码或者终止进程。 内存管理单元（MMU）的保护：CPU 中的内存管理单元（MMU）和页表机制确保用户态无法访问内核态内存，防止恶意用户进程修改或读取内核数据。 安全性示例：系统调用如何保护内核 举个简单的例子，假设一个用户态线程要进行文件写操作，它会调用 write 系统调用：\n用户态线程调用 write(fd, buffer, size)，通过陷阱指令进入内核态。 内核会接管控制，首先检查文件描述符 fd 是否有效，是否可以写操作。 然后，内核检查 buffer 是否是合法的用户空间地址，并确保该地址处的内存区域可以被读取。 如果所有检查都通过，内核才会进行实际的写操作。 操作完成后，内核返回用户态，并将控制交还给用户线程。 在这个过程中，内核确保了线程无法通过不合法的 fd 或 buffer 来执行有害操作，整个操作在严格受控的范围内进行。\n总结 在单个 CPU 系统中，线程进入内核态确实意味着权限提升，但操作系统通过硬件支持和系统调用接口的设计来保证安全性。 权限切换的过程并不直接暴露内核，内核会对系统调用进行严格的校验和控制。 即使权限提升了，恶意线程仍然无法绕过操作系统的保护机制，内核态的执行是受控制的，不会随意执行用户态提供的有害代码。 实际中，对于这里的场景：发送一个消息并期待收到回复，你可以抛开这种方法并获得简单的多的设计，L4就是采用了后者。\n有关简单的设计在一篇著名的论文中有提到，论文是Improving IPC by Kernel Design ，这篇论文在今天要讨论的论文前几年发布。相比上面的慢设计，它有几点不同：\n其中一点是，它是同步的（Synchronized）。\n所以这里不会丢下消息并等待另一个进程去获取消息，这里的send会等待消息被接收，并且recv会等待回复消息被发送。\n如果我是进程P1，我想要发送消息，我会调用send。send并不会拷贝我的消息到内核的缓存中，P1的send会等待P2调用recv。\nP2要么已经在内核中等待接收消息，要么P1的send就要等P2下一次调用recv。\n当P1和P2都到达了内核中，也就是P1因为调用send进入内核，P2因为调用recv进入内核，这时才会发生一些事情。\n这种方式快的一个原因是，如果P2已经在recv中，P1在内核中执行send可以直接跳回到P2的用户空间，从P2的角度来看，就像是从recv中返回一样，这样就不需要context switching或者线程调度。\n相比保存寄存器，出让CPU，通过线程调度找到一个新的进程来运行，这是一种快得多的方式。\nP1的send知道有一个正在等待的recv，它会立即跳转到P2，就像P2从自己的recv系统调用返回一样。\n这种方式也被称为unbuffered。它不需要buffer一部分原因是因为它是同步的。\n当send和recv都在内核中时，内核可以直接将消息从用户空间P1拷贝到用户空间P2，而不用先拷贝到内核中，再从内核中拷出来。\n因为现在消息收发的两端都在等待另一端系统调用，这意味着它们消息收发两端的指针都是确定的。\nrecv会指定它想要消息被投递的位置，所以在这个时间点，我们知道两端的数据内存地址，内核可以直接拷贝消息，而不是需要先拷贝到内核。\n如果消息超级小，比如说只有几十个字节，它可以在寄存器中传递，而不需要拷贝，你可以称之为Zero Copy。\n前面说过，发送方只会在P2进入到recv时继续执行，之后发送方P1会直接跳转到P2进程中。\n从P1进入到内核的过程中保存P1的用户寄存器，这意味着，如果P1要发送的消息很短，它可以将消息存放到特定的寄存器中。\n当内核返回到P2进程的用户空间时，会恢复保存了的寄存器，这意味着当内核从recv系统调用返回时，特定寄存器的内容就是消息的内容，因此完全不需要从内存拷贝到内存，也不需要移动数据，消息就存放在寄存器中，可以非常快的访问到。当然，这只对短的消息生效。\n对于非常长的消息，L4可以在一个IPC消息中携带一个Page映射，所以对于巨大的消息，比如说从一个文件读取数据，你可以发送一个物理内存Page，这个Page会被再次映射到目标Task地址空间，这里也没有拷贝。\n这里提供的是共享Page的权限。所以短的消息很快，非常长的消息也非常快。对于长的消息，你需要调整目的Task的Page Table，但是这仍然比拷贝快的多。\n最后一个L4使用的技巧是，如果它发现这是个RPC，有request和response，并且有非常标准的系统调用包括了send和recv，你或许会结合这两个系统调用，以减少用户态和内核态的切换。\n所以对于RPC这种特别的场景，同时也是人们使用IPC的一个常见场景，有一个call系统调用，它基本上结合了send和recv，区别是这里不会像两个独立的系统调用一样，先返回到用户空间，再次进入到内核空间。\n在消息的接收端，会有一个sendrecv系统调用将回复发出，之后等待来自任何人的request消息。这里基本是发送一个回复再加上等待接收下一个request，这样可以减少一半的内核态和用户态切换。\n实际中，所有的这些优化，对于短的RPC请求这样一个典型的场景，可以导致20倍速度的提升。这是论文中给出的对比之前慢设计提升的性能倍数。这个数字很了不起。Improving IPC by Kernel Design这篇论文是由今天这篇论文的同一个作者在前几年发表的，因为现在IPC可以变得非常的快，它使得人们可以更加认同微内核。\n学生提问：当使用这些系统调用时，进程是什么时候发送和接收消息的？\nRobert教授：对于包含request和response的RPC，进程使用call和sendrecv这一对系统调用，而不是send和recv。\n对于call，你会传入两个参数，你想要发送的消息，以及你要存放回复消息的位置，这个系统调用在内核中会结合发送和接收两个功能。\n你可以认为这是一种hack，因为IPC使用的是如此频繁，它值得一些hack来使得它变得更快。\n学生提问：在上面的图中，P2会调用recv系统调用，P2怎么知道应该去调用这个系统调用？\nRobert教授：在RPC的世界中，我们有client会发送request到server，server会做一些事情并返回。\n因为P2是一个server，我们会假设P2会一直在一个while循环中，它随时准备从任何client接收消息，做一些数据处理工作，比如在数据库中查找数据，之后再发送回复，然后再回到循环的最开始再等待接收消息。\n所以我们期望P2将所有时间都花费在等待从任何一个客户端接收消息上。\n前面讨论的设计需要依赖P2进程在暂停运行时，一直位于内核的recv系统调用中，并等待下一个request。\n这样，下一个request才可以直接从这个系统调用返回，这种快速路径在这里的设计中超级有效率。\n学生提问：这里提到从P1返回到P2，为了能返回到P1，需要P2发送response吗？\nRobert教授：是的，我们期望P2发送一个response，发送response与发送request是同一个代码路径，只是方向相反（之前是P1到P2现在是P2到P1），所以当P2发送一个response，这会导致返回到P1。\nP1实际调用的是call系统调用，通过从call系统调用返回到P1，会将P2的response送到P1。\n这里与你们以为的通常的设置略有不同，通常情况下，你从P1通过系统调用进入到内核，在内核中执行系统调用然后再返回，所有的工作都在P1这边，这也是pipe的read/write的工作方式。\n在这里，P1进入到内核，但是却返回到了P2。所以这里有点奇怪，但是却非常的快。\n远程过程调用（RPC） 和 进程间通信（IPC） 是两个相关但不同的概念，它们之间的关系以及其他 IPC 机制可以通过如下方式理解：\nRPC 和 IPC 的关系 进程间通信（IPC，Inter-Process Communication） 是指两个或多个进程之间交换数据或信号的机制。由于现代操作系统中每个进程都有自己独立的内存空间，进程间通信是必要的，因为进程无法直接访问彼此的内存。\n远程过程调用（RPC，Remote Procedure Call） 是一种特定的 IPC 方法。它允许一个进程通过调用某个“远程”的函数或过程，来请求另一个进程执行某个任务，就像调用本地函数一样。RPC 的背后可能隐藏了复杂的网络或本地通信操作，但对于用户而言，调用是透明的。\nRPC 是 IPC 的一种实现方式\nRPC 是一种封装良好的 IPC，常用于进程间的远程通信（进程可能在同一台机器上，也可能跨网络）。它将进程间的消息传递抽象为“过程调用”的形式，从而让开发者不必关心底层的消息传递细节。\n常见的 IPC 机制 除了 RPC 之外，还有许多其他的 IPC 机制，具体包括：\n（1）管道（Pipe）\n无名管道：提供单向的字节流通信，通常用于父子进程间的数据传递。一个进程写入数据，另一个进程从管道的另一端读取。 命名管道（FIFO）：允许不相关的进程之间进行双向通信，类似于无名管道，但可以由多个进程使用。 （2）消息队列（Message Queue）：消息队列是一种**先进先出（FIFO）**的消息传递方式，允许进程以消息的形式发送和接收数据。它可以在内核中存储一系列的消息，并提供同步和异步消息传递的能力。\n（3）共享内存（Shared Memory）：共享内存允许多个进程直接访问同一段内存，这是一种非常高效的 IPC 方式，因为数据不需要在进程之间复制。但是，共享内存本身不提供同步机制，所以通常需要结合其他同步原语（如信号量）使用，以避免数据竞争。\n（4）信号量（Semaphore）：信号量是一种同步机制，用于控制进程对共享资源的访问。它并不用于传递数据，而是用于解决多个进程访问同一资源时的竞争问题。\n（5）信号（Signals）：信号是一种异步的消息传递机制，主要用于通知进程某个事件的发生。例如，操作系统可以通过信号通知某个进程时间片用完，或者出现了需要处理的事件（如终止进程）。\n（6）套接字（Sockets）：套接字是一种强大的 IPC 机制，允许进程通过网络协议（如 TCP/IP）进行通信。它可以用于本地进程间通信（Unix 域套接字），也可以用于跨网络的远程通信。\n（7）内存映射文件（Memory-Mapped Files）：内存映射文件将文件内容直接映射到进程的虚拟地址空间中，允许多个进程通过映射相同的文件进行通信和数据共享。\nRPC 与其他 IPC 的对比 抽象层次：RPC 将进程间的通信抽象为远程函数调用，隐藏了底层的通信细节。其他 IPC 机制（如共享内存、消息队列）则更接近底层的实现，开发者需要处理细节。\n透明性：RPC 的一个特点是它使得进程间通信看起来像是本地函数调用，隐藏了通信的复杂性。其他 IPC 机制如管道、消息队列等则不提供这样的抽象，开发者需要明确处理消息的发送、接收和同步。\n效率：共享内存通常是最快的 IPC 方式，因为数据无需复制。然而，RPC 的透明性和易用性往往带来一些额外的性能开销，尤其是在涉及跨网络通信时。\n其他 IPC 示例和场景 文件锁（File Locking）：多个进程可以通过文件锁机制协调对文件的访问，避免数据竞争。这不是直接传递数据的 IPC，但是一种协调多进程访问共享资源的方式。\nDBus：这是 Linux 系统中常用的一种消息总线机制，提供了一个高层次的消息传递平台，允许系统中的应用程序和服务进行通信。\n总结：\nRPC 是一种特殊的 IPC 实现方式，通过封装函数调用的形式，简化了进程间的通信。 IPC 包含多种方式，包括管道、消息队列、共享内存、信号等，每种方式都有自己的使用场景和性能特点。 L4 微内核中的优化，如 call 和 sendrecv，是为提高 RPC 性能而设计的，减少了频繁的用户态和内核态切换。 Run Linux on top of L4 micro kernel 前一节对于IPC的优化使得人们开始认真考虑使用微内核替代monolithic kernel。\n然而，这里仍然有个问题，即使IPC很快了，操作系统的剩余部分从哪里去获取？现在的微内核大概只有一个完整操作系统的百分之几，我们该怎么处理操作系统剩下的部分？\n这个问题通常会在一些有着相对较少资源的学校研究项目中被问到，我们需要从某个地方获取到所有这些用户空间服务。\n实际上在一些特殊的应用场合，以上的问题并不是问题，比如说我们运行的一些设备的控制器，例如车里的点火控制器，只运行了几千行代码，它并且不需要一个文件系统，这样我们就只需要很少的用户空间内容，微内核也特别适合这种应用程序。\n但是微内核项目发起时，人们非常有雄心壮志，人们想的是完全替换操作系统，人们希望可以构建一些运行在工作站，服务器等各种地方的微内核操作系统，并取代大的monolithic kernel。\n对于这种场景，你需要一个传统操作系统所需要的所有内容。\n一种可能是，重新以微内核的方式，以大量的进程实现所有的内容。实际上有项目在这么做，但是这涉及到大量的工作。\n具体的说，比如我想要使用笔记本电脑，我的电脑必须要有emacs和我最喜欢的C编译器，否则我肯定不会用你的操作系统。\n这意味着，微内核要想获得使用，它必须支持现有的应用程序，它必须兼容或者提供相同的系统调用或者更高层的服务接口，它必须能够完全兼容一些现有的操作系统，例如Unix，Linux，这样人们才愿意切换到微内核。\n所以这些微内核项目都面临一个具体的问题，它们怎么兼容一些为Linux，Windows写的应用程序？对于论文中提到的项目，也就是L4，对标的是Linux。\n与其写一些完全属于自己的新的用户空间服务，并模仿Linux，论文中决定采用一种容易的多的方法，其实许多项目也都采用了这种方法，也就是简单的将一个现有的monolithic kernel运行在微内核之上，而不是重新实现一些新的东西。\n这就是今天论文要介绍的内容。\n在今天论文 的讨论中，L4微内核位于底部，但是同时，一个完整的Linux作为一个巨大的服务运行在用户空间进程中。听起来有点奇怪，一般的kernel都是运行在硬件之上，而现在Linux kernel是一个用户空间进程。\n实际上，如你在QEMU上运行XV6时所见，内核也是运行在用户空间。Linux kernel不过就是一个程序，对其做一些修改它就可以运行在用户空间，所以现在Linux需要被修改。论文中提到需要对Linux的底层做一些修改，例如Linux中期望能直接修改Page Table的内容，读写CPU寄存器。Linux中一部分需要被修改以将它们改成调用L4微内核的系统调用，或者发送IPC，而不是直接访问硬件。\n但是Linux的大部分内容都可以不做修改而直接运行。所以按照这种方式，作为Linux的一部分，现在得到了文件系统，网络支持，各种设备驱动等等，而不需要自己实现这些。这里的实现方式是将Linux内核作为一个L4 Task运行，每一个Linux进程又作为一个独立的L4 Task运行。所以当你登录到Linux中时，你要它运行一个Shell或者terminal，它会在用户空间创建一个L4 Task来运行这个Linux程序。所以现在有一个Task运行Linux，以及N个Task来运行每一个你在Linux中启动的进程。Linux不会直接修改进程的Page Table，而是会向L4发送正确的IPC让L4来修改进程的Page Table。\n这里有很多小的改动，其中一个有意思的地方是，当VI想要执行一个系统调用时，VI并不知道它运行在L4之上，在上面的方案中，所有的程序都以为它们运行在Linux中。当VI要执行系统调用时，L4并不支持，因为VI要执行的是Linux系统调用而不是L4系统调用。所以对于Linux进程，会有一个小的库与之关联，这个库会将类似于fork，exec，pipe，read，write的系统调用，转换成发送到Linux kernel Task的IPC消息，并等待Linux kernel Task的返回，然后再返回到进程中。\n从VI的角度看起来好像就是从系统调用返回了。所以这些小的库会将系统调用转成发送到Linux kernel Task的IPC消息。这意味着，如果Linux kernel Task没有做其他事情的话，它会在一个recv系统调用中等待接收从任何一个进程发来的下一个系统调用请求IPC。\n这导致了这里的Linux和普通的Linux明显不同的工作方式。在普通的Linux中，就像XV6一样，会有一个内核线程对应每一个用户空间进程。当用户空间进程调用系统调用时，内核会为这个系统调用运行一个内核线程。并且，在普通的Linux中，如果内核在内核线程之间切换，这基本上意味着从一个用户进程切换到另一个用户进程。\n所以这里Linux kernel的内核线程以及当Linux完成工作之后要运行的用户进程之间有一对一的关系。在这里架构中，这种一对一的关系断了，这里的Linux kernel运行在一个L4线程中。\n然而，就像XV6一样，这个线程会使用与XV6中的context switching非常相似的技术，在与每个用户进程对应的内核线程之间切换。\n不过这些内核线程完全是在Linux中实现的，与L4线程毫无关系，唯一的L4线程就是运行了Linux kernel的控制线程。但是哪个用户进程可以运行，是由L4决定的。\n所以在这里的设置中，Linux kernel或许在内核线程中执行来自VI的系统调用，同时，L4又使得Shell在用户空间运行了。\n这在XV6或者Linux极不可能发生，在这两个系统中，活跃的内核线程和用户进程有直接的对应关系，而L4会运行它喜欢的任何Task。\n因为Linux kernel中的内核线程都是私有的实现，Linux可以同时执行不同阶段的多个系统调用，或许一个进程在它的内核线程中在等待磁盘，这时Linux可以运行另一个进程的内核线程来处理另一个进程的系统调用。\n你或许会想知道为什么不直接使用L4线程来实现Linux内的内核线程，或者说Linux为什么要实现自己内部的内核线程，而不是使用L4线程，答案是，\n在论文发表时，还没有用到多核CPU硬件，他们使用的是单核CPU硬件。所以在内核中同时运行多个内核线程并没有性能优势，因为只有一个CPU核，所以第二个线程不能执行，由于硬件的限制，一次只能执行一个线程。\n另一个或许是更强大的原因是，在论文发表时，他们使用的Linux版本并不支持将Linux kernel运行在多个CPU核上。\n所以他们使用的是旧版本的单核Linux，一次只能期望在内核中使用一个CPU，它并没有类似于XV6的spinlock，可以使得它能正确的在内核中使用多核。\n所以在Linux内核中使用多个L4线程并没有性能优势。如果一定要使用的话，在没有性能优势的前提下，又需要加入spinlock和其他的内容来支持并发。所以论文中没有在Linux内核使用L4线程。\n这种架构的一个缺点是，在普通原生的Linux中，存在大量复杂的线程调度机制，例如在不同进程上增加优先级，确保调度公平性等等。\nLinux可以在你的笔记本上运行这些机制，因为Linux控制了哪些进程可以运行在哪些CPU核上。\n但是在这里的架构中，Linux完全控制不了哪些进程可以运行，因为现在是L4而不是Linux在完成调度，这些进程都是被L4所调度。\n所以这里的架构失去了Linux的调度能力，这是这种架构的缺点，我相信L4的后续版本有一些方法能够让Linux通知L4调度器，来给某个进程更高优先级等等。\nL4 Linux性能分析 你应该问自己：通过论文 可以学到有关微内核的什么内容呢？\n对于我们来说，论文中有很多有趣的有关微内核是如何运行，有关Linux是如何运行的小的知识点，以及你该如何设计这么一个系统。\n但是论文并没有回答这个问题：微内核是不是一个好的设计？论文只是讨论了微内核是否有足够的性能以值得使用。\n论文之所以讨论这个内容的原因是，在论文发表的前5-10年，有一场著名的测试针对一种更早的叫做MACH的微内核。\n它也运行了与L4类似的结构，但是内部的设计完全不一样。通过测试发现，当按照前一节的架构运行时，MACH明显慢于普通的Unix。\n这里有很多原因，比如IPC系统并没有如你期望的一样被优化，这样会有更多的用户空间和内核空间的转换，cache-miss等等。\n有很多原因使得MACH很慢。但是很多人并不关心原因，只是看到了这个测试结果，发现MACH慢于原生的操作系统，并坚信微内核是无可救药的低效，几乎不可能足够快且足够有竞争力。很多人相信应该都使用monolithic kernel。\n今天的论文像是对于这种观点的一个反驳，论文中的观点是，你可以构建类似上一节的架构，如果你花费足够的精力去优化性能，你可以获取与原生操作系统相比差不多的性能。\n因此，你不能只是因为性能就忽视微内核。今天的论文要说明的是，你可以因为其他原因不喜欢微内核，但是你不能使用性能作为拒绝微内核的原因。\n达成这一点的一个重要部分是，IPC被优化的快得多了，相应的技术在18.5中提到过。\n论文的表二做了性能对比，运行在硬件上的原生Linux执行一个简单的系统调用getpid花费1.7us，对于上一节的实现，需要发送一个IPC request，并获取一个IPC response，以实现getpid系统调用，这需要花费接近4us，这是原生Linux的两倍多。\n主要是因为这里有两倍的工作，这里涉及到两次用户空间到内核空间的切换，而不是一个简单的系统调用。\n这也说明L4已经将这种基于IPC的系统调用的代价降到了最低，也就是2倍于一个原生Linux的系统调用。因此，它可以做的大概与你期望的一样好。\n当然这里的系统调用仍然只有原生Linux一半的速度。现在还不清楚这是否是一个灾难，还是说并没有问题。\n如果你执行大量的系统调用或许就是个问题；如果你执行了相对较少的系统调用，或者系统调用本身就有很多工作，或者你的系统调用比getpid要复杂的多，这又或许不是个问题。\n论文中通过使用AIM做的测试结果，给出了答案。测试结果在论文的图8。\nAIM会执行各种系统调用，例如read/write文件，创建进程等等。\n从图8可以看出，在AIM设置的一个更完整的应用中，基于L4的Linux之比原生Linux慢几个百分点。\n因此，理想情况下你可以期望你想要运行在计算机上行的应用，如果在L4+Linux上运行可以与运行在原生操作系统上一样快。\n因为可以以近似原生Linux的速度运行，所以你们现在应该认真对待微内核。图8是一个非常不错的结果，有点超出预期。\n让时间快进20年，如果之前所说，现在人们实际上在一些嵌入式系统中使用L4，尤其在智能手机里有很多L4实例在运行，它们与Unix并没有兼容性。\n在一些更通用的场景下，像是工作站和服务器， 微内核从来没有真正的流行过，并不是因为这里的设计有什么问题，只是为了能够吸引一些软件，微内核需要做的更好，这样人们才会有动力切换到微内核。\n对于人们来说很难决定微内核是否足够好，这样才值得让他们经历从现在正在运行的Linux或者其他系统迁移到微内核的所需要的各种麻烦事。\n所以，微内核从来没有真正流行过，因为它们并没有明显的更好。\n另一方面来看，微内核的很多思想都有持久的影响。\n人们实现了更加灵活和有趣的方法来在微内核上使用虚拟内存。这些复杂的多的接口导致了mmap这样的系统调用合并到了例如Linux的主流操作系统中。 论文中将一个操作系统作为一个用户程序运行另一个操作系统之上，今天以另一种方式非常流行的存在：在Virtual Machine Monitor上运行虚拟机。这种方式在各种场景，例如云主机上，都有使用。 为了让内核能够具有一个用户空间服务一样的可扩展性，在Linux中演进成了可加载的内核模块，这使得你可以在线修改Linux的工作方式。 当然，这里基于IPC的Client-Server支持，也在macOS有所体现，macOS中也有好用的IPC。 以上就是这节课的所有内容，有什么问题吗？\n学生提问：论文4.3 Dual-Space Mistake能介绍一下吗？\nRobert教授：这里稍微有点复杂。\n这里的一部分背景是，论文发表时的Linux，甚至直到最近，当你运行在x86上，且运行在用户空间时，使用的Page Table同时会有用户空间的内存Page，以及所有的内核内存Page。\n所以当你执行系统调用，并跳转到内核中，内核已经映射到了Page Table中，因此不需要切换Page Table。\n所以当你执行一个系统调用时，代价要小得多，因为这里没有切换Page Table。\n如果你回想我们之前介绍的内容，trampoline代码会切换Page Table（注，也就是更新SATP寄存器，详见6.5）。\n这是个代价很高的操 作，因为这会涉及到清除TLB。所以出于性能的考虑，Linux将内核和用户进程映射到同一个Page Table，进而导致更快的系统调用。\n论文中期望的是，当用户空间进程向Linux发送一个系统调用，并且Linux的内核线程在处理系统调用，Page Table也包含发送系统调用的进程的所有虚拟内存映射，这会使得作为系统调用参数传入的虚拟内存地址查找更加的简单。但是为什么这里不能很好工作？\n首先，L4并不知道这里的任何具体实现，在L4的眼里这就是两个进程。\n当你从一个进程发送IPC到另一个进程，L4只是会切换Page Table。\n由于现在Linux的系统调用是基于L4实现的，没有办法在系统调用的过程中保持Page Table，因为L4在两个进程间切换时总是会切换Page Table。\n所以这里不能得到系统调用时不切换Page Table带来的性能优势。\n我认为这里希望得到可以在内核中直接使用用户空间的虚拟内存地址的便利，但是这意味着在Linux内核中需要知道是在执行哪个进程的系统调用，并使用那个进程的Page Table。\n当然L4并不知道这里的细节，它只是给每个进程关联了一个Page Table。\n所以L4只会给Linux关联一个Page Table，Linux并没有办法在处理不同进程的系统调用时使用不同的Page Table。\n为了解决这个问题，论文中为每个进程都做了共享内存拷贝，每一个共享内存拷贝都有内核的所有内存，所以都有相同的数据结构。\n因为每个进程都有一个kernel task与之关联，因此可以使得L4切换到合适的Page Table同时包含了进程和内核的内存。\n我认为这里可以工作，但是忘记了这里是否会很慢之类的，因为这里有大量的任务。\n这是个复杂的故事，我不知道解释清楚了没有。\n学生提问：看起来一些任务更适合在内核中，但是内核的方案中，要么所有东西都在内核要么都不在。所以要么你有一个monolithic kernel可以完成所有的事情，要么有个micro kernel什么也不做。我认为虚拟内存、文件系统和一些其他的事情在内核中做会非常的有效。不能有些系统具备有一些功能，然后你又可以选择用不用这些功能吗？\nRobert教授：所有你说的都有非常有道理。\n实际上有很多微内核相关的项目都构建了各种hybrid内核，MACH有一些不同的版本，其中一些就是hybrid内核，这些内核的核心是包括了IPC的微内核，同时在内核中又是一个完整的Unix系统，比如MACH 2.5就是这样一个hybrid内核，其中一些东西是按照微内核的方式构建，一些东西又是按照Unix方式构建。\n现代的macOS也以与你描述类似的方式构建，macOS本身是个完整的操作系统，包含了文件系统，同时也很好的支持了IPC和其他用来构建微内核的东西。Google Fuchsia也实现了一些这里的想法。\n","date":"2024-10-03T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08118os-organization/bg_hu_44c49d43b6364b92.jpg","permalink":"https://echudet.github.io/p/mit6s08118os-organization/","title":"MIT6S081：18OS organization"},{"content":"Lecture 17 Virtual memory for applications 课前预习 论文速览 论文链接 在简介部分提到，论文的核心讨论点是：虚拟内存（Virtual Memory）不仅仅可以用来扩展操作系统的地址空间，还能用于很多其他的目的，例如内存共享、写时复制（copy-on-write）以及处理保护违规。作者探讨了操作系统应提供给用户程序的虚拟内存基本原语，特别是这些原语如何支持不同的用户级应用。\n在虚拟内存原语部分提到，论文中提到的关键虚拟内存操作包括：\ntrap： 处理用户模式下的页故障。 prot1 \u0026amp; protN： 降低单页或多页的访问权限。 unprot： 增加页面的访问权限。 dirty： 返回自上次检查以来被修改的页面列表。 map2： 将同一个物理页面映射到不同的虚拟地址。 这些原语旨在支持中粒度的同步操作和多处理器系统上的高效内存管理。\n在虚拟内存应用部分，论文展示了多种利用虚拟内存原语的应用，包括：\n并发垃圾回收： 通过页面保护技术检测未扫描的对象，以实现垃圾收集器与程序的同步。 共享虚拟内存： 在多计算机网络上实现页面级别的内存一致性。 并发检查点保存： 通过设置页面为只读，允许程序在不间断的情况下进行内存保存操作。 世代垃圾回收： 使用页面保护机制减少对老年内存段的修改检查开销。 持久性存储： 将内存映射文件作为跨程序执行周期的数据结构，使用虚拟内存实现持久性和事务管理。 扩展寻址能力： 用虚拟内存技术处理超过32位地址空间的大量数据。 数据压缩分页： 使用压缩技术减少磁盘检索时间。 堆溢出检测： 使用页错误机制检测堆溢出，而无需手动检查每次内存分配。 在虚拟内存原语的性能部分，作者对多个操作系统（例如 Ultrix、SunOS 和 Mach）进行了基准测试，结果显示各系统在处理页面保护、陷阱处理和转换缓冲区（TLB）一致性上的性能存在显著差异。\n在系统设计考虑部分提到，操作系统在设计时应重视以下几个方面：\nTLB一致性： 在多处理器系统中，确保高效的TLB一致性对于减少过时TLB条目带来的非法访问至关重要。 页面大小： 较小的页面可以减少故障处理延迟，但具体选择应根据应用的需求进行调整。 访问受保护页面： 用户模式服务例程可能需要在保持页面保护的情况下访问页面，操作系统应支持这种访问方式。 在结论部分作者指出，现代硬件和操作系统应将虚拟内存视为用户级应用的关键组件。这些原语能够在系统规模不断扩大时保持高效。操作系统设计中必须重视虚拟内存机制的高效性，如TLB一致性、页面大小灵活性和快速的页面保护机制。\nGenerational Garbage Collection（分代垃圾回收） 虚拟内存保护的重要应用之一是分代垃圾回收（Generational Garbage Collection），这是一种高效的算法，依赖于动态分配记录的两个特性，通常用于 LISP 和其他编程语言中：\n年轻记录更有可能迅速死亡，而老记录则有更大的存活可能。如果一个记录已经存活了很长时间，那么它很可能继续存活；相反，一个新的记录更可能只是一个临时的中间值。 年轻记录通常指向老记录，因为在 LISP 和函数式编程语言中，分配一个新记录时，它通常会被初始化为指向已经存在的记录。 根据特性1，大部分垃圾回收器的工作应集中在较年轻的记录上；根据特性2，垃圾回收器可以利用这一特性来优化回收效率。分配的记录会被保存在内存中的不同区域 $G_i$，称为“代”（Generations）。每代中的记录具有相似的年龄，$G_i$ 代中的所有记录都比 $G_{i+1}$代中的记录更老。基于观察，较低代 $G_i$ 通常不会指向较高代 $G_j$，因此垃圾回收器通常只在最年轻的代中进行回收，因为那里含有最多的垃圾。\n为了回收某一代，回收器需要知道所有指向这一代的指针。这些指针可能存在于机器寄存器、全局变量或堆栈中。由于年轻记录几乎不会指向老记录，通常只有对现有记录的赋值操作才可能打破这一假设。因此，垃圾回收器需要检查每次修改，以确保不会破坏这一规则。\n要检测这种指针的赋值操作，通常有以下两种方法：\n使用专用硬件支持，进行修改检测。 通过编译器检查赋值，需要执行额外的指令（大约2条或更多）。幸运的是，LISP、Smalltalk 等语言中的非初始化赋值相对较少，因此额外的检测开销约占总执行时间的5-10%。 虚拟内存硬件可以通过“脏页”机制来检测对老对象的修改。如果操作系统提供了“脏页”（dirty）功能，垃圾回收器可以检查被修改的页面，找出从老代到新代的指针，并进行处理。如果没有这种服务，垃圾回收器可以使用页面保护机制：将老代的页面设置为只读，任何对它们的写操作都会引发异常。用户模式的异常处理器可以将触发异常的页面添加到列表中供垃圾回收器扫描，之后页面必须被解除保护以允许写入操作。\n这一算法的多个变体表现出良好的性能。随着堆和内存的增大，这种机制开始优于其他技术。\n该技术依赖于以下虚拟内存原语：\ntrap：捕获指针赋值异常。 protN：将多个页面设置为只读。 unprot：解除页面的只读保护。 此外，由于用户模式处理器处理页面的时间与页面大小无关，而垃圾回收器扫描页面的时间与页面大小成正比，因此使用较小的页面大小可能更为合适。\n这部分总结了分代垃圾回收的工作原理及其如何利用虚拟内存原语提高效率。\n在**Generational Garbage Collection（分代垃圾回收）**部分中，指针起到了连接不同代对象的作用。新生成的对象（新代）往往会指向已经存在的老对象（老代）。论文提到，这种指向老代的指针行为在分配时很常见，而从老代指向新代的指针是较为罕见的。为了确保在垃圾回收过程中能够有效处理这些异常情况，需要专门监控这些指针变化。\n分代垃圾回收机制中，垃圾收集器重点处理新生成的对象，因为这些对象更可能迅速变为垃圾。为了高效回收，垃圾收集器只需要知道指向新代（年轻代）的指针，这些指针可能存在于寄存器、全局变量、或堆栈中。然而，根据观察，老代几乎不会有指针指向新代。唯一的可能性是通过赋值操作使得老代指向新代。\n因此，为了检测这种从老代指向新代的异常赋值操作，系统会在修改堆对象时进行检查，以确保不违反“新代指向老代”的假设。如果检测到从老代指向新代的指针，垃圾收集器需要做进一步处理。操作系统或硬件可以通过脏页机制（dirty page）来辅助这个过程：垃圾收集器可以扫描那些被标记为“脏页”的内存，找出其中的老代指向新代的指针并处理这些指针。\n如果操作系统没有提供脏页机制，垃圾收集器可以通过页面保护机制来实现这一目的：通过将老代的页面设置为只读模式，任何对这些页面的写操作都会引发页错误（trap）。用户态的错误处理程序会捕获这些错误，将涉及的页面添加到待扫描列表中，并解除该页面的只读保护以继续允许写操作。最后，在垃圾回收时，收集器会扫描这些列表中的页面，以查找并处理可能指向新代的指针。\n“找出从老代到新代的指针，并进行处理”，具体来说，这个过程如下：\n检测指针变动：当老代对象尝试指向新代时，会通过页面保护或脏页机制检测到该指针变动。 处理指针：垃圾收集器扫描相关页面，识别出从老代指向新代的指针。 解除保护：识别和处理后，解除该页面的只读保护，恢复其正常的读写权限。 这一机制允许系统高效地处理指针赋值带来的潜在错误，同时优化了垃圾回收的效率，尤其是在内存增大的情况下。\nPersistent stores(持久存储) 持久存储（Persistent Stores）是用于创建超越单个程序调用的动态分配堆。程序可以遍历和修改数据结构、提交更改，甚至可以在不进行永久更改的情况下放弃这些更改。\n在持久存储中，指针遍历（即读取操作）的设计与在主存储器中的数据读取操作（fetch）一样快速。这是因为持久存储通常被映射到磁盘文件上，从而允许在访问存储的新部分时触发页面错误（类似于内存中的正常操作）。\n因此，就读取数据而言，持久存储确实可以像正常的内存获取操作一样高效。然而，写入操作则需要额外处理，例如跟踪“脏页”（dirty pages）和提交更改到磁盘，这可能会引入一定的延迟，具体取决于实现方式。\n总结一下，从持久存储中读取数据的效率可以与普通的内存获取操作一样迅速，但写入操作可能会因为需要提交更改而稍有延迟。\nExtending addressability 在“扩展寻址能力”部分，作者讨论了如何处理持久存储增长到超过处理器地址空间的情况。\n由于现代磁盘驱动器（尤其是光盘）可以存储大量数据，持久存储有可能增长到包含超过232232个对象，因此32位的地址空间无法足够支持。虽然单个程序运行期间可能访问的对象数量不会超过232232，但整个数据库的规模可能会超出这一限制。\n为了解决这个问题，可以修改持久存储的机制，使得在内存中的对象使用32位地址，而在磁盘上的对象使用64位地址。具体的做法是，将磁盘上的页面设定为内存页面的两倍大小。当从磁盘加载页面到内存时，64位的磁盘指针将通过翻译表转换为32位的内存指针。当这些32位指针首次被解引用时，可能会触发页面错误，页面错误处理程序会从磁盘中加载另一个页面，并将其转换为短指针。\nExtending addressability翻译表 翻译表只为单次执行过程中访问的对象创建条目，这就是为什么32位的指针在内存中仍然足够使用。内存中的指针可以指向尚未被访问的页面，而这些页面还未被分配到内存中，但翻译表中会有条目指出它们在磁盘上的位置。\n这个方案的概念源自Smalltalk-80中的LOOM系统，而通过页面错误机制实现这一点是较新的方法。\n关于“翻译表只为单次执行过程中访问的对象创建条目，这就是为什么32位的指针在内存中仍然足够使用”的理解，我来帮您详细解释一下：\n这里的核心观点是：32位指针在内存中仍然足够使用，因为在任何一次程序运行时，不会访问超过232232个对象。也就是说，虽然持久存储的大小可能超过232232个对象（这时需要64位指针来表示这些对象在磁盘上的位置），但在单次程序执行中，程序通常只会访问这些对象中的一小部分。因此，在内存中，使用32位指针就足够了。\n翻译表的作用是：当从磁盘加载某个页面时，64位的磁盘指针通过翻译表转换成32位内存指针。只有访问到特定的对象时，才需要为该对象分配内存地址，并在翻译表中创建相应的条目。这意味着翻译表只包含当前程序运行中实际访问的对象。由于每次程序运行时访问的对象总数远远小于232232，因此32位指针足以处理这些内存中的对象。\n因此，32位指针在内存中足够使用，是因为翻译表动态地只为当前执行过程中访问到的对象分配内存和创建条目，内存中不需要同时容纳所有的对象。\nData-compression paging **数据压缩分页（Data-compression paging）**如下：\n在典型的链式数据结构中，许多字会指向相邻的对象，许多字是空（nil）值。那些包含整数而不是指针的字通常包含小整数或零。简而言之，平均每个字的熵很小；此外，垃圾收集器可以将互相指向的对象放置在邻近位置，从而将每个字的熵减少到大约7个比特。\n因此，通过使用数据压缩算法，一个由32位字组成的页面可以被压缩到大约四分之一页面大小。与其将最近未使用的页面直接分页到磁盘，不如先将它们压缩后放回主存储器中【36】。这样，当这些虚拟页面再次需要时，解压缩可能比从磁盘中取回要快得多。当然，经过长时间未使用后，压缩后的页面仍然可以被发送到磁盘中。\n当然，数据压缩分页可以在操作系统中以透明方式实现，对用户进程完全不可见【28】。然而，由于垃圾收集器可以移动对象以最小化它们的熵，如果用户进程能够控制如何以及何时进行压缩，可能会获得更好的效果。\n这个算法需要以下支持：\ntrap：用于捕捉页面访问故障。 prot1（或protN）：用于保护或解除保护某个页面（或一组页面），这通常需要谨慎的缓冲操作。 unprot：用于解除对页面的保护。 此外，还需要判断哪些页面是最近没有被使用的。可以通过偶尔保护页面（测试是否有引用）来完成，也可以通过操作系统和硬件的帮助来实现这一点。\n通过一个例子来解释“数据压缩分页”的过程。\n假设我们有一个应用程序使用链式数据结构来管理对象，这些对象相互引用，并且很多字段都是空值（nil），或者是常见的小整数（如0）。这些对象存储在内存的多个页面中，每个页面大小为4KB。随着程序的运行，有些页面可能长时间不被访问，因此可以将这些页面从内存中移出。\n在传统的虚拟内存系统中，未使用的页面会被直接分页到磁盘，等到需要时再从磁盘取回。但如果我们使用“数据压缩分页”的方法，这个过程会有所不同。\n压缩未使用的页面：当系统检测到某个页面长时间未被访问时，而不是直接将其分页到磁盘，系统首先对页面进行数据压缩。由于页面中的很多数据是空值或者小整数，这些数据的熵很低，可以使用数据压缩算法将这些4KB的页面压缩到大约1KB大小。 保留在内存中：压缩后的页面不会立即被分页到磁盘，而是依然保存在主存储器中，只是占用的空间大大减少了。这样，当这些页面再次需要时，程序可以直接从内存中解压缩页面，而不需要从磁盘读取。解压缩的速度通常比从磁盘读取要快得多。 需要时解压缩：当程序再次访问到这些页面时，系统会检测到该页面已经被压缩，这时只需进行快速的解压缩操作，就可以将页面恢复到原来的状态，让程序继续运行。 最终分页到磁盘：如果页面长时间不被访问，系统仍然可以选择将压缩后的页面最终分页到磁盘，但通过这种方法，系统可以减少频繁的磁盘访问，从而提高效率。 这个过程的一个关键优势在于：对于内存中那些长时间不被访问的页面，通过压缩操作可以在不牺牲性能的前提下释放大量内存。而且，由于解压缩比从磁盘读取快得多，这样的机制比传统的分页系统更高效。\nHeap overflow detection 进程或线程的栈需要保护，以防止溢出访问。一种常见且实际的技术是将栈顶上方的页面标记为无效或不可访问。任何对这些页面的内存访问都会导致页面错误，操作系统可以捕捉该错误并通知用户程序栈溢出。在大多数Unix实现中，栈页面在首次使用之前不会被分配；操作系统对页面错误的响应是分配物理页面，标记为可访问，并继续执行而不通知用户进程（除非资源限制被超过）。\n这种技术需要trap（捕获异常）、protN（减少多个页面的访问权限）和unprot（增加页面的访问权限）。但由于页面错误的发生率很低（大多数进程不会使用太多的栈空间），效率并不是一个问题。\n同样的技术可以用于检测垃圾回收系统中的堆溢出【2】。通常，堆溢出通过在每次内存分配时执行比较和条件分支来检测。通过让用户进程在一块由保护页面（guard page）终止的内存区域中分配新记录，可以消除比较和条件分支。当可分配的内存区域用完时，页面错误陷阱会调用垃圾回收器。通常可以安排不需要重新调整内存保护，因为在回收后可以重新使用相同的分配区域。因此，这项技术只需要prot1（减少单个页面的访问权限）和trap。\n在这里，陷阱的效率非常重要。某些语言的实现可能每50条指令就分配一个新单元。在分代垃圾回收器中，分配区域的大小可能非常小，以便让最年轻的一代完全适合于数据缓存。例如，一个64KB的分配区域可以容纳16,000个8字节的链表单元。在一个非常频繁分配的系统中（如将激活记录存储在堆上的系统），这样一小部分数据将是存活的，因此垃圾回收所需的时间本身将非常短。\n因此，我们可以得出：\n堆溢出之前执行的指令数量：$64K/8×50=400K$\n使用比较和分支的开销指令数量：$64K/8×2=16K$\n如果处理一个陷阱需要1200个周期（这是典型的情况，见第4节），那么这种技术可以将开销从4%减少到0.3%，这是一个值得的节省。如果陷阱处理时间更长，那么这种技术将不那么高效。\n由于还有其他好的技术可以减少堆限制检查的开销，比如在展开的循环中结合连续分配的限制检查，因此虚拟内存的这一应用可能是本文讨论的所有应用中最不有趣的。\n文中提到，传统的堆溢出检测通常是在每次内存分配时，通过比较和条件分支（compare and branch）的方式来检测是否发生了堆溢出。这种方法需要在每次分配时额外执行两条指令来进行比较和条件分支，因此在系统中，这个过程会增加额外的指令开销。\n而通过使用trap机制，这种额外的指令开销可以被消除。系统可以通过设置一个保护页面（guard page），当内存分配超出可分配的范围时，程序会触发页面错误（page fault），然后调用垃圾收集器来处理这个溢出。这就不再需要每次分配时进行比较和条件分支的操作，减少了指令数量。\n指令数量的关系计算：\n在传统的比较和分支方法中，每次内存分配都需要两条指令来进行检查。 在使用trap的情况下，这些检查可以通过页面错误来触发，减少了频繁的比较和分支操作。 trap所需的周期数是指，当程序触发页面错误时，操作系统或硬件需要花费的时间来处理该错误。文中提到，处理一次页面错误的trap大约需要1200个周期。相比之下，程序执行普通指令（如加法）所需的时间要少得多。例如，在一个典型的处理器上，执行一条指令可能只需要1到几个周期。\n文中提到：\n如果没有trap机制，堆溢出检测每次都需要比较和分支，这会导致**4%**的性能开销。 使用trap后，虽然每次页面错误处理需要大约1200个周期，但它只会在堆溢出时触发，而不是在每次分配时进行比较和分支，因此可以将性能开销减少到0.3%。 trap的原理是利用页面保护机制。当程序尝试访问一个受保护的页面时，系统会捕获这个错误，并触发一个陷阱（trap）。操作系统或用户态的错误处理程序会处理这个错误，然后可能会解除页面的保护，允许程序继续访问该页面。这个过程的关键在于减少程序在正常情况下的指令开销，而只在必要时通过页面错误机制处理特殊情况。\n在文中的例子中，垃圾收集器会在分配区域末端设置一个保护页面。当程序试图超出分配区域时，系统触发页面错误，这时垃圾收集器被调用，清理堆内存并允许程序继续运行。\nVM primitive performance 几乎我们在本文中描述的所有算法都属于以下两类之一。第一类算法会将页面批量保护起来（使其无法访问），然后在每次页面错误（page-fault）陷阱发生时，逐个解除这些页面的保护状态。第二类算法则是对页面进行单独的保护和解除保护操作。由于 protN（批量保护）、trap（陷阱）和 unprot（解除保护）始终是一起使用的，如果其中一个操作效率很高，而其他操作非常慢，那么该操作系统的性能也不会很好。\n我们对整体的用户模式虚拟内存性能进行了两项测试。第一项测试是测量 prot1、trap 和 unprot 的总耗时，方法是对以下基准程序执行100次：\n访问一个随机被保护的页面，然后在错误处理程序中保护另一个页面，并解除对发生错误的页面的保护。 为了得到更准确的时间测量，这个过程重复了100次。\n第二项测试是测量 protN、trap 和 unprot 的总耗时，基准程序的步骤如下：\n保护100个页面； 以随机顺序访问每个页面； 在错误处理程序中解除对发生错误的页面的保护。 在计时开始之前，这两个程序都会对每个页面进行写操作，以消除填充缓存和TLB（转换后备缓冲区）所带来的暂时性影响。\n我们对几种操作系统的性能进行了比较，包括 Ultrix、SunOS 和 Mach，分别在不同平台上运行这些基准程序。为了校准测量结果，我们还给出了单条指令（add）的执行时间，该时间是通过一个包含20条指令的循环（包括18个加法操作、一个比较和一个分支）测量出来的。我们还给出了一个 trap（陷阱处理程序）的时间，它不改变任何内存保护；这个时间对于堆溢出检测是有用的。结果见表1。需要注意的是，这个基准测试不是一个“整体操作系统吞吐量”的测试【27】，不应该受到磁盘速度的影响；它测量的是用户级程序对虚拟内存服务的CPU处理性能。\n我们还尝试在同一进程中使用两个不同的虚拟地址映射同一个物理页面，分别使用 SunOS 和 Ultrix 的共享内存操作（shmop），以及 Mach 中的 vmamap。SunOS 和 Mach 支持此操作，但 Ultrix 不允许在同一进程中使用 shmat 连接同一个共享内存对象到不同地址。\n显然，不同操作系统在相同硬件上的性能差异很大。这表明某些或所有系统中都有很大的改进空间。此外，一些操作系统的某些版本在 mprotect 调用后没有正确刷新 TLB，这表明许多操作系统实现者并没有认真对待这个功能。\n重要的是，这些操作系统服务的效率要高。这里的论点比“高效是好的”这个空洞的说法更具体。对于磁盘分页而言，页面错误通常意味着要等待20毫秒才能让磁盘旋转到正确的扇区；因此，3至5毫秒的错误处理开销几乎不会被视为错误处理延迟的贡献者。但是在本文中描述的算法中，错误将完全由CPU处理。例如，我们已经实现了一个垃圾收集器，在20 MIPS的机器上，处理一个页错误需要执行大约10条指令来完成对 to-space 的每个字（word）的操作。在一个页大小为4096字节（1024字）的系统中，计算处理时间大约为500微秒。如果操作系统的错误处理和页面保护开销为1200微秒（如平均所示），那么操作系统显然成为瓶颈。\n如果程序表现出良好的访问局部性，那么垃圾收集产生的错误将很少，操作系统的开销就不那么重要。但对于实时程序，它们必须满足严格的延迟要求，即使偶尔出现的“慢错误”也会造成问题。例如，如果客户端程序不能被中断超过1毫秒，那么500微秒的错误处理计算时间几乎没有给操作系统提供额外1200微秒的空间！(这个问题在我们考虑多个连续错误时变得更加复杂；详见【11】)\n为了比较不同架构上的虚拟内存基元性能，我们用处理器速度对测量值进行了归一化。图4显示了各处理器在保护一个页面、发生错误以及解除保护一个页面所需时间内可以执行的 add 指令数。\n我们的基准测试表明，不同操作系统对虚拟内存基元的实现效率存在很大的差异。在 NX/2 操作系统上运行的基于 Intel 80386 的机器【29】（一种用于 iPSC/2 超立方体多计算机的简单操作系统）在我们的基准测试中表现最佳。其归一化基准性能约为表现最差的系统（在 Sparcstation 上运行的 Mach）的十倍。显然，这些基元没有固有的原因一定会慢。硬件和操作系统设计者应将内存保护性能视为设计过程中需要权衡的一个重要方面。\nTLB的回顾 在现代计算机系统中，**转换后备缓冲区（TLB, Translation Lookaside Buffer）**是一个缓存，用来存储虚拟地址到物理地址的映射。它的作用是加速虚拟内存系统中地址转换的过程，避免每次都需要通过页表（page table）来查找地址映射。\n当程序访问一个虚拟地址时，处理器首先会在TLB中查找对应的物理地址。如果找到（TLB命中），处理器可以快速访问数据；如果找不到（TLB未命中），系统会去页表中查找并更新TLB。\n在进行内存访问控制时，可能会改变某些页面的访问权限。比如，程序可能设置某个页面为可访问或不可访问。此时，TLB 中缓存的映射信息可能已经过时，而这会影响系统的行为。\n当页面被设为“更易访问”时，TLB 中的过时信息是无害的。\n假设之前一个页面是不可访问的（例如，页面被保护以防止读写），并且TLB中缓存了这个页面的不可访问状态。现在，如果我们将该页面设为可访问的，但TLB 中依然存在不可访问的过时条目，那么最坏的情况是系统会因为TLB未更新，错误地认为页面不可访问，从而触发一次TLB未命中或者页错误（page fault）。此时，系统会发现页面实际上是可访问的，于是会更新TLB并允许访问。虽然会多一次错误处理，但这不会影响程序的正确性，因为页面本身现在是可访问的。\n当页面被设为“不易访问”时，TLB 中的过时信息可能导致非法访问。\n如果页面原本是可访问的，而现在被设置为不可访问，但TLB 中依然缓存着可访问的过时映射，那么程序仍可能通过过时的 TLB 条目访问该页面。这时，系统没有触发访问保护机制，程序可能进行非法读写操作，导致数据不安全或者程序崩溃。因此，这种情况是有害的。\n举个例子：内存页面权限更改\n场景1：页面设为更易访问\n假设有一个页面 P，最初是不可写的（只读），程序试图写入该页面时触发了保护机制，系统将该页面设为可写。但由于TLB中还缓存着页面 P 的不可写权限（过时信息），程序接下来的写操作可能触发TLB未命中。系统会发现这是过时信息，重新查找并确认页面 P 现在是可写的，于是更新TLB条目，并允许程序写入。这只是一次额外的查找，最终程序能够正常运行，因此无害。\n场景2：页面设为不易访问\n现在设想页面 P 原来是可写的，但为了保护数据，系统将该页面设为只读。如果 TLB 中仍然缓存着原来的可写权限，那么程序接下来的写操作将绕过系统的保护机制，导致非法修改。由于TLB条目没有更新，操作系统不会立即发现这个问题，直到后来数据不一致或崩溃。因此，这种过时的 TLB 信息会导致非法访问，必须避免。\n所以，\n设为更易访问时：过时的 TLB 条目导致一次额外的处理（如TLB未命中或页错误），但不会影响数据安全或程序正确性。\n设为不易访问时：过时的 TLB 条目会导致非法访问，破坏系统保护机制，必须通过刷新 TLB 来保证页面访问权限的正确性。\n这也是为什么在内存访问权限变得更严格（如设为不可访问）时，必须确保所有处理器的 TLB 中的信息一致。否则，就可能导致安全漏洞或程序异常。\nTLB Consistency 许多在本文中介绍的算法，都会以批量方式减少内存的可访问性，并在每次访问时逐页增加内存的可访问性。这对于并发垃圾收集、分代垃圾收集、并发检查点、持久存储和扩展寻址能力来说尤其如此。\n这对多处理器来说是一件好事，因为它能够解决转换后备缓冲区（TLB）一致性问题。当一个页面被设为更易访问时，TLB 中的过时信息是无害的，最多只会导致一个虚假的、容易修复的 TLB 未命中或 TLB 错误。然而，当页面被设为不易访问时，TLB 中的过时信息可能导致非法访问此页面。为了防止这种情况，有必要从每个可能包含该页面的 TLB 中刷新此页面。这个**“清除（shootdown）”**过程可以通过软件完成，即通过中断每个其他处理器并要求它们刷新 TLB 中的该页面，或者通过各种总线系统在硬件上完成【7，32】。\n如果有许多处理器需要中断，软件清除过程可能会非常昂贵。我们解决清除问题的方案是批量清除；覆盖多个页面的清除成本并不比单个页面清除的成本高多少，当处理多个页面时，清除的成本可以被分摊，单个页面的成本变得可以忽略不计。本文中描述的那些以批量方式保护页面的算法“无意中”利用了批量清除的优势。\n批量操作的概念是根据我们在本文中描述的算法结构自然得出的，但它也可以用于解决传统的磁盘分页中的清除问题。在磁盘分页中，为了释放物理页面供其他虚拟页面使用，页面被设置为不易访问（即“分页出去”）。如果操作系统能够维持一个较大的未使用物理页面的预留空间，那么它可以以批量方式执行分页操作（以补充预留空间）；这样可以将清除成本分摊到整个批量操作中。因此，尽管有人声称软件解决方案表现良好，但可能需要硬件支持【7】，通过批量处理，硬件支持可能就不再必要。\n这段内容主要讨论了如何通过批量处理页面访问控制来优化 TLB 的一致性问题，特别是在多处理器系统中批量处理能够显著减少系统开销。\n它是如何应用在传统的磁盘分页问题中的。\n首先，文中提到的“批量操作”是指同时对多个页面执行某种操作（例如设置访问权限或执行清除），而不是一次只对单个页面进行操作。这种批量操作在处理大量页面时更有效，因为可以将多个页面的处理开销合并，降低每个页面操作的成本。\n在传统的虚拟内存系统中，当内存不足时，操作系统会通过磁盘分页将不经常使用的物理内存页面分页到磁盘（即“交换出去”）。当需要这些页面时，再从磁盘读取回内存。这个过程中涉及到将内存页面标记为“不易访问”或“不可访问”，以释放物理内存。\n问题： 当页面被设置为不可访问时，系统需要确保这些页面的旧访问权限不会留在TLB中。否则，处理器可能仍然使用过时的权限，导致非法访问这些已分页出去的页面。为了防止这种情况发生，操作系统需要将这些页面从各个处理器的 TLB 中清除（即清除操作或 shootdown）。\n在传统的分页过程中，当操作系统分页出去页面时，如果每次都对每一个页面单独执行清除操作（逐个刷新每个页面的 TLB 条目），这个开销会很大，尤其是在多处理器系统中。每个处理器都必须被中断，要求它们刷新 TLB，可能会造成性能瓶颈。\n文中提出的批量操作解决方案可以用于优化这个过程：\n批量清除： 而不是逐个处理每个页面的清除（刷新 TLB），可以将多个页面一起批量执行。清除多个页面的成本与清除单个页面的成本相差不大，因此通过批量处理可以将开销分摊到多个页面上，从而降低每个页面的平均处理成本。 预留空间： 通过保持一部分未使用的物理内存页面（预留空间），操作系统可以一次性批量将多个虚拟页面分页出去，释放物理内存。这种批量分页操作减少了单个页面分页出去时所需的清除次数和中断处理器的次数，提升了性能。 在传统的磁盘分页中，操作系统通常需要依赖硬件支持来加速这些分页和清除操作。然而，如果批量处理做得足够高效，那么这些分页和清除操作的开销会显著降低，可能就不再需要特殊的硬件支持。这意味着即便没有硬件优化，操作系统仍然可以通过批量操作来实现较高的性能。\n文中通过将批量处理应用到磁盘分页，解决了以下问题：\nTLB 清除的开销： 多个页面的清除成本可以被分摊，减少了逐个页面执行清除时的成本。 硬件依赖性降低： 批量处理可以使分页操作的效率提升，降低对硬件支持的需求。 最终，批量操作不仅适用于本文中讨论的高级算法（如垃圾收集），也可以用于传统的磁盘分页系统，以更高效地释放物理内存和管理页面权限。\nOptimal page size 选择页面大小是一项重要的设计决策，受多种因素的影响。\n首先，小页面的主要优点是：当从磁盘读取或写入内存时，传输时间是线性增加的，而查找时间是固定的。因此，较小的页面减少了未使用数据传输到磁盘的浪费。这对于具有糟糕的局部性的应用程序来说尤为重要，因为它们的每个页面中只有很少的字被访问。\n另一方面，大页面有助于减少页表的开销，尤其是在使用二维或多级页表时。此外，大页面能够显著降低TLB未命中的频率，因为TLB每个条目可以映射更大的内存区域。如果TLB条目的数量是固定的，页面越大，每个条目能覆盖的内存区域就越多，从而减少了TLB未命中的可能性。\n因此，页面大小的选择涉及在减少磁盘传输时间和减少TLB未命中之间的权衡。为此，有几条经验法则：\n对于随机内存访问，即当程序没有表现出良好的局部性时，小页面往往更有效，因为它们减少了未使用数据的传输。 对于表现出良好局部性的程序，大页面更有利，因为它们能够减少TLB未命中和页表开销。 现代磁盘和内存系统的性能特征表明，随着传输速率的提高，大页面的相对优势在增加。 有些系统允许在运行时调整页面大小，而有些系统则通过使用多个页面大小（称为“大页面支持”）来允许某些进程使用较大的页面。后一种方法通常在大型科学应用或数据库系统中使用，它们具有良好的局部性并且需要大量内存。\n总而言之，最佳页面大小的选择依赖于应用程序的特性以及系统的具体硬件特征。\nAccess to protected pages \u0026ldquo;Access to protected pages\u0026quot;部分讲述了在多处理器系统中，用户模式的服务程序需要访问那些对客户端线程不可访问的页面。这种需求出现在并发垃圾收集、扩展寻址能力、共享虚拟内存以及数据压缩分页等算法中。\n实现对受保护页面的访问有几种方式：\n在同一个地址空间中，给相同页面提供多个映射，并且每个映射具有不同的保护级别。例如，垃圾收集器可以通过一个“非标准”地址访问to-space的页面，而mutator看到的to-space页面则是受保护的。 提供一个系统调用来复制受保护区域的内存。垃圾收集器可以在处理每个页面时调用该系统调用三次：一次用于从from-space复制记录到to-space；一次用于扫描to-space页面之前；还有一次是在扫描结束后，在让mutator访问该页面之前。这种方法效率较低，因为需要频繁地复制数据。 在允许共享页面的操作系统中，垃圾收集器可以运行在不同的重量级进程中，这个进程有一个不同的页表。这种方法的问题在于每次垃圾收集页面故障时需要进行两次昂贵的重量级上下文切换。然而，在多处理器系统中，可能通过远程过程调用（RPC）通知另一个已经处于正确上下文中的处理器，这样该选项可能会更加有吸引力。 垃圾收集器可以在操作系统内核中运行。这种方式效率最高，但可能不适合因为垃圾收集器并不应该运行在内核中。此外，每种编程语言的运行时数据格式不同，内核可能无法处理所有语言的垃圾收集。 文中建议，对于物理寻址缓存的计算机架构来说，在同一地址空间中提供多重虚拟地址映射是一个干净且高效的解决方案。它不需要重量级上下文切换，也不需要复制数据结构，也不需要将垃圾收集器运行在内核中。缺点是每个物理页面可能需要在页表中占用两个不同的条目，增加了物理内存的需求，不过这个增量很小。\n在具有虚拟寻址缓存的系统中，多重虚拟地址映射可能会带来缓存一致性问题，但这在垃圾收集算法中很容易解决。在垃圾收集器扫描页面时，mutator无法访问该页面，因此mutator地址空间中不会有该页面的缓存行。扫描结束后，垃圾收集器应刷新该页面的缓存行，从而避免不一致性问题。\n这段内容强调了在并发计算环境中如何实现受保护页面的高效访问，以及不同方法的优缺点。\n垃圾收集器在处理每个页面时调用系统调用三次，是因为它们分别对应不同的步骤，以确保页面的有效同步和访问控制。具体作用如下：\n第一次调用：从 from-space 复制到 to-space。垃圾收集器需要将已经不再使用的对象从一个内存区域 (from-space) 复制到另一个内存区域 (to-space)。为了进行这次复制操作，垃圾收集器需要调用系统以访问并复制受保护的内存页面。 第二次调用：在扫描页面之前。在垃圾收集器准备扫描 to-space 中的新页面时，它需要确保该页面中不再包含任何指向旧空间 (from-space) 的指针。为了在扫描过程中进行这项操作，垃圾收集器需要再次通过系统调用获取对该页面的访问权限。 第三次调用：扫描结束后，将页面提供给 mutator 访问。当垃圾收集器完成了对页面的扫描并确保页面中所有的指针都已更新为指向 to-space 的新对象后，它会调用系统，让 mutator (主线程) 可以安全地访问该页面，从而使程序的其他部分能够继续正常运行。 每次调用系统调用的主要目的在于保证内存页面的访问权限和内容在不同阶段的正确性和一致性。这样设计的好处在于保证了并发垃圾收集过程中，collector 和 mutator 之间的同步，同时也确保了指针的正确更新和数据的安全性。\n多重虚拟地址映射可能会带来缓存一致性问题的原因与计算机体系结构中的缓存机制有关。缓存的主要目的是提高内存访问速度，将常用数据存储在较靠近处理器的高速存储器中，而不是每次都从较慢的主内存中读取。缓存通常是按照虚拟地址进行索引的，这意味着相同的物理内存页面如果被映射到多个虚拟地址上，那么系统在不同的虚拟地址下访问该页面时，可能会在缓存中存储多个副本。由于这些副本可以存在于缓存的不同位置，这就可能导致缓存中的数据不一致问题。\n当系统中存在多个虚拟地址指向同一物理页面时，以下情况可能会发生：\n缓存不一致性：一个虚拟地址指向的物理页面可能会被更新，但另一个映射到该物理页面的虚拟地址却没有反映这些更新，因为它的缓存副本没有被同步更新。这会导致两个虚拟地址对应的缓存中的数据不同步，出现数据不一致的情况。 数据同步问题：如果同一个物理页面有多个缓存副本，当系统从某个虚拟地址修改数据后，如果不及时更新或清除其他虚拟地址的缓存副本，处理器在访问这些虚拟地址时可能会读取过时的数据，从而导致缓存一致性问题。 缓存通常会根据虚拟地址进行分配，如果同一个物理页面通过多个虚拟地址映射，这些虚拟地址可能在缓存中占用不同的缓存行，而这正是造成不一致的根本原因。这种不一致性可能导致程序的错误运行，尤其是在垃圾回收等需要保证内存一致性的场景下。\n所以，每个虚拟地址不会抽出一个特定的缓存块，但同一个物理页面可能通过多个虚拟地址映射，从而在缓存中创建多个条目，增加了保持这些条目一致的复杂性。\nIs this too much to ask? 有些Unix的实现，在某些机器上，提供了非常干净且同步的信号处理机制。引发页面故障的指令会调用信号处理程序，而不会改变处理器的其他状态；后续的指令不会执行，等等。信号处理程序可以完全同步地访问机器寄存器，改变内存映射或机器寄存器，然后重新开始执行故障指令。然而，在高度流水化的机器上，可能会存在多个未处理的页面故障【26】，并且可能在注意到故障之前，多个故障后的指令已经将其结果写入寄存器；这些指令可以恢复执行，但不能重新启动。\n当用户程序依赖于同步行为时，它们在流水线机器上运行会很困难。现代UNIX系统让用户程序主动参与内存管理功能，允许它们显式地操作其内存映射。这在某种程度上就像是发出了“通往地狱的镌刻请柬”【26】。\n如果所描述的这些算法确实与快速流水线机器不兼容，那将是一个严重的问题。幸运的是，除了一个例子之外，我们描述的所有算法都是足够异步的。从机器的角度来看，它们的行为非常像传统的磁盘分页：发生故障，提供物理页面，使该页面在页表中可访问，然后继续执行。\n例外的是堆溢出检测：页面故障会启动垃圾回收，垃圾回收会修改寄存器（通过将其指向堆记录的新位置），然后恢复执行。存储下一分配单元地址的寄存器会被调整以指向分配空间的起始位置。之前发生故障的指令会重新执行，但这次它不会再发生故障，因为它正在存储到一个不同的位置。\n这种行为在高度流水化的机器上是不可接受的（除非像在VAX 8800上那样【10】有硬件用于“撤销”那些已经完成的后续指令或地址模式副作用）。实际上，即便在Motorola 68020上，使用页面故障来检测堆溢出也不可靠。\n因此，除了堆溢出检测之外，我们提出的所有算法对于硬件来说不会比普通磁盘分页带来更多问题，可以退还这张“通往地狱的请柬”；然而，操作系统必须确保为硬件提供足够的支持，以正确恢复半同步陷阱处理程序的故障操作。\nheap overflow detection and other algorithms 堆溢出检测算法与其他内存管理算法在虚拟内存管理中的相同点和不同点如下：\nSimilarities\n依赖页面保护机制： 无论是堆溢出检测还是其他内存管理算法，二者都依赖于页面保护机制来监控内存访问。这些算法利用设置页面为“不可访问”或“只读”等来触发页面错误（page fault），并在发生错误时进行相应处理。 通过页面错误进行内存管理： 堆溢出检测与诸如并发垃圾回收（concurrent garbage collection）等其他算法都依赖于操作系统提供的页面错误处理机制，以便在内存使用达到某个限度时进行回收或管理。 内存同步机制： 两种算法都涉及多线程或多处理器环境下的内存同步。在并发垃圾回收算法中，利用页面保护机制来同步不同线程对同一内存区域的访问。而堆溢出检测则使用页面错误触发垃圾回收，以防止超出可分配内存。 Differences\n触发条件不同： 堆溢出检测算法：该算法主要依赖于页面错误来检测堆内存溢出。当分配空间到达预设的保护页面时，触发页面错误，并调用垃圾回收器来整理内存。 其他算法（例如并发垃圾回收或世代垃圾回收）：这些算法的页面错误触发更多与内存中对象的移动或跨线程同步相关。并发垃圾回收利用页面保护来防止线程之间的不正确数据访问，而世代垃圾回收通过监测跨代指针来进行内存管理。 错误处理的复杂性： 堆溢出检测：在堆溢出检测中，页面错误的处理相对简单，主要目的是调用垃圾回收器以释放内存并更新指针。垃圾回收完成后，内存可以再次分配。错误处理不涉及复杂的线程同步或指针更新。 并发垃圾回收：并发垃圾回收的页面错误处理则更加复杂。除了要处理指针的移动外，还要确保线程之间的正确同步，防止一个线程在另一个线程完成垃圾回收之前访问被移动的对象。 性能影响： 堆溢出检测：堆溢出检测中，页面错误的发生频率与内存分配频率直接相关。如果内存分配速度快，页面错误将频繁发生，从而导致更多的垃圾回收调用。文档中提到，堆溢出检测中，每分配约50次内存就会引发一次页面错误。 其他算法：其他算法（如世代垃圾回收）依赖于内存分代的特点，仅在较长时间内的对象进行回收时才会触发页面错误。因此，这些算法在处理频率上通常较低，尤其在新对象的分配主要集中在年轻代时。 适应硬件的能力： 堆溢出检测：在高度流水线化的处理器上，堆溢出检测可能面临问题。文档中指出，由于处理器可能在注意到页面错误之前已经执行了多个指令，因此无法可靠地恢复执行错误指令，尤其是在如Motorola 68020等架构中，堆溢出检测机制并不稳定。 其他算法：并发垃圾回收和共享虚拟内存等其他算法则更具异步性，对现代硬件架构更具适应性。这些算法通常不依赖于特定的指令状态，而是处理页面错误后继续执行程序。 堆溢出检测算法和其他内存管理算法都依赖页面错误和页面保护机制来进行内存管理，但在处理复杂性、触发条件和性能影响上存在显著不同。堆溢出检测主要用于检测堆内存的溢出并触发垃圾回收，而其他算法则更多用于并发处理、同步和跨代内存管理。此外，堆溢出检测在高度流水线处理器上面临更大的挑战，而并发垃圾回收等算法则具有更好的硬件适应性。\n正课 应用程序使用虚拟内存所需要的特性 今天的话题是用户应用程序使用的虚拟内存，它主要是受这篇1991年的论文 的启发。\n首先，你们已经知道了，操作系统内核以非常灵活的方式使用了虚拟内存Page Table。\n你们已经通过Lazy Allocation Lab，Copy on Write Lab，以及XV6中的各种内存实现了解到了这一点。\n而今天论文中的核心观点是，用户应用程序也应该从灵活的虚拟内存中获得收益，也就是说用户应用程序也可以使用虚拟内存。\n用户应用程序本身就是运行在虚拟内存之上，我们这里说的虚拟内存是指：User Mode或者应用程序想要使用与内核相同的机制，来产生Page Fault并响应Page Fault（注，详见Lec08，内核中几乎所有的虚拟内存技巧都基于Page Fault）。\n也就是说User Mode需要能够修改PTE的Protection位（注，Protection位是PTE中表明对当前Page的保护，对应了4.3中的Writeable和Readable位）或者Privileged level。\n今天的论文，通过查看6-7种不同的应用程序，来说明用户应用程序使用虚拟内存的必要性。\n这些应用程序包括了：\nGarbage Collector Data Compression Application Shared Virtual Memory 你可以发现这都是一些非常不同的应用程序，并且它们都依赖虚拟内存的一些特性来正常工作。\n所以第一个问题是，上面的应用程序需要的特性是什么？所以我们先来讨论一下需要的特性是什么？\n首先，你需要trap来使得发生在内核中的Page Fault可以传播到用户空间，然后在用户空间的handler可以处理相应的Page Fault，之后再以正常的方式返回到内核并恢复指令的执行。\n这个特性是必须的，否则的话，你不能基于Page Fault做任何事情。\n第二个特性是Prot1，它会降低了一个内存Page的accessability。\naccessability的意思是指内存Page的读写权限。\n内存Page的accessability有不同的降低方式，例如，将一个可以读写的Page变成只读的，或者将一个只读的Page变成完全没有权限。\n除了对于每个内存Page的Prot1，还有管理多个Page的ProtN。\nProtN基本上等效于调用N次Prot1，那为什么还需要有ProtN？\n因为单次ProtN的损耗比Prot1大不了多少，使用ProtN可以将成本分摊到N个Page，使得操作单个Page的性能损耗更少。\n在使用Prot1时，你需要修改PTE的bit位，并且在Prot1的结束时，需要清除TLB（注，详见4.4 Translation Lookaside Buffer），而清除TLB比较费时。\n如果能对所有需要修改的内存Page集中清理一次TLB，就可以将成本分摊。\n所以ProtN等效于修改PTE的bit位N次，再加上清除一次TLB。\n如果执行了N次Prot1，那就是N次修改PTE的bit位，再加上清除N次TLB，所以ProtN可以减少清除TLB的次数，进而提升性能。\n下一个特性是Unprot，它增加了内存Page的accessability，例如将本来只读的Page变成可读可写的。\n除此之外，还需要能够查看内存Page是否是Dirty。\n以及map2。map2使得一个应用程序可以将一个特定的内存地址空间映射两次，并且这两次映射拥有不同的accessability。\n（注，也就是一段物理内存对应两份虚拟内存，并且两份虚拟内存有不同的accessability）。\nXV6在用户程序中支持以上任意的特性吗？\n除了有类似于trap及其相关的alarm hander之外，XV6不支持任何一个以上的特性。\nXV6只有一个最小化的Unix接口，并不支持以上任何虚拟内存特性。\n尽管在XV6的内核中包含了所有的可用的虚拟内存的机制，但是并没有以系统调用的形式将它们暴露给用户空间。\n论文的观点是，任何一个好的操作系统都应该以系统调用的形式提供以上特性，以供应用程序使用。\n所以自然的，这就引出了另一个问题，当今的Unix系统的功能范围是什么？以上特性属于Unix的范畴吗？\n如果你查看现在的Unix系统，例如Linux，你会发现，或许并不与论文中描述的完全一样，但是这些特性都存在。\n在论文那个年代（1991年），某些操作系统只包含了部分以上特性，但是如今这些特性都已经在现代的Unix系统中广泛支持了。接下来我们看一下如何实现这些特性。\n支持应用程序使用虚拟内存的系统调用 mmap的系统调用 第一个或许也是最重要的一个，是一个叫做mmap的系统调用。\n它接收某个对象，并将其映射到调用者的地址空间中。\n举个例子，如果你想映射一个文件，那么你需要将文件描述符传递给mmap系统调用。\nmmap系统调用有许多令人眼花缭乱的参数（注，mmap的具体说明可以参考man page ）：\n第一个参数是一个你想映射到的特定地址，如果传入null表示不指定特定地址，这样的话内核会选择一个地址来完成映射，并从系统调用返回。\n第二个参数是想要映射的地址段长度len。\n第三个参数是Protection bit，例如读写R|W。\n第四个参数我们会跳过不做讨论，它的值可以是MAP_PRIVATE。它指定了如果你更新了传入的对象，会发生什么。\n（注，第四个参数是flags，MAP_PRIVATE是其中一个值，在mmap文件的场景下，MAP_PRIVATE表明更新文件不会写入磁盘，只会更新在内存中的拷贝，详见man page ）。\n第五个参数是传入的对象，在上面的例子中就是文件描述符。\n第六个参数是offset。\n通过上面的系统调用，可以将文件描述符指向的文件内容，从起始位置加上offset的地方开始，映射到特定的内存地址（如果指定了的话），并且连续映射len长度。\n这使得你可以实现Memory Mapped File，你可以将文件的内容带到内存地址空间，进而只需要方便的通过普通的指针操作，而不用调用read/write系统调用，就可以从磁盘读写文件内容。\n这是一个方便的接口，可以用来操纵存储在文件中的数据结构。\n实际上，你们将会在下个lab实现基于文件的mmap，下个lab结合了XV6的文件系统和虚拟内存，进而实现mmap。\nmmap还可以用作他途。\n除了可以映射文件之外，还可以用来映射匿名的内存（Anonymous Memory）。\n这是sbrk（注，详见8.2）的替代方案，你可以向内核申请物理内存，然后映射到特定的虚拟内存地址。\nmmap是实现应用程序虚拟内存的核心系统调用之一，我们稍后会将它与之前提到的特性关联起来。\n除此之外，还需要有一些系统调用来支持论文中讨论到的特性。\nmprotect系统调用 （注，详见man page ）。\n当你将某个对象映射到了虚拟内存地址空间，你可以修改对应虚拟内存的权限，这样你就可以以特定的权限保护对象的一部分，或者整个对象。\n举个例子，通过上图对于mprotect的调用将权限设置成只读（R），这时，对于addr到addr+len这段地址，load指令还能执行，但是store指令将会变成Page Fault。\n类似的，如果你想要将一段地址空间变成完成不可访问的，那么可以在mprotect中的权限参数传入None，那么任何对于addr到addr+len这段地址的访问，都会生成Page Fault。\nmunmap 对应mmap还有一个系统调用munmap，它使得你可以移除一个地址或者一段内存地址的映射关系。\nsigaction 最后一个系统调用是sigaction，它本质上是用来处理signal。\n它使得应用程序可以设置好一旦特定的signal发生了，就调用特定的函数。\n可以给它传入函数f作为特定signal的handler。\n在Page Fault的场景下，生成的signal是segfault。\n你或许之前在用户代码中看过了segfault，通常来说当发生segfault时，应用程序会停止运行并crash。\n但是如果应用程序为segfault signal设置了handler，发生segfault时，应用程序不会停止，相应的handler会被内核调用，然后应用程序可以在handler中响应segfault。\n当内核发现Page Fault时，或许会通过修复Page Table来使得应用程序还能继续执行。\n与内核响应Page Fault的方式类似，在这里的handler中或许会调用mprotect来修改内存的权限来避免segfault，这样应用程序的指令就可以恢复运行。\n与sigaction类似的有sigalarm（译注：sigalarm 是traps lab 中实现的一个系统调用，不属于标准Unix 接口），在sigalarm中可以设置每隔一段时间就调用handler。\nsigaction也可以实现这个功能，只是它更加的通用，因为它可以响应不同类型的signal。\n学生提问：看起来mprotect暗示了你可以为单独的地址添加不同的权限，然而在XV6中，我们只能为整个Page设置相同的权限，这里是有区别吗？\nFrans教授：不，这里并没有区别，它们都在Page粒度工作。如果你好奇的话，有一个单独的系统调用可以查看Page的大小。\n虚拟内存特性 如果你回想前面提到过的虚拟内存特性，我们可以将它们对应到这一节描述的Unix接口中来。\ntrap对应的是sigaction系统调用 Prot1，ProtN和Unprot可以使用mprotect系统调用来实现。mprotect足够的灵活，你可以用它来修改一个Page的权限，也可以用它来修改多个Page的权限。当修改多个Page的权限时，可以获得只清除一次TLB的好处。 查看Page的Dirty位要稍微复杂点，并没有一个直接的系统调用实现这个特性，不过你可以使用一些技巧完成它，我稍后会介绍它。 map2也没有一个系统调用能直接对应它，通过多次调用mmap，你可以实现map2特性。 或许并不完全受这篇论文所驱动，但是内核开发人员已经在操作系统中为现在的应用程序提供了这些特性。\n接下来，我将在框架层面简单介绍一下这些特性是如何实现的，之后再看看应用程序是如何使用这些特性。\n虚拟内存系统如何支持用户应用程序 有关实现，有两个方面较为有趣。\n第一个是虚拟内存系统为了支持这里的特性，具体会发生什么？\n这里我们只会讨论最重要的部分，并且它也与即将开始的mmap lab有一点相关，因为在mmap lab中你们将要做类似的事情。\n在现代的Unix系统中，地址空间是由硬件Page Table来体现的，在Page Table中包含了地址翻译。\n在现代的 Unix 系统中，地址空间的管理确实依赖于硬件级别的页表（Page Table），但这并不意味着用户或操作系统完全无法控制它。我们可以理解为：\n硬件辅助而非硬件独立控制\n页表的硬件实现：页表的实际翻译过程由硬件（比如 CPU 的内存管理单元 MMU）来执行。CPU 会根据页表中的内容将虚拟地址翻译成物理地址，这种翻译过程在访问内存时是由硬件快速完成的。\n软件管理与硬件依赖：操作系统在软件层面负责构建、修改和管理页表的内容，决定虚拟地址到物理地址的映射关系。虽然硬件负责执行实际的地址翻译，但操作系统通过系统调用和内核机制对页表进行控制，从而间接控制了地址空间。\n为什么称为“硬件Page Table”\n性能的考量：地址翻译是频繁的基本操作，每次访问内存时都需要进行。这种高频率的操作如果完全由软件完成会非常耗时。因此，硬件（CPU）来负责翻译加速，以保证性能。页表硬件机制可以使这些翻译在数个 CPU 时钟周期内完成。\n页表的多级结构：现代 CPU 使用多级页表（例如 x86 架构中的四级页表）来高效地管理大规模地址空间。这些复杂的结构由硬件支持并高效处理。\n用户的控制能力\n间接控制：用户态程序无法直接操作页表，但可以通过操作系统提供的接口（比如 mmap 或 malloc）来请求内存分配，操作系统会根据这些请求更新页表。\n内核态控制：在内核态，操作系统完全可以控制页表，包括创建新的页表、修改页表项、设置权限等。这些操作最终都影响到用户态进程的虚拟内存布局。\n因此，“硬件 Page Table”指的是地址翻译依赖于硬件完成，而不是说用户或操作系统完全无法控制页表。操作系统仍然可以通过内核态管理页表并间接影响地址空间，只不过实际的地址翻译过程由硬件（CPU 和 MMU）来加速处理，以提高效率。这种设计是为了平衡性能与灵活性，使得系统能够高效地管理和访问内存。\n但是通常来说，地址空间还包含了一些操作系统的数据结构，这些数据结构与任何硬件设计都无关，它们被称为Virtual Memory Areas（VMAs）。\nVMA会记录一些有关连续虚拟内存地址段的信息。\n在一个地址空间中，可能包含了多个section，每一个section都由一个连续的地址段构成，对于每个section，都有一个VMA对象。\n连续地址段中的所有Page都有相同的权限，并且都对应同一个对象VMA\n例如一个进程的代码是一个section，数据是另一个section，它们对应不同的VMA，VMA还可以表示属于进程的映射关系，例如下面提到的Memory Mapped File。\n举个例子，如果进程有一个Memory Mapped File，那么对于这段地址，会有一个VMA与之对应，VMA中会包含文件的权限，以及文件本身的信息，例如文件描述符，文件的offset等。\n在接下来的mmap lab中，你们将会实现一个非常简单版本的VMA，并用它来实现针对文件的mmap系统调用。你可以在VMA中记录mmap系统调用参数中的文件描述符和offset。\n第二个部分我们了解的就不多了，它或许值得仔细看一下，也就是User level trap是如何实现的？\n我们假设一个PTE被标记成invalid或者只读，而你想要向它写入数据。\n这时，CPU会跳转到kernel中的固定程序地址，也就是XV6中的trampoline代码（注，详见6.2）。\nkernel会保存应用程序的状态，在XV6中是保存到trapframe。\n之后再向虚拟内存系统查询，现在该做什么呢？\n虚拟内存系统或许会做点什么，例如在lazy lab和copy-on-write lab中，trap handler会查看Page Table数据结构。\n而在我们的例子中会查看VMA，并查看需要做什么。\n举个例子，如果是segfault，并且应用程序设置了一个handler来处理它，那么\nsegfault事件会被传播到用户空间 并且通过一个到用户空间的upcall在用户空间运行handler 在handler中或许会调用mprotect来修改PTE的权限 之后handler返回到内核代码 最后，内核再恢复之前被中断的进程。 当内核恢复了中断的进程时，如果handler修复了用户程序的地址空间，那么程序指令可以继续正确的运行，如果哪里出错了，那么会通过trap再次回到内核，因为硬件还是不能翻译特定的虚拟内存地址。\n\u0026ldquo;segfault事件会被传播到用户空间\u0026rdquo; 是什么意思？是指segfault这个信号会从内核态传给用户态吗？怎么传的？\n是的，\u0026ldquo;segfault事件会被传播到用户空间\u0026rdquo; 意思是内核会检测到段错误（segfault）这种内存访问异常，然后通过一种信号机制将这个信息传递给用户态进程。\n具体来说，在现代类 Unix 系统中，当进程试图非法访问内存（如访问一个无效的地址或尝试向只读页面写入数据）时，内核会捕获到这个异常并触发一个信号，通常是 SIGSEGV 信号（表示“段错误”）。这个信号会通知用户进程，内存访问出错。\n传播机制如下：\n内核捕获异常：CPU检测到非法内存访问，触发陷阱（trap）进入内核，内核负责处理该事件。 生成信号（signal）：内核会向引发该异常的进程生成一个信号，比如 SIGSEGV，这个信号会被记录到进程的待处理信号队列中。 向用户态传播：当进程被调度运行时，内核会检查该进程的待处理信号队列。如果有信号，内核会中断该进程的正常执行流程，并将控制权交给事先注册的信号处理程序（handler）。 如果进程注册了信号处理程序（handler），那么信号处理程序会在用户态被执行，处理这个异常。 如果没有设置处理程序，系统会终止进程并生成一个core dump（进程崩溃时的内存转储）。 在这个过程中，进程是trap，然后进入trampoline，存好上下文，进入内核，然后又从内核回到用户空间，执行handler吗？这个handler怎么有权限更改PTE的？\n是的，你描述的过程基本正确。以下是详细的解释：\n处理过程：\ntrap发生：当用户进程尝试写入一个被标记为 invalid 或 只读 的页面时，CPU触发异常，进入内核态。 进入trampoline代码：内核首先通过 trampoline 代码进入内核，并保存当前进程的上下文信息（如寄存器、程序计数器等）。在 XV6 系统中，这些信息会被存储在 trapframe 结构中。 查询虚拟内存系统：内核的trap handler会查看页面表（PTE），以及与虚拟内存相关的其他数据结构（如VMA，虚拟内存区域）。根据这些信息，内核决定接下来如何处理。 如果是合法的页面缺失（Page Fault），可能会分配一个新的页面或执行 Copy-on-Write 机制。 如果是段错误（segfault），内核会发送 SIGSEGV 信号给用户进程。 信号处理机制：如果用户程序设置了 SIGSEGV 信号的处理程序，内核会通过一种机制称为 upcall（上行调用）返回到用户态，让用户进程执行该信号处理函数（handler）。这是通过修改 trapframe 中保存的上下文来完成的，修改的内容包括将程序计数器（PC）指向用户态的 handler 地址。 handler修改PTE的权限：\n信号处理函数（handler）可以调用系统提供的接口来修改 PTE（页表项）。具体来说，虽然用户态程序无法直接操作页表，但它可以通过系统调用来间接修改页表。比如：\n调用 mprotect：mprotect 是一个系统调用，允许用户程序请求内核修改特定页面的访问权限。通过调用 mprotect，用户可以请求将某些页面的权限从只读修改为可写，或从无效状态修改为有效状态。 当 handler 中调用 mprotect 时，控制权会再次转交给内核，由内核负责对页表进行修改。内核可以直接操作 PTE，将其修改为新的权限配置（例如将页面从只读修改为可写）。这并不是用户态程序直接更改 PTE，而是通过系统调用请求内核来完成修改。\n内核恢复进程的执行：\n恢复上下文：handler 执行完毕后，控制权再次回到内核。内核会检查 trapframe，恢复之前保存的进程上下文，确保进程能够从上次中断的地方继续执行。 继续执行原程序：如果 handler 成功修复了页面（例如通过调用 mprotect 修改了页面的权限），那么进程会从触发 trap 的指令处继续执行。如果地址空间依然有问题（例如 handler 没有正确处理），则进程会再次触发 trap。 所以，总的来说：\n“segfault事件传播到用户空间” 是指通过信号机制，内核将 segfault 这个异常信息传递给用户态，用户态的 handler 处理程序可以在用户空间执行。 进程从trap到trampoline再到用户空间 的确是一个循环的过程。handler 本身无法直接更改 PTE 的权限，但它可以通过系统调用（如 mprotect）向内核请求修改，最终由内核来执行 PTE 的更新操作。 这种机制在性能和安全性之间取得了平衡：用户程序通过信号机制间接地影响页表内容，而页表的实际修改仍然由内核完成。\n学生提问：当我们允许用户针对Page Fault来运行handler代码时，这不会引入安全漏洞吗？\nFrans教授：这是个很好的问题。会有安全问题吗？你们怎么想的？这会破坏User/kernel或者不同进程之间的隔离性吗？或者从另一个角度来说，你的问题是sigalarm会破坏隔离性吗？\n当我们执行upcall的时候，upcall会走到设置了handler的用户空间进程中，所以handler与设置了它的应用程序运行在相同的context，相同的Page Table中。\n所以handler唯一能做的事情就是影响那个应用程序，并不能影响其他的应用程序，因为它不能访问其他应用程序的Page Table，或者切换到其他应用程序的Page Table。所以这里还好。\n当然，如果handler没有返回，或者做了一些坏事，最终内核还是会杀掉进程。所以唯一可能出错的地方就是进程伤害了自己，但是它不能伤害任何其他进程。\n构建大的缓存表 接下来，我将通过介绍几个例子来看一下如何使用之前介绍的内容。\n我会从一个非常简单的例子开始，之后我们会看一下Garbage Collector，因为很多同学都问了Garbage Collector的问题，所以GC是一个可以深入探讨的好话题。\n首先我想讨论的是一个非常简单的应用，它甚至都没有在论文中提到，但它却是展示这节课的内容非常酷的一个方法。\n这个应用里是构建一个大的缓存表，什么是缓存表？它是用来记录一些运算结果的表单。\n举个例子，你可以这么想，下面是我们的表单，它从0开始到n。\n表单记录的是一些费时的函数运算的结果，函数的参数就是0到n之间的数字。\n如果这个表单在最开始的时候就预计算好了，那么当你想知道f(i)的结果是什么时，你需要做的就是查看表单的i槽位，并获取f(i)的值。\n这样你可以将一个费时的函数运算转变成快速的表单查找，所以这里一个酷的技巧就是预先将费时的运算结果保存下来。如果相同的计算需要运行很多很多次，那么预计算或许是一个聪明的方案。\n这里的挑战是，表单可能会很大，或许会大过物理内存，这里可以使用论文提到的虚拟内存特性来解决这个挑战。\n首先，你需要分配一个大的虚拟地址段，但是并不分配任何物理内存到这个虚拟地址段。\n这里只是从地址空间获取了很大一段地址，并说我将要使用地址空间的这部分来保存表单。\n但是现在表单中并没有内容，表单只是一段内存地址。\n如果你现在查找表单的i槽位，会导致Page Fault。\n所以这里的计划是，在发生Page Fault时，先针对对应的虚拟内存地址分配物理内存Page，之后计算f(i)，并将结果存储于tb[i]，也就是表单的第i个槽位，最后再恢复程序的运行。\n这种方式的优势是，如果你需要再次计算f(i)，你不需要在进行任何费时的计算，只需要进行表单查找。\n即使接下来你要查找表单的i+1槽位，因为一个内存Page可能可以包含多个表单项，这时也不用通过Page Fault来分配物理内存Page。\n不过如果你一直这么做的话，因为表单足够大，你最终还是会消耗掉所有的物理内存。\n所以Page Fault Handler需要在消耗完所有的内存时，回收一些已经使用过的物理内存Page。\n当然，你需要修改已经被回收了的物理内存对应的PTE的权限，这样在将来使用对应地址段时，就可以获得Page Fault。\n所以你需要使用Prot1或者ProtN来减少这些Page的accessbility。\n学生提问：在分配物理内存Page时，我们需要让操作系统映射到地址空间的特定地址，否则的话可能会映射到任意地址，是吧？\nFrans教授：操作系统会告知是哪个地址，并且这里可能是任意的地址。\n为了更具体的描述这里的应用，我这里有个小的实现，我们可以看一看这里是如何使用现有的Unix特性。\n1 2 3 4 5 6 7 8 9 int main(int argc,char *argv[]) { page_size = sysconf(_SC_PAGESIZE); printf(\u0026#34;page_size is %ld\\n\u0026#34;,page_size); setup_sqrt_region(); test_sqrt_region(); return 0; } 在main函数中，首先调用setup_sqrt_region函数，它会从地址空间分配地址段，但是又不实际分配物理Page。之后调用test_sqrt_region。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 static void test_sqrt_region(void) { int i,pos = rand()%(MAX_SQRTS 1); double correct_sqrt; printf(\u0026#34;Validating square root table contents...\\n\u0026#34;); srand (0xDEADBEEF); for(i = 0; i \u0026lt; 500000; i++){ if(i%2==0) pos = rand()%(MAX_SQRTS - 1); else pos += 1; calculate_sqrts(\u0026amp;correct_sqrt,pos,1); if (sqrts[pos] != correct_sqrt){ fprintf(stderr, \u0026#34;Square root is incorrect.Expected %f,got %f.\\n\u0026#34;, correct_sqrt, sqrts[pos]); exit(EXIT_FAILURE); } } printf(\u0026#34;All tests passed!\\n\u0026#34;); } 在test_sqrt_region中，会以随机数来遍历表单，并通过实际运算对应的平方根值，来检查表单中相应位置值是不是保存了正确的平方根值。\n在test_sqrt_region运行的过程中，会产生Page Fault，因为现在还并没有分配任何物理内存Page。\n应用程序该如何收到Page Fault呢？\n在setup_sqrt_region函数中有一段代码，通过将handle_sigsegv函数注册到了SIGSEGV事件。\n这样当segfault或者Page Fault发生时，内核会调用handle_sigsegv函数。\nhandle_sigsegv函数与你们之前看过很多很多次的trap代码非常相似。\n它首先会获取触发Page Fault的地址， 之后调用mmap对这个虚拟内存地址分配一个物理内存Page（注，这里是mmap映射匿名内存）。这里的虚拟内存地址就是我们想要在表单中用来保存数据的地址。 然后我们为这个Page中所有的表单项都计算对应的平方根值，之后就完事了。 这个应用程序有点极端，它在运行的时候只会使用一个物理内存Page，所以不论上一次使用的Page是什么，在handle_sigsegv的最后都会通过munmap释放它。\n所以我们有一个巨大的表单，但是它只对应一个物理内存Page。\n接下来我将运行一下这个应用程序。test_sqrt_region会随机查看表单的内容，所以可以假设这会触发很多Page Fault，但是可以看出表单中的所有内容都能通过检查。\n所以，尽管这里有一个巨大的表单用来保存平方根，但是实际在物理内存中只有一个内存Page。\n这是一个简单的例子，它展示了用户应用程序使用之前提到的虚拟内存特性之后可以做的一些酷的事情。\n学生提问：能再讲一下为什么一个物理内存Page就可以工作吗？我觉得这像是lazy allocation，但是区别又是什么呢？\nFrans教授：当我们刚刚完成设置时，我们一个内存Page都没有，setup_sqrt_region分配了一个地址段，但是又立即通过munmap将与这个地址段关联的内存释放了。所以在启动的最开始对于表单并没有一个物理内存Page与之关联。\n之后，当我们得到了一个Page Fault，这意味着整个表单对应的地址中至少有一个Page没有被映射，虽然实际上我们一个Page都没有映射。\n现在我们得到了一个Page Fault，我们只需要映射一个Page，在这个Page中，我们会存入i，i+1。。。的平方根（注，因为一个Page4096字节，一个double8个字节，所以一个Page可以保存512个表单项）。\n因为这是第一个Page Fault，之前并没有映射了内存Page，所以不需要做任何事情。\n之后，程序继续运行并且查找了表单中的更多项，如果查找一个没有位于已分配Page上的表单项时，会得到另一个Page Fault。\n这时，在handle_sigsegv会分配第二个内存Page，并为这个Page计算平方根的值。之后会munmap记录在last_page_base中的内存。\nBaker\u0026rsquo;s Real-Time Copying Garbage Collector 接下来我会讨论另一个例子，也就是Garbage Collector（注，后面将Garbage Collector和Garbage Collection都简称为GC），并且我也收到了很多有关Garbage Collector的问题。\nGC是指编程语言替程序员完成内存释放，这样程序员就不用像在C语言中一样调用free来释放内存。\n对于拥有GC的编程语言，程序员只需要调用类似malloc的函数来申请内存，但是又不需要担心释放内存的过程。\nGC会决定内存是否还在使用，如果内存并没有被使用，那么GC会释放内存。\nGC是一个很好的特性，有哪些编程语言带有GC呢？Java，Python，Golang，几乎除了C和Rust，其他所有的编程语言都带有GC。\n你可以想象，GC有很大的设计空间。\n这节课讨论的论文并没有说什么样的GC是最好的，它只是展示了GC可以利用用户空间虚拟内存特性。\n论文中讨论了一种特定的GC，这是一种copying GC。什么是copying GC？假设你有一段内存作为heap，应用程序从其中申请内存。\n你将这段内存分为两个空间，其中一个是from空间，另一个是to空间。\n当程序刚刚启动的时候，所有的内存都是空闲的，应用程序会从from空间申请内存。假设我们申请了一个类似树的数据结构。\n树的根节点中包含了一个指针指向另一个对象，这个对象和根节点又都包含了一个指针指向第三个对象，这里构成了一个循环。\n或许应用程序在内存中还有其他对象，但是没有别的指针指向这些对象，所以所有仍然在使用的对象都可以从根节点访问到。\n在某个时间，或许因为之前申请了大量的内存，已经没有内存空间给新对象了，也就是说整个from空间都被使用了。\nCopying GC的基本思想是将仍然在使用的对象拷贝到to空间去，具体的流程是从根节点开始拷贝。\n每一个应用程序都会在一系列的寄存器或者位于stack上的变量中保存所有对象的根节点指针，通常来说会存在多个根节点，但是为了说明的简单，我们假设只有一个根节点。\n拷贝的流程会从根节点开始向下跟踪，所以最开始将根节点拷贝到了to空间，但是现在根节点中的指针还是指向着之前的对象。\n之后，GC会扫描根节点对象。因为程序的运行时知道对象的类型是什么，当然也就知道对象中的指针。\n接下来GC会将根节点对象中指针指向的对象也拷贝到to空间，很明显这些也是还在使用中的对象。\n当一个对象被拷贝到to空间时，根节点中的指针会被更新到指向拷贝到了to空间的对象。\n在之后的过程中，我们需要记住这个对象已经被拷贝过了。\n所以，我们还会存储一些额外的信息来记住相应的对象已经保存在了to空间，这里会在from空间保留一个forwarding指针。\n这里将对象从from空间拷贝到to空间的过程称为forward。\n接下来还剩下一个对象，我们将这个对象从from空间拷贝到to空间，这个对象还包含一个指针指向第二个对象。\n但是通过查看指针可以看到这个对象已经被拷贝了，并且我们已经知道了这个对象被拷贝到的地址（注，也就是之前在from空间留下的forwarding指针）。\n所以我们可以直接更新第三个对象的指针到正确的地址。\n现在与根节点相关的对象都从from空间移到了to空间，并且所有的指针都被正确的更新了，所以现在我们就完成了GC，from空间的所有对象都可以被丢弃，并且from空间现在变成了空闲区域。\n论文中讨论的是一种更为复杂的GC算法，它被称为Baker算法，这是一种很老的算法。\n它的一个优势是它是实时的，这意味着它是一种incremental GC（注，incremental GC是指GC并不是一次做完，而是分批分步骤完成）。\n在Baker算法中，我们还是有from和to两个空间。假设其中还是包含了上面介绍的几个对象。\n这里的基本思想是，GC的过程没有必要停止程序的运行并将所有的对象都从from空间拷贝到to空间，然后再恢复程序的运行。\nGC开始之后，唯一必要的事情，就是将根节点拷贝到to空间。\n所以现在根节点被拷贝了，但是根节点内的指针还是指向位于from空间的对象。\n根节点只是被拷贝了并没有被扫描，其中的指针还没有被更新。\n如果应用程序调用了new来申请内存，那就再扫描几个对象，并将这些对象从from空间forward到to空间。\n这很好，因为现在我们将拷贝heap中还在使用的所有对象的过程，拆分成了渐进的步骤。\n每一次调用new都使得整个拷贝过程向前进一步。\n当然应用程序也会使用这里对象所指向的指针。\n举个例子，现在当根节点需要读出其指向的一个对象时，这个对象仍然在from空间。这是危险的，因为我们不应该跟踪from空间的指针（注，换言之GC时的指针跟踪都应该只在同一个空间中完成）。\n所以每次获取一个指针指向的对象时（dereference），你需要检查对象是否在在from空间，如果是的话，将其从from空间forward到to空间。\n所以应用程序允许使用指针，但是编译器需要对每个指针的访问都包上一层检查，这样我们就可以保证在to空间的任何指针指向的是位于to空间的对象。\n我们需要确保这一点，因为在最后当GC完成了所有对象的跟踪之后，我们会清空from部分并重用这部分内存。\n论文对于这里的方案提出了两个问题：\n第一个是每次dereference都需要有以上的额外步骤，每次dereference不再是对于内存地址的单个load或者store指令，而是多个load或者store指令，这增加了应用程序的开销。\n第二个问题是并不能容易并行运行GC。\n如果程序运行在多核CPU的机器上，并且你拥有大量的空闲CPU，我们本来可以将GC运行在后台来遍历对象的图关系，并渐进的拷贝对象。\n但是如果应用程序也在操作对象，那么这里可能会有抢占。应用程序或许在运行dereference检查并拷贝一个对象，而同时GC也在拷贝这个对象。\n如果我们不够小心的话，我们可能会将对象拷贝两遍，并且最后指针指向的不是正确的位置。所以这里存在GC和应用程序race condition的可能。\n使用虚拟内存特性的GC 论文中介绍，如果拥有了前面提到的虚拟内存特性，你可以使用虚拟内存来减少指针检查的损耗，并且以几乎零成本的代价来并行运行GC。\n这里的基本思想是将heap内存中from和to空间，再做一次划分，每一个部分包含scanned，unscanned两个区域。\n在程序启动，或者刚刚完成了from和to空间的切换时，整个空间都是unscanned，因为其中还没有任何对象。\n之后的过程与前面描述的相同，在开始GC时，我们将根节点对象拷贝到to空间，但是根节点中的指针还是指向了位于from空间的对象。\n现在unscanned区域包括了所有的对象（注，现在只有根节点），我们会将unscanned区域的权限设置为None。\n这意味着，当开始GC之后，应用程序第一次使用根节点，它会得到Page Fault，因为这部分内存的权限为None。\n在Page Fault Handler中，GC需要扫描位于内存Page中所有的对象，然后将这些对象所指向的其他对象从from空间forward到to空间。\n所以，在GC最开始的时候，我们将根节点拷贝过来了；之后在Page Fault Handler中通过扫描，将根节点指向的对象也都拷贝过来了。\n在我们的例子中根节点指向的只有两个对象，这两个对象会被拷贝到unscanned区域中，而根节点会被标记成scanned。\n在我们扫描完一个内存Page中的对象时，我们可以通过Unprot（注，详见17.1）恢复对应内存Page的权限。\n之后，应用程序就可以访问特定的对象，因为我们将对象中的指针转换成了可以安全暴露给应用程序的指针（注，因为这些指针现在指向了位于to空间的对象），所以应用程序可以访问这些指针。\n当然这些指针对应的对象中还没有被扫描。如果dereference这些指针，我们会再次得到Page Fault，之后我们会继续扫描。\n这种方案的好处是，它仍然是递增的GC，因为每次只需要做一小部分GC的工作。\n除此之外，它还有额外的优势：现在不需要对指针做额外的检查了（注，也就是不需要查看指针是不是指向from空间，如果是的话，将其forward到to空间）。\n或者说指针检查还在，只是现在通过虚拟内存相关的硬件来完成了。\n学生提问：刚刚说到在Handler里面会扫描一个Page中的所有对象，但是对象怎么跟内存Page对应起来呢？\nFrans教授：在最开始的时候，to空间是没有任何对象的。\n当需要forward的时候，我刚刚描述的是拷贝一个对象，但是实际上拷贝的是一个内存Page中的N个对象，这样它们可以填满整个Page。\n所以现在我们在to空间中，有N个对象位于一个Page中，并且它们都没有被扫描。\n之后某个时间，Page Fault Handler会被调用，GC会遍历这个内存Page上的N个对象，并检查它们的指针。\n对于这些指针，GC会将对应的对象拷贝到to空间的unscanned区域中。\n之后，当应用程序使用了这些未被扫描的对象，它会再次得到Page Fault，进而再扫描这些对象，以此类推。\n学生提问：在完成了GC之后，会切换from和to空间吗？\nFrans教授：最开始我们使用的是from空间，当用完了的时候，你会将对象拷贝到to空间，一旦完成了扫描，from空间也被完全清空了，你可以切换两个空间的名字。现在会使用to空间来完成内存分配。直到它也满了，你会再次切换。\n在理解 to 空间的 unscanned 区域时，首先要明确 to 空间的作用及其与 from 空间的关系。这通常涉及到一种叫做 半空间垃圾回收算法（或称 复制收集算法），其中内存被划分为两个区域：from 空间和 to 空间。\n在复制收集算法中：\nFrom 空间：存放当前所有正在使用的对象。 To 空间：在垃圾回收过程中，用于接收从 from 空间拷贝过来的活动对象（即被引用的对象）。 to 空间的 unscanned 区域 是指尚未被扫描的对象区域。当垃圾回收（GC）开始时，to 空间为空。GC 会将所有活动对象从 from 空间拷贝到 to 空间，并将这些对象放入 unscanned 区域。\n一旦对象被拷贝到 to 空间，它就会被标记为未扫描（unscanned）。 随后，GC 会依次扫描 to 空间中的对象。扫描的过程中，GC 会查看这些对象的指针，如果指针引用了其他对象，就将这些对象也拷贝到 to 空间。 每次扫描时，未被扫描的对象会被移出 unscanned 区域。这样，可以确保所有活动对象都被处理到，而不活动（垃圾）对象则被丢弃。 Frans 教授提到“拷贝的是一个内存 Page 中的多个对象”，这是为了提高效率。在分页（Paging）机制下：\n系统通常会按页（Page）为单位处理内存，而非单个对象。 这意味着，在需要时，GC 会一次性地将整个页中的多个对象拷贝到 to 空间。这是因为单次页内存的映射和传输速度要比逐个拷贝对象更快。 当 GC 将整个页面中的对象拷贝到 to 空间时，它们可以填满整个页，避免频繁的页内存操作和碎片化问题。 在复制过程中，只拷贝活动对象。垃圾回收器通过对象的引用关系来判断哪些对象是活动的，哪些对象是垃圾：\n从 from 空间拷贝对象到 to 空间时，GC 只处理那些仍然被程序引用的对象。这些对象是通过扫描根对象引用链找到的，而不活动的对象（没有被引用）不会被拷贝。 所以，即使是一个页面中的部分对象是垃圾，GC 也不会将这些垃圾对象拷贝到 to 空间。 在垃圾回收完成后：\nfrom 空间中原有的对象会被标记为可以回收。此时，to 空间中的对象成为新的活动对象集。 空间交换：from 和 to 空间交换角色。原来的 to 空间变为新的 from 空间，用于分配和存放新对象；原来的 from 空间则被完全清空，作为下一次 GC 的目标。 在垃圾回收过程中，to 空间的 unscanned 区域存放的是从 from 空间拷贝过来的、尚未被扫描的活动对象。通过扫描这些对象，GC 可以递归地找到并拷贝其他活动对象，避免将垃圾对象转移到 to 空间。这样可以保证 to 空间在完成后只包含当前的活动对象，垃圾被完全丢弃在 from 空间中等待清空。\n论文中提到使用虚拟内存的另一个好处是，它简化了GC的并发。\nGC现在可以遍历未被扫描的内存Page，并且一次扫描一个Page，同时可以确保应用程序不能访问这个内存Page，因为对于应用程序来说，未被扫描的内存Page权限为None。\n虚拟内存硬件引入了这种显式的同步机制，或者说对于抢占的保护。\n现在只有GC可以访问未被扫描的内存Page，而应用程序不能访问。\n所以这里提供了自动的并发，应用程序可以运行并完成它的工作，GC也可以完成自己的工作，它们不会互相得罪，因为一旦应用程序访问了一个未被扫描的Page，它就会得到一个Page Fault。\n而GC也永远不会访问扫描过的Page，所以也永远不会干扰到应用程序。所以这里以近乎零成本获取到了并发性。\n但是实际上有个麻烦的问题。回到我们之前那张图，我们在heap中有from空间，to空间。在to空间中又分为了unscanned和scanned区域，对于应用程序来说，unscanned区域中的Page权限为None。\n这就引出了另一个问题，GC怎么能访问这个区域的内存Page？因为对于应用程序来说，这些Page是inaccessible。\n这里的技巧是使用map2（注，详见17.1）。\n这里我们会将同一个物理内存映射两次，第一次是我们之前介绍的方式，也就是为应用程序进行映射，第二次专门为GC映射。\n在GC的视角中，我们仍然有from和to空间。在to空间的unscanned区域中，Page具有读写权限。\n所以GC可以遍历这些内存Page，读取内容并forward必要的对象。\n这里使用了map2将物理内存映射到应用程序地址空间中两次的能力，其中每次映射都有不同的权限，这样这里的场景才能工作。\n学生提问：GC和应用程序是不是有不同的Page Table？\nFrans教授：不，它们拥有相同的Page Table。它们只是将物理内存映射到了地址空间的两个位置，也就是Page Table的两个位置。在一个位置，PTE被标记成invalid，在另一个位置，PTE被标记成可读写的。\n使用虚拟内存特性的GC代码展示 为了更清晰的说明上一节的内容，我这里有个针对论文中方法的简单实现，我可以肯定它包含了一些bug，因为我并没有认真的测试它。\n首先，应用程序使用的API包括了new和readptr。\n1 2 3 4 //API to collector struct elem *readptr(struct elem **ptr); struct elem *new(); readptr会检查指针是否位于from空间，如果是的话，那么它指向的对象需要被拷贝。\n当然，当我们使用虚拟内存时，这里的readptr成本会比较低，它会直接返回参数。\n在这个简单的例子中，我有一个循环链表，并且有两个根节点，其中一个指向链表的头节点，另一个指向链表的尾节点。\n应用程序线程的工作是循环1000次，每次创建list，再检查list。\n1 2 3 4 5 6 7 8 void app_thread(void *x){ for (int i 0;i\u0026lt;1000;i++) { make_clist(); check_clist(LISTSZ); } } 所以它会产生大量的垃圾，因为每次make_clist完成之后，再次make_clist，上一个list就成为垃圾了。所以GC必然会有一些工作要做。\nmake_clist的代码有点丑，主要是因为每个指针都需要被readptr检查包围。\n通常这里的检查代码是由编译器生成的。\n但是我这里并没有一个针对带GC的编程语言的编译器，所以我只能模仿一个编译器可能生成的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void make_clist(void){ struct elem *e; root head = new(); readptr(\u0026amp;root_head)-\u0026gt;val = 0; readptr(\u0026amp;root_head)-\u0026gt;next = readptr(\u0026amp;root_head); root_last = readptr(\u0026amp;root_head); for (int i = 1;i \u0026lt; LISTSZ;i++){ e = new(); readptr(\u0026amp;e)-\u0026gt;next = readptr(\u0026amp;root_head); readptr(\u0026amp;e)-\u0026gt;val = i; root_head = readptr(\u0026amp;e); readptr(\u0026amp;root_last)-\u0026gt;next = readptr(\u0026amp;root_head); check_clist(i+1); } } make_clist会构建一个LISTSZ大小的链表，分配新的元素，并将新元素加到链表的起始位置，之后更新链表尾指针指向链表新的起始位置，这样就能构成一个循环链表。\n这里更有趣的部分是，GC部分怎么实现。\n首先让我们看看如果没有虚拟内存会怎样。我们只需要查看两个API：new和readptr。\n以上就是new的实现，先不考虑这里的mutex，因为这是为基于虚拟内存的实现提供的。\n先假设我们不需要扫描，也不需要collect。\n接下来会检查是否有足够的空间，如果有足够的空间，我们就将指针地址增加一些，以分配内存空间给新的对象，最后返回。\n如果没有足够的空间，我们需要调用flip，也就是运行GC。\nflip首先会切换from和to指针，之后将这个应用程序的两个根节点从from空间forward到to空间。接下来我们看一下forward函数。\n这个函数会forward指针o指向的对象，首先检查指针o是不是在from空间，如果是的话，并且之前没有被拷贝过，那么就将它拷贝到to空间。\n如果之前拷贝过，那么就可以用to空间的指针代替对象指针，并将其返回。\n对于readptr，如果我们没有使用虚拟内存。\n会对指针p做forward操作，forward操作的意思是如果对象在from空间，那么就将其拷贝到to空间，所以这里会有耗时的检查。\n接下来我们看一下这里如何使用虚拟内存。\n首先是设置内存，通过shm_open 创建一个Share-memory object，shm_open是一个Linux/Uinx系统调用。Share-memory object表现的像是一个文件，但是它并不是一个文件，它位于内存，并没有磁盘文件与之对应，如果你愿意的话，可以认为它是一个位于内存的文件系统。\n之后我们裁剪这个Shared-memory object到from和to空间的大小。\n之后我们通过mmap先将其映射一次，以供mutator也就是实际的应用程序使用。然后再映射一次，以供GC使用。这里shm_open，ftruncate，和两次mmap，等效于map2。\n回过去看之前的代码，\n使用了虚拟内存之后，readptr将不做任何事情，直接将参数返回。\n当然，如果我们使用这里的指针，并且指针对应的对象位于unscanned区域，我们会得到Page Fault。\n在Page Fault hanlder中，GC会运行scan函数。\n但是scan函数是以GC对应的PTE来运行的，所以它能工作。\n而同时，应用程序或者mutator不能访问这些Page，如果访问了的话，这会产生Page Fault。\n一旦scan执行完成，handler中会将Page设置成对应用程序可访问的（注，也就是调用mprotect）。\n在flip函数中，\n完成from和to空间的切换时，如果使用了虚拟内存，我们会通过mprotect将整个to空间对应用程序标记成不可访问的。\n之后GC将root_head和root_last移到to空间中，这样应用程序就不能访问这两个对象，任何时候应用程序需要访问这两个对象，都会导致一个Page Fault。\n在Page Fault handler中，GC可以将其他对象从from空间拷贝到to空间，然后再Unprot对应的Page。\n在Page Fault handler中，先scan内存Page，再将内存Page标记成对应用程序可访问的这个顺序是至关重要的。\n因为如果你先将内存Page标记成应用程序可访问的，然后再扫描它，如果有多个应用程序线程，那么应用程序可能会查看到unscanned区域的对象。\n当然我们要禁止这一点（注，因为为了避免抢占，unscanned区域只能GC访问），所以这里的代码是先扫描，再增加内存的访问权限，这样应用程序就可以安全的访问这些内存Page。\n接下来，我总结一下这节课的内容。有一个问题，你应该在这里使用虚拟内存吗？或者说这里的这些技巧值得吗？\n许多的GC并没有使用虚拟内存，而是通过编译器生成的代码来完成GC，并且还有各种其他的技巧来减少性能损耗。所以GC的大部分场景都可以通过一些额外的指令来完成。\n这对于一个编译器，程序运行时，或者编程语言来说，并不是一个太糟糕的选择，因为编译器就可以完成这些操作。\n但是如果没有程序运行时或者编译器，那么这个过程就会很痛苦。所以对于一些完全没有编译器参与的应用程序，例如checkpointing，shared-virtual memory，它们的确需要这里提到的虚拟内存特性。\n实际中，足够多的应用程序开发人员发现了这些特性的价值，所以今天的操作系统都支持了这些虚拟内存特性。\n很多人问了这个问题，从91年（论文发表的年份）至今，虚拟内存系统发生了什么改变？其中一个改变是，大部分的Unix系统都支持了这些虚拟内存特性了，并且从91年至今有许多变化。\n或许很难想象，但是在虚拟内存系统中有持续的开发，所以如果你查看Linux的git log，你可以发现在内核的各个方面都有持续的开发，其中包括了对虚拟内存系统的持续开发。在过去有一些重大的改变，比如说：\n现在的Page Table是5级的，这样可以处理非常大的地址 可以通过地址空间标识符来处理TLB flush 大概一年前，一种叫做KPTI（kernel page table isolation）的功能被引入，它是针对Meltdown attack的功能 虚拟内存系统绝对不是一个静态的系统，几乎Linux内核的所有方向都不是静态的。\n几乎每两个月在内核的不同方向都会有大量的更新。所以每个子系统时不时的就会被重写。\n学生提问：VMA中的连续地址是什么意思？\nFrans教授：这里是指连续的虚拟内存地址，比如说一个VMA表示1000-2000这段地址。如果你有另一段地址，2100-2200，那么它会有属于自己的VMA。所以每个VMA覆盖了一段连续的地址，中间不会有中断。你们将会在mmap lab中看到这样的设计是更加的合理的。你们可以认为对于每个mmap系统调用，如果地址没有重叠的话，都会有一个VMA。\n学生提问：GC什么时候会停止，什么时候又会再开始？我认为GC可以一直运行，如果它是并发的。\nFrans教授：是的，基于虚拟内存的解决方案一个酷的地方在于，GC可以一直运行。它可以在没有unscanned对象时停止。\n学生提问：但是你需要遍历所有在from空间的对象，你怎么知道已经遍历了所有的对象呢？\nFrans教授：你会从根节点开始扫描整个对象的图，然后拷贝到to空间。在某个时间点，你不再添加新的对象了，因为所有的对象已经被拷贝过了。当你不再添加新的对象，你的unscanned区域就不再增长，如果它不再增长，那么你就遍历了所有的对象（注，可以想象一个普通的DFS或者BFS过程）。\n","date":"2024-10-02T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08117virtual-memory-for-applications/bg_hu_33a60326029931f1.png","permalink":"https://echudet.github.io/p/mit6s08117virtual-memory-for-applications/","title":"MIT6S081：17Virtual memory for applications"},{"content":"Lecture 16 File system performance and fast crash recovery 课前预习 论文链接 什么是文件系统 对于任何文件系统，我们都需要什么功能？文件系统所服务的操作系统有明确的要求。文件系统对应用程序的表现方式是：一个操作系统通常需要遵守某些约定的文件名，并且文件具有某些以特定方式解释的属性。\n然而，文件系统的许多内部方面没有那么受约束，文件系统实现者可以在一定程度上自由地设计这些方面。磁盘上数据的布局（或者，如果文件系统不是本地的，它的网络协议）、内部缓存的细节以及用于调度磁盘IO的算法——在不违反文件系统应用程序接口规范的前提下，这些都是可以改变的。\n我们可能选择一种而不是另一种设计的原因有很多。与旧文件系统的兼容性可能是一个问题：例如，Linux提供了一个UMSDOS文件系统，它在标准MSDOS磁盘文件结构的基础上实现了POSIX文件系统的语义学。\n当试图解决Linux上文件系统恢复时间过长的问题时，我们牢记许多目标：\n使用新文件系统不会严重影响性能； 不得破坏与现有应用程序的兼容性 文件系统的可靠性不得以任何方式受到损害。 当我们谈论文件系统的可靠性时，有许多问题利害攸关。就本特定项目而言，我们主要关心的是恢复崩溃文件系统内容的可靠性，我们可以确定其中的几个方面：\n保持（Preservation）：崩溃前磁盘上稳定的数据永远不会被损坏。显然，崩溃时正在写入的文件不能保证完全完好无损，但是恢复系统不能碰磁盘上已经安全的任何文件。\n可预测性（Predictability）：我们必须恢复的故障模式应该是可预测的，以便我们可靠地恢复。\n原子性（Atomicity）：许多文件系统操作需要大量独立的IO来完成。一个很好的例子是将文件从一个目录重命名到另一个目录。如果这样的文件系统操作在磁盘上完全完成，或者在恢复完成后完全撤销，恢复就是原子性的。（对于重命名的例子，恢复应该在崩溃后保留提交给磁盘的旧文件名或新文件名，但不能两者都保留。）\n日志文件的工作很简单：它在我们提交事务的过程中记录文件系统元数据块的新内容。日志的唯一其他要求是我们必须能够原子地提交它包含的事务。\n我们向日志写入三种不同类型的数据块：元数据块、描述符块和头块（metadata, descriptor and header blocks）。\n日志元数据块包含由事务更新的单个文件系统元数据块的全部内容。这意味着，无论我们对文件系统元数据块做了多么小的更改，我们都必须写出整个日志块来记录更改。然而，由于两个原因，这一成本相对较低：\n无论如何，日志写入非常快，因为对日志的大多数写入都是顺序的，我们可以很容易地将日志IO批处理成大型集群，磁盘控制器可以有效地处理这些集群； 通过将更改后的元数据缓冲区的全部内容从文件系统缓存写入日志，我们可以避免在日志代码中执行大量CPU工作。 Linux内核已经为我们提供了一种非常有效的机制，可以将buffer cache中现有块的内容写到磁盘上的不同位置。buffer cache中的每个缓冲区都由一个名为buffer_head的结构体描述，该结构体包括缓冲区的数据要写到哪个磁盘块的信息。如果我们想将整个缓冲区块在不干扰buffer_head的情况下写入新位置，我们可以简单地创建一个新的临时buffer_head，将旧的描述复制到其中，然后编辑临时buffer_head中的设备块编号字段，以指向日志文件中的块。然后，我们可以将临时buffer_head直接提交给设备IO系统，并在IO完成后丢弃它。\n描述符块是描述其他日志元数据块的日志块，每当我们要将元数据块写出到日志时，我们需要记录下元数据通常安置在哪些磁盘块，这样恢复机制就可以将元数据复制回主文件系统中。在日志中的每一组元数据块之前都会写出一个描述符块，其中包含要写入的元数据块的数量加上它们的磁盘块号。\n描述符块和元数据块都按顺序写入日志，每当我们运行超过末尾时，都会从日志的开头重新开始。在任何时候，我们都维护当前的日志头（最后写入的块的块号）和尾部（日志中尚未取消固定的最老的块，如下所述）。每当我们用完日志空间时——日志的头部已经循环回来并赶上了尾部——我们会停止新的日志写入，直到日志的尾部被清理干净，以释放更多的空间。\n最后，日志文件包含一些位于固定位置的头块。这些头块记录了日志的当前头部和尾部，加上序列号。在恢复时，头块被扫描以找到序列号最高的块，当我们在恢复过程中扫描日志时，我们只是运行从尾部到头部的所有日志块，就像头块中记录的那样。\n在某个时候，要么是因为上次提交后我们已经等了足够长的时间，要么是因为日志中的空间不足，我们希望将未完成的文件系统更新作为一个新的复合事务提交到日志中。\n复合事务被完全提交后，我们仍然没有完成它。我们需要跟踪记录在事务中的元数据缓冲区，这样我们就可以注意到它们何时被写回磁盘上的主位置。\n回想一下，当我们提交事务时，新更新的文件系统块位于日志中，但尚未同步回磁盘上的永久家块（家块就是写入操作对应的磁盘中文件系统对应的块，我们需要保持旧块的这种不同步，以防在提交日志之前崩溃）。一旦提交了日志，磁盘上的旧版本就不再重要，我们可以在闲暇时将缓冲区写回它们的主位置。但是，在同步完这些缓冲区之前，我们不能删除日志中数据的副本。\n要完全提交并完成事务的检查点，我们将经历以下阶段：\n关闭事务。在此刻，我们会建立一个新的事务以记录未来开始的任何文件系统操作。任何现有的、不完整的操作仍然会使用现有的事务：我们不能在多个事务上拆分单个文件系统操作！ 开始将事务刷新到磁盘。在一个单独的log-writer内核线程的上下文中，我们开始向日志写入所有被事务修改过的元数据缓冲区。在这个阶段，我们还必须写入任何依赖数据（参见上面的部分：事务解剖）。 提交缓冲区后，将其标记以固定事务，直到它不再脏（它已通过通常的写回机制写回主存储）。 等待此事务中所有未完成的文件系统操作完成。我们可以在所有操作完成之前安全地开始写日志，允许这两个步骤在某种程度上重叠会更快。 等待所有未完成的事务更新完全记录在日志中。 更新日志头块以记录日志的新头部和尾部，将事务提交到磁盘。space released in the journal can now be reused by a later transaction. 当我们将事务的更新缓冲区写到日志中时，我们将它们标记以将事务固定在日志中。只有当这些缓冲区已同步到磁盘上的主缓冲区时，它们才会解除固定。只有当事务的最后一个缓冲区取消固定时，我们才能重用事务占用的日志块。当发生这种情况时，写入另一组日志头，记录日志尾部的新位置。日志中释放的空间现在可以由以后的事务重用。 正课 Why logging 这节课讲的是Linux中的广泛使用的ext3文件系统所使用的logging系统，同时我们也会讨论在高性能文件系统中添加log需要面对的一些问题。\n首先我会花几分钟来回顾一下，为什么我们要学习logging。\n之所以我们认为logging很重要，是因为这是一个巨大的成功和重要的思想，几乎可以认为logging是一种魔法，它可以用在任何一个已知的存储系统的故障恢复流程中，它在很多地方都与你想存储的任何数据相关。\n所以你们可以在大量的存储场景中看到log，比如说数据库，文件系统，甚至一些需要在crash之后恢复的定制化的系统中。\n你们也可以发现log作为从故障中恢复的机制，在分布式系统中也有大量的应用。\n因为log是一种可以用来表示所有在crash之前发生事情的数据结构，如果能够理解log，那么就可以更容易的从crash中恢复过来。\n除此之外，当你尝试构建高性能logging系统时，log本身也有大量有意思的地方。\n这里有一个术语，当我们谈到log时，与今天阅读的论文 所用到的journal是同一件事情，它们是同义词。\n除此之外，今天的论文是有关向ext2增加journal，并得到ext3文件系统（注，所以可以认为ext3文件系统就是ext2加上了logging系统）。\n接下来我将从这几个方面来讨论ext3文件系统：某种程度上将其与XV6进行对比；解释ext3是如何修复XV6的logging存在的性能问题；解释ext3在故障恢复时语义上的一些变化。\nXV6 File system logging回顾 首先来回顾一下XV6的logging系统。\n我们有一个磁盘用来存储XV6的文件系统，你可以认为磁盘分为了两个部分：\n首先是文件系统目录的树结构，以root目录为根节点，往下可能有其他的目录，我们可以认为目录结构就是一个树状的数据结构。\n假设root目录下有两个子目录D1和D2，D1目录下有两个文件F1和F2，每个文件又包含了一些block。\n除此之外，还有一些其他并非是树状结构的数据，比如bitmap表明了每一个data block是空闲的还是已经被分配了。\n注，Frans和Robert在这里可能有些概念不统一，对于Frans来说，目录内容应该也属于文件内容，目录是一种特殊的文件，详见14.3；而对于Robert来说，目录内容是metadata。\ninode，目录内容，bitmap block，我们将会称之为metadata block，另一类就是持有了文件内容的block，或者叫data block。\n除了文件系统之外，XV6在磁盘最开始的位置还有一段log。\nXV6的log相对来说比较简单，它有header block，之后是一些包含了有变更的文件系统block，这里可以是metadata block也可以是data block。\nheader block会记录之后的每一个log block应该属于文件系统中哪个block，假设第一个log block属于block 17，第二个属于block 29。\n在计算机上，我们会有一些用户程序调用write/create系统调用来修改文件系统。\n在内核中存在block cache，最初write请求会被发到block cache。\nblock cache就是磁盘中block在内存中的拷贝，所以最初对于文件block或者inode的更新走到了block cache。\n在write系统调用的最后，这些更新都被拷贝到了log中，之后我们会更新header block的计数来表明当前的transaction已经结束了。\n在文件系统的代码中，任何修改了文件系统的系统调用函数中，某个位置会有begin_op，表明马上就要进行一系列对于文件系统的更新了，不过在完成所有的更新之前，不要执行任何一个更新。\n在begin_op之后是一系列的read/write操作。最后是end_op，用来告诉文件系统现在已经完成了所有write操作。\n所以在begin_op和end_op之间，所有的write block操作只会走到block cache中。当系统调用走到了end_op函数，文件系统会将修改过的block cache拷贝到log中。\n在拷贝完成之后，文件系统会将修改过的block数量，通过一个磁盘写操作写入到log的header block，这次写入被称为commit point。\n在commit point之前，如果发生了crash，在重启时，整个transaction的所有写磁盘操作最后都不会应用。\n在commit point之后，即使立即发生了crash，重启时恢复软件会发现在log header中记录的修改过的block数量不为0，接下来就会将log header中记录的所有block，从log区域写入到文件系统区域。\n这里实际上使得系统调用中位于begin_op和end_op之间的所有写操作在面对crash时具备原子性。\n也就是说，要么文件系统在crash之前更新了log的header block，这样所有的写操作都能生效；要么crash发生在文件系统更新log的header block之前，这样没有一个写操作能生效。\n在crash并重启时，必须有一些恢复软件能读取log的header block，并判断里面是否记录了未被应用的block编号，如果有的话，需要写（也有可能是重写）log block到文件系统中对应的位置；如果没有的话，恢复软件什么也不用做。\n这里有几个超级重要的点，不仅针对XV6，对于大部分logging系统都适用：\n包括XV6在内的所有logging系统，都需要遵守write ahead rule。\n这里的意思是，任何时候如果一堆写操作需要具备原子性，系统需要先将所有的写操作记录在log中，之后才能将这些写操作应用到文件系统的实际位置。\n也就是说，我们需要预先在log中定义好所有需要具备原子性的更新，之后才能应用这些更新。\nwrite ahead rule是logging能实现故障恢复的基础。\nwrite ahead rule使得一系列的更新在面对crash时具备了原子性。\n另一点是，XV6对于不同的系统调用复用的是同一段log空间，但是直到log中所有的写操作被更新到文件系统之前，我们都不能释放或者重用log。\n我将这个规则称为freeing rule，它表明我们不能覆盖或者重用log空间，直到保存了transaction所有更新的这段log，都已经反应在了文件系统中。\n所以在XV6中，end_op做了大量的工作，首先是将所有的block记录在log中，之后是更新log header。\n在没有crash的正常情况，文件系统需要再次将所有的block写入到磁盘的文件系统中。\n磁盘中的文件系统更新完成之后，XV6文件系统还需要删除header block记录的变更了的block数量，以表明transaction已经完成了，之后就可以重用log空间。\n在向log写入任何新内容之前，删除header block中记录的block数量也很重要。\n因为你不会想要在header block中记录的还是前一个transaction的信息，而log中记录的又是一个新的transaction的数据。\n可以假设新的transaction对应的是与之前不同的block编号的数据，这样的话，在crash重启时，log中的数据会被写入到之前记录的旧的block编号位置。所以我们必须要先清除header block。\nfreeing rule的意思就是，在从log中删除一个transaction之前，我们必须将所有log中的所有block都写到文件系统中。\n这些规则使得，就算一个文件系统更新可能会复杂且包含多个写操作，但是每次更新都是原子的，在crash并重启之后，要么所有的写操作都生效，要么没有写操作能生效。\n要介绍Linux的logging方案，就需要了解XV6的logging有什么问题？为什么Linux不使用与XV6完全一样的logging方案？这里的回答简单来说就是XV6的logging太慢了。\nXV6中的任何一个例如create/write的系统调用，需要在整个transaction完成之后才能返回。所以在创建文件的系统调用返回到用户空间之前，它需要完成所有end_op包含的内容，这包括了：\n将所有更新了的block写入到log 更新header block 将log中的所有block写回到文件系统分区中 清除header block 之后才能从系统调用中返回。\n在任何一个文件系统调用的commit过程中，不仅是占据了大量的时间，而且其他系统调用也不能对文件系统有任何的更新。\n所以这里的系统调用实际上是一次一个的发生，而每个系统调用需要许多个写磁盘的操作。\n这里每个系统调用需要等待它包含的所有写磁盘结束，对应的技术术语被称为synchronize。\nXV6的系统调用对于写磁盘操作来说是同步的（synchronized），所以它非常非常的慢。\n在使用机械硬盘时，它出奇的慢，因为每个写磁盘都需要花费10毫秒，而每个系统调用又包含了多个写磁盘操作。\n所以XV6每秒只能完成几个更改文件系统的系统调用。如果我们在SSD上运行XV6会快一些，但是离真正的高效还差得远。\n另一件需要注意的更具体的事情是，在XV6的logging方案中，每个block都被写了两次。第一次写入到了log，第二次才写入到实际的位置。\n虽然这么做有它的原因，但是ext3可以一定程度上修复这个问题。\next3 file system log format ext3文件系统就是基于今天要阅读的论文 ，再加上几年的开发得到的，并且ext3也曾经广泛的应用过。\next3是针对之前一种的文件系统（ext2）logging方案的修改，所以ext3就是在几乎不改变之前的ext2文件系统的前提下，在其上增加一层logging系统。\n所以某种程度来说，logging是一个容易升级的模块。\next3的数据结构与XV6是类似的。\n在内存中，存在block cache，这是一种write-back cache（注，区别于write-through cache，指的是cache稍后才会同步到真正的后端）。\nblock cache中缓存了一些block，其中的一些是干净的数据，因为它们与磁盘上的数据是一致的；\n其他一些是脏数据，因为从磁盘读出来之后被修改过；\n有一些被固定在cache中，基于前面介绍的write-ahead rule和freeing rule，不被允许写回到磁盘中。\n除此之外，ext3还维护了一些transaction信息。它可以维护多个在不同阶段的transaction的信息。\n每个transaction的信息包含有：\n一个序列号 一系列该transaction修改的block编号。这些block编号指向的是在cache中的block，因为任何修改最初都是在cache中完成。 以及一系列的handle，handle对应了系统调用，并且这些系统调用是transaction的一部分，会读写cache中的block 在磁盘上，与XV6一样：\n会有一个文件系统树，包含了inode，目录，文件等等 会有bitmap block来表明每个data block是被分配的还是空闲的 在磁盘的一个指定区域，会保存log 目前为止，这与XV6非常相似。\n主要的区别在于ext3可以同时跟踪多个在不同执行阶段的transaction。\n接下来我们详细看一下ext3的log中有什么，这与XV6中的log有点不一样。\n在log的最开始，是super block。\n这是log的super block，而不是文件系统的super block。\nlog的super block包含了log中第一个有效的transaction的起始位置和序列号。\n起始位置就是磁盘上log分区的block编号，序列号就是前面提到的每个transaction都有的序列号。\nlog是磁盘上一段固定大小的连续的block。\nlog中，除了super block以外的block存储了transaction。\n每个transaction在log中包含了：\n一个descriptor block，其中包含了log数据对应的实际block编号，这与XV6中的header block很像。 之后是针对每一个block编号的更新数据。 最后当一个transaction完成并commit了，会有一个commit block 因为log中可能有多个transaction，commit block之后可能会跟着下一个transaction的descriptor block，data block和commit block。\n所以log可能会很长并包含多个transaction。我们可以认为super block中的起始位置和序列号属于最早的，排名最靠前的，并且是有效的transaction。\n这里有一些细节对于后面的内容很重要。\n在crash之后的恢复过程会扫描log，为了将descriptor block和commit block与data block区分开，descriptor block和commit block会以一个32bit的魔法数字作为起始。\n这个魔法数字不太可能出现在数据中，并且可以帮助恢复软件区分不同的block。\n学生提问：有没有可能使用一个descriptor block管理两个transaction？是不是只能一个transaction结束了才能开始下一个transaction？\nRobert教授：Log中会有多个transaction，但是的确一个时间只有一个正在进行的transaction。\n上面的图片没能很好的说明这一点，当前正在进行的transaction对应的是正在执行写操作的系统调用。\n所以当前正在进行的transaction只存在于内存中，对应的系统调用只会更新cache中的block，也就是内存中的文件系统block。\n当ext3决定结束当前正在进行的transaction，它会做两件事情：\n首先开始一个新的transaction，这将会是下一个transaction；其次将刚刚完成的transaction写入到磁盘中，这可能要花一点时间。\n所以完整的故事是，磁盘上的log分区有一系列旧的transaction，这些transaction已经commit了，除此之外，还有一个位于内存的正在进行的transaction。\n在磁盘上的transaction，只能以log记录的形式存在，并且还没有写到对应的文件系统block中。\nlogging系统在后台会从最早的transaction开始，将transaction中的data block写入到对应的文件系统中。\n当整个transaction的data block都写完了，之后logging系统才能释放并重用log中的空间。\n所以log其实是个循环的数据结构，如果用到了log的最后，logging系统会从log的最开始位置重新使用。\n记住这里的log的结构，它对于后面的内容也很重要。\next3如何提升性能 ext3通过3种方式提升了性能：\n首先，它提供了异步的（asynchronous）系统调用，也就是说系统调用在写入到磁盘之前就返回了，系统调用只会更新缓存在内存中的block，并不用等待写磁盘操作。不过它可能会等待读磁盘。 第二，它提供了批量执行（batching）的能力，可以将多个系统调用打包成一个transaction。 最后，它提供了并发（concurrency）。 这些基本上就是ext3有的，而XV6没有的特性。接下来我将一一介绍这里的特性。\n学生提问：有关batching，XV6不是也支持多个系统调用同时执行start_op和end_op，然后再一起commit吗？\nRobert教授：是的，XV6具备有限能力的batching。\n首先是异步的系统调用。\n这表示系统调用修改完位于缓存中的block之后就返回，并不会触发写磁盘。\n所以这里明显的优势就是系统调用能够快速的返回。\n同时它也使得I/O可以并行的运行，也就是说应用程序可以调用一些文件系统的系统调用，但是应用程序可以很快从系统调用中返回并继续运算，与此同时文件系统在后台会并行的完成之前的系统调用所要求的写磁盘操作。\n这被称为I/O concurrency，如果没有异步系统调用，很难获得I/O concurrency，或者说很难同时进行磁盘操作和应用程序运算，因为同步系统调用中，应用程序总是要等待磁盘操作结束才能从系统调用中返回。\n另一个异步系统调用带来的好处是，它使得大量的批量执行变得容易。\n异步系统调用的缺点是系统调用的返回并不能表示系统调用应该完成的工作实际完成了。\n举个例子，如果你创建了一个文件并写了一些数据然后关闭文件并在console向用户输出done，最后你把电脑的电给断了。\n尽管所有的系统调用都完成了，程序也输出了done，但是在你重启之后，你的数据并不一定存在。\n这意味着，在异步系统调用的世界里，如果应用程序关心可能发生的crash，那么应用程序代码应该更加的小心。\n这在XV6并不是什么大事，因为如果XV6中的write返回了，那么数据就在磁盘上，crash之后也还在。\n而ext3中，如果write返回了，你完全不能确定crash之后数据还在不在。\n所以一些应用程序的代码应该仔细编写，例如对于数据库，对于文本编辑器，我如果写了一个文件，我不想在我写文件过程断电然后再重启之后看到的是垃圾文件或者不完整的文件，我想看到的要么是旧的文件，要么是新的文件。\n所以文件系统对于这类应用程序也提供了一些工具以确保在crash之后可以有预期的结果。\n这里的工具是一个系统调用，叫做fsync，所有的UNIX都有这个系统调用。\n这个系统调用接收一个文件描述符作为参数，它会告诉文件系统去完成所有的与该文件相关的写磁盘操作，在所有的数据都确认写入到磁盘之后，fsync才会返回。\n所以如果你查看数据库，文本编辑器或者一些非常关心文件数据的应用程序的源代码，你将会看到精心放置的对于fsync的调用。\nfsync可以帮助解决异步系统调用的问题。\n对于大部分程序，例如编译器，如果crash了编译器的输出丢失了其实没什么，所以许多程序并不会调用fsync，并且乐于获得异步系统调用带来的高性能。\n学生提问：这是不是有时也被称为flush，因为我之前经常听到这个单词？\nRobert教授：是的，一个合理的解释fsync的工作的方式是，它flush了所有文件相关的写磁盘操作到了磁盘中，之后再返回，所以flush也是针对这个场景的一个合理的单词。\n以上就是异步系统调用，下一个ext3使用的技术是批量执行（batching）。\n在任何时候，ext3只会有一个open transaction。\next3中的一个transaction可以包含多个不同的系统调用。\n所以ext3是这么工作的：它首先会宣告要开始一个新的transaction，接下来的几秒所有的系统调用都是这个大的transaction的一部分。\n我认为默认情况下，ext3每5秒钟都会创建一个新的transaction，所以每个transaction都会包含5秒钟内的系统调用，这些系统调用都打包在一个transaction中。\n在5秒钟结束的时候，ext3会commit这个包含了可能有数百个更新的大transaction。\n为什么这是个好的方案呢？\n首先它在多个系统调用之间分摊了transaction带来的固有的损耗。\n固有的损耗包括写transaction的descriptor block和commit block；在一个机械硬盘中需要查找log的位置并等待磁碟旋转，\n这些都是成本很高的操作，现在只需要对一批系统调用执行一次，而不用对每个系统调用执行一次这些操作，所以batching可以降低这些损耗带来的影响。\n另外，它可以更容易触发write absorption。\n经常会有这样的情况，你有一堆系统调用最终在反复更新相同的一组磁盘block。\n举个例子，如果我创建了一些文件，我需要分配一些inode，inode或许都很小只有64个字节，一个block包含了很多个inode，所以同时创建一堆文件只会影响几个block的数据。\n类似的，如果我向一个文件写一堆数据，我需要申请大量的data block，我需要修改表示block空闲状态的bitmap block中的很多个bit位，如果我分配到的是相邻的data block，它们对应的bit会在同一个bitmap block中，所以我可能只是修改一个block的很多个bit位。\n所以一堆系统调用可能会反复更新一组相同的磁盘block。\n通过batching，多次更新同一组block会先快速的在内存的block cache中完成，之后在transaction结束时，一次性的写入磁盘的log中。\n这被称为write absorption，相比一个类似于XV6的同步文件系统，它可以极大的减少写磁盘的总时间。\n最后就是disk scheduling。\n假设我们要向磁盘写1000个block，不论是在机械硬盘还是SSD（机械硬盘效果会更好），一次性的向磁盘的连续位置写入1000个block，要比分1000次每次写一个不同位置的磁盘block快得多。\n我们写log就是向磁盘的连续位置写block。\n通过向磁盘提交大批量的写操作，可以更加的高效。\n这里我们不仅通过向log中连续位置写入大量block来获得更高的效率，甚至当我们向文件系统分区写入包含在一个大的transaction中的多个更新时，如果我们能将大量的写请求同时发送到驱动，即使它们位于磁盘的不同位置，我们也使得磁盘可以调度这些写请求，并以特定的顺序执行这些写请求，这也很有效。\n在一个机械硬盘上，如果一次发送大量需要更新block的写请求，驱动可以对这些写请求根据轨道号排序。\n甚至在一个固态硬盘中，通过一次发送给硬盘大量的更新操作也可以稍微提升性能。\n所以，只有发送给驱动大量的写操作，才有可能获得disk scheduling。这是batching带来的另一个好处。\next3使用的最后一个技术就是concurrency，相比XV6这里包含了两种concurrency。\n首先ext3允许多个系统调用同时执行，所以我们可以有并行执行的多个不同的系统调用。\n在ext3决定关闭并commit当前的transaction之前，系统调用不必等待其他的系统调用完成，它可以直接修改作为transaction一部分的block。\n许多个系统调用都可以并行的执行，并向当前transaction增加block，这在一个多核计算机上尤其重要，因为我们不会想要其他的CPU核在等待锁。\n在XV6中，如果当前的transaction还没有完成，新的系统调用不能继续执行。\n而在ext3中，大多数时候多个系统调用都可以更改当前正在进行的transaction。\n另一种ext3提供的并发是，可以有多个不同状态的transaction同时存在。\n所以尽管只有一个open transaction可以接收系统调用，但是其他之前的transaction可以并行的写磁盘。\n这里可以并行存在的不同transaction状态包括了：\n首先是一个open transaction 若干个正在commit到log的transaction，我们并不需要等待这些transaction结束。当之前的transaction还没有commit并还在写log的过程中，新的系统调用仍然可以在当前的open transaction中进行。 若干个正在从cache中向文件系统block写数据的transaction 若干个正在被释放的transaction，这个并不占用太多的工作 通常来说会有位于不同阶段的多个transaction，新的系统调用不必等待旧的transaction提交到log或者写入到文件系统。\n对比之下，XV6中新的系统调用就需要等待前一个transaction完全完成。\n学生提问：如果一个block cache正在被更新，而这个block又正在被写入到磁盘的过程中，会怎样呢？\nRobert教授：这的确会是一个问题，这里有个潜在的困难点，因为transaction写入到log中的内容只能包含由该transaction中的系统调用所做的更新，而不能包含在该transaction之后的系统调用的更新。\n因为如果这么做了的话，那么可能log中会只包含系统调用的部分更新，而我们需要确保transaction包含系统调用的所有更新。所以我们不能承担transaction包含任何在该transaction之后的更新的风险。\next3是这样解决这个问题的，当它决定结束当前的open transaction时，它会在内存中拷贝所有相关的block，之后transaction的commit是基于这些block的拷贝进行的。\n所以transaction会有属于自己的block的拷贝。为了保证这里的效率，操作系统会使用copy-on-write（注，详见8.4）来避免不必要的拷贝，这样只有当对应的block在后面的transaction中被更新了，它在内存中才会实际被拷贝。\nconcurrency之所以能帮助提升性能，是因为它可以帮助我们并行的运行系统调用，我们可以得到多核的并行能力。\n如果我们可以在运行应用程序和系统调用的同时，来写磁盘，我们可以得到I/O concurrency，也就是同时运行CPU和磁盘I/O。这些都能帮助我们更有效，更精细的使用硬件资源。\next3文件系统调用格式 接下来我们大概过一下Linux中的文件系统调用，并介绍抽象上每个系统调用的结构。\n在Linux的文件系统中，我们需要每个系统调用都声明一系列写操作的开始和结束。\n实际上在任何transaction系统中，都需要明确的表示开始和结束，这样之间的所有内容都是原子的。\n所以系统调用中会调用start函数。\next3需要知道当前正在进行的系统调用个数，所以每个系统调用在调用了start函数之后，会得到一个handle，它某种程度上唯一识别了当前系统调用。\n当前系统调用的所有写操作都是通过这个handle来识别跟踪的（注，handle是ext3 transaction中的一部分数据，详见16.3）。\n之后系统调用需要读写block，它可以通过get获取block在buffer中的缓存，同时告诉handle这个block需要被读或者被写。\n当这个系统调用结束时，它会调用stop函数，并将handle作为参数传入。\n除非transaction中所有已经开始的系统调用都完成了，transaction是不能commit的。\n因为可能有多个transaction，文件系统需要有种方式能够记住系统调用属于哪个transaction，这样当系统调用结束时，文件系统就知道这是哪个transaction正在等待的系统调用，所以handle需要作为参数传递给stop函数。\n因为每个transaction都有一堆block与之关联，修改这些block就是transaction的一部分内容，所以我们将handle作为参数传递给get函数是为了告诉logging系统，这个block是handle对应的transaction的一部分。\nstop函数并不会导致transaction的commit，它只是告诉logging系统，当前的transaction少了一个正在进行的系统调用。\ntransaction只能在所有已经开始了的系统调用都执行了stop之后才能commit。所以transaction需要记住所有已经开始了的handle，这样才能在系统调用结束的时候做好记录。\next3 transaction commit步骤 基于上面的系统调用的结构，接下来我将介绍commit transaction完整的步骤。\n每隔5秒，文件系统都会commit当前的open transaction，下面是commit transaction涉及到的步骤：\n首先需要阻止新的系统调用。\n当我们正在commit一个transaction时，我们不会想要有新增的系统调用，我们只会想要包含已经开始了的系统调用，所以我们需要阻止新的系统调用。\n这实际上会损害性能，因为在这段时间内系统调用需要等待并且不能执行。\n第二，需要等待包含在transaction中的已经开始了的系统调用们结束。\n所以我们需要等待transaction中未完成的系统调用完成，这样transaction能够反映所有的写操作。\n一旦transaction中的所有系统调用都完成了，也就是完成了更新cache中的数据，那么就可以开始一个新的transaction，并且让在第一步中等待的系统调用继续执行。\n所以现在需要为后续的系统调用开始一个新的transaction。\n还记得ext3中的log包含了descriptor，data和commit block吗？\n现在我们知道了transaction中包含的所有的系统调用所修改的block，因为系统调用在调用get函数时都将handle作为参数传入，表明了block对应哪个transaction。\n接下来我们可以更新descriptor block，其中包含了所有在transaction中被修改了的block编号。\n我们还需要将被修改了的block，从缓存中写入到磁盘的log中。\n之前有同学问过，新的transaction可能会修改相同的block，所以在这个阶段，我们写入到磁盘log中的是transaction结束时，对于相关block cache的拷贝。所以这一阶段是将实际的block写入到log中。\n接下来，我们需要等待前两步中的写log结束。\n之后我们可以写入commit block。\n接下来我们需要等待写commit block结束。\n结束之后，从技术上来说，当前transaction已经到达了commit point，也就是说transaction中的写操作可以保证在面对crash并重启时还是可见的。\n如果crash发生在写commit block之前，那么transaction中的写操作在crash并重启时会丢失。\n接下来我们可以将transaction包含的block写入到文件系统中的实际位置。\n在第9步中的所有写操作完成之后，我们才能重用transaction对应的那部分log空间。\n在一个非常繁忙的系统中，log的头指针一直追着尾指针在跑（注，也就是说一直没有新的log空间）。\n在当前最早的transaction的所有步骤都完成之前，或许不能开始commit一个新的transaction，因为我们需要重复利用最早的transaction对应的log空间。\n不过人们通常会将log设置的足够大，让这种情况就不太可能发生。\n学生提问：你刚刚说没有进程会等待这些步骤完成，那么这些步骤是在哪里完成的呢？\nRobert教授：这些是在后台的内核线程完成的\n学生提问：我有个有关重用log空间的问题，假设我们使用了一段特定的log空间，并且这段log空间占据了是刚刚释放出来的所有log空间，但是还不够，那么文件系统会等待另一部分的log空间释放出来吗，还是会做点别的？\nRobert教授：是的，会等待。让我画张图来确保我回答的是正确的问题。我们可以认为log是磁盘中的一段线性空间，假设现存的transaction中最早的是T7，之后是T8，T9，我们想要将T10放在T9之后的空闲区域。\n我们或许要等待T7将所有的block写入到文件系统对应的位置，这样我们才能释放T7对应的空间。这意味着T10中的步骤需要暂停以等待T7释放出来。这是你的问题吗？\n同一个学生：是的，所以可能是这样，我先写入T10的block到现有的log空闲区域，但是如果最后log足够大并且我们用光了空闲区域，我们就需要等待T7的空间被释放出来，是吗？\nRobert教授：是的，如果需要写入的数据足够多，并且log迅速的用光了。我们甚至都不能在释放log空间之前开始新的系统调用。如果你们关注细节的话，这里会有一些潜在的死锁。首先系统调用需要预声明需要多少个block，这样logging系统才知道对于该transaction需要多少log空间，因为我们不会在没有足够空间来commit transaction时，开始一个新的transaction（注，难道不能将不能写入到磁盘log中的transaction先缓存在内存中吗？虽然这样可能会导致堆积）。\n学生提问：如果新的transaction需要的空间走到了T8，那么现在就需要等待T7，T8结束，这是怎么工作的呢？\nRobert教授：图中的T7，T8，T9其中的系统调用都完成了，并且都已经在commit到log中了。在上面的图中，我们会直接开始T10，新的系统调用会写入到transaction T10，最终当T10需要commit到log中，并且它大到需用用到T8的空间时，它需要等待T7，T8结束。\n文件系统会记录每个transaction的大小，这样文件系统就知道要等待多少个之前的transaction结束。所以这里还有不少的记录工作，这样文件系统才能理解所有旧的transaction的状态。\n有关如何重用log空间，这里有个小细节。\n在log的最开始有一个super block，所以在任何时候log都是由一个super block和一些transaction组成。\n假设T4是最新的transaction，之前是T1，T2，T3。\n我们是否能重用一段log空间，取决于相应的transaction，例如T2，是否已经commit并且写入到文件系统的实际位置中，这样在crash并重启时就不需要重新执行这段transaction了。\n同时也取决于T2之前的的所有transaction是否已经被释放了。所有的这些条件都满足时，我们就可以释放并重用T2对应的log空间。\next3 file system恢复过程 为了简化重启时恢复软件的工作，当决定释放某段log空间时，文件系统会更新super block中的指针将其指向当前最早的transaction的起始位置。\n之后如果crash并重启，恢复软件会读取super block，并找到log的起始位置。\n所以如果crash了，内存中的所有数据都会消失，例如文件系统中记录的哪些block被写入到了磁盘中这些信息都会丢失，所以可以假设这时内存中没有可用的数据，唯一可用的数据存在于磁盘中。\n当然我们这里的讨论都是基于磁盘还是完好的，所以你可以认为只是一次电力故障，系统突然停止了运行过程，在电力恢复时，断电那一瞬间磁盘中的数据还存在。我们并没有考虑磁盘被损坏或者被摧毁的情况。\ncrash或许会打断任何在进行中的transaction，或许transaction正在commit，或许transaction正在向文件系统写block。\n让我重新画一个例子，我们在log中有一个super block，之后是transaction T6，T7，T8，在T8之后是一个已近被释放了log空间的T5，假设T8已经用了T5的一部分空间。\n并且现在super block指向的是T6的起始位置，因为T6是最早的transaction。\n现在crash并重启，恢复软件读取super block就可以知道log的起始位置，之后恢复软件会在log中一直扫描并尝试找到log的结束位置，现在我们需要有一种方式确定log的结束位置。\n我们知道每个transaction包含了一个descriptor block，里面记录了该transaction中包含了多少个data block，假设descriptor block记录了17个block，那么恢复软件会扫描17个data block，最后是commit block。\n这样可以一直扫描到T8。\n在扫描T8时有两种可能，一种可能是T8完成了commit，并且包含了commit block。\n这时恢复软件并不知道T8就是最后一个transaction，所以它会接着看T8的commit block的下一个block，来看看这是不是一个有效的descriptor block。\n我们知道这不是一个descriptor block，而是一个包含在T5内的随机block。\n现在的问题是恢复软件如何可靠的区分出来呢？是的，每个descriptor和commit block都以某个魔法数字作为起始，这是一个32bit的数字。\n所以如果扫描完了T8，下一个block以魔法数字作为起始，那么恢复软件就会认为这是一个descriptor block。\n（注，也有可能T5正好完美的跟在T8后面，也就是说T8的commit block之后就是T5的descriptor block，同时T5的commit block也存在，所以这里必然还需要一些其他的机制，我猜是用到了transaction的序列号）\n但是，现在我们看到的block可能是包含了任意数据的data block，所以它可能是文件中的一个data block并且也是以魔法数字作为起始。\n所以这里的最后一个细节是，logging系统需要能区分一个以魔法数字作为起始的descriptor block和一个以魔法数字作为起始的data block。\n你可以想到各种方法来实现这种区分，ext3是这样做的，当它向log写一个block时，如果这个block既不是descriptor block也不是commit block，但是又以魔法数字作为起始，文件系统会以0替换前32bit，并在transaction的descriptor block中为该data block设置一个bit。\n这个bit表示，对应的data block本来是以魔法数字作为起始，但是现在我们将其替换成了0。而恢复软件会检查这个bit位，在将block写回到文件系统之前，会用魔法数字替换0。\n因此，在log中，除了descriptor和commit block，不会有其他的block以这32bit的魔法数字作为起始。\n所以我们不会有模棱两可的判断，如果一个commit block之后的block以魔法数字作为起始，那么它必然是一个descriptor block。\n所以恢复软件会从super block指向的位置开始一直扫描，直到：\n某个commit block之后的一个block并不是descriptor block 某个commit block之后是descriptor block，但是根据descriptor block找到的并不是一个commit block 这时，恢复软件会停止扫描，并认为最后一个有效的commit block是log的结束位置。\n或许在最后一个commit block之后会跟一个并没有commit完成的transaction（注，上面的第二种情况），但是恢复软件会忽略未完成的transaction，因为这个transaction并没有包含所有的写操作，所以它并不能原子性的恢复。\n之后恢复软件会回到log的最开始位置，并将每个log block写入到文件系统的实际位置，直到走到最后一个有效的commit block。\n之后才是启动剩下的操作系统，并且运行普通的程序。在恢复完成之前，是不能运行任何程序的，因为这个时候文件系统并不是有效的。\n学生提问：XV6相比这里的log机制，缺少了什么呢？\nRobert教授：XV6主要缺失的是在log中包含多个transaction的能力，在XV6的log中最多只会有一个transaction，所以在XV6中缺少了并发的能力。\n比如说当我在执行transaction T7的系统调用时，ext3可以同时向磁盘提交T6，而这在XV6中这是不可能的，因为log只保存了一个transaction。\n所以我们必须先完成一个transaction的所有工作，之后才能开始下一个transaction。所以XV6是简单且正确的，但是缺少了并发的能力。\n学生提问：但是在XV6我还是可以有多个transaction，只是说不能异步的执行它们，对吗？\nRobert教授：这里其实有点模糊，XV6实际上允许在一个transaction中包含多个系统调用（注，详见15.8），所以XV6有一些并发和batching的能力，但是当XV6决定要commit一个transaction时，在完全完成这个transaction之前，是不能执行任何新的系统调用的。\n因为直到前一个transaction完全完成，并没有log空间来存放新的系统调用。\n所以XV6要么是在运行一些系统调用，要么是在commit transaction，但是它不能同时干这两件事情，而ext3可以同时干这两件事情。\n为什么新transaction需要等前一个transaction中系统调用执行完成 以上就是ext3中相对来说直观的部分。实际上还有一些棘手的细节我想讨论一下。\n之前我提到过，ext3中存在一个open transaction，但是当ext3决定要关闭该transaction时，它需要等待该transaction中的所有系统调用都结束，之后才能开始新的transaction。\n假设我们现在有transaction T1，其中包含了多个系统调用。\n如果我们想要关闭T1，我们需要停止接收新的系统调用，因为我们想要等待现有的系统调用结束，这样才能commit transaction。\n所以直到这些系统调用都结束了，在ext3中不能允许开始任何新的系统调用。\n所以只有在T1中的系统调用完成之后，才能开始在接下来的transaction T2中接收系统调用。\n在这之间有一段时间，新的系统调用是被拦截的，这降低了性能，因为我们本来应该执行系统调用的但是又不被允许。\n这里的问题是，直到T1中所有的系统调用都结束之前，ext3为什么不让T2中的系统调用开始执行呢？\n让我们来看一下没有这个限制条件可能会带来的错误的场景。\n我们假设T1只包含了一个系统调用，这是一个create系统调用用来创建文件x。\n在create系统调用结束之前，文件系统决定开始一个新的transaction T2用来接收create之后的所有系统调用。\n我们假设T2在T1结束之前就开始了，T2对另一个文件y调用了unlink系统调用。unlink会释放与y关联的inode。\n假设在下面的时间点T2将inode标记为空闲的，create会为x分配inode，或许它在之后的一个时间点分配了inode。\n因为create在unlink释放inode之后分配的inode，它可能会重用同一个inode，所以x可能会获得y的inode，假设是inode 17。\n目前为止没有问题，因为unlink本来就是释放inode。\n当T1中的create结束之后，我们会关闭T1，在最后我们会将T1的所有更新都写入到磁盘的log中。\n之后unlink还要花点时间才能结束，但是在它结束之前计算机crash了。\n在重启并运行恢复软件时，可以发现T1已经commit了，而T2没有。\n所以恢复软件会完全忽略T2，这意味着T2中的unlink就跟没有发生过一样，恢复软件不会执行T2中的unlink，也就不会删除文件y。\n所以crash并重启之后y文件仍然存在，并还在使用inode 17。\n然而T1又完成了，x文件使用的也是inode 17，所以现在我们错误的有了两个文件都使用了相同的inode，这意味着它们共享了文件内容，向一个文件写数据会神奇的出现在另一个文件中。\n这完全是错误的，因为我们本来想的是删除y，并为x分配一个空闲的inode，而不是一个已经在使用中的inode。\n这里可以这么想，T2中的unlink修改了一个block，最终这个修改过的block被前一个transaction所使用。\nT2中修改的信息，被T1所使用了，这意味着我们丢失了T2的原子性。\n因为T2的目标是unlink的效果要么是全发生，要么是完全不发生。但是刚刚的例子中，因为T1使用了T2中释放的inode，这意味着T2中部分修改已经生效了，但是其他的修改随着crash又丢失了。\n或许你可以想到一些修复这里问题的方法，或许T1可以发现inode是由后一个transaction释放的而不去使用它。\n而ext3采用了一个非常简单的方法，在前一个transaction中所有系统调用都结束之前，它不允许任何新的系统调用执行。\n所以transaction T1也就不可能看到之后的transaction包含的更新。\n因为直到T1 commit了，整个unlink都不被允许执行。\n学生提问：当你关闭一个open transaction时，具体会发生什么呢？会对当前的缓存做一个快照吗？\nRobert教授：会的，当我们关闭一个transaction，文件系统会拷贝被transaction中的系统调用所修改的所有block，之后transaction才会commit这些block。后面的transaction会在真正的block cache上运行。当将block都commit到log之后，对于block cache的拷贝就可以丢弃了。\n总结 最后我希望同学们记住的有关logging和ext3的是：\nlog是为了保证多个步骤的写磁盘操作具备原子性。在发生crash时，要么这些写操作都发生，要么都不发生。这是logging的主要作用。\nlogging的正确性由write ahead rule来保证。\n你们将会在故障恢复相关的业务中经常看到write ahead rule或者write ahead log（WAL）。write ahead rule的意思是，你必须在做任何实际修改之前，将所有的更新commit到log中。在稍后的恢复过程中完全依赖write ahead rule。对于文件系统来说，logging的意义在于简单的快速恢复。log中可能包含了数百个block，你可以在一秒中之内重新执行这数百个block，不管你的文件系统有多大，之后又能正常使用了。\n最后有关ext3的一个细节点是，它使用了批量执行和并发来获得可观的性能提升，不过同时也带来了可观的复杂性的提升。\n学生提问：你刚刚说有一个文件系统线程会做这里所有的工作，那么只能有一个这样的线程，否则的话就会有不同步的问题了，对吗？\nRobert教授：或许真的只有一个线程，我其实不知道有多少个线程，但是1是个不错的数字，因为logging的正确性取决于旧的transaction要在新的transaction之前提交。\n但是逻辑上来说又没有必要只有一个线程，你可以想象不同的transaction使用不同的线程来提交（注，只要锁加的合适多个线程应该也是没问题的）。\n学生提问：当你在讨论crash的时候，你有一个图是T8正在使用之前释放的T5的空间，如果T8在crash的时候还没有commit，并且T5的commit block正好在T8的descriptor block所指定的位置，这样会不会不正确的表明T8已经被commit了（注，这时T8有一个假的commit block）？\nRobert教授：让我尝试画出这里的场景。\n首先我们有一个古老的transaction T5，因为log的循环特性，在顺序上T8位于T5之前。\n因为T5已经被释放了，T8正在蚕食T5的log空间。\n假设T8没有完成commit，但是如果完成commit的话，T8的commit block会写到T5的commit block位置。\nT8并没有能写入commit block，T8前面所有的block都写入了，但是最后跟的是T5的commit block。\n这里的答案是，descriptor block和commit block都有transaction的序列号，所以T8的descriptor block里面的序列号是8，但是T5的commit block里面的序列号是5，所以两者不能匹配。\n学生提问：我们可以在transaction T8开始的时候就知道它的大小吗？\nRobert教授：这是个复杂的问题。\n当T8作为活跃的transaction开始时，系统调用会写入数据，这时文件系统并不知道T8有多大。\n当文件系统开始commit T8时，是知道T8有多大的，因为文件系统只会在T8中所有的系统调用都结束之后才commit它，而在那个时间点，文件系统知道所有的写操作，所以就知道T8究竟有多大。\n除此之外，descriptor block里面包含了所有block的实际编号，所以当写入transaction的第一个block，也就是descriptor block时，logging系统知道T8会包含多少个block。\n学生提问：为什么不在descriptor block里面记录commit信息。虽然这样可能不太好，因为要回到之前的一个位置去更新之前的一个block。\nRobert教授：所以这里的提议是，与其要一个专门的commit block，可以让descriptor block来实现commit block的功能。\nXV6与这个提议非常像，我认为可以这么做，至少在ext3中这么做了不会牺牲性能。\n你需要像XV6一样来组织这里的结构，也就是需要在descriptor block包含某个数据表明这是一个已经提交过的transaction。\n这样做的话，可以节省一个commit block的空间，但是不能节省整个时间。\nLinux文件系统的后续版本实现了你的提议，ext4做了以下工作来更有效的写commit block。\next4会同时写入所有的data block和commit block，它并不是等待所有的data block写完了之后才写的commit block。\n但是这里有个问题，磁盘可以无序的执行写操作，所以磁盘可能会先写commit block之后再写data block。\n如果中间有了crash，那么我们有了commit block，但是却没有全部的data block。\next4通过在commit block中增加校验和来避免这种问题。\n所以commit block写入之后发生了crash，如果data block没有全写入那么校验和不能得出正确的结果，恢复软件可以据此判断出错了。\next4可以通过这种方式在机械硬盘上写入一批block而避免磁碟旋转，进而提升磁盘性能。\n学生提问：log中的data block是怎么写入到文件系统中的？\nRobert教授：这个问题有多个答案。\n对于data block，ext3有三种模式，但是我只记得两个，journaled data和ordered data（注，第三种是writeback）。\n当你在配置ext3文件系统时，你需要告诉Linux你想要哪种模式。\n如果你想要的是journaled data，文件内容就是写入到log中，如果你向一个文件写数据，这会导致inode更新，log中会包含文件数据和更新了的inode，也就是说任何更新了的block都会记录在log中。这种方法非常慢，因为数据需要先写在log中，再写到文件系统中。所以journaled data很直观，但是很慢。\nordered data是最流行的模式，它不会将文件数据写入到log中，只会将metadata block，例如inode，目录block，写入到log中，文件的内容会直接写到文件系统的实际位置中。\n所以这种模式要快得多，因为你不用将文件内容写两次。但是它也会导致更多的复杂性，因为你不能随时写入文件内容。假设你执行一个写操作导致一个新的block被分配给一个文件，并将包含了新分配block编号的inode写入到log中并commit，在实际写入文件内容至刚刚分配的data block之前发生crash。\n在稍后的恢复流程中，你将会看到包含了新分配的block编号的inode，但是对应data block里面的内容却属于之前使用了这个data block的旧的文件。\n如果你运行的是一个类似Athena的多用户系统，那么可能就是一个用户拥有一个文件，其中的内容又属于另一个用户已经删除的文件，如果我们不是非常小心的处理写入数据和inode的顺序就会有这样的问题。\next3的ordered data通过先写入文件内容到磁盘中，再commit修改了的inode来解决这里的问题。\n如果你是个应用程序，你写了一个文件并导致一个新的文件系统data block被分配出来，文件系统会将新的文件内容写到新分配的data block中，之后才会commit transaction，进而导致inode更新并包含新分配的data block编号。\n如果在写文件数据和更新inode之间发生了crash，你也看不到其他人的旧的数据，因为这时就算有了更新了的data block，但是也没关系，因为现在不仅inode没有更新，连bitmap block也没更新，相应的data block还处于空闲状态，并且可以分配给其他的程序，你并不会因此丢失block。\n这里的效果就是我们写了一个data block但是最终并没有被任何文件所使用。\nQ\u0026amp;A 拷贝具体过程 在 ext3 文件系统中，当关闭一个开放的事务（open transaction）时，涉及的拷贝过程是为了保证事务的一致性和数据的原子性。让我们逐步解析这个拷贝过程，了解它的各个环节及其对后续事务的影响。\n拷贝的目的主要是为了确保事务的一致性和隔离性。在事务提交之前，文件系统需要把修改过的块（block）从缓存中复制出来，确保这些修改在事务提交到日志（log）之前不会被其他事务覆盖。\n拷贝的对象：拷贝的对象是 被当前事务（T1）修改过的所有数据块，包括文件内容块（data blocks）、inode 元数据块、目录块等。具体来说，任何在这个事务期间被修改的缓存块（block cache），都需要被拷贝。 为什么要拷贝？：因为在事务提交之前，这些数据块仍然是可修改的。如果没有拷贝，后续的系统调用或者新的事务（T2）可能会修改这些相同的块，从而导致在 T1 还未提交时，出现不一致的状态。通过拷贝，可以确保 T1 的事务在独立的副本上执行，直到它的修改完全提交到日志。 拷贝的过程在事务关闭并准备提交时进行。具体过程如下：\n拷贝的时机：当 T1 事务关闭并进入提交状态时，文件系统会将该事务中所有被修改的块拷贝出来。这些块的拷贝存放在内存中，形成事务的一个快照，即将所有被修改的块保存到独立的区域，以保证这些块不会被新的事务修改。 拷贝的方式：拷贝并不是从磁盘到内存，而是 缓存块（block cache）到独立的事务区域，这可以理解为内存中的一个独立缓冲区。这个缓冲区专门保存当前事务修改的块。 什么时候清除拷贝的内容？\n清除的时机：当事务 T1 被成功提交到日志（即 descriptor block、data block 和 commit block 已全部写入日志），并且文件系统确认事务完成后，拷贝的这些块就不再需要了。这时，拷贝的内容会被丢弃，因为它们已经持久化到日志或者写回到实际的磁盘上。 清除的机制：系统会释放这些拷贝占用的内存资源，这样可以让新的事务和系统调用继续使用这些块。在 T1 事务提交并清除后，新的事务就可以直接使用这些被修改过的块，而不再需要访问拷贝的副本。 拷贝的内容是事务 T1 中修改的块。这些拷贝的存在对后续事务 T2 有着重要的影响，主要体现在以下几点：\n保证事务隔离：拷贝使得 T1 和后续事务 T2 在操作相同块时互相隔离。即使 T2 需要修改与 T1 相同的数据块，它也会在缓存中进行自己的修改，而不会影响 T1 的拷贝内容。这种隔离确保了不同事务之间的修改不会互相干扰。 确保事务原子性：拷贝的块在事务 T1 完全提交之前，是不会被其他事务修改的。这意味着即使 T2 修改了某个块，T1 的快照仍然保持一致。只有当 T1 提交并将修改持久化到磁盘后，T2 才能够基于 T1 的最终结果继续进行。 写时复制机制：这其实是一种**写时复制（Copy-on-Write, CoW）**机制。后续的事务在操作时，如果需要修改 T1 已经修改过的块，系统会先进行块的拷贝，然后让 T2 基于新副本进行修改，而 T1 的拷贝内容保持不变，直到 T1 提交。这种机制有效保障了数据一致性。 拷贝过程的完整总结\n事务关闭时的拷贝：当事务 T1 关闭，文件系统会将当前事务中所有被修改的块从缓存中拷贝到一个独立的区域，确保这些块的修改不会被后续事务覆盖或干扰。 拷贝内容的保存位置：拷贝的块在内存中独立保存，形成事务 T1 的“快照”。后续事务会在未被拷贝的缓存块上执行，而不会直接修改这些已经拷贝的块。 清除拷贝的时机：当 T1 成功提交到日志，并且文件系统确认修改持久化到磁盘后，这些拷贝的块会被丢弃，释放内存资源。 后续事务的影响：拷贝确保了事务的隔离性和原子性。T1 的修改对 T2 是透明的，直到 T1 完全提交后，T2 才会基于 T1 的结果进行修改。 总的来说：\n拷贝的对象 是当前事务修改的所有块。 拷贝的位置 是内存中的独立区域。 清除的时机 是当事务提交完成后，系统确认修改已经持久化。 拷贝对后续事务的影响 是确保事务之间的隔离性和一致性，防止数据被后续事务不正确地修改。 这个机制保障了每个事务的操作都是独立的，修改不会被其他事务破坏，同时能确保在崩溃恢复时维持文件系统的一致性。\n日志空间的操作执行的顺序 在 ext3 文件系统中，日志（log）空间的操作是按照 事务提交顺序 来执行的，并不是并发执行的。文件系统采用的是 顺序日志写入 模式，这样能够保证数据的一致性和恢复时的确定性。\n当一个事务被提交时，文件系统会按照以下顺序执行操作：\nDescriptor block：描述当前事务要修改的块编号。 Data block：写入缓存中修改过的数据。 Commit block：标志事务的完成。 所有这些 block 都按照顺序依次写入日志空间，并确保 在写入 commit block 之前，所有的 data block 都已经写入日志。换句话说，事务的所有操作按照顺序写入日志，这种顺序性保证了日志的完整性和恢复时的安全性。\n当一个事务完成日志写入，并且 commit block 写入成功后，这个事务的所有操作就被认为是提交完成的。然而，这时修改的数据块还仅仅存在于日志中，而不是文件系统的实际位置。\n接下来，文件系统会将这些已经提交的事务数据（data block）从日志写入到实际的文件系统（也称为“写回”过程）。这个过程也是按照 事务提交的顺序 来执行的，而不是并发执行。\n顺序写回数据：文件系统会按照事务提交的顺序依次将数据块从日志空间写回到文件系统的实际存储位置。虽然这个写回过程可能与系统中的其他操作并发进行，但每个事务的写入顺序是依赖事务提交顺序的。 避免并发冲突：为了避免文件系统的并发写入冲突，事务的写回过程是顺序化的。一个事务必须在之前的事务完全写入后，才能开始写回下一个事务。这确保了日志恢复的一致性和文件系统状态的可预测性。 如果系统崩溃或故障，在系统重启时，恢复软件会从日志中按照事务的提交顺序，逐一将数据块恢复到文件系统中。恢复过程严格按照日志中的顺序执行，确保事务按提交顺序被恢复到文件系统的实际位置。\n总的来说：\n提交顺序执行：虽然日志写入和写回的过程可能会与系统的其他操作并发进行，但每个事务的操作是按照它们提交的顺序依次执行的。文件系统确保事务的提交和数据写入严格遵循顺序，避免了并发冲突和数据不一致。 顺序性的重要性：保持顺序写入和写回能够保证日志恢复过程的可预测性和文件系统的一致性。 commit block ext3 文件系统使用 commit block 来判断一个事务是否已经成功提交，因此读取日志空间的最后一个 block 是否是 commit block 是判断该事务是否已完成的关键方式。具体流程解释：\n事务提交的流程：\n在一个事务的执行过程中，首先写入 descriptor block，该 block 描述了事务中哪些块会被修改。 紧接着，文件系统会写入 data block，这些数据块包含了文件系统实际修改的内容。 最后，文件系统会写入 commit block。这标志着这个事务已经完成。 判断事务是否完成：\n当系统在检查日志空间时，它会读取每个事务的最后一个 block。如果发现最后的 block 是 commit block，这就表明该事务已完成并成功提交。 如果系统崩溃或中断，重启时，文件系统会扫描日志空间来恢复事务。如果一个事务中没有 commit block，则表示这个事务没有完成，因此系统会回滚该事务的所有操作，确保文件系统的一致性。 具体来说，文件系统在重启时会：\n扫描日志空间中的所有 block。 找到事务的 descriptor block 和对应的 data block。 检查事务的最后一个 block 是否是 commit block。如果 commit block 存在，表示事务完成，数据可以被写入文件系统的永久位置；如果 commit block 缺失，则该事务未完成，文件系统会丢弃该事务的所有修改。 为什么 commit block 重要：\n一致性保障：commit block 是文件系统的一个检查点，确保在事务执行的过程中，即使发生崩溃或系统故障，文件系统仍然能够恢复到一致的状态。commit block 之前的事务被认为是完全提交的，而 commit block 之前未写入的事务将被回滚。 事务回滚：如果在提交事务时，系统未能写入 commit block（比如系统崩溃），重启后的文件系统会回滚这一事务，以避免数据的不一致。只有包含 commit block 的事务数据才能被认为是持久化的。 在系统崩溃或非正常关闭的情况下，ext3 文件系统的日志恢复机制会通过以下步骤恢复文件系统的状态：\n扫描日志空间：系统启动后，文件系统会扫描日志空间，寻找每个事务的 descriptor block 和 data block。 检查 commit block：系统检查事务的最后一个 block 是否为 commit block。 如果 commit block 存在：表示事务完成，文件系统会将日志中的 data block 数据写回到文件系统的实际存储位置。 如果 commit block 不存在：表示事务未完成，文件系统会回滚这个事务，丢弃该事务的所有操作，保持文件系统的原始状态。 总结：\ncommit block 是判断事务是否完成的核心标志。文件系统通过检查日志空间中的最后一个 block 是否为 commit block 来判断该事务是否已成功提交。 如果 commit block 存在，说明事务成功完成，系统会把日志中的数据写回到文件系统实际的位置。如果 commit block 不存在，说明事务中断，系统会回滚该事务。 这个设计确保了在崩溃恢复或系统重启时，ext3 文件系统可以有效地维持数据的一致性和完整性。\next3 文件系统的日志结构 在 ext3 文件系统中，日志 (log) 机制是基于事务日志（Journal）的，它用于跟踪对文件系统的修改，确保在系统崩溃或掉电等意外情况下可以恢复一致的文件系统状态。ext3 的日志是按事务提交的方式进行的，每个事务包含多个 block。具体来说，日志中的 block 结构分为几类，分别是descriptor block、data block 和 commit block。ext3 文件系统的日志结构如下。\nSuperblock： 这是文件系统的描述符，记录了日志的元信息。它保存了日志的位置、大小、状态等关键信息。Superblock 在整个文件系统中只有一个，并且包含关于日志区（Journal Area）的一些元数据。 日志空间 (Journal Area)： 日志区域由多个 block 组成，用于记录文件系统的事务操作。这些 block 依次连接组成一个循环日志空间。日志空间的大小是在创建文件系统时指定的。 日志中的 block 类型： Descriptor Block：这个 block 是每个事务的开头部分，用于描述这个事务中的修改。具体来说，它列出了哪些块在这个事务中被修改了，并且这些块的位置（即磁盘上的块编号）。Descriptor block 让文件系统在恢复时知道哪些块被修改。 Data Block：这些 block 存储具体的修改数据。它们紧跟在 descriptor block 后面。数据块包含了文件系统中的实际数据修改，比如某个文件的内容或元数据的修改。 Commit Block：这个 block 标记事务的结束，它告诉文件系统这个事务已经完成，所有的修改都可以视为已提交。Commit block 是事务提交的关键点。如果系统在 commit block 写入之前崩溃，整个事务都会被回滚。如果 commit block 已经写入，那么事务被视为成功提交。 一个完整的事务通常包含以下几个步骤：\nDescriptor Block： 事务开始时，首先会写入一个 descriptor block，这个 block 列出了哪些磁盘块在此次事务中将被修改。 Data Block： 接下来，紧跟在 descriptor block 之后，系统会将实际的数据（data block）写入日志。这些数据来自于缓存（buffer），并且每个数据块对应 descriptor block 中列出的某个块。 Commit Block： 在所有数据块写入日志之后，文件系统会写入 commit block。这个 block 标记着这个事务已经完成，所有在这个事务中的修改都可以被持久化到磁盘的真实位置。 Commit block 并不是在每个数据块后立即写入，而是在整个事务的所有数据块写入日志之后，才会写入日志区域的最后一个 block。它的作用是表明这个事务已经提交成功。如果系统在写入 commit block 之前崩溃，那么即使之前写入的数据块已经在日志中，事务也会在系统重启时被回滚。\n具体来说，commit block 记录了当前事务的结束，并且在崩溃恢复时，系统会根据 commit block 判断哪些事务已经提交、哪些事务需要回滚：\n事务提交成功：如果 commit block 已经写入日志，那么表示事务已经完整提交，系统可以安全地将数据块写入文件系统的实际位置。 事务未提交：如果系统崩溃时 commit block 尚未写入，则说明事务未完成。系统在恢复时会回滚这次事务，确保数据一致性。 日志块的生命周期：\n事务日志写入后，commit block 确认提交：当事务的 descriptor block 和 data block 写入日志后，文件系统会写入 commit block。一旦 commit block 写入成功，这个事务就被认为提交完成。此时，文件系统可以将这些数据块（data block）从日志区域持久化写入实际文件系统的数据结构中。 是否所有 block 都变成 commit block？：并不是说写入到磁盘的所有 data block 会变成 commit block。commit block 只是一个标识符，标记事务的提交成功。data block 依然是数据块，它们并不会变成 commit block。 日志空间重用：一旦事务成功提交并且数据已经写入文件系统的真实位置，日志空间中的那些被使用的 block（包括 descriptor block、data block 和 commit block）会被标记为可重用。接下来进行的事务可以继续使用这部分日志空间进行新的事务记录。这就是日志的循环利用机制。 日志提交后的流程：\n写入磁盘：提交事务成功之后，文件系统会将事务中涉及到的 data block 写入文件系统的实际位置（不是日志空间）。这些数据块会从缓存中持久化到文件系统对应的区域。 日志空间重用：当数据块写入实际文件系统后，相应的日志空间会被释放，以便记录新的事务。 总结：\n日志空间结构：日志空间中包含 descriptor block（描述修改的块）、data block（存储具体数据的块）和 commit block（标记事务完成的块）。 commit block：它是在事务的所有 data block 写入日志之后写入，用于标记事务的成功提交。 数据持久化与重用：事务提交后，数据写入文件系统实际位置，日志空间被释放，可以循环使用。 这种设计确保了文件系统的一致性，特别是在崩溃或故障恢复的场景下，通过日志系统，ext3 能够有效恢复文件系统的状态。\n为何使用魔法数字 在 descriptor block 中记录哪些是 data block，而不让 data block 自身存储魔法数字是合理的。这确实能避免 data block 中与魔法数字相关的冲突问题，但 ext3 文件系统中使用魔法数字的方式有其历史原因和技术权衡，具体来说：\n魔法数字在文件系统中扮演着一个识别块类型的重要角色。通过在每个 block 中使用魔法数字，文件系统可以快速识别 block 的类型并执行相应的操作。虽然你建议可以只在 descriptor block 中标识哪些是 data block，但魔法数字的存在带来了几项关键好处：\n快速识别与检查：魔法数字使得文件系统在处理 block 时能够快速识别这是一个 data block 还是 descriptor block，而不需要依赖其他结构（如 descriptor block）来查找并区分块的类型。这种设计能够提高处理和识别的效率，特别是在日志恢复和崩溃后的检查过程中。 自我描述性：每个 block 都是自我描述的，魔法数字提供了一种内嵌的机制，便于在调试和修复时直接通过读取 block 数据来了解它的类型。这种自我描述的设计在维护和诊断中具有很大的价值。 如果我们完全依赖 descriptor block 来标记哪些 block 是 data block，虽然能减少对魔法数字的需求，但会引入一些潜在的复杂性和风险：\n恢复过程的复杂化：在崩溃恢复时，文件系统需要扫描事务日志来重建状态。如果所有的 block 类型信息仅存储在 descriptor block 中，那么在恢复过程中，文件系统需要额外查找和关联每个 block 和其对应的 descriptor block。这个过程会增加恢复的复杂度和时间。相反，魔法数字让每个 block 自带标识，恢复过程更加直接和高效。 处理时间增加：如果每次访问某个 block 时，系统都必须回溯到对应的 descriptor block 来确定其类型，文件系统的处理效率会降低。通过在 block 本身使用魔法数字，系统可以立即判断 block 的类型，而不必在事务日志中查找。 一致性检查困难：在某些场景下，比如调试、日志审计或者磁盘一致性检查，如果 block 本身没有类型标识符（即魔法数字），系统或运维人员必须依赖 descriptor block 来进行检查。如果 descriptor block 丢失或损坏，可能会导致无法正确识别和恢复 data block。 ext3 的设计是平衡了性能和数据一致性。在大多数情况下，魔法数字提供了更直接的块类型识别方式，而不是让文件系统依赖 descriptor block 逐一去匹配。在实践中：\n用魔法数字快速识别块类型 提供了高效的方式，在极端情况下（如崩溃恢复或诊断中），可以立即识别 block 的类型。 避免数据冲突：通过将某些具有冲突魔法数字的 data block 前 32 位替换为 0，并在 descriptor block 中设置标志位来记住这个修改，ext3 提供了一种既能避免冲突又保持性能的折衷方案。 另外，文件系统的设计往往受到历史版本的影响。魔法数字的使用是 UNIX 系统中常见的一种自我描述性机制，它有助于跨平台和跨版本的兼容。尽管你提出的依赖 descriptor block 的方法在某些情况下更为简洁，但从设计的演化角度来看，使用魔法数字已经成为一种通用的、广泛接受的机制，修改这一点可能会影响到已有的工具和工作流。\n虽然理论上可以只依赖 descriptor block 来标识哪些 block 是 data block，省去魔法数字，但 ext3 通过使用魔法数字和补充的 bit 标记系统，确保了性能、可靠性和恢复过程的高效性。魔法数字提供了一种快速的、块级别的识别方式，使文件系统在正常运行和崩溃恢复过程中更加高效。这种设计是在可靠性、性能和历史兼容性之间的平衡折衷。\n为什么在事务提交过程中，系统会阻止新的系统调用？ 注：其实正课末尾已解释\n在事务提交过程中，阻止新的系统调用是为了确保文件系统在处理当前事务时，保持一致性和数据的完整性。这种做法虽然会暂时影响性能（因为新的系统调用需要等待），但从文件系统的一致性角度来看，它是必要的。为什么阻止新的系统调用：\n事务的原子性和一致性要求： 事务（transaction）在文件系统中是一个原子操作的集合，所有的操作要么全部成功提交，要么全部失败回滚。在提交事务时，文件系统需要确保这个事务中的所有系统调用都已经完成，且所有修改的数据块都已经记录在日志中。 如果允许在事务提交过程中开始新的系统调用，可能会导致数据不一致。新的系统调用可能会修改与当前事务相同的数据块。如果这些修改在事务提交的过程中发生，文件系统可能无法区分哪些修改属于当前正在提交的事务，哪些属于新的事务，从而导致事务日志记录不完整或错误。 防止数据块的竞争修改： 假设在提交事务时，有一个新的系统调用尝试修改与当前事务相同的磁盘块（block）。这就可能导致两个系统调用对同一个 block 进行操作，产生写入冲突。为了避免这种竞争条件的出现，文件系统在提交事务时会阻止新的系统调用。 如果不阻止新的系统调用，新的系统调用可能会修改缓存中的相同 block，文件系统将无法保证提交的事务反映了缓存的正确状态。这会使得事务提交后的状态与预期的事务日志内容不符，损害文件系统的一致性。 事务日志的完整性： 在提交事务的过程中，文件系统会将修改过的 block 写入事务日志的 data block，并记录在 descriptor block 中。当写入完成并提交 commit block 后，事务才能认为已经安全地写入磁盘，数据的持久性才得以保证。 如果此时允许新的系统调用开始，新的写入可能会干扰正在进行的日志写入过程。这样，文件系统可能无法正确识别哪些 block 是由新系统调用修改的，哪些属于当前事务，从而破坏日志的完整性。 事务提交的最终确定性： 当事务提交进入最后阶段（如写入 commit block 时），文件系统的状态必须是确定的。所有涉及到的系统调用都必须结束，并且系统需要保证事务中的所有修改已经记录在日志中。如果此时允许新的系统调用开始，可能会打破这种确定性，使得日志中的数据变得不可靠。 为了确保事务在提交之后可以正确恢复和回滚，阻止新的系统调用是必要的，这样可以保证提交点（commit point）前后文件系统的状态是一致的。 阻止新的系统调用不会影响文件系统的有序性，反而有助于保持事务之间的严格顺序和隔离性。具体来说：\n系统调用的顺序性：在阻止新的系统调用的时间段内，文件系统仅仅是延迟了新系统调用的开始，但并没有打乱系统调用的顺序。事务提交之后，新的系统调用将按顺序继续执行。因此，文件系统的操作顺序仍然是有序的。 隔离性：文件系统中的事务需要提供一定程度的隔离性，确保在事务 A 提交前，事务 B 的修改不会影响事务 A。如果不阻止新的系统调用，这种隔离性将无法得到保证，可能导致事务 A 和事务 B 的修改交错进行，进而导致数据不一致。 阻止新的系统调用在事务提交过程中是为了保证事务的一致性、原子性和日志的完整性。虽然这种做法会暂时影响系统的性能，但它确保了文件系统的正确性，避免了数据冲突和不一致的发生。阻止新的系统调用并不会影响系统调用的有序性，相反，它帮助维护了事务的隔离性和数据的可靠性。\nhandle 的作用 handle 在 ext3 文件系统中的角色，确实是用于标识系统调用并追踪与该系统调用相关的所有写操作。让我们从你的描述出发，澄清 handle 的作用以及它在 ext3 文件系统中的实际用途。\n在 ext3 文件系统中，每个系统调用在开始时会调用 start 函数，随后会得到一个 handle。这个 handle 是用来唯一标识当前系统调用的。\n唯一标识系统调用：每个系统调用在其生命周期内，handle 充当它的唯一识别符，帮助文件系统将其跟踪。文件系统中的所有写操作都通过这个 handle 关联到相应的系统调用和事务上。 跟踪写操作：在系统调用的过程中，如果它需要访问或修改某个 block（例如通过 get_block 函数），handle 会作为参数传递，表明该 block 的修改是由哪个系统调用发起的。 因此，handle 不仅用于标识系统调用，同时也用于跟踪该系统调用中的所有写操作。每个系统调用与其操作的 block 通过 handle 建立联系。\n虽然 handle 是系统调用的标识符，但它也与整个事务的执行紧密关联。ext3 文件系统会周期性地将当前事务提交到磁盘，在事务提交过程中，handle 的作用包括以下几点：\n识别系统调用所属的事务：文件系统需要知道哪些系统调用属于当前事务。当某个系统调用读取或修改 block 时，handle 传递给 logging 系统，表明这些 block 是该事务的一部分。因此，handle 也关联了系统调用与事务的关系。 跟踪事务中的系统调用进度：在事务提交时，所有与该事务相关的系统调用必须完成（即所有 handle 所代表的系统调用必须调用 stop 函数）才能进行提交。stop 函数通过 handle 通知文件系统，该系统调用已经完成，事务中的操作减少一个。 阻止新系统调用：在事务提交过程中，系统会阻止新的系统调用，以确保所有尚未完成的系统调用能够有序完成，并且所有的修改在提交时是一致的。 文件系统的日志（log）包含 descriptor block、data block 和 commit block。在系统调用过程中，通过 handle 确定哪些 block 被修改：\n当一个系统调用修改某个 block 时，文件系统会记录这些修改，并在事务提交时写入 descriptor block，标明哪些块被修改。 这些 block 编号会被记录到 descriptor block 中，随后在数据被写入到日志后，再写入 commit block，表明事务的所有修改都已完成并提交。 在 ext3 文件系统中，handle 主要用于以下几个方面：\n唯一标识系统调用：每个系统调用通过 handle 进行唯一标识，帮助文件系统跟踪哪些 block 被修改。 关联事务和系统调用：通过 handle，文件系统可以将系统调用与对应的事务关联起来，并确保所有系统调用完成后事务才能提交。 管理日志：handle 还用于将系统调用与其修改的 block 关联，确保这些修改能够正确写入日志和最终提交到文件系统中。 在整个过程中，handle 起到的核心作用是帮助文件系统管理和跟踪每个系统调用以及它对文件系统的修改操作。\n传递 block 编号的具体时机是在事务执行的过程中，尤其是在系统调用读取或写入某个数据块时。以下是几个关键点：\n读取时：当系统调用需要读取一个 block 时，文件系统会根据当前事务的 handle 记录哪些 block 被读取，并在事务日志中记录。 写入时：当系统调用修改某个 block 时，这个 block 的编号会被记录为修改的 block，并最终写入 descriptor block，确保在提交事务时，这些 block 能够正确写入到磁盘的 log 中。 ","date":"2024-10-01T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08116file-system-performance-and-fast-crash-recovery/bg_hu_52fde5a4b9e5e11f.jpg","permalink":"https://echudet.github.io/p/mit6s08116file-system-performance-and-fast-crash-recovery/","title":"MIT6S081：16File system performance and fast crash recovery"},{"content":"Lec15 Crash recovery 课前预习 日志层 文件系统设计中最有趣的问题之一是崩溃恢复。\n出现此问题的原因是，许多文件系统操作都涉及到对磁盘的多次写入，并且在完成写操作的部分子集后崩溃可能会使磁盘上的文件系统处于不一致的状态。\n例如，假设在文件截断（将文件长度设置为零并释放其内容块）期间发生崩溃。根据磁盘写入的顺序，崩溃可能会留下对标记为空闲的内容块的引用的inode，也可能留下已分配但未引用的内容块。\n后者相对来说是良性的，但引用已释放块的inode在重新启动后可能会导致严重问题。\n重新启动后，内核可能会将该块分配给另一个文件，现在我们有两个不同的文件无意中指向同一块。\n如果xv6支持多个用户，这种情况可能是一个安全问题，因为旧文件的所有者将能够读取和写入新文件中的块，而新文件的所有者是另一个用户。\nXv6通过简单的日志记录形式解决了文件系统操作期间的崩溃问题。\nxv6系统调用不会直接写入磁盘上的文件系统数据结构。\n相反，它会在磁盘上的log（日志）中放置它希望进行的所有磁盘写入的描述。\n一旦系统调用记录了它的所有写入操作，它就会向磁盘写入一条特殊的commit（提交）记录，表明日志包含一个完整的操作。\n此时，系统调用将写操作复制到磁盘上的文件系统数据结构。\n完成这些写入后，系统调用将擦除磁盘上的日志。\n如果系统崩溃并重新启动，则在运行任何进程之前，文件系统代码将按如下方式从崩溃中恢复。\n如果日志标记为包含完整操作，则恢复代码会将写操作复制到磁盘文件系统中它们所属的位置。\n如果日志没有标记为包含完整操作，则恢复代码将忽略该日志。恢复代码通过擦除日志完成。\n为什么xv6的日志解决了文件系统操作期间的崩溃问题？\n如果崩溃发生在操作提交之前，那么磁盘上的登录将不会被标记为已完成，恢复代码将忽略它，并且磁盘的状态将如同操作尚未启动一样。 如果崩溃发生在操作提交之后，则恢复将重播操作的所有写入操作，如果操作已开始将它们写入磁盘数据结构，则可能会重复这些操作。 在任何一种情况下，日志都会使操作在崩溃时成为原子操作：恢复后，要么操作的所有写入都显示在磁盘上，要么都不显示。\n日志设计 日志驻留在超级块中指定的已知固定位置。\n超级块 是 xv6 文件系统中的一个重要结构，它存储了文件系统的基本信息和元数据。超级块在文件系统中扮演着类似于“目录”的角色，记录了关于文件系统布局的全局信息。超级块是存储在文件系统的特定块中，并且它会在文件系统被挂载时读入内存\n超级块的主要作用是提供以下关键信息：\n文件系统的总体布局：超级块中包含文件系统中各个区域的大小和位置，例如： inode（索引节点）区域的起始位置和数量。 数据块区域的起始位置和数量。 位图的起始位置，用于管理块的分配。 块的总数和 inode 的总数：超级块记录了文件系统中总的块数和 inode 数。inode 是文件系统中的一种数据结构，每个 inode 对应一个文件或目录，存储了它们的元数据信息（如文件大小、权限等）。 文件系统的其他元数据：例如文件系统的版本号、磁盘块大小等信息。 它由一个头块（header block）和一系列更新块的副本（logged block）组成。\n头块包含一个扇区号数组（每个logged block对应一个扇区号）以及日志块的计数。\n磁盘上的头块中的计数或者为零，表示日志中没有事务；或者为非零，表示日志包含一个完整的已提交事务，并具有指定数量的logged block。\n在事务提交（commit）时Xv6才向头块写入数据，在此之前不会写入，并在将logged blocks复制到文件系统后将计数设置为零。\n因此，事务中途崩溃将导致日志头块中的计数为零；提交后的崩溃将导致非零计数。\n注：logged block表示已经记录了操作信息的日志块，而log block仅表示日志块\n每个系统调用的代码都指示写入序列的起止，考虑到崩溃，写入序列必须具有原子性。\n为了允许不同进程并发执行文件系统操作，日志系统可以将多个系统调用的写入累积到一个事务中。\n因此，单个提交可能涉及多个完整系统调用的写入。\n为了避免在事务之间拆分系统调用，日志系统仅在没有文件系统调用进行时提交。\n同时提交多个事务的想法称为组提交（group commit）。\n组提交减少了磁盘操作的数量，因为成本固定的一次提交分摊了多个操作。\n组提交还同时为磁盘系统提供更多并发写操作，可能允许磁盘在一个磁盘旋转时间内写入所有这些操作。\nXv6的virtio驱动程序不支持这种批处理，但是Xv6的文件系统设计允许这样做。\nXv6在磁盘上留出固定的空间来保存日志。\n事务中系统调用写入的块总数必须可容纳于该空间。\n这导致两个后果：任何单个系统调用都不允许写入超过日志空间的不同块。\n这对于大多数系统调用来说都不是问题，但其中两个可能会写入许多块：write和unlink。\n一个大文件的write可以写入多个数据块和多个位图块以及一个inode块；unlink大文件可能会写入许多位图块和inode。\nXv6的write系统调用将大的写入分解为适合日志的多个较小的写入，unlink不会导致此问题，因为实际上Xv6文件系统只使用一个位图块。\n日志空间有限的另一个后果是，除非确定系统调用的写入将可容纳于日志中剩余的空间，否则日志系统无法允许启动系统调用。\n代码：日志 在系统调用中一个典型的日志使用就像这样：\n1 2 3 4 5 6 7 begin_op(); ... bp = bread(...); bp-\u0026gt;data[...] = ...; log_write(bp); ... end_op(); begin_op（kernel/log.c:126）等待直到日志系统当前未处于提交中，并且直到有足够的未被占用的日志空间来保存此调用的写入。\nlog.outstanding统计预定了日志空间的系统调用数；为此保留的总空间为log.outstanding乘以MAXOPBLOCKS。\n递增log.outstanding会预定空间并防止在此系统调用期间发生提交。代码保守地假设每个系统调用最多可以写入MAXOPBLOCKS个不同的块。\nlog_write（kernel/log.c:214）充当bwrite的代理。它将块的扇区号记录在内存中，在磁盘上的日志中预定一个槽位，并调用bpin将缓存固定在block cache中，以防止block cache将其逐出。\n注：固定在block cache是指在缓存不足需要考虑替换时，不会将这个block换出，因为事务具有原子性：假设块45被写入，将其换出的话需要写入磁盘中文件系统对应的位置，而日志系统要求所有内存必须都存入日志，最后才能写入文件系统。\nbpin是通过增加引用计数防止块被换出的，之后需要再调用bunpin\n在提交之前，块必须留在缓存中：在提交之前，缓存的副本是修改的唯一记录；只有在提交后才能将其写入磁盘上的位置；同一事务中的其他读取必须看到修改。\nlog_write会注意到在单个事务中多次写入一个块的情况，并在日志中为该块分配相同的槽位。这种优化通常称为合并（absorption）。\n例如，包含多个文件inode的磁盘块在一个事务中被多次写入是很常见的。通过将多个磁盘写入合并到一个磁盘中，文件系统可以节省日志空间并实现更好的性能，因为只有一个磁盘块副本必须写入磁盘。\n注：日志需要写入磁盘，以便重启后读取，但日志头块和日志数据块也会在block cache中有一个副本\nend_op（kernel/log.c:146）首先减少未完成系统调用的计数。\n如果计数现在为零，则通过调用commit()提交当前事务。这一过程分为四个阶段。\nwrite_log()（kernel/log.c:178）将事务中修改的每个块从缓冲区缓存复制到磁盘上日志槽位中。\nwrite_head()（kernel/log.c:102）将头块写入磁盘：这是提交点，写入后的崩溃将导致从日志恢复重演事务的写入操作。\ninstall_trans（kernel/log.c:69）从日志中读取每个块，并将其写入文件系统中的适当位置。\n最后，end_op写入计数为零的日志头；这必须在下一个事务开始写入日志块之前发生，以便崩溃不会导致使用一个事务的头块和后续事务的日志块进行恢复。\nrecover_from_log（kernel/log.c:116）是由initlog（kernel/log.c:55）调用的，而它又是在第一个用户进程运行（kernel/proc.c:539）之前的引导期间由fsinit（kernel/fs.c:42）调用的。它读取日志头，如果头中指示日志包含已提交的事务，则模拟end_op的操作。\n日志的一个示例使用发生在filewrite（kernel/file.c:135）中。事务如下所示：\n1 2 3 4 5 begin_op(); ilock(f-\u0026gt;ip); r = writei(f-\u0026gt;ip, ...); iunlock(f-\u0026gt;ip); end_op(); 这段代码被包装在一个循环中，该循环一次将大的写操作分解为几个扇区的单个事务，以避免日志溢出。作为此事务的一部分，对writei的调用写入许多块：文件的inode、一个或多个位图块以及一些数据块。\n代码：块分配器 文件和目录内容存储在磁盘块中，磁盘块必须从空闲池中分配。xv6的块分配器在磁盘上维护一个空闲位图，每一位代表一个块。0表示对应的块是空闲的；1表示它正在使用中。程序mkfs设置对应于引导扇区、超级块、日志块、inode块和位图块的比特位。\n块分配器提供两个功能：balloc分配一个新的磁盘块，bfree释放一个块。\nBalloc中位于kernel/fs.c:71的循环从块0到sb.size（文件系统中的块数）遍历每个块。它查找位图中位为零的空闲块。\n如果balloc找到这样一个块，它将更新位图并返回该块。为了提高效率，循环被分成两部分。\n外部循环读取位图中的每个块。内部循环检查单个位图块中的所有BPB位。\n由于任何一个位图块在buffer cache中一次只允许一个进程使用，因此，如果两个进程同时尝试分配一个块，可能会发生争用。\nBfree（kernel/fs.c:90）找到正确的位图块并清除正确的位。同样，bread和brelse隐含的独占使用避免了显式锁定的需要。\n与本章其余部分描述的大部分代码一样，必须在事务内部调用balloc和bfree。\n正课 File system crash概述 今天的课程是有关文件系统中的Crash safety。\n这里的Crash safety并不是一个通用的解决方案，而是只关注一个特定的问题的解决方案，也就是crash或者电力故障可能会导致在磁盘上的文件系统处于不一致或者不正确状态的问题。\n当我说不正确的状态时，是指例如一个data block属于两个文件，或者一个inode被分配给了两个不同的文件。\n这个问题可能出现的场景可能是这样，当你在运行make指令时，make与文件系统会有频繁的交互，并读写文件，但是在make执行的过程中断电了，可能是你的笔记本电脑没电了，也可能就是停电了，之后电力恢复之后，你重启电脑并运行ls指令，你会期望你的文件系统仍然在一个好的可用的状态。\n这里我们关心的crash或者故障包括了：\n在文件系统操作过程中的电力故障\n在文件系统操作过程中的内核panic\n包括XV6在内的大部分内核都会panic，panic可能是由内核bug引起，它会突然导致你的系统故障，但是你肯定期望能够在重启之后还能使用文件系统。\n文件系统的操作都包含了多个步骤，在多个步骤的错误位置crash或者电力故障了，存储在磁盘上的文件系统可能会是一种不一致的状态，之后可能会发生一些坏的事情。\n这类特定问题的解决方法，也就是logging。\n这是一个最初来自于数据库世界的很流行的解决方案，现在很多文件系统都在使用logging。\n之所以它很流行，是因为它是一个很好用的方法。\nXV6的实现非常简单，几乎是最简单的实现logging的方法，因为我们只是为了演示关键的思想。\n由于XV6实现的较为简单，XV6中的logging存在一个缺点，它的性能并不咋样，尽管logging系统原则上来说可以获得好的性能。\n在下节课我们将通过学习Linux的ext3文件系统所使用的logging系统，来看一下如何实现一个高性能logging系统。\n这是我们最后一节有关XV6的课程。这节课之后，我们将切换到阅读论文。\n因为这节课讲完了之后，我们就覆盖了操作系统的基本概念，我们可以通过阅读论文看一些更高级的操作系统思想。\n这节课关注的场景。类似于创建文件，写文件这样的文件系统操作，都包含了多个步骤的写磁盘操作。\n如何创建一个文件，这里多个步骤的顺序是（注，实际步骤会更多，详见14.5）：\n分配inode，或者在磁盘上将inode标记为已分配 之后更新包含了新文件的目录的data block 如果在这两个步骤之间，操作系统crash了。这时可能会使得文件系统的属性被破坏。\n这里的属性是指，每一个磁盘block要么是空闲的，要么是只分配给了一个文件。即使故障出现在磁盘操作的过程中，我们期望这个属性仍然能够保持。\n如果这个属性被破坏了，那么重启系统之后程序可能会运行出错，比如：\n操作系统可能又立刻crash了，因为文件系统中的一些数据结构现在可能处于一种文件系统无法处理的状态。 或者，更可能的是操作系统没有crash，但是数据丢失了或者读写了错误的数据。 File system crash示例 为了更清晰的理解这里的风险，让我们看一些基于XV6的例子，并看一下哪里可能出错。\nXV6有一个非常简单的文件系统和磁盘数据的排布方式。\n在super block之后就是log block，我们今天主要介绍的就是log block。\nlog block之后是inode block，每个block可能包含了多个inode。\n之后是bitmap block，它记录了哪个data block是空闲的。\n最后是data block，这里包含了文件系统的实际数据。\n操作系统与磁盘block的交互过程，创建一个文件涉及到了多个操作：\n首先是分配inode，因为首先写的是block 33 之后inode被初始化，然后又写了一次block 33 之后是写block 46，是将文件x的inode编号写入到x所在目录的inode的data block中 之后是更新root inode，因为文件x创建在根目录，所以需要更新根目录的inode的size字段，以包含这里新创建的文件x 最后再次更新了文件x的inode 在出现电力故障之后，因为内存数据保存在RAM中，所有的内存数据都丢失了。\n所有的进程数据，所有的文件描述符，内存中所有的缓存都没有了，因为内存数据不是持久化的。\n我们唯一剩下的就是磁盘上的数据，因为磁盘的介质是持久化的，所以只有磁盘上的数据能够在电力故障之后存活。\n基于这些事实，如果我们在上面的位置出现故障，并且没有额外的机制，没有logging，会有多糟糕呢？我们这里会有什么风险？\n在这个位置，我们先写了block 33表明inode已被使用，之后出现了电力故障，然后计算机又重启了。\n这时，我们丢失了刚刚分配给文件x的inode。这个inode虽然被标记为已被分配，但是它并没有放到任何目录中，所以也就没有出现在任何目录中，因此我们也就没办法删除这个inode。\n所以在这个位置发生电力故障会导致我们丢失inode。\n你或许会认为，我们应该改一改代码，将写block的顺序调整一下，这样就不会丢失inode了。所以我们可以先写block 46来更新目录内容，之后再写block 32来更新目录的size字段，最后再将block 33中的inode标记为已被分配。\n这里的效果是一样的，只是顺序略有不同。并且这样我们应该可以避免丢失inode的问题。那么问题来了，这里可以工作吗？我们应该问问自己，如果在下面的位置发生了电力故障会怎样？\n在这个位置，目录被更新了，但是还没有在磁盘上分配inode。\n电力故障之后机器重启，文件系统会是一个什么状态？或者说，如果我们读取根目录下的文件x，会发生什么，因为现在在根目录的data block已经有了文件x的记录？\n是的，我们会读取一个未被分配的inode，因为inode在crash之前还未被标记成被分配。更糟糕的是，如果inode之后被分配给一个不同的文件，这样会导致有两个应该完全不同的文件共享了同一个inode。\n如果这两个文件分别属于用户1和用户2，那么用户1就可以读到用户2的文件了。所以上面的解决方案也不好。\n所以调整写磁盘的顺序并不能彻底解决我们的问题，我们只是从一个问题换到了一个新的问题。\n让我们再看一个例子，这个例子中会向文件x写入“hi”（注，也就是14.5介绍的第二个部分）\n一旦成功的创建了文件x，之后会调用write系统调用，我们在上节课看到了write系统调用也执行了多个写磁盘的操作。\n首先会从bitmap block，也就是block 45中，分配data block，通过从bitmap中分配一个bit，来表明一个data block已被分配。 上一步分配的data block是block 595，这里将字符“h”写入到block 595。 将字符“i”写入到block 595。 最后更新文件夹x的inode来更新size字段。 这里我们也可以问自己一个问题，我们在下面的位置crash了会怎样？\n这里我们从bitmap block中分配了一个data block，但是又还没有更新到文件x的inode中。\n当我们重启之后，磁盘处于一个特殊的状态，这里的风险是什么？是的，我们这里丢失了data block，因为这个data block被分配了，但是却没有出现在任何文件中，因为它还没有被记录在任何inode中。\n你或许会想，是因为这里的顺序不对才会导致丢失data block的问题。我们应该先写block 33来更新inode来包含data block 595（同样的问题，这个时候data block都还没有分配怎么知道是595），之后才通过写block 45将data block 595标记为已被分配。\n所以，为了避免丢失data block，我们将写block的顺序改成这样。现在我们考虑一下，如果故障发生在这两个操作中间会怎样？(略)\n这时inode会认为data block 595属于文件x，但是在磁盘上它还被标记为未被分配的。之后如果另一个文件被创建了，block 595可能会被另一个文件所使用。所以现在两个文件都会在自己的inode中记录block 595。如果两个文件属于两个用户，那么两个用户就可以读写彼此的数据了。很明显，我们不想这样，文件系统应该确保每一个data block要么属于且只属于一个文件，要么是空闲的。所以这里的修改会导致磁盘block在多个文件之间共享的安全问题，这明显是错误的。\n所以这里的问题并不在于操作的顺序，而在于我们这里有多个写磁盘的操作，这些操作必须作为一个原子操作出现在磁盘上。\nFile system logging 我们这节课要讨论的针对文件系统crash之后的问题的解决方案，其实就是logging。\n这是来自于数据库的一种解决方案。它有一些好的属性：\n解决方案。它有一些好的属性：\n首先，它可以确保文件系统的系统调用是原子性的。比如你调用create/write系统调用，这些系统调用的效果是要么完全出现，要么完全不出现，这样就避免了一个系统调用只有部分写磁盘操作出现在磁盘上。 其次，它支持快速恢复（Fast Recovery）。在重启之后，我们不需要做大量的工作来修复文件系统，只需要非常小的工作量。这里的快速是相比另一个解决方案来说，在另一个解决方案中，你可能需要读取文件系统的所有block，读取inode，bitmap block，并检查文件系统是否还在一个正确的状态，再来修复。而logging可以有快速恢复的属性。 最后，原则上来说，它可以非常的高效，尽管我们在XV6中看到的实现不是很高效。 如何构建一个logging系统，并同时具有原子性的系统调用，快速恢复和高性能，而今天，我们只会关注前两点。\nlogging的基本思想还是很直观的。\n首先，你将磁盘分割成两个部分，其中一个部分是log，另一个部分是文件系统，文件系统可能会比log大得多。\n1.（log write）当需要更新文件系统时，我们并不是更新文件系统本身。\n假设我们在内存中缓存了bitmap block，也就是block 45。\n当需要更新bitmap时，我们并不是直接写block 45，而是将数据写入到log中，并记录这个更新应该写入到block 45。\n对于所有的写 block都会有相同的操作，例如更新inode，也会记录一条写block 33的log。\n所以基本上，任何一次写操作都是先写入到log，我们并不是直接写入到block所在的位置，而总是先将写操作写入到log中。\n2.（commit op）之后在某个时间，当文件系统的操作结束了，比如说我们前一节看到的4-5个写block操作都结束，并且都存在于log中，我们会commit文件系统的操作。\n这意味着我们需要在log的某个位置记录属于同一个文件系统的操作的个数，例如5。\n3.（install log）当我们在log中存储了所有写block的内容时，如果我们要真正执行这些操作，只需要将block从log分区移到文件系统分区。\n我们知道第一个操作该写入到block 45，我们会直接将数据从log写到block45，第二个操作该写入到block 33，我们会将它写入到block 33，依次类推。\n4.（clean log）一旦完成了，就可以清除log。清除log实际上就是将属于同一个文件系统的操作的个数设置为0。\n以上就是log的基本工作方式。\n为什么这样的工作方式是好的呢？假设我们crash并重启了。在重启的时候，文件系统会查看log的commit记录值，如果是0的话，那么什么也不做。\n如果大于0的话，我们就知道log中存储的block需要被写入到文件系统中，很明显我们在crash的时候并不一定完成了install log，我们可能是在commit之后，clean log之前crash的。\n所以这个时候我们需要做的就是reinstall（注，也就是将log中的block再次写入到文件系统），再clean log。\n这里的方法之所以能起作用，是因为可以确保当发生crash（并重启之后），我们要么将写操作所有相关的block都在文件系统中更新了，要么没有更新任何一个block，我们永远也不会只写了一部分block。\n为什么可以确保呢？我们考虑crash的几种可能情况。\n在第1步和第2步之间crash会发生什么？\n在重启的时候什么也不会做，就像系统调用从没有发生过一样，也像crash是在文件系统调用之前发生的一样。这完全可以，并且也是可接受的。\n在第2步和第3步之间crash会发生什么？\n在这个时间点，所有的log block都落盘了，因为有commit记录，所以完整的文件系统操作必然已经完成了。我们可以将log block写入到文件系统中相应的位置，这样也不会破坏文件系统。所以这种情况就像系统调用正好在crash之前就完成了。\n在install（第3步）过程中和第4步之前这段时间crash会发生什么？\n在下次重启的时候，我们会redo log，我们或许会再次将log block中的数据再次拷贝到文件系统。这样也是没问题的，因为log中的数据是固定的，我们就算重复写了文件系统，每次写入的数据也是不变的。\n重复写入并没有任何坏处，因为我们写入的数据可能本来就在文件系统中，所以多次install log完全没问题。当然在这个时间点，我们不能执行任何文件系统的系统调用。我们应该在重启文件系统之前，在重启或者恢复的过程中完成这里的恢复操作。\n换句话说，install log是幂等操作（注，idempotence，表示执行多次和执行一次效果一样），你可以执行任意多次，最后的效果都是一样的。\n学生提问：因为这里的接口只有read/write，但是如果我们做append操作，就不再安全了，对吧？\nFrans教授：某种程度来说，append是文件系统层面的操作，在这个层面，我们可以使用上面介绍的logging机制确保其原子性（注，append也可以拆解成底层的read/write）。\n学生提问：当正在commit log的时候crash了会发生什么？比如说你想执行多个写操作，但是只commit了一半。\nFrans教授：在上面的第2步，执行commit操作时，你只会在记录了所有的write操作之后，才会执行commit操作。所以在执行commit时，所有的write操作必然都在log中。而commit操作本身也有个有趣的问题，它究竟会发生什么？如我在前面指出的，commit操作本身只会写一个block。文件系统通常可以这么假设，单个block或者单个sector的write是原子操作（注，有关block和sector的区别详见14.3）。这里的意思是，如果你执行写操作，要么整个sector都会被写入，要么sector完全不会被修改。所以sector本身永远也不会被部分写入，并且commit的目标sector总是包含了有效的数据。而commit操作本身只是写log的header，如果它成功了只是在commit header中写入log的长度，例如5，这样我们就知道log的长度为5。这时crash并重启，我们就知道需要重新install 5个block的log。如果commit header没能成功写入磁盘，那这里的数值会是0。我们会认为这一次事务并没有发生过。这里本质上是write ahead rule，它表示logging系统在所有的写操作都记录在log中之前，不能install log。\nLogging的实现方式有很多，我这里展示的指示一种非常简单的方案，这个方案中clean log和install log都被推迟了。\n接下来我会运行这种非常简单的实现方式，之后在下节课我们会看到更加复杂的logging协议。\n不过所有的这些协议都遵循了write ahead rule，也就是说在写入commit记录之前，你需要确保所有的写操作都在log中。\n在这个范围内，还有大量设计上的灵活性可以用来设计特定的logging协议。\n在XV6中，我们会看到数据有两种状态，是在磁盘上还是在内存中。内存中的数据会在crash或者电力故障之后丢失。\nXV6的log结构如往常一样也是极其的简单。\n我们在最开始有一个header block，也就是我们的commit record，里面包含了：\n数字n代表有效的log block的数量 每个log block的实际对应的block编号 之后就是log的数据，也就是每个block的数据，依次为bn0对应的block的数据，bn1对应的block的数据以此类推。这就是log中的内容，并且log也不包含其他内容。\n当文件系统在运行时，在内存中也有header block的一份拷贝，拷贝中也包含了n和block编号的数组。\n这里的block编号数组就是log数据对应的实际block编号，并且相应的block也会缓存在block cache中，这个在Lec14有介绍过。\n与前一节课对应，log中第一个block编号是45，那么在block cache的某个位置，也会有block 45的cache。\nlog_write函数 接下来让我们看一些代码来帮助我们理解这里是怎么工作的。\n前面我提过事务（transaction），也就是我们不应该在所有的写操作完成之前写入commit record。\n这意味着文件系统操作必须表明事务的开始和结束。\n在XV6中，以创建文件的sys_open为例（在sysfile.c文件中）每个文件系统操作，都有begin_op和end_op分别表示事物的开始和结束。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 uint64 sys_open(void) { char path[MAXPATH]; int fd, omode; struct file *f; struct inode *ip; int n; if((n = argstr(0, path, MAXPATH)) \u0026lt; 0 || argint(1, \u0026amp;omode) \u0026lt; 0) return -1; begin_op();//THIS if(omode \u0026amp; O_CREATE){ ip = create(path, T_FILE, 0, 0); if(ip == 0){ end_op(); return -1; } } else { if((ip = namei(path)) == 0){ end_op(); return -1; } ilock(ip); if(ip-\u0026gt;type == T_DIR \u0026amp;\u0026amp; omode != O_RDONLY){ iunlockput(ip); end_op(); return -1; } } if(ip-\u0026gt;type == T_DEVICE \u0026amp;\u0026amp; (ip-\u0026gt;major \u0026lt; 0 || ip-\u0026gt;major \u0026gt;= NDEV)){ iunlockput(ip); end_op(); return -1; } if((f = filealloc()) == 0 || (fd = fdalloc(f)) \u0026lt; 0){ if(f) fileclose(f); iunlockput(ip); end_op(); return -1; } if(ip-\u0026gt;type == T_DEVICE){ f-\u0026gt;type = FD_DEVICE; f-\u0026gt;major = ip-\u0026gt;major; } else { f-\u0026gt;type = FD_INODE; f-\u0026gt;off = 0; } f-\u0026gt;ip = ip; f-\u0026gt;readable = !(omode \u0026amp; O_WRONLY); f-\u0026gt;writable = (omode \u0026amp; O_WRONLY) || (omode \u0026amp; O_RDWR); if((omode \u0026amp; O_TRUNC) \u0026amp;\u0026amp; ip-\u0026gt;type == T_FILE){ itrunc(ip); } iunlock(ip); end_op();//THIS return fd; } begin_op表明想要开始一个事务，在最后有end_op表示事务的结束。\n并且事务中的所有写block操作具备原子性，这意味着这些写block操作要么全写入，要么全不写入。\nXV6中的文件系统调用都有这样的结构，最开始是begin_op，之后是实现系统调用的代码，最后是end_op。\n在end_op中会实现commit操作。\n在begin_op和end_op之间，磁盘上或者内存中的数据结构会更新。\n但是在end_op之前，并不会有实际的改变（注，也就是不会写入到实际的block中）。\n在end_op时，我们会将数据写入到log中，之后再写入commit record或者log header。\n这里有趣的是，当文件系统调用执行写磁盘时会发生什么？\n让我们看一下fs.c中的ialloc，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Allocate an inode on device dev. // Mark it as allocated by giving it type type. // Returns an unlocked but allocated and referenced inode. struct inode* ialloc(uint dev, short type) { int inum; struct buf *bp; struct dinode *dip; for(inum = 1; inum \u0026lt; sb.ninodes; inum++){ bp = bread(dev, IBLOCK(inum, sb)); dip = (struct dinode*)bp-\u0026gt;data + inum%IPB; if(dip-\u0026gt;type == 0){ // a free inode memset(dip, 0, sizeof(*dip)); dip-\u0026gt;type = type; log_write(bp); // mark it allocated on the disk brelse(bp); return iget(dev, inum); } brelse(bp); } panic(\u0026#34;ialloc: no inodes\u0026#34;); } 在这个函数中，并没有直接调用bwrite，这里实际调用的是log_write函数。\nlog_write是由文件系统的logging实现的方法。\n任何一个文件系统调用的begin_op和end_op之间的写操作总是会走到log_write。\nlog_write函数位于log.c文件，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Caller has modified b-\u0026gt;data and is done with the buffer. // Record the block number and pin in the cache by increasing refcnt. // commit()/write_log() will do the disk write. // // log_write() replaces bwrite(); a typical use is: // bp = bread(...) // modify bp-\u0026gt;data[] // log_write(bp) // brelse(bp) void log_write(struct buf *b) { int i; if (log.lh.n \u0026gt;= LOGSIZE || log.lh.n \u0026gt;= log.size - 1) panic(\u0026#34;too big a transaction\u0026#34;); if (log.outstanding \u0026lt; 1) panic(\u0026#34;log_write outside of trans\u0026#34;); acquire(\u0026amp;log.lock); for (i = 0; i \u0026lt; log.lh.n; i++) { if (log.lh.block[i] == b-\u0026gt;blockno) // log absorbtion break; } log.lh.block[i] = b-\u0026gt;blockno; if (i == log.lh.n) { // Add new block to log? bpin(b); log.lh.n++; } release(\u0026amp;log.lock); } log_write还是很简单直观的，我们已经向block cache中的某个block写入了数据。\n比如写block 45，我们已经更新了block cache中的block 45。\n接下来我们需要在内存中记录，在稍后的commit中，要将block 45写入到磁盘的log中。\n这里的代码先获取log header的锁，之后再更新log header。\n首先代码会查看block 45是否已经被log记录了。\n如果是的话，其实不用做任何事情，因为block 45已经会被写入了。\n这种忽略的行为称为log absorbtion。\n如果block 45不在需要写入到磁盘中的block列表中，接下来会对n加1，并将block 45记录在列表的最后。\n之后，这里会通过调用bpin函数将block 45固定在block cache中，我们稍后会介绍为什么要这么做（注，详见15.8）。\n以上就是log_write的全部工作了。\n任何文件系统调用，如果需要更新block或者说更新block cache中的block，都会将block编号加在这个内存数据中（注，也就是log header在内存中的cache），除非编号已经存在。\n学生提问：这是不是意味着，bwrite不能直接使用？\nFrans教授：是的，可以这么认为，文件系统中的所有bwrite都需要被log_write替换。\nend_op函数 位于log.c中的end_op函数中会发生什么？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // called at the end of each FS system call. // commits if this was the last outstanding operation. void end_op(void) { int do_commit = 0; acquire(\u0026amp;log.lock); log.outstanding -= 1; if(log.committing) panic(\u0026#34;log.committing\u0026#34;); if(log.outstanding == 0){ do_commit = 1; log.committing = 1; } else { // begin_op() may be waiting for log space, // and decrementing log.outstanding has decreased // the amount of reserved space. wakeup(\u0026amp;log); } release(\u0026amp;log.lock); if(do_commit){ // call commit w/o holding locks, since not allowed // to sleep with locks. commit(); acquire(\u0026amp;log.lock); log.committing = 0; wakeup(\u0026amp;log); release(\u0026amp;log.lock); } } 可以看到，即使是这么简单的一个文件系统也有一些微秒的复杂之处，代码的最开始就是一些复杂情况的处理（注，15.8有这部分的解释）。\n我直接跳到正常且简单情况的代码。在简单情况下，没有其他的文件系统操作正在处理中。\n这部分代码非常简单直观，首先调用了commit函数。让我们看一下commit函数的实现，\n1 2 3 4 5 6 7 8 9 10 11 static void commit() { if (log.lh.n \u0026gt; 0) { write_log(); // Write modified blocks from cache to log write_head(); // Write header to disk -- the real commit install_trans(0); // Now install writes to home locations log.lh.n = 0; write_head(); // Erase the transaction from the log } } commit中有两个操作：\n首先是write_log。\n这基本上就是将所有存在于内存中的log header中的block编号对应的block，从block cache写入到磁盘上的log区域中（注，也就是将变化先从内存拷贝到log中）。\nwrite_head会将内存中的log header写入到磁盘中。\n看一下write_log的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Copy modified blocks from cache to log. static void write_log(void) { int tail; for (tail = 0; tail \u0026lt; log.lh.n; tail++) { struct buf *to = bread(log.dev, log.start+tail+1); // log block struct buf *from = bread(log.dev, log.lh.block[tail]); // cache block memmove(to-\u0026gt;data, from-\u0026gt;data, BSIZE); bwrite(to); // write the log brelse(from); brelse(to); } } 函数中依次遍历log中记录的block，并写入到log中。\n它首先读出log block，将cache中的block拷贝到log block，最后再将log block写回到磁盘中。\n这样可以确保需要写入的block都记录在log中。但是在这个位置，我们还没有commit，现在我们只是将block存放在了log中。\n如果我们在这个位置也就是在write_head之前crash了，那么最终的表现就像是transaction从来没有发生过。\n接下来看一下write_head函数，我之前将write_head称为commit point。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Write in-memory log header to disk. // This is the true point at which the // current transaction commits. static void write_head(void) { struct buf *buf = bread(log.dev, log.start); struct logheader *hb = (struct logheader *) (buf-\u0026gt;data); int i; hb-\u0026gt;n = log.lh.n; for (i = 0; i \u0026lt; log.lh.n; i++) { hb-\u0026gt;block[i] = log.lh.block[i]; } bwrite(buf); brelse(buf); } 函数也比较直观，首先读取log的header block。\n将n拷贝到block中，将所有的block编号拷贝到header的列表中。最后再将header block写回到磁盘。\n函数中的倒数第2行，bwrite是实际的commit point吗？如果crash发生在这个bwrite之前，会发生什么？\n这时虽然我们写了log的header block，但是数据并没有落盘。\n所以crash并重启恢复时，并不会发生任何事情。那crash发生在bwrite之后会发生什么呢？\n这时header会写入到磁盘中，当重启恢复相应的文件系统操作会被恢复。在恢复过程的某个时间点，恢复程序可以读到log header并发现比如说有5个log还没有install，恢复程序可以将这5个log拷贝到实际的位置。\n所以这里的bwrite就是实际的commit point。\n在commit point之前，transaction并没有发生，在commit point之后，只要恢复程序正确运行，transaction必然可以完成。\n回到commit函数，在commit point之后，就会实际应用transaction。\n这里很直观，就是读取log block再查看header这个block属于文件系统中的哪个block，最后再将log block写入到文件系统相应的位置。\n让我们看一下install_trans函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Copy committed blocks from log to their home location static void install_trans(int recovering) { int tail; for (tail = 0; tail \u0026lt; log.lh.n; tail++) { struct buf *lbuf = bread(log.dev, log.start+tail+1); // read log block struct buf *dbuf = bread(log.dev, log.lh.block[tail]); // read dst memmove(dbuf-\u0026gt;data, lbuf-\u0026gt;data, BSIZE); // copy block to dst bwrite(dbuf); // write dst to disk if(recovering == 0) bunpin(dbuf); brelse(lbuf); brelse(dbuf); } } 这里先读取log block，再读取文件系统对应的block。\n将数据从log拷贝到文件系统，最后将文件系统block缓存落盘。\n这里实际上就是将block数据从log中拷贝到了实际的文件系统block中。\n当然，可能在这里代码的某个位置会出现问题，但是这应该也没问题，因为在恢复的时候，我们会从最开始重新执行过。\n在commit函数中，install结束之后，会将log header中的n设置为0，再将log header写回到磁盘中。将n设置为0的效果就是清除log。\n学生提问：install_trans函数在写block的时候，先写的缓存。可不可以优化一下直接写磁盘而不写缓存让代码运行的更快一些？\nFrans教授：这里的接口是不太好。你可能会想问反正都要写入新数据，为什么要先读出目标block来。这里的代码肯定还有很多优化空间，但是为了看起来简单我们并没有这么做。\n以上就是commit内容。\nFile system recovering 接下来我们看一下发生在XV6的启动过程中的文件系统的恢复流程。\n当系统crash并重启了，在XV6启动过程中做的一件事情就是调用initlog函数。\n1 2 3 4 5 6 7 8 9 10 11 12 void initlog(int dev, struct superblock *sb) { if (sizeof(struct logheader) \u0026gt;= BSIZE) panic(\u0026#34;initlog: too big logheader\u0026#34;); initlock(\u0026amp;log.lock, \u0026#34;log\u0026#34;); log.start = sb-\u0026gt;logstart; log.size = sb-\u0026gt;nlog; log.dev = dev; recover_from_log(); } initlog基本上就是调用recover_from_log函数。\n1 2 3 4 5 6 7 8 static void recover_from_log(void) { read_head(); install_trans(1); // if committed, copy from log to disk log.lh.n = 0; write_head(); // clear the log } recover_from_log先调用read_head函数从磁盘中读取header，之后调用install_trans函数。\n这个函数之前在commit函数中也调用过，它就是读取log header中的n，然后根据n将所有的log block拷贝到文件系统的block中。recover_from_log在最后也会跟之前一样清除log。\n这就是恢复的全部流程。\n如果我们在install_trans函数中又crash了，也不会有问题，因为之后再重启时，XV6会再次调用initlog函数，再调用recover_from_log来重新install log。\n如果我们在commit之前crash了多次，在最终成功commit时，log可能会install多次。\n学生提问：如果一个进程向磁盘写了一些数据，但是在commit之前进程出现了故障，假设故障之后进程退出了，这样会有问题吗？\nFrans教授：简单回答是没问题，因此磁盘不会被更新，所以效果就像文件系统操作没有发生过一样。并且进程并不能在故障后恢复，唯一能在故障之后还能保持的是保存在磁盘中的状态。（注，应该是没有理解问题。进程通过write系统调用成功写入的数据，就算在成功落盘之前进程异常退出了，内核还是会写入到磁盘中，前提是内核还在运行。）\nLog写磁盘流程 我已经在bwrite函数中加了一个print语句。\nbwrite函数是block cache中实际写磁盘的函数，所以我们将会看到实际写磁盘的记录。\n在上节课（Lec 14）我将print语句放在了log_write中，log_write只能代表文件系统操作的记录，并不能代表实际写磁盘的记录。\n我们这里会像上节课一样执行echo \u0026ldquo;hi\u0026rdquo; \u0026gt; x，并看一下实际的写磁盘过程。\n很明显这里的记录要比只在log_write中记录要长的多。\n之前的log_write只有11条记录（注，详见14.5）但是可以看到实际上背后有很多个磁盘写操作，让我们来分别看一下这里的写磁盘操作：\n首先是前3行的bwrite 3，4，5。\n因为block 3是第一个log data block，所以前3行是在log中记录了3个写操作。这3个写操作都保存在log中，并且会写入到磁盘中的log部分。\n第4行的bwrite 2。\n因为block 2是log的起始位置，也就是log header，所以这条是commit记录。\n第5，6，7行的bwrite 33，46，32。\n这里实际就是将前3行的log data写入到实际的文件系统的block位置，这里实际是install log。\n第8行的bwrite 2，是清除log（注，也就是将log header中的n设置为0）。\n到此为止，完成了实际上的写block 33，46，32这一系列的操作。第一部分是log write，第二部分是install log，每一部分后面还跟着一个更新commit记录（注，也就是commit log和clean log）。\n学生提问：可以从这里的记录找到一次文件操作的begin_op和end_op位置吗？\nFrans教授：大概可以知道。我们实际上不知道begin_op的位置，但是所有的文件系统操作都从begin_op开始。更新commit记录必然在end_op中，所以我们可以找到文件系统操作的end_op位置，之后就是begin_op（注，其实这里所有的操作都在end_op中，只需要区分每一次end_op的调用就可以找到begin_op）。\n所以以上就是XV6中文件系统的logging介绍，即使是这么一个简单的logging系统也有一定的复杂度。\n这里立刻可以想到的一个问题是，通过观察这些记录，这是一个很有效的实现吗？很明显不是的，因为数据被写了两次。\n如果我写一个大文件，我需要在磁盘中将这个大文件写两次。所以这必然不是一个高性能的实现，为了实现Crash safety我们将原本的性能降低了一倍。\n当你们去读ext3论文时，你们应该时刻思考如何避免这里的性能降低一倍的问题。\nFile system challenges 前面说到XV6的文件系统有一定的复杂性，接下来我将介绍一下三个复杂的地方或者也可以认为是三个挑战。\n第一个是cache eviction。 假设transaction还在进行中，我们刚刚更新了block 45，正要更新下一个block，而整个buffer cache都满了并且决定撤回block 45。\n在buffer cache中撤回block 45意味着我们需要将其写入到磁盘的block 45位置，这里会不会有问题？如果我们这么做了的话，会破坏什么规则吗？\n是的，如果将block 45写入到磁盘之后发生了crash，就会破坏transaction的原子性。\n这里也破坏了前面说过的write ahead rule，write ahead rule的含义是，你需要先将所有的block写入到log中，之后才能实际的更新文件系统block。\n所以buffer cache不能撤回任何还位于log的block。\n前面在介绍log_write函数时，其中调用了一个叫做bpin的函数，这个函数的作用就如它的名字一样，将block固定在buffer cache中。\n它是通过给block cache增加引用计数来避免cache撤回对应的block。\n在之前（注，详见14.6）我们看过，如果引用计数不为0，那么buffer cache是不会撤回block cache的。\n相应的在将来的某个时间，所有的数据都写入到了log中，我们可以在cache中unpin block（注，在15.5中的install_trans函数中会有unpin，因为这时block已经写入到了log中）。\n所以这是第一个复杂的地方，我们需要pin/unpin buffer cache中的block。\n第二个挑战是，文件系统操作必须适配log的大小。 在XV6中，总共有30个log block（注，详见14.3）。\n当然我们可以提升log的尺寸，在真实的文件系统中会有大得多的log空间。\n但是无所谓啦，不管log多大，文件系统操作必须能放在log空间中。\n如果一个文件系统操作尝试写入超过30个block，那么意味着部分内容需要直接写到文件系统区域，而这是不被允许的，因为这违背了write ahead rule。\n所以所有的文件系统操作都必须适配log的大小。\n为什么XV6的log大小是30？因为30比任何一个文件系统操作涉及的写操作数都大，Robert和我看了一下所有的文件系统操作，发现都远小于30，所以就将XV6的log大小设为30。\n我们目前看过的一些文件系统操作，例如创建一个文件只包含了写5个block。\n实际上大部分文件系统操作只会写几个block。\n你们可以想到什么样的文件系统操作会写很多很多个block吗？是的，写一个大文件。如果我们调用write系统调用并传入1M字节的数据，这对应了写1000个block，这看起来会有很严重的问题，因为这破坏了我们刚刚说的“文件系统操作必须适配log的大小”这条规则。\n让我们看一下file.c文件中的file_write函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Write to file f. // addr is a user virtual address. int filewrite(struct file *f, uint64 addr, int n) { int r, ret = 0; if(f-\u0026gt;writable == 0) return -1; if(f-\u0026gt;type == FD_PIPE){ ret = pipewrite(f-\u0026gt;pipe, addr, n); } else if(f-\u0026gt;type == FD_DEVICE){ if(f-\u0026gt;major \u0026lt; 0 || f-\u0026gt;major \u0026gt;= NDEV || !devsw[f-\u0026gt;major].write) return -1; ret = devsw[f-\u0026gt;major].write(1, addr, n); } else if(f-\u0026gt;type == FD_INODE){ // write a few blocks at a time to avoid exceeding // the maximum log transaction size, including // i-node, indirect block, allocation blocks, // and 2 blocks of slop for non-aligned writes. // this really belongs lower down, since writei() // might be writing a device like the console. int max = ((MAXOPBLOCKS-1-1-2) / 2) * BSIZE; int i = 0; while(i \u0026lt; n){ int n1 = n - i; if(n1 \u0026gt; max) n1 = max; begin_op(); ilock(f-\u0026gt;ip); if ((r = writei(f-\u0026gt;ip, 1, addr + i, f-\u0026gt;off, n1)) \u0026gt; 0) f-\u0026gt;off += r; iunlock(f-\u0026gt;ip); end_op(); if(r != n1){ // error from writei break; } i += r; } ret = (i == n ? n : -1); } else { panic(\u0026#34;filewrite\u0026#34;); } return ret; } 从这段代码可以看出，如果写入的block数超过了30，那么一个写操作会被分割成多个小一些的写操作。\n这里整个写操作不是原子的，但是这还好，因为write系统调用的语义并不要求所有1000个block都是原子的写入，它只要求我们不要损坏文件系统。\n所以XV6会将一个大的写操作分割成多个小的写操作，每一个小的写操作通过独立的transaction写入。这样文件系统本身不会陷入不正确的状态中。\n这里还需要注意，因为block在落盘之前需要在cache中pin住，所以buffer cache的尺寸也要大于log的尺寸。\n最后一个挑战是并发文件系统调用。 让我先来解释一下这里会有什么问题，再看对应的解决方案。\n假设我们有一段log，和两个并发的执行的transaction，其中transaction t0在log的前半段记录，transaction t1在log的后半段记录。可能我们用完了log空间，但是任何一个transaction都还没完成。\n现在我们能提交任何一个transaction吗？我们不能，因为这样的话我们就提交了一个部分完成的transaction，这违背了write ahead rule，log本身也没有起到应该的作用。\n所以必须要保证多个并发transaction加在一起也适配log的大小。\n所以当我们还没有完成一个文件系统操作时，我们必须在确保可能写入的总的log数小于log区域的大小的前提下，才允许另一个文件系统操作开始。\nXV6通过限制并发文件系统操作的个数来实现这一点。\n在begin_op中，我们会检查当前有多少个文件系统操作正在进行。\n如果有太多正在进行的文件系统操作，我们会通过sleep停止当前文件系统操作的运行，并等待所有其他所有的文件系统操作都执行完并commit之后再唤醒。\n这里的其他所有文件系统操作都会一起commit。\n有的时候这被称为group commit，因为这里将多个操作像一个大的transaction一样提交了，这里的多个操作要么全部发生了，要么全部没有发生。\n学生提问：group commit有必要吗？不能当一个文件系统操作结束的时候就commit掉，然后再commit其他的操作吗？\nFrans教授：如果这样的话你需要非常非常小心。因为有一点我没有说得很清楚，我们需要保证write系统调用的顺序。如果一个read看到了一个write，再执行了一次write，那么第二个write必须要发生在第一个write之后。在log中的顺序，本身就反应了write系统调用的顺序，你不能改变log中write系统调用的执行顺序，因为这可能会导致对用户程序可见的奇怪的行为。所以必须以transaction发生的顺序commit它们，而一次性提交所有的操作总是比较安全的，这可以保证文件系统处于一个好的状态。\n最后我们再回到最开始，看一下begin_op，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // called at the start of each FS system call. void begin_op(void) { acquire(\u0026amp;log.lock); while(1){ if(log.committing){ sleep(\u0026amp;log, \u0026amp;log.lock); } else if(log.lh.n + (log.outstanding+1)*MAXOPBLOCKS \u0026gt; LOGSIZE){ // this op might exhaust log space; wait for commit. sleep(\u0026amp;log, \u0026amp;log.lock); } else { log.outstanding += 1; release(\u0026amp;log.lock); break; } } } 首先，如果log正在commit过程中，那么就等到log提交完成，因为我们不能在install log的过程中写log；\n其次，如果当前操作是允许并发的操作个数的后一个，那么当前操作可能会超过log区域的大小，我们也需要sleep并等待所有之前的操作结束；\n最后，如果当前操作可以继续执行，需要将log的outstanding字段加1，最后再退出函数并执行文件系统操作。\n再次看一下end_op函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // called at the end of each FS system call. // commits if this was the last outstanding operation. void end_op(void) { int do_commit = 0; acquire(\u0026amp;log.lock); log.outstanding -= 1; if(log.committing) panic(\u0026#34;log.committing\u0026#34;); if(log.outstanding == 0){ do_commit = 1; log.committing = 1; } else { // begin_op() may be waiting for log space, // and decrementing log.outstanding has decreased // the amount of reserved space. wakeup(\u0026amp;log); } release(\u0026amp;log.lock); if(do_commit){ // call commit w/o holding locks, since not allowed // to sleep with locks. commit(); acquire(\u0026amp;log.lock); log.committing = 0; wakeup(\u0026amp;log); release(\u0026amp;log.lock); } } 在最开始首先会对log的outstanding字段减1，因为一个transaction正在结束；\n其次检查committing状态，当前不可能在committing状态，所以如果是的话会触发panic；\n如果当前操作是整个并发操作的最后一个的话（log.outstanding == 0），接下来立刻就会执行commit；\n如果当前操作不是整个并发操作的最后一个的话，我们需要唤醒在begin_op中sleep的操作，让它们检查是不是能运行。\n（注，这里的outstanding有点迷，它表示的是当前正在并发执行的文件系统操作的个数，MAXOPBLOCKS定义了一个操作最大可能涉及的block数量。在begin_op中，只要log空间还足够，就可以一直增加并发执行的文件系统操作。所以XV6是通过设定了MAXOPBLOCKS，再间接的限定支持的并发文件系统操作的个数）\n所以，即使是XV6中这样一个简单的文件系统，也有一些复杂性和挑战。\n最后让我总结一下：\n这节课讨论的是使用logging来解决crash safety或者说多个步骤的文件系统操作的安全性。这种方式对于安全性来说没有问题，但是性能不咋地。\n学生提问：前面说到cache size至少要跟log size一样大，如果它们一样大的话，并且log pin了30个block，其他操作就不能再进行了，因为buffer中没有额外的空间了。\nFrans教授：如果buffer cache中没有空间了，XV6会直接panic。这并不理想，实际上有点恐怖。所以我们在挑选buffer cache size的时候希望用一个不太可能导致这里问题的数字。这里为什么不能直接返回错误，而是要panic？因为很多文件系统操作都是多个步骤的操作，假设我们执行了两个write操作，但是第三个write操作找不到可用的cache空间，那么第三个操作无法完成，我们不能就直接返回错误，因为我们可能已经更新了一个目录的某个部分，为了保证文件系统的正确性，我们需要撤回之前的更新。所以如果log pin了30个block，并且buffer cache没有额外的空间了，会直接panic。当然这种情况不太会发生，只有一些极端情况才会发生。\n","date":"2024-09-29T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08115crash-recovery/bg_hu_59c1eae71e7c3edc.jpg","permalink":"https://echudet.github.io/p/mit6s08115crash-recovery/","title":"MIT6S081：15Crash recovery"},{"content":"Lecture 14 File Systems 预习 概述 xv6文件系统实现分为七层，如图8.1所示。\n文件描述符（File descriptor） 路径名（Pathname） 目录（Directory） 索引结点（Inode） 日志（Logging） 缓冲区高速缓存（Buffer cache） 磁盘（Disk） 图8.1 XV6文件系统的层级\n磁盘层读取和写入virtio硬盘上的块。\n缓冲区高速缓存层缓存磁盘块并同步对它们的访问，确保每次只有一个内核进程可以修改存储在任何特定块中的数据。\n日志记录层允许更高层在一次事务（transaction）中将更新包装到多个块，并确保在遇到崩溃时自动更新这些块（即，所有块都已更新或无更新）。\n索引结点层提供单独的文件，每个文件表示为一个索引结点，其中包含唯一的索引号（i-number）和一些保存文件数据的块。\n目录层将每个目录实现为一种特殊的索引结点，其内容是一系列目录项，每个目录项包含一个文件名和索引号。\n路径名层提供了分层路径名，如***/usr/rtm/xv6/fs.c***，并通过递归查找来解析它们。\n文件描述符层使用文件系统接口抽象了许多Unix资源（例如，管道、设备、文件等），简化了应用程序员的工作。\n文件系统必须有将索引节点和内容块存储在磁盘上哪些位置的方案。\n为此，xv6将磁盘划分为几个部分，如图8.2所示。\n文件系统不使用块0（它保存引导扇区）。\n块1称为超级块：它包含有关文件系统的元数据（文件系统大小（以块为单位）、数据块数、索引节点数和日志中的块数）。\n从2开始的块保存日志。\n日志之后是索引节点，每个块有多个索引节点。\n然后是位图块，跟踪正在使用的数据块。\n其余的块是数据块：每个都要么在位图块中标记为空闲，要么保存文件或目录的内容。\n超级块由一个名为mkfs的单独的程序填充，该程序构建初始文件系统。\n本章的其余部分将从缓冲区高速缓存层开始讨论每一层。\n注意那些在较低层次上精心选择的抽象可以简化较高层次的设计的情况。\nBuffer cache层 Buffer cache有两个任务：\n同步对磁盘块的访问，以确保磁盘块在内存中只有一个副本，并且一次只有一个内核线程使用该副本 缓存常用块，以便不需要从慢速磁盘重新读取它们。代码在bio.c中。 Buffer cache层导出的主接口主要是bread和bwrite；\n前者获取一个buf，其中包含一个可以在内存中读取或修改的块的副本，后者将修改后的缓冲区写入磁盘上的相应块。\n内核线程必须通过调用brelse释放缓冲区。\nBuffer cache每个缓冲区使用一个睡眠锁，以确保每个缓冲区（因此也是每个磁盘块）每次只被一个线程使用；bread返回一个上锁的缓冲区，brelse释放该锁。\n让我们回到Buffer cache。\nBuffer cache中保存磁盘块的缓冲区数量固定，这意味着如果文件系统请求还未存放在缓存中的块，Buffer cache必须回收当前保存其他块内容的缓冲区。\nBuffer cache为新块回收最近使用最少的缓冲区。这样做的原因是认为最近使用最少的缓冲区是最不可能近期再次使用的缓冲区。\n代码：Buffer cache Buffer cache是以双链表表示的缓冲区。\nmain（kernel/main.c:27）调用的函数binit使用静态数组buf（kernel/bio.c:43-52）中的NBUF个缓冲区初始化列表。\n对Buffer cache的所有其他访问都通过bcache.head引用链表，而不是buf数组。\n缓冲区有两个与之关联的状态字段。\n字段valid表示缓冲区是否包含块的副本。\n字段disk表示缓冲区内容是否已交给磁盘，这可能会更改缓冲区（例如，将数据从磁盘写入data）。\nBread（kernel/bio.c:93）调用bget为给定扇区（kernel/bio.c:97）获取缓冲区。\n如果缓冲区需要从磁盘进行读取，bread会在返回缓冲区之前调用virtio_disk_rw来执行此操作。\nBget（kernel/bio.c:59）扫描缓冲区列表，查找具有给定设备和扇区号（kernel/bio.c:65-73）的缓冲区。\n如果存在这样的缓冲区，bget将获取缓冲区的睡眠锁。然后Bget返回锁定的缓冲区。\n如果对于给定的扇区没有缓冲区，bget必须创建一个，这可能会重用包含其他扇区的缓冲区。\n它再次扫描缓冲区列表，查找未在使用中的缓冲区（b-\u0026gt;refcnt = 0）：任何这样的缓冲区都可以使用。\nBget编辑缓冲区元数据以记录新设备和扇区号，并获取其睡眠锁。\n注意，b-\u0026gt;valid = 0的布置确保了bread将从磁盘读取块数据，而不是错误地使用缓冲区以前的内容。\n每个磁盘扇区最多有一个缓存缓冲区是非常重要的，并且因为文件系统使用缓冲区上的锁进行同步，可以确保读者看到写操作。\nBget的从第一个检查块是否缓存的循环到第二个声明块现在已缓存（通过设置dev、blockno和refcnt）的循环，一直持有bcache.lock来确保此不变量。\n这会导致检查块是否存在以及（如果不存在）指定一个缓冲区来存储块具有原子性。\nbget在bcache.lock临界区域之外获取缓冲区的睡眠锁是安全的，因为非零b-\u0026gt;refcnt防止缓冲区被重新用于不同的磁盘块。\n睡眠锁保护块缓冲内容的读写，而bcache.lock保护有关缓存哪些块的信息。\n如果所有缓冲区都处于忙碌，那么太多进程同时执行文件系统调用；bget将会panic。\n一个更优雅的响应可能是在缓冲区空闲之前休眠，尽管这样可能会出现死锁。\n一旦bread读取了磁盘（如果需要）并将缓冲区返回给其调用者，调用者就可以独占使用缓冲区，并可以读取或写入数据字节。\n如果调用者确实修改了缓冲区，则必须在释放缓冲区之前调用bwrite将更改的数据写入磁盘。\nBwrite（kernel/bio.c:107）调用virtio_disk_rw与磁盘硬件对话。\n当调用方使用完缓冲区后，它必须调用brelse来释放缓冲区(brelse是b-release的缩写，这个名字很隐晦，但值得学习：它起源于Unix，也用于BSD、Linux和Solaris）。\nbrelse（kernel/bio.c:117）释放睡眠锁并将缓冲区移动到链表的前面（kernel/bio.c:128-133）。\n移动缓冲区会使列表按缓冲区的使用频率排序（意思是释放）：列表中的第一个缓冲区是最近使用的，最后一个是最近使用最少的。\nbget中的两个循环利用了这一点：在最坏的情况下，对现有缓冲区的扫描必须处理整个列表，但首先检查最新使用的缓冲区（从bcache.head开始，然后是下一个指针），在引用局部性良好的情况下将减少扫描时间。\n选择要重用的缓冲区时，通过自后向前扫描（跟随prev指针）选择最近使用最少的缓冲区。\n这个流程描述了 Buffer cache 在 XV6 操作系统中的管理方式。\nBuffer cache 是通过双向链表来表示缓冲区。使用 binit 函数初始化缓存，创建了一个包含 NBUF 个缓冲区的静态数组 buf，并通过 bcache.head 引用缓冲区链表进行所有访问。每个缓冲区有两个状态字段：\nvalid：表示缓冲区是否包含有效的磁盘块副本。 disk：表示缓冲区内容是否已提交到磁盘。 bread 函数调用 bget 来获取给定扇区的缓冲区。它根据设备号和扇区号查找缓冲区。如果找到匹配的缓冲区，锁定它并返回。如果没有找到，则会尝试重用未使用的缓冲区。\nbget 进行两轮扫描：\n第一轮：扫描查找是否已经有匹配的缓冲区缓存。如果找到，锁定并返回。 第二轮：如果没有找到匹配的缓存，则扫描未使用的缓冲区，修改元数据记录新设备和扇区号，并返回。 确保缓存的一致性和唯一性：\n为确保同一个磁盘扇区在任何时刻只有一个缓存缓冲区，bget 在两个扫描中都持有 bcache.lock，使得查找和缓存块的过程是原子操作。缓冲区的引用计数 refcnt 确保缓冲区不会在锁定后被其他进程重用。\n读取与写入：当调用者获取到缓冲区后，可以读取或者写入数据。如果缓冲区需要从磁盘读取数据，bread 会通过 virtio_disk_rw 执行读取操作。如果调用者修改了缓冲区内容，必须通过调用 bwrite 函数将更改写入磁盘。\n释放缓冲区：调用者使用完缓冲区后，必须通过 brelse 来释放缓冲区。brelse 释放睡眠锁，并将缓冲区移动到链表前面，这样链表会按最近使用顺序排序（LRU策略）。\n缓存管理与优化\n通过链表的双向扫描，bget减少了缓冲区查找和重用的开销： 向前扫描是从最近使用的缓冲区开始查找，这种局部性提高了效率。 向后扫描时选择最近最少使用的缓冲区来重用，保证了缓存的合理利用。 如果所有缓冲区都处于忙碌状态，bget 会触发 panic，虽然更优雅的处理方式是等待空闲缓冲区，但可能导致死锁。 Buffer cache 管理流程的核心在于利用双向链表来组织缓冲区，并通过局部性优化缓冲区的查找和重用。同时，通过锁机制确保多个进程之间对缓冲区的访问和操作是安全的。\n索引结点层 术语inode（即索引结点）可以具有两种相关含义之一。\n它可能是指包含文件大小和数据块编号列表的磁盘上的数据结构。\n或者“inode”可能指内存中的inode，它包含磁盘上inode的副本以及内核中所需的额外信息。\n磁盘上的inode都被打包到一个称为inode块的连续磁盘区域中。\n每个inode的大小都相同，因此在给定数字n的情况下，很容易在磁盘上找到第n个inode。\n事实上，这个编号n，称为inode number或i-number，是在具体实现中标识inode的方式。\n磁盘上的inode由struct dinode（kernel/fs.h:32）定义。\n1 2 3 4 5 6 7 8 9 // On-disk inode structure struct dinode { short type; // File type short major; // Major device number (T_DEVICE only) short minor; // Minor device number (T_DEVICE only) short nlink; // Number of links to inode in file system uint size; // Size of file (bytes) uint addrs[NDIRECT+1]; // Data block addresses }; 字段type区分文件、目录和特殊文件（设备）。type为零表示磁盘inode是空闲的。\n字段nlink统计引用此inode的目录条目数，以便识别何时应释放磁盘上的inode及其数据块。\n字段size记录文件中内容的字节数。\n字段addrs数组记录保存文件内容的磁盘块的块号。\n内核将活动的inode集合保存在内存中；struct inode（kernel/file.h:17）是磁盘上struct dinode的内存副本。\n只有当有C指针引用某个inode时，内核才会在内存中存储该inode。\nref字段统计引用内存中inode的C指针的数量，如果引用计数降至零，内核将从内存中丢弃该inode。\niget和iput函数分别获取和释放指向inode的指针，修改引用计数。\n指向inode的指针可以来自文件描述符、当前工作目录和如exec的瞬态内核代码。\nxv6的inode代码中有四种锁或类似锁的机制。\nicache.lock保护以下两个不变量：inode最多在缓存中出现一次；缓存inode的ref字段记录指向缓存inode的内存指针数量。\n每个内存中的inode都有一个包含睡眠锁的lock字段，它确保以独占方式访问inode的字段（如文件长度）以及inode的文件或目录内容块。\n如果inode的ref大于零，则会导致系统在cache中维护inode，而不会对其他inode重用此缓存项。\n最后，每个inode都包含一个nlink字段（在磁盘上，如果已缓存则复制到内存中），该字段统计引用文件的目录项的数量；如果inode的链接计数大于零，xv6将不会释放inode。\niget()返回的struct inode指针在相应的iput()调用之前保证有效：inode不会被删除，指针引用的内存也不会被其他inode重用。\niget()提供对inode的非独占访问，因此可以有许多指向同一inode的指针。\n文件系统代码的许多部分都依赖于iget()的这种行为，既可以保存对inode的长期引用（如打开的文件和当前目录），也可以防止争用，同时避免操纵多个inode（如路径名查找）的代码产生死锁。\niget返回的struct inode可能没有任何有用的内容。\n为了确保它保存磁盘inode的副本，代码必须调用ilock。\n这将锁定inode（以便没有其他进程可以对其进行ilock），并从磁盘读取尚未读取的inode。\niunlock释放inode上的锁。\n将inode指针的获取与锁定分离有助于在某些情况下避免死锁，例如在目录查找期间。\n多个进程可以持有指向iget返回的inode的C指针，但一次只能有一个进程锁定inode。\ninode缓存只缓存内核代码或数据结构持有C指针的inode。\n它的主要工作实际上是同步多个进程的访问；缓存是次要的。\n如果经常使用inode，在inode缓存不保留它的情况下buffer cache可能会将其保留在内存中。\ninode缓存是直写的，这意味着修改已缓存inode的代码必须立即使用iupdate将其写入磁盘。\ninode究竟是什么？\ninode是一种用于描述文件的结构，可以表示为磁盘上的inode或内存中的inode：\n磁盘上的inode：是存储在磁盘上的数据结构，用于保存文件的元数据（例如文件大小、文件类型、数据块编号等）。 内存中的inode：是磁盘inode的副本，保存在内存中，并且包含内核需要的额外信息。内存中的inode用于跟踪活动文件和目录。 inode和dinode有什么关系？\ndinode 是 磁盘上的inode 的结构定义，用于表示存储在磁盘上的文件的元数据。 inode 是 内存中的dinode 的副本，它通过引用 dinode 的数据来进行文件操作。此外，内存中的 inode 还包含额外的信息，如引用计数和锁等。 总的来说，dinode 是磁盘上的结构，而 inode 是其在内存中的表现形式，并增加了内核所需的额外字段。\n和inode/dinode相关的字段及其作用\ndinode（磁盘上的inode）字段（kernel/fs.h:32）：\ntype：表示文件的类型，如普通文件、目录、设备文件等。type = 0 表示该inode为空闲状态。\nnlink：引用该inode的目录项的数量。用于跟踪文件的链接数量，如果 nlink 降为零，意味着没有文件指向该inode，系统可以释放inode及其关联的磁盘块。\nsize：记录文件的字节数，即文件的大小。\naddrs：保存文件内容的磁盘块编号列表，指向文件内容在磁盘上的存储位置。\ninode（内存中的inode）字段（kernel/file.h:17）\nref：引用此inode的内存指针的数量。当 ref \u0026gt; 0 时，该inode保存在内存中。 lock：inode的睡眠锁，确保独占访问inode的字段及文件内容块，防止多个进程同时操作相同的inode，造成竞争。 dev：inode所属的设备号，指向文件所在的设备。 inum：inode number（i-number），用于标识inode，是磁盘上inode的编号。 valid：用于标记此内存inode是否包含有效的磁盘inode数据。 nlink：引用计数字段，与磁盘上的 nlink 相同，用于同步两个层次的inode数据。 和inode/dinode相关的函数及其作用\niget（kernel/fs.c）： 作用：从磁盘加载inode到内存，返回指向该inode的指针。该函数不提供inode的独占访问，但确保返回的inode指针在 iput 调用之前是有效的。 功能：它管理内存中的inode，增减引用计数，确保内存中的inode不会在多个进程间被错误地重用。 iput（kernel/fs.c）： 作用：减少inode的引用计数。当引用计数降为零时，释放inode，并可能从内存中丢弃该inode。 功能：负责清理不再使用的inode，避免内存浪费。 ilock（kernel/fs.c）： 作用：锁定inode，确保inode的独占访问，通常在修改inode或访问文件数据时使用。 功能：从磁盘中读取未加载的inode，并保护多个进程对inode的并发访问。 iunlock（kernel/fs.c）： 作用：解锁inode，允许其他进程继续对inode进行操作。 功能：确保操作完成后，释放锁，使其他进程可以安全访问inode。 iupdate（kernel/fs.c）： 作用：将内存中的inode更新到磁盘，确保内存inode的更改反映到磁盘上的inode。 功能：保持磁盘上inode和内存中inode的一致性，避免数据丢失。 brelse： 作用：释放缓冲区并将其返回到缓存中。 功能：当对inode或文件操作结束时，缓解资源争用，将缓冲区释放回来。 总结：\ninode 是文件的核心元数据结构，磁盘上通过 dinode 定义，内存中通过 inode 管理。 两者通过字段如 type、nlink 和 size 共同维护文件的状态和信息。 关键函数如 iget、iput、ilock、iunlock 负责对 inode 的引用、锁定和同步，确保文件系统在并发访问下的正确性。 在文件系统中，link 指的是一个文件或目录在不同的目录项（directory entries）中有多个指向同一个 inode 的引用。这些指向同一个 inode 的引用称为 硬链接（hard links）。\nnlink 的含义\nnlink 是 inode 结构中的一个字段，记录了 指向该 inode 的目录项数量。简单来说，nlink 统计了文件有多少个硬链接。 当 nlink 变为 0 时，意味着没有任何目录项再指向该 inode，此时文件的实际内容（包括占用的磁盘块）就可以被删除。 假设我们有一个文件系统，文件 file1 存在于目录 /home/user 中。\n创建文件 file1：\n当创建文件 file1 时，文件系统会为它分配一个新的 inode。\n该 inode 会包含 file1 的元数据，比如文件大小、存储位置等。\n同时，inode 的 nlink 字段被设置为 1，表示有一个目录项（/home/user/file1）指向这个 inode。\n1 2 /home/user/file1 -\u0026gt; inode 100 nlink = 1 创建硬链接 file2 到 file1：\n现在，我们执行命令 ln /home/user/file1 /home/user/file2，这是在同一个目录下创建一个新的文件名 file2，但 file2 和 file1 共享相同的 inode。\n也就是说，file1 和 file2 指向同一个 inode，它们共享文件的元数据和内容。\n此时，inode 的 nlink 字段增加到 2，因为有两个目录项（/home/user/file1 和 /home/user/file2）指向同一个 inode。\n1 2 3 /home/user/file1 -\u0026gt; inode 100 /home/user/file2 -\u0026gt; inode 100 nlink = 2 删除文件 file1：\n当我们删除 file1 时（通过 rm /home/user/file1），文件系统会移除目录项 /home/user/file1，但由于 nlink 仍为 1（file2 还存在），文件的 inode 和其数据依然保留。\n只有当 nlink 降为 0 时，文件的实际数据才会被删除。\n1 2 /home/user/file2 -\u0026gt; inode 100 nlink = 1 删除 file2： 如果之后也删除了 file2，nlink 变为 0，此时该 inode 不再有任何目录项指向它，文件系统会将它的元数据和占用的磁盘块释放。 总结：\nnlink 记录了指向同一个文件（inode）的硬链接数。 当创建多个硬链接时，所有的链接指向同一个 inode，因此它们共享文件的内容和元数据。 只有当 nlink 变为 0 时，文件的内容和 inode 才会被真正删除。 代码：Inodes 为了分配新的inode（例如，在创建文件时），xv6调用ialloc（kernel/fs.c:196）。\nialloc类似于balloc：它一次一个块地遍历磁盘上的索引节点结构体，查找标记为空闲的一个。\n当它找到一个时，它通过将新type写入磁盘来声明它，然后末尾通过调用iget（kernel/fs.c:210）从inode缓存返回一个条目。\nialloc的正确操作取决于这样一个事实：一次只有一个进程可以保存对bp的引用：ialloc可以确保其他进程不会同时看到inode可用并尝试声明它。\nIget（kernel/fs.c:243）在inode缓存中查找具有所需设备和inode编号的活动条目（ip-\u0026gt;ref \u0026gt; 0）。\n如果找到一个，它将返回对该inode的新引用（kernel/fs.c:252-256）。\n在iget扫描时，它会记录第一个空槽（kernel/fs.c:257-258）的位置，如果需要分配缓存项，它会使用这个槽。\n在读取或写入inode的元数据或内容之前，代码必须使用ilock锁定inode。\nIlock（kernel/fs.c:289）为此使用睡眠锁。一旦ilock以独占方式访问inode，它将根据需要从磁盘（更可能是buffer cache）读取inode。\n函数iunlock（kernel/fs.c:317）释放睡眠锁，这可能会导致任何睡眠进程被唤醒。\nIput（kernel/fs.c:333）通过减少引用计数（kernel/fs.c:356）释放指向inode的C指针。\n如果这是最后一次引用，inode缓存中该inode的槽现在将是空闲的，可以重用于其他inode。\n如果iput发现没有指向inode的C指针引用，并且inode没有指向它的链接（发生于无目录），则必须释放inode及其数据块。\nIput调用itrunc将文件截断为零字节，释放数据块；将索引节点类型设置为0（未分配）；并将inode写入磁盘（kernel/fs.c:338）。\niput中释放inode的锁定协议值得仔细研究。\n一个危险是并发线程可能正在ilock中等待使用该inode（例如，读取文件或列出目录），并且不会做好该inode已不再被分配的准备。\n这不可能发生，因为如果缓存的inode没有链接，并且ip-\u0026gt;ref为1，那么系统调用就无法获取指向该inode的指针。\n那一个引用是调用iput的线程所拥有的引用。\n的确，iput在icache.lock的临界区域之外检查引用计数是否为1，但此时已知链接计数为零，因此没有线程会尝试获取新引用。\n另一个主要危险是，对ialloc的并发调用可能会选择iput正在释放的同一个inode。\n这只能在iupdate写入磁盘以使inode的type为零后发生。\n这个争用是良性的：分配线程将客气地等待获取inode的睡眠锁，然后再读取或写入inode，此时iput已完成。\niput()可以写入磁盘。\n这意味着任何使用文件系统的系统调用都可能写入磁盘，因为系统调用可能是最后一个引用该文件的系统调用。\n即使像read()这样看起来是只读的调用，也可能最终调用iput()。\n这反过来意味着，即使是只读系统调用，如果它们使用文件系统，也必须在事务中进行包装。\niput()和崩溃之间存在一种具有挑战性的交互。\niput()不会在文件的链接计数降至零时立即截断文件，因为某些进程可能仍在内存中保留对inode的引用：进程可能仍在读取和写入该文件，因为它已成功打开该文件。\n但是，如果在最后一个进程关闭该文件的文件描述符之前发生崩溃，则该文件将被标记为已在磁盘上分配，但没有目录项指向它。\n文件系统以两种方式之一处理这种情况。\n简单的解决方案用于恢复时：重新启动后，文件系统会扫描整个文件系统，以查找标记为已分配但没有指向它们的目录项的文件。\n如果存在任何此类文件，接下来可以将其释放。\n第二种解决方案不需要扫描文件系统。\n在此解决方案中，文件系统在磁盘（例如在超级块中）上记录链接计数降至零但引用计数不为零的文件的i-number。\n如果文件系统在其引用计数达到0时删除该文件，则会通过从列表中删除该inode来更新磁盘列表。\n恢复时，文件系统将释放列表中的任何文件。\nXv6没有实现这两种解决方案，这意味着inode可能被标记为已在磁盘上分配，即使它们不再使用。这意味着随着时间的推移，xv6可能会面临磁盘空间不足的风险。\n多个进程或线程可能同时尝试访问同一个 inode，而这些进程通过锁机制（如 ilock）来确保独占访问。在某些情况下，某些线程可能正在等待获取这个 inode 的锁，比如在读取文件内容或者列出目录时（因为列出目录也是在访问目录 inode），这些操作需要独占锁。然而，一个潜在的危险是，如果 iput() 释放了该 inode，而其他线程正在等待这个 inode 的锁，这些线程可能会面对该 inode 已被删除或释放的情况。\n具体流程可以分解为以下步骤：\n进程 A 获得 inode 锁： 进程 A 通过 ilock() 获得对某个 inode 的独占锁，执行文件读取或其他操作。此时 ip-\u0026gt;ref 表示有指针引用该 inode。 进程 B 等待 inode 锁： 进程 B 也尝试访问同一个 inode，但由于锁已被进程 A 占用，进程 B 进入等待状态，等待 ilock() 返回。 进程 A 调用 iput()： 进程 A 完成了对 inode 的操作，并调用 iput() 来减少引用计数，准备释放该 inode。此时，iput() 检查 inode 的引用计数 ip-\u0026gt;ref 和链接计数 nlink。 如果 inode 的链接计数 nlink = 0 并且引用计数 ip-\u0026gt;ref = 1，意味着没有其他进程引用该 inode，这时 iput() 可以将其释放。 潜在的问题：进程 B 继续等待： 如果没有锁定协议，那么当 iput() 释放 inode 后，进程 B 可能获得了一个已经被释放的 inode。为了避免这种情况，锁定机制确保了 inode 的引用计数和锁的使用没有冲突，也就是说，iput() 不会释放一个仍有其他进程等待的 inode，避免了资源错误释放。 \u0026ldquo;截断\u0026quot;文件的意思\n截断（truncate） 是指 减少文件的内容或大小。当我们说 \u0026ldquo;iput() 不会立即截断文件\u0026rdquo; 时，意思是即便某个文件的链接计数 nlink 已经为零（即没有目录项再指向这个文件），并且不再有任何进程打开这个文件，iput() 也不会立刻释放与文件相关的磁盘空间。 这是因为进程可能仍然持有该文件的文件描述符，仍在访问文件。因此，在 iput() 中，仅当最后一个进程关闭该文件描述符，且文件的链接计数为零时，系统才会真正释放文件占用的磁盘空间。 目录项与 inode 的关系\n目录项（directory entry） 是 文件系统中存储文件名与 inode 号的映射。目录项记录了文件名，并指向相应的 inode。换句话说，每个文件或目录都有一个或多个目录项，它们通过 inode number（i-number） 引用磁盘上的 inode 结构。 当创建文件时，文件系统会为该文件分配一个 inode，并在目录中创建一个目录项，目录项保存文件名以及 inode 的编号。这就是文件名和 inode 之间的联系。 \u0026ldquo;没有目录项指向它\u0026rdquo; 的意思是，当一个文件的链接计数 nlink 降为零时，说明文件系统中没有任何目录项再指向这个 inode。换句话说，所有指向该文件的硬链接都已被删除，意味着该文件的文件名已经从文件系统中消失了。 文件描述符和目录项的关系： 打开文件时，进程会通过文件描述符访问该文件，文件描述符实际上是对 inode 的引用，而不是对目录项的直接引用。 当所有进程关闭文件描述符，并且文件的 nlink 为零时，说明没有任何文件名指向该 inode，文件的磁盘空间才会真正释放。 具体流程和时间点\n进程 A 调用 ilock()： 时间点：当进程 A 需要访问某个文件的 inode（比如读取文件或列出目录）。 结果：进程 A 获得了 inode 的独占锁。 进程 B 进入等待： 时间点：进程 B 也需要访问同一个 inode，但锁被进程 A 占用，因此 B 进入等待状态。 结果：进程 B 等待，无法立即获得 inode 锁。 进程 A 调用 iput()： 时间点：进程 A 完成操作后，调用 iput() 释放 inode 引用。 结果： 如果 nlink = 0 且 ip-\u0026gt;ref = 1，iput() 开始准备释放该 inode。 如果 iput() 发现有其他线程（进程 B）正在等待访问 inode，它会确保不会在有引用时释放 inode。 iput() 写入磁盘： 时间点：当 iput() 发现需要释放的 inode 在磁盘上有更新时，它会调用 iupdate() 写入磁盘，将 inode 的 type 设为 0。 结果：此时 inode 被标记为无效（type 为 0），并且没有目录项指向它。 系统崩溃时： 时间点：如果系统在进程 B 正在等待时崩溃，并且 inode 的 nlink = 0 但 ref \u0026gt; 0，文件可能被保留在磁盘上（没有目录项指向它，但文件还在）。 结果：此类文件会在系统重新启动时面临被回收或者手动删除的情况。 总结\n并发线程在访问 inode 时，锁定机制确保了不会出现 inode 被错误释放的情况。 截断文件是指释放文件的磁盘空间，这仅在文件的 nlink 和 ref 都为零时发生。 目录项是文件名和 inode 之间的映射关系，当所有目录项删除且文件描述符关闭时，inode 才会真正释放。 具体流程中，iput() 检查 inode 的状态，决定是否立即释放它，同时处理可能的竞争条件。 ip-\u0026gt;ref（引用计数） 作用：ip-\u0026gt;ref 是 内存中的引用计数，用于跟踪有多少 内核指针（C 指针） 指向该 inode。每当某个系统调用获取对 inode 的引用时（例如，打开文件、使用文件描述符），引用计数就会增加。每当一个引用不再需要时（例如，关闭文件），引用计数就会减少。\n示例：\n当进程打开一个文件时，ip-\u0026gt;ref 增加。 当进程关闭文件时，ip-\u0026gt;ref 减少。 作用场景：当 ip-\u0026gt;ref = 1 时，通常表示内核中只有一个指针还引用该 inode（通常是当前 iput() 调用所持有的引用）。当引用计数降为 0 时，表示内存中没有任何系统调用或进程再引用该 inode，它可以被释放。\nnlink（链接计数） 作用：nlink 是 磁盘上目录项的计数，用于跟踪有多少个 目录项（directory entry） 指向该 inode。每当文件被硬链接时，链接计数增加；每当删除一个硬链接时，链接计数减少。\n示例：\n创建文件时，nlink = 1，表示一个目录项（即文件名）指向该 inode。 创建一个硬链接（ln 命令）时，nlink 增加。删除文件或硬链接时，nlink 减少。 作用场景：当 nlink = 0 时，表示没有目录项指向该 inode，文件的名字已经从文件系统中删除。即使文件还在内存中被引用（例如，进程还在使用文件描述符），文件系统也不再通过目录访问到它。\n两者的区别与关系 ip-\u0026gt;ref 计数内存中的引用：它反映的是内核当前有多少指针指向这个 inode，通常与文件描述符、当前工作目录等相关。当 ip-\u0026gt;ref = 0 时，意味着没有进程再引用这个 inode，内核可以从内存中释放它。 nlink 计数磁盘上的链接：它反映的是磁盘上有多少文件名（目录项）指向这个 inode。nlink = 0 时，表示文件在文件系统中不再可见，即已经被从目录中移除。 为什么两者都需要统计？ 独立作用：这两者统计不同层次的引用。nlink 反映文件在文件系统中的可见性，决定何时删除文件的目录项；而 ip-\u0026gt;ref 则是内存中的引用，决定 inode 何时可以从内存中释放。 结合决定 inode 何时释放：即便 nlink = 0，也不能立刻释放 inode，因为文件可能仍然在被进程使用（ip-\u0026gt;ref \u0026gt; 1）。只有当 nlink = 0 且 ip-\u0026gt;ref = 1 时，iput() 可以安全地释放 inode——此时，文件已经从文件系统中删除，没有目录项指向它，并且没有进程再引用它。 流程示例 当删除一个文件时，nlink 会降到 0，文件从目录中消失。但如果一个进程已经打开了该文件并正在使用它，ip-\u0026gt;ref 会大于 1。 直到最后一个进程关闭文件描述符，ip-\u0026gt;ref 也降为 1，iput() 最终会释放 inode。这时，文件的所有数据会被清除，inode 和磁盘块被回收。 代码：Inode包含内容 磁盘上的inode结构体struct dinode包含一个size和一个块号数组（见图8.3）。\ninode数据可以在dinode的addrs数组列出的块中找到。\n前面的NDIRECT个数据块被列在数组中的前NDIRECT个元素中；这些块称为直接块（direct blocks）。\n接下来的NINDIRECT个数据块不在inode中列出，而是在称为间接块（indirect block）的数据块中列出。\naddrs数组中的最后一个元素给出了间接块的地址。\n因此，可以从inode中列出的块加载文件的前12 kB（NDIRECT x BSIZE）字节，而只有在查阅间接块后才能加载下一个256 kB（NINDIRECT x BSIZE）字节。\n这是一个很好的磁盘表示，但对于客户端来说较复杂。\n函数bmap管理这种表示，以便实现我们将很快看到的如readi和writei这样的更高级例程。\nbmap(struct inode *ip, uint bn)返回索引结点ip的第bn个数据块的磁盘块号。\n如果ip还没有这样的块，bmap会分配一个。\n函数bmap（kernel/fs.c:378）从简单的情况开始：前面的NDIRECT个块在inode本身中列出（kernel/fs.c:383-387）中。\n下面NINDIRECT个块在ip-\u0026gt;addrs[NDIRECT]的间接块中列出。\nBmap读取间接块（kernel/fs.c:394），然后从块内的正确位置（kernel/fs.c:395）读取块号。\n如果块号超过NDIRECT+NINDIRECT，则bmap调用panic崩溃；writei包含防止这种情况发生的检查（kernel/fs.c:490）。\nBmap根据需要分配块。ip-\u0026gt;addrs[]或间接块中条目为零表示未分配块。当bmap遇到零时，它会用按需分配的新块（kernel/fs.c:384-385）（kernel/fs.c:392-393）替换它们。\nitrunc释放文件的块，将inode的size重置为零。Itrunc（kernel/fs.c:410）首先释放直接块（kernel/fs.c:416-421），然后释放间接块中列出的块（kernel/fs.c:426-429），最后释放间接块本身（kernel/fs.c:431-432）。\nBmap使readi和writei很容易获取inode的数据。\nReadi（kernel/fs.c:456）首先确保偏移量和计数不超过文件的末尾。\n开始于超过文件末尾的地方读取将返回错误（kernel/fs.c:461-462），而从文件末尾开始或穿过文件末尾的读取返回的字节数少于请求的字节数（kernel/fs.c:463-464）。\n主循环处理文件的每个块，将数据从缓冲区复制到dst（kernel/fs.c:466-474）。\nwritei（kernel/fs.c:483）与readi相同，但有三个例外：从文件末尾开始或穿过文件末尾的写操作会使文件增长到最大文件大小（kernel/fs.c:490-491）；循环将数据复制到缓冲区而不是输出（kernel/fs.c:36）；如果写入扩展了文件，writei必须更新其大小（kernel/fs.c:504-511）。\nreadi和writei都是从检查ip-\u0026gt;type == T_DEV开始的。这种情况处理的是数据不在文件系统中的特殊设备；我们将在文件描述符层返回到这种情况。\n函数stati（kernel/fs.c:442）将inode元数据复制到stat结构体中，该结构通过stat系统调用向用户程序公开。\n代码：目录层 目录的内部实现很像文件。\n其inode的type为T_DIR，其数据是一系列目录条目（directory entries）。\n每个条目（entry）都是一个struct dirent（kernel/fs.h:56），其中包含一个名称name和一个inode编号inum。\n名称最多为DIRSIZ（14）个字符；如果较短，则以NUL（0）字节终止。\ninode编号为零的条目是空的。\n函数dirlookup（kernel/fs.c:527）在目录中搜索具有给定名称的条目。\n如果找到一个，它将返回一个指向相应inode的指针，解开锁定，并将*poff设置为目录中条目的字节偏移量，以满足调用方希望对其进行编辑的情形。\n如果dirlookup找到具有正确名称的条目，它将更新*poff并返回通过iget获得的未锁定的inode。\nDirlookup是iget返回未锁定indoe的原因。\n调用者已锁定dp，因此，如果对.，当前目录的别名，进行查找，则在返回之前尝试锁定indoe将导致重新锁定dp并产生死锁(还有更复杂的死锁场景，涉及多个进程和..，父目录的别名。.不是唯一的问题。）调用者可以解锁dp，然后锁定ip，确保它一次只持有一个锁。\n函数dirlink（kernel/fs.c:554）将给定名称和inode编号的新目录条目写入目录dp。\n如果名称已经存在，dirlink将返回一个错误（kernel/fs.c:560-564）。\n主循环读取目录条目，查找未分配的条目。\n当找到一个时，它会提前停止循环（kernel/fs.c:538-539），并将off设置为可用条目的偏移量。\n否则，循环结束时会将off设置为dp-\u0026gt;size。\n无论哪种方式，dirlink都会通过在偏移off处写入（kernel/fs.c:574-577）来向目录添加一个新条目。\n目录的结构和作用 目录（Directory）是由一系列目录条目（directory entries）组成的，目录本身就是一个类型为T_DIR的 inode。目录条目是struct dirent类型的结构，每个条目包含：\nname：文件名或目录名，最多可有 14 个字符（DIRSIZ），如果文件名较短，则以空字符（NUL）结束。 inum：该目录条目对应的 inode 编号。如果 inum 为 0，表示这个条目是空的，未使用。 函数概览及其作用\ndirlookup（kernel/fs.c:527）：\n作用：在目录中查找给定名称的条目，返回指向对应 inode 的指针。\n位置：文件系统层次，用于搜索目录内是否有指定名称的文件或子目录。\n功能：\n1. **查找目录条目**：扫描目录中的条目，通过匹配文件名找到相应的 inode。 返回 inode：如果找到匹配的条目，返回未锁定的 inode，并设置该条目的字节偏移量（*poff）。这个偏移量表示条目在目录数据中的位置，方便后续修改。 避免死锁：由于 dirlookup 返回的是未锁定的 inode，调用者在处理 inode 时可以解锁目录，避免像对 \u0026ldquo;.\u0026quot;（当前目录）和 \u0026ldquo;..\u0026quot;（父目录）这样的条目产生的死锁问题。 dirlink（kernel/fs.c:554）：\n作用：向目录中写入一个新条目，将给定的名称和 inode 编号关联起来。\n位置：文件系统层次，用于将文件或子目录添加到目录中。\n功能：\n检查名称是否存在：在目录中查找是否已有同名条目。如果存在，返回错误。 寻找空条目：如果条目不存在，它扫描目录，寻找空闲的目录条目（inum 为 0），如果找到，记录下这个条目的偏移量。 写入新条目：将新的目录条目写入目录，将给定的名称和 inode 编号保存到偏移位置。 偏移量的含义\n偏移量 是目录条目在 目录文件数据 中的位置。目录本质上是一个特殊的文件，其中包含每个条目的 inode 编号和对应的文件名。偏移量是目录条目在这个文件中的字节位置，用于定位、修改或删除目录条目。 在 dirlookup 中，偏移量（*poff）被设置为找到的条目的位置，便于后续修改该条目。 在 dirlink 中，偏移量指向空闲条目的位置或者目录末尾，用于添加新条目。 目录和文件之间的定位过程 目录与文件的定位：文件系统通过 目录项（directory entry） 将文件名与 inode 关联。每个目录条目保存文件名和 inode 编号。当我们访问一个文件时，系统先从目录中找到与文件名对应的 inode 编号，然后通过这个编号定位到磁盘上对应的 inode 结构。 查找过程（dirlookup）：\n目录本身是一个 inode，它保存了该目录下所有文件的目录条目。\ndirlookup 函数扫描目录条目，查找与给定文件名匹配的条目，并返回对应的 inode。\n一旦找到对应的 inode 编号，文件系统就可以通过这个 inode 编号进一步读取文件的元数据（如文件大小、数据块位置等）。\n添加过程（dirlink）：\n在添加新文件或目录时，dirlink 首先查找目录中的空条目位置。\n然后，它将文件名和 inode 编号写入该条目，关联文件名与 inode。\n总结 目录是由一系列目录条目组成的文件结构，每个条目记录了文件名和 inode 编号的对应关系。 关键函数： dirlookup：用于在目录中查找文件名，返回对应 inode 并提供目录条目的偏移量。 dirlink：用于向目录中添加新条目，将文件名和 inode 编号关联。 偏移量用于定位目录条目在目录文件中的位置，便于读取或修改。 目录项与 inode 的关系是：目录项通过 inode 编号定位文件，inode 保存了文件的元数据及存储信息。 代码：路径名 路径名查找涉及一系列对dirlookup的调用，每个路径组件调用一个。\nNamei（kernel/fs.c:661）计算path并返回相应的inode。\n函数nameiparent是一个变体：它在最后一个元素之前停止，返回父目录的inode并将最后一个元素复制到name中。\n两者都调用通用函数namex来完成实际工作。\n路径名查找和基本流程\n路径名查找 涉及对 dirlookup 的多次调用，每个路径组件对应一次查找。例如，查找路径 /a/b/c 涉及查找 /a、b、c。 namei（kernel/fs.c:661）：负责计算路径名并返回对应的 inode。 nameiparent：是 namei 的变体，它在查找路径时返回最后一个路径元素的 父目录 的 inode，并将最后一个路径元素复制到 name。 两个函数都调用了通用函数 namex 来完成实际的路径查找工作。 Namex（kernel/fs.c:626）首先决定路径计算的开始位置。\n如果路径以斜线开始，则计算从根目录开始；否则，从当前目录开始（kernel/fs.c:630-633）。\n然后，它使用skipelem依次考察路径的每个元素（kernel/fs.c:635）。\n循环的每次迭代都必须在当前索引结点ip中查找name。\n迭代首先给ip上锁并检查它是否是一个目录。\n如果不是，则查找失败（kernel/fs.c:636-640）(锁定ip是必要的，不是因为ip-\u0026gt;type可以被更改，而是因为在ilock运行之前，ip-\u0026gt;type不能保证已从磁盘加载。）\n如果调用是nameiparent，并且这是最后一个路径元素，则根据nameiparent的定义，循环会提前停止；最后一个路径元素已经复制到name中，因此namex只需返回解锁的ip（kernel/fs.c:641-645）。\n最后，循环将使用dirlookup查找路径元素，并通过设置ip = next（kernel/fs.c:646-651）为下一次迭代做准备。当循环用完路径元素时，它返回ip。\nnamex 函数的查找逻辑\nnamex（kernel/fs.c:626）首先决定从哪里开始查找路径： 如果路径以斜杠 / 开头，从根目录开始。 如果路径不是以斜杠开头，从当前目录开始（kernel/fs.c:630-633）。 然后通过 skipelem 分解路径的每个组件（例如，a、b、c）。 每次迭代，namex 锁定当前的 inode（ip）并检查它是否是一个目录（kernel/fs.c:636-640）。如果不是目录，查找失败。 处理最后一个路径元素\n如果调用的是 nameiparent，且查找到了路径的最后一个元素，那么 namex 会提前停止，并将最后一个路径元素的名称复制到 name，返回 父目录 的 inode（kernel/fs.c:641-645）。 否则，namex 会使用 dirlookup 查找当前目录中的下一个路径元素，并为下一次迭代设置新的 inode（kernel/fs.c:646-651）。 namex过程可能需要很长时间才能完成：它可能涉及多个磁盘操作来读取路径名中所遍历目录的索引节点和目录块（如果它们不在buffer cache中）。\nXv6经过精心设计，如果一个内核线程对namex的调用在磁盘I/O上阻塞，另一个查找不同路径名的内核线程可以同时进行。\nNamex分别锁定路径中的每个目录，以便在不同目录中进行并行查找。\n路径名查找中的 I/O 操作和并发\nnamex 可能需要多个磁盘操作来读取遍历路径中的目录和 inode，特别是在相关目录和 inode 不在 buffer cache 中时。这可能导致磁盘 I/O 延迟。 xv6 设计允许多个内核线程并发查找不同的路径名。每个目录在查找时都被分别加锁，使得不同线程可以同时访问不同的目录，避免彼此阻塞。 这种并发性带来了一些挑战。\n例如，当一个内核线程正在查找路径名时，另一个内核线程可能正在通过取消目录链接来更改目录树。\n一个潜在的风险是，查找可能正在搜索已被另一个内核线程删除且其块已被重新用于另一个目录或文件的目录。\nXv6避免了这种竞争。\n例如，在namex中执行dirlookup时，lookup线程持有目录上的锁，dirlookup返回使用iget获得的inode。\nIget增加索引节点的引用计数。\n只有在从dirlookup接收inode之后，namex才会释放目录上的锁。\n现在，另一个线程可以从目录中取消inode的链接，但是xv6还不会删除inode，因为inode的引用计数仍然大于零。\n并发风险和解决方案\n并发风险：当一个线程在执行路径查找时，另一个线程可能在删除目录或更改目录树。查找可能正在访问已被删除的目录条目，其块可能被重新分配给其他文件或目录。 解决方案：在 namex 中，路径查找线程在执行 dirlookup 时会锁定目录。dirlookup 返回一个通过 iget 获取的 inode，并增加其引用计数。只有在获取到 inode 后，才会释放目录的锁。这样，即使另一个线程删除该 inode，它也不会立即被回收，因为引用计数仍然大于零。 另一个风险是死锁。\n例如，查找“.”时，next指向与ip相同的inode。在释放ip上的锁之前锁定next将导致死锁。\n在释放ip上的锁之前锁定next将导致死锁。\n为了避免这种死锁，namex在获得下一个目录的锁之前解锁该目录。\n这里我们再次看到为什么iget和ilock之间的分离很重要。\n死锁问题和解决方案\n死锁风险：如果在查找路径名如 \u0026quot;.\u0026quot; 时，next inode 和 ip 是同一个 inode，在释放 ip 的锁之前尝试锁定 next 会导致死锁。 解决方案：为了避免这种死锁，namex 在锁定下一个目录（next）之前，先解锁当前的 inode（ip）。这也是为什么 xv6 将 iget 和 ilock 分离的原因 关键函数和变量\nnamei：计算路径并返回对应的 inode。 nameiparent：返回父目录的 inode 并将最后的路径元素复制到 name。 namex：通用路径查找逻辑，处理路径分解、目录锁定与释放。 skipelem：解析路径名中的下一个元素。 dirlookup：查找目录中的指定条目，返回对应 inode。 iget：通过 inode 编号获取对应的 inode 并增加引用计数。 ilock：锁定 inode，确保对其独占访问。 文件描述符层 Unix界面的一个很酷的方面是，Unix中的大多数资源都表示为文件，包括控制台、管道等设备，当然还有真实文件。文件描述符层是实现这种一致性的层。\n正如我们在第1章中看到的，Xv6为每个进程提供了自己的打开文件表或文件描述符。\n每个打开的文件都由一个struct file（kernel/file.h:1）表示，它是inode或管道的封装，加上一个I/O偏移量。\n每次调用open都会创建一个新的打开文件（一个新的struct file）：如果多个进程独立地打开同一个文件，那么不同的实例将具有不同的I/O偏移量。\n另一方面，单个打开的文件（同一个struct file）可以多次出现在一个进程的文件表中，也可以出现在多个进程的文件表中。\n如果一个进程使用open打开文件，然后使用dup创建别名，或使用fork与子进程共享，就会发生这种情况。\n引用计数跟踪对特定打开文件的引用数。\n可以打开文件进行读取或写入，也可以同时进行读取和写入。\nreadable和writable字段可跟踪此操作。\n系统中所有打开的文件都保存在全局文件表ftable中。\n文件表具有分配文件（filealloc）、创建重复引用（filedup）、释放引用（fileclose）以及读取和写入数据（fileread和filewrite）的函数。\n前三个函数遵循现在熟悉的形式。\nFilealloc（kernel/file.c:30）扫描文件表以查找未引用的文件（f-\u0026gt;ref == 0），并返回一个新的引用；filedup（kernel/file.c:48）增加引用计数；fileclose（kernel/file.c:60）将其递减。\n当文件的引用计数达到零时，fileclose会根据type释放底层管道或inode。\n函数filestat、fileread和filewrite实现对文件的stat、read和write操作。\nFilestat（kernel/file.c:88）只允许在inode上操作并且调用了stati。\nFileread和filewrite检查打开模式是否允许该操作，然后将调用传递给管道或inode的实现。如果文件表示inode，fileread和filewrite使用I/O偏移量作为操作的偏移量，然后将文件指针前进该偏移量（kernel/file.c:122-123）（kernel/file.c:153-154）。\n管道没有偏移的概念。\n回想一下，inode的函数要求调用方处理锁（kernel/file.c:94-96）（kernel/file.c:121-124）（kernel/file.c:163-166）。inode锁定有一个方便的副作用，即读取和写入偏移量以原子方式更新，因此，对同一文件的同时多次写入不能覆盖彼此的数据，尽管他们的写入最终可能是交错的。\n在 Unix 系统中，有一个很有趣的特点：几乎所有的资源都可以表示为文件。不只是硬盘上的文件，像控制台、管道（即进程间通信的通道）这些设备也被视为“文件”。为了实现这种一致性，Unix 使用了一个叫做“文件描述符层”的机制。\n在 Unix 系统中，每个进程都有自己独立的“打开文件表”，也就是我们常说的“文件描述符”。这个表格包含了每个进程当前打开的所有文件。每个打开的文件由一个 struct file 结构体表示（这个结构体定义在 kernel/file.h:1），它封装了文件的元信息（如文件的 inode，或者管道）和文件的读写位置（即 I/O 偏移量）。\n当你调用 open 打开一个文件时，操作系统会为这个文件创建一个新的 struct file 实例。所以，如果两个进程独立打开同一个文件，它们会得到不同的 struct file 实例，这样每个进程可以有各自独立的 I/O 偏移量。另一方面，如果一个进程通过 dup 复制文件描述符，或者通过 fork 创建子进程与子进程共享文件，那么它们可能会指向同一个 struct file 实例。\n每个 struct file 实例都有一个引用计数器，用于追踪有多少个文件描述符指向它。文件可以以只读、只写或读写的方式打开，struct file 中的 readable 和 writable 字段分别记录文件的读写权限。\n系统中所有进程的所有打开文件都被存储在一个全局文件表 ftable 中。这个表有几个关键的操作函数：\nfilealloc：分配一个新的文件对象。 filedup：为一个已经打开的文件创建一个重复的引用。 fileclose：释放对文件的引用。 fileread 和 filewrite：读写文件数据。 前三个函数 (filealloc、filedup、fileclose) 实现起来相对简单：\nfilealloc（定义在 kernel/file.c:30）通过扫描全局文件表，找到一个未被引用的文件对象（f-\u0026gt;ref == 0）并返回一个新的引用。 filedup（定义在 kernel/file.c:48）增加该文件的引用计数。 fileclose（定义在 kernel/file.c:60）减少引用计数。如果计数变成零，表示没有进程再使用这个文件，fileclose 会释放文件的底层资源（比如管道或 inode）。 文件操作函数 filestat、fileread 和 filewrite：\nfilestat（定义在 kernel/file.c:88）用于获取文件的状态信息，但只能对 inode 类型的文件执行此操作，并会调用 stati 函数获取信息。 fileread 和 filewrite 函数用于读取和写入文件数据。它们首先会检查文件的打开模式，确保允许进行所需的操作，然后将具体的操作传递给管道或 inode 的实现层。 如果文件是一个 inode，fileread 和 filewrite 会根据文件的 I/O 偏移量进行读写操作，之后会将该偏移量向前推进（定义在 kernel/file.c:122-123 和 kernel/file.c:153-154）。 如果文件是一个管道，管道并不支持偏移量，所以操作逻辑会有所不同。 同时，所有 inode 操作都要求调用方自己负责管理锁定（在 kernel/file.c:94-96、kernel/file.c:121-124 和 kernel/file.c:163-166 中实现）。这种锁定有一个额外的好处：它确保了对同一个文件的并发写入不会相互覆盖，虽然数据写入的顺序可能会交错。\n代码：系统调用 通过使用底层提供的函数，大多数系统调用的实现都很简单（请参阅kernel/sysfile.c）。\n有几个调用值得仔细看看。\n函数sys_link和sys_unlink编辑目录，创建或删除索引节点的引用。\n它们是使用事务能力的另一个很好的例子。\nsys_link（kernel/sysfile.c:120）从获取其参数开始，两个字符串分别是old和new（kernel/sysfile.c:125）。\n假设old存在并且不是一个目录（kernel/sysfile.c:129-132），sys_link会增加其ip-\u0026gt;nlink计数。\n然后sys_link调用nameiparent来查找new（kernel/sysfile.c:145）的父目录和最终路径元素，并创建一个指向old的inode（kernel/sysfile.c:148）的新目录条目。\nnew的父目录必须存在并且与现有inode位于同一设备上：inode编号在一个磁盘上只有唯一的含义。\n如果出现这样的错误，sys_link必须返回并减少ip-\u0026gt;nlink。\n在 xv6 文件系统中，ip 和 dp 是常用的变量名，通常分别代表 inode 指针 和 目录 inode 指针。\n事务简化了实现，因为它需要更新多个磁盘块，但我们不必担心更新的顺序。\n他们要么全部成功，要么什么都不做。\n例如在没有事务的情况下，在创建一个链接之前更新ip-\u0026gt;nlink会使文件系统暂时处于不安全状态，而在这两者之间发生的崩溃可能会造成严重破坏。\n对于事务，我们不必担心这一点\nSys_link为现有inode创建一个新名称。\n函数create（kernel/sysfile.c:242）为新inode创建一个新名称。\n它是三个文件创建系统调用的泛化：带有O_CREATE标志的open生成一个新的普通文件，mkdir生成一个新目录，mkdev生成一个新的设备文件。\n与sys_link一样，create从调用nameiparent开始，以获取父目录的inode。\n然后调用dirlookup检查名称是否已经存在（kernel/sysfile.c:252）。\n如果名称确实存在，create的行为取决于它用于哪个系统调用：open的语义与mkdir和mkdev不同。\n如果create是代表open（type == T_FILE）使用的，并且存在的名称本身是一个常规文件，那么open会将其视为成功，create也会这样做（kernel/sysfile.c:256）。否则，这是一个错误（kernel/sysfile.c:257-258）。\n如果名称不存在，create现在将使用ialloc（kernel/sysfile.c:261）分配一个新的inode。如果新inode是目录，create将使用.和..条目对它进行初始化。\n最后，既然数据已正确初始化，create可以将其链接到父目录（kernel/sysfile.c:274）。\nCreate与sys_link一样，同时持有两个inode锁：ip和dp。不存在死锁的可能性，因为索引结点ip是新分配的：系统中没有其他进程会持有ip的锁，然后尝试锁定dp。\n使用create，很容易实现sys_open、sys_mkdir和sys_mknod。\nSys_open（kernel/sysfile.c:287）是最复杂的，因为创建一个新文件只是它能做的一小部分。如果open被传递了O_CREATE标志，它将调用create（kernel/sysfile.c:301）。\n否则，它将调用namei（kernel/sysfile.c:307）。Create返回一个锁定的inode，但namei不锁定，因此sys_open必须锁定inode本身。\n这提供了一个方便的地方来检查目录是否仅为读取打开，而不是写入。\n假设inode是以某种方式获得的，sys_open分配一个文件和一个文件描述符（kernel/sysfile.c:325），然后填充该文件（kernel/sysfile.c:337-342）。\n请注意，没有其他进程可以访问部分初始化的文件，因为它仅位于当前进程的表中。\n在我们还没有文件系统之前，第7章就研究了管道的实现。\n函数sys_pipe通过提供创建管道对的方法将该实现连接到文件系统。\n它的参数是一个指向两个整数的指针，它将在其中记录两个新的文件描述符。然后分配管道并安装文件描述符。\n在 Unix 操作系统中，系统调用是用户程序与内核交互的主要途径。虽然系统调用的实现依赖于底层的操作函数，但大多数系统调用的代码相对简单（可以在 kernel/sysfile.c 中找到）。接下来，让我们重点看看几个值得关注的调用。\nsys_link 和 sys_unlink这两个系统调用用于操作目录，分别创建或删除对文件的引用（即链接）。这两个函数展示了事务功能的另一个好处——它们可以确保多个磁盘操作要么全部成功，要么全部失败。\nsys_link：它的功能是为一个已有文件创建一个新的名字（也就是硬链接）。它首先获取两个参数，分别是旧路径old和新路径new\n。假设old文件存在且不是一个目录，sys_link会增加这个文件的链接计数（ip-\u0026gt;nlink），表示这个文件有了更多的引用。\n然后它会调用 nameiparent 函数，找到 new 的父目录，并在该目录下为 old 文件创建一个新条目。如果新条目的父目录不存在，或者新条目和旧条目不在同一个磁盘上，sys_link 就会返回错误并撤销刚刚增加的引用计数。 事务使得这种操作更为安全，因为所有的磁盘块更新要么一起成功，要么一起失败。如果没有事务，某个步骤（如增加链接计数）出错，可能会导致文件系统不一致。\ncreate 函数用于创建一个新文件，它是三个系统调用的泛化：open（带 O_CREATE 标志时创建一个普通文件）、mkdir（创建目录）和 mkdev（创建设备文件）。\n与sys_link类似，create从获取父目录开始。首先调用nameiparent获取父目录的 inode，并使用dirlookup检查要创建的名称是否已经存在。 如果名称存在，create 的行为取决于调用者是 open、mkdir 还是 mkdev。如果调用者是 open 且要创建的对象是一个普通文件，那么 create 会将其视为成功。否则，这是一个错误。 如果名称不存在，create 会分配一个新的 inode，并根据不同的文件类型（如目录）初始化它。之后，它会将新创建的文件链接到父目录。 create 和 sys_link 都会同时持有两个 inode 的锁：新分配的文件（ip）和父目录（dp）。由于 ip 是新分配的，其他进程不可能持有 ip 的锁，这就避免了死锁的可能性。\nsys_open、sys_mkdir 和 sys_mknod：通过 create 函数，很容易实现 sys_open（打开文件）、sys_mkdir（创建目录）和 sys_mknod（创建设备文件）。其中，sys_open 是最复杂的，因为它不仅仅负责创建新文件。\n如果调用时传递了O_CREATE标志，sys_open就会调用create来创建一个新文件。否则，它会调用namei获取文件的 inode（namei不会锁定 inode，因此sys_open需要自己加锁）。 在获得 inode 后，sys_open 会为该文件分配一个文件结构和文件描述符，并将其添加到进程的文件表中。 注意，此时文件只存在于当前进程的文件表中，其他进程无法访问这个“部分初始化”的文件，因此不存在竞争问题。\nsys_pipe：在第7章中我们已经探讨了管道的实现。sys_pipe 函数将管道系统连接到文件系统中。它的作用是创建一对管道：一个用于读取，一个用于写入。调用时，传递的参数是一个指向两个整数的指针，这两个整数用于记录管道的两个文件描述符。随后，sys_pipe 会分配管道并在文件描述符表中为其安装对应的读写端。\n真实世界 实际操作系统中的buffer cache比xv6复杂得多，但它有两个相同的用途：缓存和同步对磁盘的访问。\n与UNIX V6一样，Xv6的buffer cache使用简单的最近最少使用（LRU）替换策略；有许多更复杂的策略可以实现，每种策略都适用于某些工作场景，而不适用于其他工作场景。\n更高效的LRU缓存将消除链表，而改为使用哈希表进行查找，并使用堆进行LRU替换。\n现代buffer cache通常与虚拟内存系统集成，以支持内存映射文件。\nXv6的日志系统效率低下。\n提交不能与文件系统调用同时发生。\n系统记录整个块，即使一个块中只有几个字节被更改。\n它执行同步日志写入，每次写入一个块，每个块可能需要整个磁盘旋转时间。\n真正的日志系统解决了所有这些问题。\n日志记录不是提供崩溃恢复的唯一方法。\n早期的文件系统在重新启动期间使用了一个清道夫程序（例如，UNIX的fsck程序）来检查每个文件和目录以及块和索引节点空闲列表，查找并解决不一致的问题。\n清理大型文件系统可能需要数小时的时间，而且在某些情况下，无法以导致原始系统调用原子化的方式解决不一致问题。\n从日志中恢复要快得多，并且在崩溃时会导致系统调用原子化。\nXv6使用的索引节点和目录的基础磁盘布局与早期UNIX相同；这一方案多年来经久不衰。\nBSD的UFS/FFS和Linux的ext2/ext3使用基本相同的数据结构。\n文件系统布局中最低效的部分是目录，它要求在每次查找期间对所有磁盘块进行线性扫描。\n当目录只有几个磁盘块时，这是合理的，但对于包含许多文件的目录来说，开销巨大。\nMicrosoft Windows的NTFS、Mac OS X的HFS和Solaris的ZFS（仅举几例）将目录实现为磁盘上块的平衡树。\n这很复杂，但可以保证目录查找在对数时间内完成（即时间复杂度为O(logn)）。\nXv6对于磁盘故障的解决很初级：如果磁盘操作失败，Xv6就会调用panic。\n这是否合理取决于硬件：如果操作系统位于使用冗余屏蔽磁盘故障的特殊硬件之上，那么操作系统可能很少看到故障，因此panic是可以的。\n另一方面，使用普通磁盘的操作系统应该预料到会出现故障，并能更优雅地处理它们，这样一个文件中的块丢失不会影响文件系统其余部分的使用。\nXv6要求文件系统安装在单个磁盘设备上，且大小不变。\n随着大型数据库和多媒体文件对存储的要求越来越高，操作系统正在开发各种方法来消除“每个文件系统一个磁盘”的瓶颈。\n基本方法是将多个物理磁盘组合成一个逻辑磁盘。\nRAID等硬件解决方案仍然是最流行的，但当前的趋势是在软件中尽可能多地实现这种逻辑。\n这些软件实现通常允许通过动态添加或删除磁盘来扩展或缩小逻辑设备等丰富功能。\n当然，一个能够动态增长或收缩的存储层需要一个能够做到这一点的文件系统：xv6使用的固定大小的inode块阵列在这样的环境中无法正常工作。\n将磁盘管理与文件系统分离可能是最干净的设计，但两者之间复杂的接口导致了一些系统（如Sun的ZFS）将它们结合起来。\nXv6的文件系统缺少现代文件系统的许多其他功能；例如，它缺乏对快照和增量备份的支持。\n现代Unix系统允许使用与磁盘存储相同的系统调用访问多种资源：命名管道、网络连接、远程访问的网络文件系统以及监视和控制接口，如/proc（注：Linux 内核提供了一种通过/proc文件系统，在运行时访问内核内部数据结构、改变内核设置的机制。proc文件系统是一个伪文件系统，它只存在内存当中，而不占用外存空间。它以文件系统的方式为访问系统内核数据的操作提供接口。）。\n不同于xv6中fileread和filewrite的if语句，这些系统通常为每个打开的文件提供一个函数指针表，每个操作一个，并通过函数指针来援引inode的调用实现。\n网络文件系统和用户级文件系统提供了将这些调用转换为网络RPC并在返回之前等待响应的函数。\n在实际的操作系统中，缓冲区缓存要比 xv6 中复杂得多，但它们有相同的两个核心功能：缓存磁盘访问 和 管理对磁盘的同步访问。Xv6 使用了一种非常简单的缓存替换策略，即“最近最少使用（LRU）”，这意味着最久未使用的缓存项将被优先替换。当然，实际系统中有很多更复杂的替换策略，每种策略在不同的应用场景中表现更好。更高效的 LRU 实现通常会使用哈希表来快速查找数据，并通过堆来管理 LRU 替换顺序。此外，现代的缓冲区缓存通常与虚拟内存系统集成，这样就可以支持像内存映射文件这样的高级功能。\nXv6 的日志系统效率很低，主要因为在提交（写入日志）时无法同时进行文件系统操作，而且系统会记录整个磁盘块，即使只修改了几个字节。这些写入操作是同步的，即每次只能写一个块，导致操作非常慢。如果磁盘需要旋转一次才能完成一个块的写入，那么这次旋转可能就是瓶颈。实际的日志系统会解决这些问题，让写入更高效。\n日志记录并不是恢复文件系统的一种唯一方法。早期的文件系统使用了清理程序（如 Unix 中的 fsck 程序）在重启时检查文件和目录的完整性，并修复不一致的问题。这种方法很慢，尤其是在文件系统很大的情况下，可能需要花费数小时。相比之下，使用日志系统的恢复速度要快得多，而且它能确保系统调用是原子性的——这意味着操作要么完全成功，要么什么都不会改变。\nXv6 中使用的 inode（索引节点）和目录结构 是从早期 Unix 继承下来的，像 BSD 的 UFS/FFS 和 Linux 的 ext2/ext3 文件系统也使用了类似的结构。文件系统中的一个低效部分是目录查找，它在查找文件时需要对目录的所有磁盘块进行线性扫描，这对于包含大量文件的目录来说是一个巨大的性能开销。现代文件系统（如 Windows 的 NTFS、Mac OS X 的 HFS、Solaris 的 ZFS）通过在磁盘上使用平衡树结构来加速目录查找，使得查找时间复杂度变成 O(logn)，大大提升了性能。\nXv6 对于磁盘故障的处理非常基础：一旦磁盘操作失败，系统就会调用 panic 进行崩溃。这种处理方式是否合理取决于硬件情况：如果系统使用的硬件有冗余机制来屏蔽磁盘故障（如 RAID），那么 panic 处理是可以接受的；但如果操作系统运行在普通硬件上，系统应该能够优雅地处理这些故障，防止单个文件损坏影响整个文件系统。\nXv6 要求文件系统安装在单一的磁盘设备上，且文件系统大小是固定的。随着数据量的不断增长，这种设计已经无法满足现代应用的需求。如今，操作系统通过多种方法来打破“每个文件系统只能对应一个磁盘”的限制。RAID 是一种常见的硬件解决方案，它通过将多个物理磁盘组合成一个逻辑磁盘。但现代趋势是更多地依靠软件来实现这些功能，并支持动态添加或删除磁盘。\nXv6 文件系统缺乏许多现代文件系统的高级功能。例如，它没有 快照（snapshot）和增量备份 的功能。快照可以在某一时刻冻结文件系统状态，从而支持快速备份，而增量备份只备份变化的数据，大大节省存储空间。\n在现代的 Unix 系统中，很多不同类型的资源（如命名管道、网络连接、远程文件系统等）都可以通过与磁盘文件相同的系统调用（如 read 和 write）进行访问。这比 xv6 中使用简单的 if 语句来判断文件类型更灵活。在这些系统中，每个打开的文件都会有一个函数指针表，不同的操作会指向不同的实现函数。例如，网络文件系统 可能会将文件读写操作转化为网络请求并等待响应。\n正课 文件系统引入 今天介绍的是文件系统。实际上我们会花三节课的时间来学习文件系统。\n前两节课基于XV6来做介绍，第三节课基于Linux来做介绍。\n实际上，这将是有关XV6的最后一个话题，在这周之后我们就讲完了XV6。\n文件系统是操作系统中除了shell以外最常见的用户接口。\n我们希望通过这几节课来理解：文件系统的背后究竟是什么原理，文件系统是如何实现的。\nXV6的文件系统与你正在使用的文件系统有什么区别。\n学生回答：其中一点是，XV6支持的文件大小远小于其他文件系统。其次一些文件的文件名也较短。第三点，我不认为XV6的文件系统有copy-on-write。\nFrans教授：很好，那有什么相似的地方吗？\n学生回答：基本的结构是类似的，比如说都有文件名，都有inode，目录等等。\nFans教授：很好，我再问一个同学，XV6的文件系统与你正在使用的文件系统有什么异同？\n学生回答：文件目录结构都是层级的。\n接下来让我列出一些文件系统突出的特性：\n其中一点刚刚有同学提到了，就是对于用户友好的文件名，具体来说就是层级的路径名，这可以帮助用户组织目录中的文件。 通过将文件命名成方便易记的名字，可以在用户之间和进程之间更简单的共享文件。 相比我们已经看过的XV6其他子系统，这一点或许是最重要的，文件系统提供了持久化。这意味着，我可以关闭一个计算机，过几天再开机而文件仍然在那，我可以继续基于文件工作。这一点与进程和其他资源不一样，这些资源在计算机重启时就会消失，之后你需要重新启动它们，但是文件系统就可以提供持久化。 出于以下原因，文件系统背后的机制还比较有意思：\n文件系统对硬件的抽象较为有用，所以理解文件系统对于硬件的抽象是如何实现的还是有点意思的。 除此之外，还有个关键且有趣的地方就是crash safety。有可能在文件系统的操作过程中，计算机崩溃了，在重启之后你的文件系统仍然能保持完好，文件系统的数据仍然存在，并且你可以继续使用你的大部分文件。如果文件系统操作过程中计算机崩溃了，然后你重启之后文件系统不存在了或者磁盘上的数据变了，那么崩溃的将会是你。所以crash safety是一个非常重要且经常出现的话题，我们下节课会专门介绍它。 如何在磁盘上排布文件系统。例如目录和文件，它们都需要以某种形式在磁盘上存在，这样当你重启计算机时，所有的数据都能恢复。所以在磁盘上有一些数据结构表示了文件系统的结构和内容。在XV6中，使用的数据结构非常简单，因为XV6是专门为教学目的创建的。真实的文件系统通常会更加复杂。但是它们都是磁盘上保存的数据结构，我们在今天的课程会重点看这部分。 最后一个有趣的话题是性能。文件系统所在的硬件设备通常都较慢，比如说向一个SSD磁盘写数据将会是毫秒级别的操作，而在一个毫秒内，计算机可以做大量的工作，所以尽量避免写磁盘很重要，我们将在几个地方看到提升性能的代码。比如说，所有的文件系统都有buffer cache或者叫block cache。同时这里会有更多的并发，比如说你正在查找文件路径名，这是一个多次交互的操作，首先要找到文件结构，然后查找一个目录的文件名，之后再去查找下一个目录等等。你会期望当一个进程在做路径名查找时，另一个进程可以并行的运行。这样的并行运行在文件系统中将会是一个大的话题。 除此之外，你会对文件系统感兴趣是因为这是接下来两个lab的内容。下一个lab完全关注在文件系统，下下个lab结合了虚拟内存和文件系统。\n即使是这周的lab，也会尝试让buffer cache可以支持更多的并发。所以这就是为什么文件系统是有趣的。\nFile system实现概述 为了理解文件系统必须提供什么能力，让我们再看一下一些与文件系统相关的基础系统调用。\n首先让我们来看一个简单的场景，假设我们创建了文件“x/y”，或者说在目录x中创建了文件y，同时我们需要提供一些标志位，现在我们还不太关心标志位所以我会忽略它。\n上面的系统调用会创建文件，并返回文件描述符给调用者。\n调用者也就是用户应用程序可以对文件描述符调用write，有关write我们在之前已经看过很多次了，这里我们向文件写入“abc”三个字符。\n从这两个调用已经可以看出一些信息了：\n首先出现在接口中的路径名是可读的名字，而不是一串数字，它是由用户选择的字符串。 write系统调用并没有使用offset作为参数，所以写入到文件的哪个位置是隐式包含在文件系统中，文件系统在某个位置必然保存了文件的offset。因为如果你再调用write系统调用，新写入的数据会从第4个字节开始。 除此之外，还有一些我们之前没有看过的有趣的系统调用。\n例如XV6和所有的Unix文件系统都支持通过系统调用创建链接，给同一个文件指定多个名字。你可以通过调用link系统调用，为之前创建的文件“x/y”创建另一个名字“x/z”。\n所以文件系统内部需要以某种方式跟踪指向同一个文件的多个文件名。\n我们还可能会在文件打开时，删除或者更新文件的命名空间。\n例如，用户可以通过unlink系统调用来删除特定的文件名。如果此时相应的文件描述符还是打开的状态，那我们还可以向文件写数据，并且这也能正常工作。\n所以，在文件系统内部，文件描述符必然与某个对象关联，而这个对象不依赖文件名。\n这样，即使文件名变化了，文件描述符仍然能够指向或者引用相同的文件对象。\n所以，实际上操作系统内部需要对于文件有内部的表现形式，并且这种表现形式与文件名无关。\n除此之外，我还想提一点。\n文件系统的目的是实现上面描述的API，也即是典型的文件系统API。\n但是，这并不是唯一构建一个存储系统的方式。\n如果只是在磁盘上存储数据，你可以想出一个完全不同的API。\n举个例子，数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。\n所以记住这一点很重要：还存在其他的方式能组织存储系统。\n我们这节课关注在文件系统，文件系统通常由操作系统提供，而数据库如果没有直接访问磁盘的权限的话，通常是在文件系统之上实现的。\n（注，早期数据库通常直接基于磁盘构建自己的文件系统，因为早期操作系统自带的文件系统在性能上较差，且写入不是同步的，进而导致数据库的ACID不能保证。不过现代操作系统自带的文件系统已经足够好，所以现代的数据库大部分构建在操作系统自带的文件系统之上）\n学生提问：link增加了了对于文件的一个引用，unlink减少了一个引用？\nFrans教授：是的。我们稍后会介绍更多相关的内容。\n学生提问：能介绍一下soft link和hard link吗？\nFrans教授：我今天不会讨论这些内容。但是你们将会在下一个File system lab中实现soft link。所以XV6本身实现了hard link，需要你们来实现soft link。\n学生提问：link是对inode做操作，而不是对文件描述符做操作，对吧？\nFrans教授：是的，link是对inode做操作，我们接下来介绍这部分内容。\n接下来我们看一下文件系统的结构。文件系统究竟维护了什么样的结构来实现前面介绍的API呢？\n首先，最重要的可能就是inode，这是代表一个文件的对象，并且它不依赖于文件名。\n实际上，inode是通过自身的编号来进行区分的，这里的编号就是个整数。\n所以文件系统内部通过一个数字，而不是通过文件路径名引用inode。\n同时，基于之前的讨论，inode必须有一个link count来跟踪指向这个inode的文件名的数量。\n一个文件（inode）只能在link count为0的时候被删除。\n实际的过程可能会更加复杂，实际中还有一个openfd count，也就是当前打开了文件的文件描述符计数。\n一个文件只能在这两个计数器都为0的时候才能被删除。\n同时基于之前的讨论，我们也知道write和read都没有针对文件的offset参数，所以文件描述符必然自己悄悄维护了对于文件的offset。\n文件系统中核心的数据结构就是inode和file descriptor。后者主要与用户进程进行交互。\n尽管文件系统的API很相近并且内部实现可能非常不一样。\n但是很多文件系统都有类似的结构。\n因为文件系统还挺复杂的，所以最好按照分层的方式进行理解。\n可以这样看：\n在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。 在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘。这里我们将磁盘中的数据保存在了内存中。 为了保证持久性，再往上通常会有一个logging层。许多文件系统都有某种形式的logging，我们下节课会讨论这部分内容，所以今天我就跳过它的介绍。 在logging层之上，XV6有inode cache，这主要是为了同步（synchronization），我们稍后会介绍。inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache。 再往上就是inode本身了。它实现了read/write。 再往上，就是文件名，和文件描述符操作。 不同的文件系统组织方式和每一层可能都略有不同，有的时候分层也没有那么严格，即使在XV6中分层也不是很严格，但是从概念上来说这里的结构对于理解文件系统还是有帮助的。\n实际上所有的文件系统都有组件对应这里不同的分层，例如buffer cache，logging，inode和路径名。\n文件系统使用磁盘 接下来，我将简单的介绍最底层，也即是存储设备。\n实际中有非常非常多不同类型的存储设备，这些设备的区别在于性能，容量，数据保存的期限等。\n其中两种最常见，并且你们应该也挺熟悉的是SSD和HDD。\n这两类存储虽然有着不同的性能，但是都在合理的成本上提供了大量的存储空间。\nSSD通常是0.1到1毫秒的访问时间，而HDD通常是在10毫秒量级完成读写一个disk block。\n这里有些术语有点让人困惑，它们是sectors和blocks。\nsector通常是磁盘驱动可以读写的最小单元，它过去通常是512字节。 block通常是操作系统或者文件系统视角的数据。它由文件系统定义，在XV6中它是1024字节。所以XV6中一个block对应两个sector。通常来说一个block对应了一个或者多个sector。 有的时候，人们也将磁盘上的sector称为block。所以这里的术语也不是很精确。\n这些存储设备连接到了电脑总线之上，总线也连接了CPU和内存。\n一个文件系统运行在CPU上，将内部的数据存储在内存，同时也会以读写block的形式存储在SSD或者HDD。\n这里的接口还是挺简单的，包括了read/write，然后以block编号作为参数。\n虽然我们这里描述的过于简单了，但是实际的接口大概就是这样。\n在内部，SSD和HDD工作方式完全不一样，但是对于硬件的抽象屏蔽了这些差异。\n磁盘驱动通常会使用一些标准的协议，例如PCIE，与磁盘交互。\n从上向下看磁盘驱动的接口，大部分的磁盘看起来都一样，你可以提供block编号，在驱动中通过写设备的控制寄存器，然后设备就会完成相应的工作。\n这是从一个文件系统的角度的描述。\n尽管不同的存储设备有着非常不一样的属性，从驱动的角度来看，你可以以大致相同的方式对它们进行编程。\n有关存储设备我们就说这么多。\n学生提问：对于read/write的接口，是不是提供了同步/异步的选项？\nFrans教授：你可以认为一个磁盘的驱动与console的驱动是基本一样的。\n驱动向设备发送一个命令表明开始读或者写，过了一会当设备完成了操作，会产生一个中断表明完成了相应的命令。\n但是因为磁盘本身比console复杂的多，所以磁盘的驱动也会比我们之前看过的console的驱动复杂的多。\n不过驱动中的代码结构还是类似的，也有bottom部分和top部分，中断和读写控制寄存器（注，详见lec09）。\n从文件系统的角度来看磁盘还是很直观的。\n因为对于磁盘就是读写block或者sector，我们可以将磁盘看作是一个巨大的block的数组，数组从0开始，一直增长到磁盘的最后。\n而文件系统的工作就是将所有的数据结构以一种能够在重启之后重新构建文件系统的方式，存放在磁盘上。虽然有不同的方式，但是XV6使用了一种非常简单，但是还挺常见的布局结构。\n通常来说：\nblock0要么没有用，要么被用作boot sector来启动操作系统。 block1通常被称为super block，它描述了文件系统。它可能包含磁盘上有多少个block共同构成了文件系统这样的信息。我们之后会看到XV6在里面会存更多的信息，你可以通过block1构造出大部分的文件系统信息。 在XV6中，log从block2开始，到block32结束。实际上log的大小可能不同，这里在super block中会定义log就是30个block。 接下来在block32到block45之间，XV6存储了inode。我之前说过多个inode会打包存在一个block中，一个inode是64字节。 之后是bitmap block，这是我们构建文件系统的默认方法，它只占据一个block。它记录了数据block是否空闲。 之后就全是数据block了，数据block存储了文件的内容和目录的内容。 通常来说，bitmap block，inode blocks和log blocks被统称为metadata block。\n它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据。\n学生提问：boot block是不是包含了操作系统启动的代码？\nFrans教授：完全正确，它里面通常包含了足够启动操作系统的代码。之后再从文件系统中加载操作系统的更多内容。\n学生提问：所以XV6是存储在虚拟磁盘上？\nFrans教授：在QEMU中，我们实际上走了捷径。QEMU中有个标志位-kernel，它指向了内核的镜像文件，QEMU会将这个镜像的内容加载到了物理内存的0x80000000。所以当我们使用QEMU时，我们不需要考虑boot sector。\n学生提问：所以当你运行QEMU时，你就是将程序通过命令行传入，然后直接就运行传入的程序，然后就不需要从虚拟磁盘上读取数据了？\nFrans教授：完全正确。\n假设inode是64字节，如果你想要读取inode10，那么你应该按照下面的公式去对应的block读取inode。\n$n_{block} = 32 + inode * 64 / 1024$\n所以inode0在block32，inode17会在block33。只要有inode的编号，我们总是可以找到inode在磁盘上存储的位置。\ninode 接下来我们看一下磁盘上存储的inode究竟是什么？首先我们前面已经看过了，这是一个64字节的数据结构。\n通常来说它有一个type字段，表明inode是文件还是目录。 nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。 size字段，表明了文件数据有多少个字节。 不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1，等等。XV6的inode中总共有12个block编号。这些被称为direct block number。这12个block编号指向了构成文件的前12个block。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。 之后还有一个indirect block number，它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。 以上基本就是XV6中inode的组成部分。\n基于上面的内容，XV6中文件最大的长度是多少呢？\n学生回答：会是268*1024字节\n是的，最大文件尺寸对应的是下面的公式。\n$size_{max} = (256 + 12) * 1024$\n可以算出这里就是268KB，这么点大小能存个什么呢？所以这是个很小的文件长度，实际的文件系统，文件最大的长度会大的多得多。那可以做一些什么来让文件系统支持大得多的文件呢？\n学生回答：可以扩展inode中indirect部分吗？\n是的，可以用类似page table的方式，构建一个双重indirect block number指向一个block，这个block中再包含了256个indirect block number，每一个又指向了包含256个block number的block。\n这样的话，最大的文件长度会大得多（注，是2562561K）。\n这里修改了inode的数据结构，你可以使用类似page table的树状结构，也可以按照B树或者其他更复杂的树结构实现。\nXV6这里极其简单，基本是按照最早的Uinx实现方式来的，不过你可以实现更复杂的结构。\n实际上，在接下来的File system lab中，你将会实现双重indirect block number来支持更大的文件。\n学生提问：为什么每个block存储256个block编号？\nFrans教授：因为每个编号是4个字节。1024/4 = 256。这又带出了一个问题，如果block编号只是4个字节，磁盘最大能有多大？是的，2的32次方（注，4TB）。有些磁盘比这个数字要大，所以通常人们会使用比32bit更长的数字来表示block编号。\n在下一个File system lab，你们需要将inode中的一个block number变成双重indirect block number，这个双重indirect block number将会指向一个包含了256个indirect block number的block，其中的每一个indirect block number再指向一个包含了256个block number的block，这样文件就可以大得多。\n接下来，我们想要实现read系统调用。\n假设我们需要读取文件的第8000个字节，那么你该读取哪个block呢？从inode的数据结构中该如何计算呢？\n对于8000，我们首先除以1024，也就是block的大小，得到大概是7。\n这意味着第7个block就包含了第8000个字节。\n所以直接在inode的direct block number中，就包含了第8000个字节的block。\n为了找到这个字节在第7个block的哪个位置，我们需要用8000对1024求余数，我猜结果是是832。\n所以为了读取文件的第8000个字节，文件系统查看inode，先用8000除以1024得到block number，然后再用8000对1024求余读取block中对应的字节。\n总结一下，inode中的信息完全足够用来实现read/write系统调用，至少可以找到哪个disk block需要用来执行read/write系统调用。\n接下来我们讨论一下目录（directory）。\n文件系统的酷炫特性就是层次化的命名空间（hierarchical namespace），你可以在文件系统中保存对用户友好的文件名。\n大部分Unix文件系统有趣的点在于，一个目录本质上是一个文件加上一些文件系统能够理解的结构。\n在XV6中，这里的结构极其简单。每一个目录包含了directory entries，每一条entry都有固定的格式：\n前2个字节包含了目录中文件或者子目录的inode编号， 接下来的14个字节包含了文件或者子目录名。 所以每个entry总共是16个字节。\n对于实现路径名查找，这里的信息就足够了。假设我们要查找路径名“/y/x”，我们该怎么做呢？\n从路径名我们知道，应该从root inode开始查找。\n通常root inode会有固定的inode编号，在XV6中，这个编号是1。\n我们该如何根据编号找到root inode呢？从前一节我们可以知道，inode从block 32开始，如果是inode1，那么必然在block 32中的64到128字节的位置。\n所以文件系统可以直接读到root inode的内容。\n对于路径名查找程序，接下来就是扫描root inode包含的所有block，以找到“y”。\n该怎么找到root inode所有对应的block呢？根据前一节的内容就是读取所有的direct block number和indirect block number。\n结果可能是找到了，也可能是没有找到。如果找到了，那么目录y也会有一个inode编号，假设是251，我们可以继续从inode 251查找，先读取inode 251的内容，之后再扫描inode所有对应的block，找到“x”并得到文件x对应的inode编号，最后将其作为路径名查找的结果返回。\n学生提问：有没有一些元数据表明当前的inode是目录而不是一个文件？\nFrans教授：有的，实际上是在inode中。inode中的type字段表明这是一个目录还是一个文件。如果你对一个类型是文件的inode进行查找，文件系统会返回错误。\n这里的结构不是很有效。\n为了找到一个目录名，你需要线性扫描。实际的文件系统会使用更复杂的数据结构来使得查找更快，当然这又是设计数据结构的问题，而不是设计操作系统的问题。\n你可以使用你喜欢的数据结构并提升性能。出于简单和更容易解释的目的，XV6使用了这里这种非常简单的数据结构。\nFile system工作示例 接下来我们看一下实际中，XV6的文件系统是如何工作的，这部分内容对于下一个lab是有帮助的。\n首先我会启动XV6，这里有件事情我想指出。启动XV6的过程中，调用了makefs指令，来创建一个文件系统。\n所以makefs创建了一个全新的磁盘镜像，在这个磁盘镜像中包含了我们在指令中传入的一些文件。makefs为你创建了一个包含这些文件的新的文件系统。\nXV6总是会打印文件系统的一些信息，所以从指令的下方可以看出有46个meta block，其中包括了：\nboot block super block 30个log block 13个inode block 1个bitmap block 之后是954个data block。所以这是一个袖珍级的文件系统，总共就包含了1000个block。在File system lab中，你们会去支持更大的文件系统。\n我还稍微修改了一下XV6，使得任何时候写入block都会打印出block的编号。我们从console的输出可以看出，在XV6启动过程中，会有一些对于文件系统的调用，并写入了block 33，45，32。\n接下来我们运行一些命令，来看一下特定的命令对哪些block做了写操作，并理解为什么要对这些block写入数据。\n我们通过echo “hi” \u0026gt; x，来创建一个文件x，并写入字符“hi”。我会将输出拷贝出来，并做分隔以方便我们更好的理解。\n这里会有几个阶段\n第一阶段是创建文件 第二阶段将“hi”写入文件 第三阶段将“\\n”换行符写入到文件 如果你去看echo的代码实现，基本就是这3个阶段。\n上面就是echo的代码，它先检查参数，并将参数写入到文件描述符1，在最后写入一个换行符。\n让我们一个阶段一个阶段的看echo的执行过程，并理解对于文件系统发生了什么。\n相比看代码，这里直接看磁盘的分布图更方便：\nwrite 33代表了写inode。\n看起来给我们分配的inode位于block 33。之所以有两个write 33，第一个是为了标记inode将要被使用。\n在XV6中，我记得是使用inode中的type字段来标识inode是否空闲，这个字段同时也会用来表示inode是一个文件还是一个目录。\n所以这里将inode的type从空闲改成了文件，并写入磁盘表示这个inode已经被使用了。第二个write 33就是实际的写入inode的内容。inode的内容会包含linkcount为1以及其他内容。\nwrite 46是向第一个data block写数据，那么这个data block属于谁呢？\n学生回答：属于根目录。\n是的，block 46是根目录的第一个block。为什么它需要被写入数据呢？\n学生回答：因为我们正在向根目录创建一个新文件。\n是的，这里我们向根目录增加了一个新的entry，其中包含了文件名x，以及我们刚刚分配的inode编号。\n接下来的write 32又是什么意思呢？block 32保存的仍然是inode，那么inode中的什么发生了变化使得需要将更新后的inode写入磁盘？是的，根目录的大小变了，因为我们刚刚添加了16个字节的entry来代表文件x的信息。\n最后又有一次write 33，我在稍后会介绍这次写入的内容，这里我们再次更新了文件x的inode， 尽管我们又还没有写入任何数据。\n以上就是第一阶段创建文件的过程。第二阶段是向文件写入“hi”。\n首先是write 45，这是更新bitmap。文件系统首先会扫描bitmap来找到一个还没有使用的data block，未被使用的data block对应bit 0。\n找到之后，文件系统需要将该bit设置为1，表示对应的data block已经被使用了。所以更新block 45是为了更新bitmap。\n接下来的两次write 595表明，文件系统挑选了data block 595。\n所以在文件x的inode中，第一个direct block number是595。因为写入了两个字符，所以write 595被调用了两次。\n第二阶段最后的write 33是更新文件x对应的inode中的size字段，因为现在文件x中有了两个字符。\n学生提问：block 595看起来在磁盘中很靠后了，是因为前面的block已经被系统内核占用了吗？\nFrans教授：我们可以看前面makefs指令，makefs存了很多文件在磁盘镜像中，这些都发生在创建文件x之前，所以磁盘中很大一部分已经被这些文件填满了。\n学生提问：第二阶段最后的write 33是否会将block 595与文件x的inode关联起来？\nFrans教授：会的。这里的write 33会发生几件事情：首先inode的size字段会更新；第一个direct block number会更新。这两个信息都会通过write 33一次更新到磁盘上的inode中。\n以上就是磁盘中文件系统的组织结构的核心，希望你们都能理解背后的原理。\nXV6创建inode代码展示 接下来我们通过查看XV6中的代码，更进一步的了解文件系统。因为我们前面已经分配了inode，我们先来看一下这是如何发生的。\nsysfile.c中包含了所有与文件系统相关的函数，分配inode发生在sys_open函数中，这个函数会负责创建文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 uint64 sys_open(void) { char path[MAXPATH]; int fd, omode; struct file *f; struct inode *ip; int n; if((n = argstr(0, path, MAXPATH)) \u0026lt; 0 || argint(1, \u0026amp;omode) \u0026lt; 0) return -1; begin_op(); if(omode \u0026amp; O_CREATE){ ip = create(path, T_FILE, 0, 0); if(ip == 0){ end_op(); return -1; } } else { if((ip = namei(path)) == 0){ end_op(); return -1; } ilock(ip); if(ip-\u0026gt;type == T_DIR \u0026amp;\u0026amp; omode != O_RDONLY){ iunlockput(ip); end_op(); return -1; } } if(ip-\u0026gt;type == T_DEVICE \u0026amp;\u0026amp; (ip-\u0026gt;major \u0026lt; 0 || ip-\u0026gt;major \u0026gt;= NDEV)){ iunlockput(ip); end_op(); return -1; } if((f = filealloc()) == 0 || (fd = fdalloc(f)) \u0026lt; 0){ if(f) fileclose(f); iunlockput(ip); end_op(); return -1; } if(ip-\u0026gt;type == T_DEVICE){ f-\u0026gt;type = FD_DEVICE; f-\u0026gt;major = ip-\u0026gt;major; } else { f-\u0026gt;type = FD_INODE; f-\u0026gt;off = 0; } f-\u0026gt;ip = ip; f-\u0026gt;readable = !(omode \u0026amp; O_WRONLY); f-\u0026gt;writable = (omode \u0026amp; O_WRONLY) || (omode \u0026amp; O_RDWR); if((omode \u0026amp; O_TRUNC) \u0026amp;\u0026amp; ip-\u0026gt;type == T_FILE){ itrunc(ip); } iunlock(ip); end_op(); return fd; } 在sys_open函数中，会调用create函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 static struct inode* create(char *path, short type, short major, short minor) { struct inode *ip, *dp; char name[DIRSIZ]; if((dp = nameiparent(path, name)) == 0) return 0; ilock(dp); if((ip = dirlookup(dp, name, 0)) != 0){ iunlockput(dp); ilock(ip); if(type == T_FILE \u0026amp;\u0026amp; (ip-\u0026gt;type == T_FILE || ip-\u0026gt;type == T_DEVICE)) return ip; iunlockput(ip); return 0; } if((ip = ialloc(dp-\u0026gt;dev, type)) == 0) panic(\u0026#34;create: ialloc\u0026#34;); ilock(ip); ip-\u0026gt;major = major; ip-\u0026gt;minor = minor; ip-\u0026gt;nlink = 1; iupdate(ip); if(type == T_DIR){ // Create . and .. entries. dp-\u0026gt;nlink++; // for \u0026#34;..\u0026#34; iupdate(dp); // No ip-\u0026gt;nlink++ for \u0026#34;.\u0026#34;: avoid cyclic ref count. if(dirlink(ip, \u0026#34;.\u0026#34;, ip-\u0026gt;inum) \u0026lt; 0 || dirlink(ip, \u0026#34;..\u0026#34;, dp-\u0026gt;inum) \u0026lt; 0) panic(\u0026#34;create dots\u0026#34;); } if(dirlink(dp, name, ip-\u0026gt;inum) \u0026lt; 0) panic(\u0026#34;create: dirlink\u0026#34;); iunlockput(dp); return ip; } create函数中首先会解析路径名并找到最后一个目录，之后会查看文件是否存在，如果存在的话会返回错误。之后就会调用ialloc（inode allocate），这个函数会为文件x分配inode。ialloc函数位于fs.c文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Allocate an inode on device dev. // Mark it as allocated by giving it type type. // Returns an unlocked but allocated and referenced inode. struct inode* ialloc(uint dev, short type) { int inum; struct buf *bp; struct dinode *dip; for(inum = 1; inum \u0026lt; sb.ninodes; inum++){ bp = bread(dev, IBLOCK(inum, sb)); dip = (struct dinode*)bp-\u0026gt;data + inum%IPB; if(dip-\u0026gt;type == 0){ // a free inode memset(dip, 0, sizeof(*dip)); dip-\u0026gt;type = type; log_write(bp); // mark it allocated on the disk brelse(bp); return iget(dev, inum); } brelse(bp); } panic(\u0026#34;ialloc: no inodes\u0026#34;); } 以上就是ialloc函数，与XV6中的大部分函数一样，它很简单，但是又不是很高效。\n它会遍历所有可能的inode编号，找到inode所在的block，再看位于block中的inode数据的type字段。\n如果这是一个空闲的inode，那么将其type字段设置为文件，这会将inode标记为已被分配。\n函数中的log_write就是我们之前看到在console中有关写block的输出。这里的log_write是我们看到的整个输出的第一个。\n以上就是第一次写磁盘涉及到的函数调用。\n这里有个有趣的问题，如果有多个进程同时调用create函数会发生什么？对于一个多核的计算机，进程可能并行运行，两个进程可能同时会调用到ialloc函数，然后进而调用bread（block read）函数。\n所以必须要有一些机制确保这两个进程不会互相影响。\n让我们看一下位于bio.c的buffer cache代码。首先看一下bread函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Return a locked buf with the contents of the indicated block. struct buf* bread(uint dev, uint blockno) { struct buf *b; b = bget(dev, blockno); if(!b-\u0026gt;valid) { virtio_disk_rw(b, 0); b-\u0026gt;valid = 1; } return b; } bread函数首先会调用bget函数，bget会为我们从buffer cache中找到block的缓存。让我们看一下bget函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Look through buffer cache for block on device dev. // If not found, allocate a buffer. // In either case, return locked buffer. static struct buf* bget(uint dev, uint blockno) { struct buf *b; acquire(\u0026amp;bcache.lock); // Is the block already cached? for(b = bcache.head.next; b != \u0026amp;bcache.head; b = b-\u0026gt;next){ if(b-\u0026gt;dev == dev \u0026amp;\u0026amp; b-\u0026gt;blockno == blockno){ b-\u0026gt;refcnt++; release(\u0026amp;bcache.lock); acquiresleep(\u0026amp;b-\u0026gt;lock); return b; } } // Not cached. // Recycle the least recently used (LRU) unused buffer. for(b = bcache.head.prev; b != \u0026amp;bcache.head; b = b-\u0026gt;prev){ if(b-\u0026gt;refcnt == 0) { b-\u0026gt;dev = dev; b-\u0026gt;blockno = blockno; b-\u0026gt;valid = 0; b-\u0026gt;refcnt = 1; release(\u0026amp;bcache.lock); acquiresleep(\u0026amp;b-\u0026gt;lock); return b; } } panic(\u0026#34;bget: no buffers\u0026#34;); } 这里的代码还有点复杂。我猜你们之前已经看过这里的代码，那么这里的代码在干嘛？\n学生回答：这里遍历了linked-list，来看看现有的cache是否符合要找的block。\n是的，我们这里看一下block 33的cache是否存在，如果存在的话，将block对象的引用计数（refcnt）加1，之后再释放bcache锁，因为现在我们已经完成了对于cache的检查并找到了block cache。之后，代码会尝试获取block cache的锁。\n所以，如果有多个进程同时调用bget的话，其中一个可以获取bcache的锁并扫描buffer cache。\n此时，其他进程是没有办法修改buffer cache的（注，因为bacche的锁被占住了）。\n之后，进程会查找block number是否在cache中，如果在的话将block cache的引用计数加1，表明当前进程对block cache有引用，之后再释放bcache的锁。\n如果有第二个进程也想扫描buffer cache，那么这时它就可以获取bcache的锁。\n假设第二个进程也要获取block 33的cache，那么它也会对相应的block cache的引用计数加1。最后这两个进程都会尝试对block 33的block cache调用acquiresleep函数。\nacquiresleep是另一种锁，我们称之为sleep lock，本质上来说它获取block 33 cache的锁。\n其中一个进程获取锁之后函数返回。\n在ialloc函数中会扫描block 33中是否有一个空闲的inode。\n而另一个进程会在acquiresleep中等待第一个进程释放锁。\n学生提问：当一个block cache的refcnt不为0时，可以更新block cache吗？因为释放锁之后，可能会修改block cache。\nFrans教授：这里我想说几点；首先XV6中对bcache做任何修改的话，都必须持有bcache的锁；其次对block 33的cache做任何修改你需要持有block 33的sleep lock。所以在任何时候，release(\u0026amp;bcache.lock)之后，b-\u0026gt;refcnt都大于0。block的cache只会在refcnt为0的时候才会被驱逐，任何时候refcnt大于0都不会驱逐block cache。所以当b-\u0026gt;refcnt大于0的时候，block cache本身不会被buffer cache修改。这里的第二个锁，也就是block cache的sleep lock，是用来保护block cache的内容的。它确保了任何时候只有一个进程可以读写block cache。\n如果buffer cache中有两份block 33的cache将会出现问题。假设一个进程要更新inode19，另一个进程要更新inode20。\n如果它们都在处理block 33的cache，并且cache有两份，那么第一个进程可能持有一份cache并先将inode19写回到磁盘中，而另一个进程持有另一份cache会将inode20写回到磁盘中，并将inode19的更新覆盖掉。\n所以一个block只能在buffer cache中出现一次。你们在完成File system lab时，必须要维持buffer cache的这个属性。\n学生提问：如果多个进程都在使用同一个block的cache，然后有一个进程在修改block，并通过强制向磁盘写数据修改了block的cache，那么其他进程会看到什么结果？\nFrans教授：如果第一个进程结束了对block 33的读写操作，它会对block的cache调用brelse（block cache release）函数。\n这个函数会对refcnt减1，并释放sleep lock。这意味着，如果有任何一个其他进程正在等待使用这个block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。\n假设两个进程都需要分配一个新的inode，且新的inode都位于block 33。如果第一个进程分配到了inode18并完成了更新，那么它对于inode18的更新是可见的。另一个进程就只能分配到inode19，因为inode18已经被标记为已使用，任何之后的进程都可以看到第一个进程对它的更新。\n这正是我们想看到的结果，如果一个进程创建了一个inode或者创建了一个文件，之后的进程执行读就应该看到那个文件。\nSleep Lock block cache使用的是sleep lock。sleep lock区别于一个常规的spinlock。我们先看来一下sleep lock。\n1 2 3 4 5 6 7 8 9 10 11 void acquiresleep(struct sleeplock *lk) { acquire(\u0026amp;lk-\u0026gt;lk); while (lk-\u0026gt;locked) { sleep(lk, \u0026amp;lk-\u0026gt;lk); } lk-\u0026gt;locked = 1; lk-\u0026gt;pid = myproc()-\u0026gt;pid; release(\u0026amp;lk-\u0026gt;lk); } 首先是acquiresleep函数，它用来获取sleep lock。\n函数里首先获取了一个普通的spinlock，这是与sleep lock关联在一起的一个锁。\n之后，如果sleep lock被持有，那么就进入sleep状态，并将自己从当前CPU调度开。\n既然sleep lock是基于spinlock实现的，为什么对于block cache，我们使用的是sleep lock而不是spinlock？\n这里其实有多种原因。对于spinlock有很多限制，其中之一是加锁时中断必须要关闭。\n所以如果使用spinlock的话，当我们对block cache做操作的时候需要持有锁，那么我们就永远也不能从磁盘收到数据。\n或许另一个CPU核可以收到中断并读到磁盘数据，但是如果我们只有一个CPU核的话，我们就永远也读不到数据了。\n出于同样的原因，也不能在持有spinlock的时候进入sleep状态（注，详见13.1）。\n所以这里我们使用sleep lock。\nsleep lock的优势就是，我们可以在持有锁的时候不关闭中断。\n我们可以在磁盘操作的过程中持有锁，我们也可以长时间持有锁。\n当我们在等待sleep lock的时候，我们并没有让CPU一直空转，我们通过sleep将CPU出让出去了。\n接下来让我们看一下brelease函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Release a locked buffer. // Move to the head of the most-recently-used list. void brelse(struct buf *b) { if(!holdingsleep(\u0026amp;b-\u0026gt;lock)) panic(\u0026#34;brelse\u0026#34;); releasesleep(\u0026amp;b-\u0026gt;lock); acquire(\u0026amp;bcache.lock); b-\u0026gt;refcnt--; if (b-\u0026gt;refcnt == 0) { // no one is waiting for it. b-\u0026gt;next-\u0026gt;prev = b-\u0026gt;prev; b-\u0026gt;prev-\u0026gt;next = b-\u0026gt;next; b-\u0026gt;next = bcache.head.next; b-\u0026gt;prev = \u0026amp;bcache.head; bcache.head.next-\u0026gt;prev = b; bcache.head.next = b; } release(\u0026amp;bcache.lock); } brelease函数中首先释放了sleep lock；之后获取了bcache的锁；之后减少了block cache的引用计数，表明一个进程不再对block cache感兴趣；最后如果引用计数为0，那么它会修改buffer cache的linked-list，将block cache移到linked-list的头部，这样表示这个block cache是最近使用过的block cache。\n这一点很重要，当我们在bget函数中不能找到block cache时，我们需要在buffer cache中腾出空间来存放新的block cache，这时会使用LRU（Least Recent Used）算法找出最不常使用的block cache，并撤回它（注，而将刚刚使用过的block cache放在linked-list的头部就可以直接更新linked-list的tail来完成LRU操作）。\n为什么这是一个好的策略呢？因为通常系统都遵循temporal locality策略，也就是说如果一个block cache最近被使用过，那么很有可能它很快会再被使用，所以最好不要撤回这样的block cache。\n以上就是对于block cache代码的介绍。这里有几件事情需要注意：\n首先在内存中，对于一个block只能有一份缓存。这是block cache必须维护的特性。 其次，这里使用了与之前的spinlock略微不同的sleep lock。与spinlock不同的是，可以在I/O操作的过程中持有sleep lock。 第三，它采用了LRU作为cache替换策略。 第四，它有两层锁。第一层锁用来保护buffer cache的内部数据；第二层锁也就是sleep lock用来保护单个block的cache。 最后让我们来总结一下，并把剩下的内容留到下节课。\n首先，文件系统是一个位于磁盘的数据结构。我们今天的主要时间都用来介绍这个位于磁盘的数据结构的内容。XV6的这个数据结构实现的很简单，但是你可以实现一个更加复杂的数据结构。 其次，我们花了一些时间来看block cache的实现，这对于性能来说是至关重要的，因为读写磁盘是代价较高的操作，可能要消耗数百毫秒，而block cache确保了如果我们最近从磁盘读取了一个block，那么我们将不会再从磁盘读取相同的block。 下节课我将会介绍crash safety，这是文件系统设计中非常棒的一部分。我们将会在crash safety讲两节课。下节课我们会看到基于log实现的crash safety机制，下下节课我们会看到Linux的ext3是如何实现的logging，这种方式要快得多。\n学生提问：我有个关于brelease函数的问题，看起来它先释放了block cache的锁，然后再对引用计数refcnt减一，为什么可以这样呢？\nFrans教授：这是个好问题。如果我们释放了sleep lock，这时另一个进程正在等待锁，那么refcnt必然大于1，而b-\u0026gt;refcnt \u0026ndash;只是表明当前执行brelease的进程不再关心block cache。如果还有其他进程正在等待锁，那么refcnt必然不等于0，我们也必然不会执行if(b-\u0026gt;refcnt == 0)中的代码。\n","date":"2024-09-28T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08114file-systems/bg_hu_89f4de69cc638d65.jpg","permalink":"https://echudet.github.io/p/mit6s08114file-systems/","title":"MIT6S081：14File Systems"},{"content":"Lecture 13 Sleep \u0026amp; Wake up 预习 sleep与wakeup 调度和锁有助于隐藏一个进程对另一个进程的存在，但到目前为止，我们还没有帮助进程进行有意交互的抽象。\n为解决这个问题已经发明了许多机制。\nXv6使用了一种称为sleep和wakeup的方法，它允许一个进程在等待事件时休眠，而另一个进程在事件发生后将其唤醒。\n睡眠和唤醒通常被称为序列协调（sequence coordination）或条件同步机制（conditional synchronization mechanisms）。\n为了说明，让我们考虑一个称为信号量（semaphore）的同步机制，它可以协调生产者和消费者。\n信号量维护一个计数并提供两个操作。\n“V”操作（对于生产者）增加计数。“P”操作（对于使用者）等待计数为非零，然后递减并返回。\n如果只有一个生产者线程和一个消费者线程，并且它们在不同的CPU上执行，并且编译器没有进行过积极的优化，那么此实现将是正确的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 struct semaphore { struct spinlock lock; int count; }; void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { while (s-\u0026gt;count == 0) ; acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } 上面的实现代价昂贵。\n如果生产者很少采取行动，消费者将把大部分时间花在while循环中，希望得到非零计数。\n消费者的CPU可以找到比通过反复轮询s-\u0026gt;count繁忙等待更有成效的工作。\n避免繁忙等待，消费者需要一种方法来释放CPU，并且只有在V增加计数后才能恢复。\n这是朝着这个方向迈出的一步，尽管我们将看到这是不够的。\n让我们想象一对调用，sleep和wakeup，工作流程如下。\nSleep(chan)在任意值chan上睡眠，称为等待通道（wait channel）。\nSleep将调用进程置于睡眠状态，释放CPU用于其他工作。\nWakeup(chan)唤醒所有在chan上睡眠的进程（如果有），使其sleep调用返回。\n如果没有进程在chan上等待，则wakeup不执行任何操作。\n我们可以将信号量实现更改为使用sleep和wakeup（更改的行添加了注释）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); // !pay attention release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { while (s-\u0026gt;count == 0) sleep(s); // !pay attention acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } P现在放弃CPU而不是自旋，这很好。\n然而，事实证明，使用此接口设计sleep和wakeup而不遭受所谓的丢失唤醒（lost wake-up）问题并非易事。\n假设P在第9行发现s-\u0026gt;count==0。\n当P在第9行和第10行之间时，V在另一个CPU上运行：它将s-\u0026gt;count更改为非零，并调用wakeup，这样就不会发现进程处于休眠状态，因此不会执行任何操作。\n现在P继续在第10行执行：它调用sleep并进入睡眠。这会导致一个问题：P正在休眠，等待调用V，而V已经被调用。\n除非我们运气好，生产者再次呼叫V，否则消费者将永远等待，即使count为非零。\n这个问题的根源是V在错误的时刻运行，违反了P仅在s-\u0026gt;count==0时才休眠的不变量。\n保护不变量的一种不正确的方法是将锁的获取（下面以黄色突出显示）移动到P中，以便其检查count和调用sleep是原子的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); // !pay attention while (s-\u0026gt;count == 0) sleep(s); s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } 人们可能希望这个版本的P能够避免丢失唤醒，因为锁阻止V在第10行和第11行之间执行。\n它确实这样做了，但它会导致死锁：P在睡眠时持有锁，因此V将永远阻塞等待锁。\n我们将通过更改sleep的接口来修复前面的方案：调用方必须将条件锁（condition lock）传递给sleep，以便在调用进程被标记为asleep并在睡眠通道上等待后sleep可以释放锁。\n如果有一个并发的V操作，锁将强制它在P将自己置于睡眠状态前一直等待，因此wakeup将找到睡眠的消费者并将其唤醒。\n一旦消费者再次醒来，sleep会在返回前重新获得锁。我们新的正确的sleep/wakeup方案可用如下（更改以黄色突出显示）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void V(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); s-\u0026gt;count += 1; wakeup(s); release(\u0026amp;s-\u0026gt;lock); } void P(struct semaphore* s) { acquire(\u0026amp;s-\u0026gt;lock); while (s-\u0026gt;count == 0) sleep(s, \u0026amp;s-\u0026gt;lock); // !pay attention s-\u0026gt;count -= 1; release(\u0026amp;s-\u0026gt;lock); } P持有s-\u0026gt;lock的事实阻止V在P检查s-\u0026gt;count和调用sleep之间试图唤醒它。然而请注意，我们需要sleep释放s-\u0026gt;lock并使消费者进程进入睡眠状态的操作是原子的。\n代码：sleep与wakeup 让我们看看sleep（kernel/proc.c:548）和wakeup（kernel/proc.c:582）的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // Atomically release lock and sleep on chan. // Reacquires lock when awakened. void sleep(void *chan, struct spinlock *lk) { struct proc *p = myproc(); // Must acquire p-\u0026gt;lock in order to // change p-\u0026gt;state and then call sched. // Once we hold p-\u0026gt;lock, we can be // guaranteed that we won\u0026#39;t miss any wakeup // (wakeup locks p-\u0026gt;lock), // so it\u0026#39;s okay to release lk. if(lk != \u0026amp;p-\u0026gt;lock){ //DOC: sleeplock0 acquire(\u0026amp;p-\u0026gt;lock); //DOC: sleeplock1 release(lk); } // Go to sleep. p-\u0026gt;chan = chan; p-\u0026gt;state = SLEEPING; sched(); // Tidy up. p-\u0026gt;chan = 0; // Reacquire original lock. if(lk != \u0026amp;p-\u0026gt;lock){ release(\u0026amp;p-\u0026gt;lock); acquire(lk); } } // Wake up all processes sleeping on chan. // Must be called without any p-\u0026gt;lock. void wakeup(void *chan) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++) { acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state == SLEEPING \u0026amp;\u0026amp; p-\u0026gt;chan == chan) { p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); } } 其基本思想是让sleep将当前进程标记为SLEEPING，然后调用sched释放CPU；\nwakeup查找在给定等待通道上休眠的进程，并将其标记为RUNNABLE。\nsleep和wakeup的调用者可以使用任何相互间方便的数字作为通道。\nXv6通常使用等待过程中涉及的内核数据结构的地址。\nsleep获得p-\u0026gt;lock（kernel/proc.c:559）。\n要进入睡眠的进程现在同时持有p-\u0026gt;lock和lk。\n在调用者（示例中为P）中持有lk是必要的：它确保没有其他进程（在示例中指一个运行的V）可以启动wakeup(chan)调用。\n既然sleep持有p-\u0026gt;lock，那么释放lk是安全的：其他进程可能会启动对wakeup(chan)的调用，但是wakeup将等待获取p-\u0026gt;lock，因此将等待sleep把进程置于睡眠状态的完成，以防止wakeup错过sleep。\n还有一个小问题：如果lk和p-\u0026gt;lock是同一个锁，那么如果sleep试图获取p-\u0026gt;lock就会自身死锁。\n但是，如果调用sleep的进程已经持有p-\u0026gt;lock，那么它不需要做更多的事情来避免错过并发的wakeup。\n当wait（kernel/proc.c:582）持有p-\u0026gt;lock调用sleep时，就会出现这种情况。\n由于sleep只持有p-\u0026gt;lock而无其他，它可以通过记录睡眠通道、将进程状态更改为SLEEPING并调用sched（kernel/proc.c:564-567）将进程置于睡眠状态。\n过一会儿，我们就会明白为什么在进程被标记为SLEEPING之前不将p-\u0026gt;lock释放（由scheduler）是至关重要的。\n在某个时刻，一个进程将获取条件锁，设置睡眠者正在等待的条件，并调用wakeup(chan)。\n在持有状态锁时调用wakeup非常重要。\n严格地说，wakeup只需跟在acquire之后就足够了（也就是说，可以在release之后调用wakeup）\nwakeup遍历进程表（kernel/proc.c:582）。\n它获取它所检查的每个进程的p-\u0026gt;lock，这既是因为它可能会操纵该进程的状态，也是因为p-\u0026gt;lock确保sleep和wakeup不会彼此错过。\n当wakeup发现一个SLEEPING的进程且chan相匹配时，它会将该进程的状态更改为RUNNABLE。调度器下次运行时，将看到进程已准备好运行。\n为什么sleep和wakeup的用锁规则能确保睡眠进程不会错过唤醒？\n休眠进程从检查条件之前的某处到标记为休眠之后的某处，要么持有条件锁，要么持有其自身的p-\u0026gt;lock或同时持有两者。\n调用wakeup的进程在wakeup的循环中同时持有这两个锁。\n因此，要么唤醒器（waker）在消费者线程检查条件之前使条件为真；要么唤醒器的wakeup在睡眠线程标记为SLEEPING后对其进行严格检查。\n然后wakeup将看到睡眠进程并将其唤醒（除非有其他东西首先将其唤醒）。\n有时，多个进程在同一个通道上睡眠；例如，多个进程读取同一个管道。\n一个单独的wakeup调用就能把他们全部唤醒。\n其中一个将首先运行并获取与sleep一同调用的锁，并且（在管道例子中）读取在管道中等待的任何数据。\n尽管被唤醒，其他进程将发现没有要读取的数据。从他们的角度来看，醒来是“虚假的”，他们必须再次睡眠。\n因此，在检查条件的循环中总是调用sleep。\n如果两次使用sleep/wakeup时意外选择了相同的通道，则不会造成任何伤害：它们将看到虚假的唤醒，但如上所述的循环将容忍此问题。\nsleep/wakeup的魅力在于它既轻量级（不需要创建特殊的数据结构来充当睡眠通道），又提供了一层抽象（调用者不需要知道他们正在与哪个特定进程进行交互）。\n代码：Pipes 使用睡眠和唤醒来同步生产者和消费者的一个更复杂的例子是xv6的管道实现。\n在第1章中看到了管道接口：写入管道一端的字节被复制到内核缓冲区，然后可以从管道的另一端读取。\n以后的章节将研究围绕管道的文件描述符支持，但现在让我们看看pipewrite和piperead的实现。\n每个管道都由一个struct pipe表示，其中包含一个锁lock和一个数据缓冲区data。\n1 2 3 4 5 6 7 8 struct pipe { struct spinlock lock; char data[PIPESIZE]; uint nread; // number of bytes read uint nwrite; // number of bytes written int readopen; // read fd is still open int writeopen; // write fd is still open }; 字段nread和nwrite统计从缓冲区读取和写入缓冲区的总字节数。\n缓冲区是环形的：在buf[PIPESIZE-1]之后写入的下一个字节是buf[0]。\n而计数不是环形。此约定允许实现区分完整缓冲区（nwrite==nread+PIPESIZE）和空缓冲区（nwrite==nread），但这意味着对缓冲区的索引必须使用buf[nread%PIPESIZE]，而不仅仅是buf[nread]（对于nwrite也是如此）。\n让我们假设对piperead和pipewrite的调用同时发生在两个不同的CPU上。\nPipewrite（kernel/pipe.c:77）从获取管道锁开始，它保护计数、数据及其相关不变量。\nPiperead（kernel/pipe.c:103）然后也尝试获取锁，但无法实现。它在acquire（kernel/spinlock.c:22）中旋转等待锁。\n当piperead等待时，pipewrite遍历被写入的字节（addr[0..n-1]），依次将每个字节添加到管道中（kernel/pipe.c:95）。\n在这个循环中缓冲区可能会被填满（kernel/pipe.c:85）。\n在这种情况下，pipewrite调用wakeup来提醒所有处于睡眠状态的读进程缓冲区中有数据等待，然后在\u0026amp;pi-\u0026gt;nwrite上睡眠，等待读进程从缓冲区中取出一些字节。\n作为使pipewrite进程进入睡眠状态的一部分，Sleep释放pi-\u0026gt;lock。\n现在pi-\u0026gt;lock可用，piperead设法获取它并进入其临界区域：它发现pi-\u0026gt;nread != pi-\u0026gt;nwrite（kernel/pipe.c:110）（pipewrite进入睡眠状态是因为pi-\u0026gt;nwrite == pi-\u0026gt;nread+PIPESIZE（kernel/pipe.c:85））。\n因此它进入for循环，从管道中复制数据（kernel/pipe.c:117），并根据复制的字节数增加nread。\n那些读出的字节就可供写入，因此piperead调用wakeup（kernel/pipe.c:124）返回之前唤醒所有休眠的写进程。\nWakeup寻找一个在\u0026amp;pi-\u0026gt;nwrite上休眠的进程，该进程正在运行pipewrite，但在缓冲区填满时停止。它将该进程标记为RUNNABLE。\n管道代码为读者和写者使用单独的睡眠通道（pi-\u0026gt;nread和pi-\u0026gt;nwrite）；这可能会使系统在有许多读者和写者等待同一管道这种不太可能的情况下更加高效。\n管道代码在检查休眠条件的循环中休眠；如果有多个读者或写者，那么除了第一个醒来的进程之外，所有进程都会看到条件仍然错误，并再次睡眠。\n代码：wait, exit和kill Sleep和wakeup可用于多种等待。\n第一章介绍的一个有趣的例子是子进程exit和父进程wait之间的交互。\n在子进程死亡时，父进程可能已经在wait中休眠，或者正在做其他事情；在后一种情况下，随后的wait调用必须观察到子进程的死亡，可能是在子进程调用exit后很久。\nxv6记录子进程终止直到wait观察到它的方式是让exit将调用方置于ZOMBIE状态，在那里它一直保持到父进程的wait注意到它，将子进程的状态更改为UNUSED，复制子进程的exit状态码，并将子进程ID返回给父进程。\n如果父进程在子进程之前退出，则父进程将子进程交给init进程，init进程将永久调用wait；因此，每个子进程退出后都有一个父进程进行清理。\n主要的实现挑战是父级和子级wait和exit，以及exit和exit之间可能存在竞争和死锁。\nWait使用调用进程的p-\u0026gt;lock作为条件锁，以避免丢失唤醒，并在开始时获取该锁（kernel/proc.c:398）。\n然后它扫描进程表。如果它发现一个子进程处于ZOMBIE状态，它将释放该子进程的资源及其proc结构体，将该子进程的退出状态码复制到提供给wait的地址（如果不是0），并返回该子进程的进程ID。\n如果wait找到子进程但没有子进程退出，它将调用sleep以等待其中一个退出（kernel/proc.c:445），然后再次扫描。\n这里，sleep中释放的条件锁是等待进程的p-\u0026gt;lock，这是上面提到的特例。\n注意，wait通常持有两个锁：它在试图获得任何子进程的锁之前先获得自己的锁；因此，整个xv6都必须遵守相同的锁定顺序（父级，然后是子级），以避免死锁。\nWait查看每个进程的np-\u0026gt;parent以查找其子进程。它使用np-\u0026gt;parent而不持有np-\u0026gt;lock，这违反了通常的规则，即共享变量必须受到锁的保护。\nnp可能是当前进程的祖先，在这种情况下，获取np-\u0026gt;lock可能会导致死锁，因为这将违反上述顺序。\n这种情况下无锁检查np-\u0026gt;parent似乎是安全的：进程的parent字段仅由其父进程更改，因此如果np-\u0026gt;parent==p为true，除非当前流程更改它，否则该值无法被更改，\nExit（*kernel/proc.c*:333）记录退出状态码，释放一些资源，将所有子进程提供给init进程，在父进程处于等待状态时唤醒父进程，将调用方标记为僵尸进程（zombie），并永久地让出CPU。\n最后的顺序有点棘手。\n退出进程必须在将其状态设置为ZOMBIE并唤醒父进程时持有其父进程的锁，因为父进程的锁是防止在wait中丢失唤醒的条件锁。\n子级还必须持有自己的p-\u0026gt;lock，否则父级可能会看到它处于ZOMBIE状态，并在它仍运行时释放它。\n锁获取顺序对于避免死锁很重要：因为wait先获取父锁再获取子锁，所以exit必须使用相同的顺序。\nExit调用一个专门的唤醒函数wakeup1，该函数仅唤醒父进程，且父进程必须正在wait中休眠（kernel/proc.c:598）。\n在将自身状态设置为ZOMBIE之前，子进程唤醒父进程可能看起来不正确，但这是安全的：\n虽然wakeup1可能会导致父进程运行，但wait中的循环在scheduler释放子进程的p-\u0026gt;lock之前无法检查子进程，所以wait在exit将其状态设置为ZOMBIE（kernel/proc.c:386）之前不能查看退出进程。\nexit允许进程自行终止，而kill（kernel/proc.c:611）允许一个进程请求另一个进程终止。\n对于kill来说，直接销毁受害者进程（即要杀死的进程）太复杂了，因为受害者可能在另一个CPU上执行，也许是在更新内核数据结构的敏感序列中间。\n因此，kill的工作量很小：它只是设置受害者的p-\u0026gt;killed，如果它正在睡眠，则唤醒它。\n受害者进程终将进入或离开内核，此时，如果设置了p-\u0026gt;killed，usertrap中的代码将调用exit。\n如果受害者在用户空间中运行，它将很快通过进行系统调用或由于计时器（或其他设备）中断而进入内核。\n如果受害者进程在sleep中，kill对wakeup的调用将导致受害者从sleep中返回。\n这存在潜在的危险，因为等待的条件可能不为真。\n但是，xv6对sleep的调用总是封装在while循环中，该循环在sleep返回后重新测试条件。\n一些对sleep的调用还在循环中测试p-\u0026gt;killed，如果它被设置，则放弃当前活动。\n只有在这种放弃是正确的情况下才能这样做。\n例如，如果设置了killed标志，则管道读写代码返回；最终代码将返回到陷阱，陷阱将再次检查标志并退出。\n一些XV6的sleep循环不检查p-\u0026gt;killed，因为代码在应该是原子操作的多步系统调用的中间。\nvirtio驱动程序（kernel/virtio_disk.c:242）就是一个例子：它不检查p-\u0026gt;killed，因为一个磁盘操作可能是文件系统保持正确状态所需的一组写入操作之一。\n等待磁盘I/O时被杀死的进程将不会退出，直到它完成当前系统调用并且usertrap看到killed标志\n真实世界 xv6调度器实现了一个简单的调度策略：它依次运行每个进程。这一策略被称为轮询调度（round robin）。\n真实的操作系统实施更复杂的策略，例如，允许进程具有优先级。其思想是调度器将优先选择可运行的高优先级进程，而不是可运行的低优先级进程。\n这些策略可能变得很复杂，因为常常存在相互竞争的目标：例如，操作系统可能希望保证公平性和高吞吐量。\n此外，复杂的策略可能会导致意外的交互，例如优先级反转（priority inversion）和航队（convoys）。\n当低优先级进程和高优先级进程共享一个锁时，可能会发生优先级反转，当低优先级进程持有该锁时，可能会阻止高优先级进程前进。\n当许多高优先级进程正在等待一个获得共享锁的低优先级进程时，可能会形成一个长的等待进程航队；一旦航队形成，它可以持续很长时间。\n为了避免此类问题，在复杂的调度器中需要额外的机制。\n睡眠和唤醒是一种简单有效的同步方法，但还有很多其他方法。\n所有这些问题中的第一个挑战是避免我们在本章开头看到的“丢失唤醒”问题。\n原始Unix内核的sleep只是禁用了中断，这就足够了，因为Unix运行在单CPU系统上。\n因为xv6在多处理器上运行，所以它为sleep添加了一个显式锁。\nFreeBSD的msleep采用了同样的方法。\nPlan 9的sleep使用一个回调函数，该函数在马上睡眠时获取调度锁，并在运行中持有；该函数用于在最后时刻检查睡眠条件，以避免丢失唤醒。\nLinux内核的sleep使用一个显式的进程队列，称为等待队列，而不是等待通道；队列有自己内部的锁。\n在wakeup中扫描整个进程列表以查找具有匹配chan的进程效率低下。\n一个更好的解决方案是用一个数据结构替换sleep和wakeup中的chan，该数据结构包含在该结构上休眠的进程列表，例如Linux的等待队列。\nPlan 9的sleep和wakeup将该结构称为集结点（rendezvous point）或Rendez。\n许多线程库引用与条件变量相同的结构；在这种情况下，sleep和wakeup操作称为wait和signal。\n所有这些机制都有一个共同的特点：睡眠条件受到某种在睡眠过程中原子级释放的锁的保护。\nwakeup的实现会唤醒在特定通道上等待的所有进程，可能有许多进程在等待该特定通道。\n操作系统将安排所有这些进程，它们将竞相检查睡眠条件。\n进程的这种行为有时被称为惊群效应（thundering herd），最好避免。\n大多数条件变量都有两个用于唤醒的原语：signal用于唤醒一个进程；broadcast用于唤醒所有等待进程。\n信号量（Semaphores）通常用于同步。\n计数count通常对应于管道缓冲区中可用的字节数或进程具有的僵尸子进程数。\n使用显式计数作为抽象的一部分可以避免“丢失唤醒”问题：使用显式计数记录已经发生wakeup的次数。计数还避免了虚假唤醒和惊群效应问题。\n终止进程并清理它们在xv6中引入了很多复杂性。\n在大多数操作系统中甚至更复杂，因为，例如，受害者进程可能在内核深处休眠，而展开其栈空间需要非常仔细的编程。\n许多操作系统使用显式异常处理机制（如longjmp）来展开栈。\n此外，还有其他事件可能导致睡眠进程被唤醒，即使它等待的事件尚未发生。\n例如，当一个Unix进程处于休眠状态时，另一个进程可能会向它发送一个signal。\n在这种情况下，进程将从中断的系统调用返回，返回值为-1，错误代码设置为EINTR。\n应用程序可以检查这些值并决定执行什么操作。\nXv6不支持信号，因此不会出现这种复杂性。\nXv6对kill的支持并不完全令人满意：有一些sleep循环可能应该检查p-\u0026gt;killed。\n一个相关的问题是，即使对于检查p-\u0026gt;killed的sleep循环，sleep和kill之间也存在竞争；后者可能会设置p-\u0026gt;killed，并试图在受害者的循环检查p-\u0026gt;killed之后但在调用sleep之前尝试唤醒受害者。\n如果出现此问题，受害者将不会注意到p-\u0026gt;killed，直到其等待的条件发生。\n这可能比正常情况要晚一点（例如，当virtio驱动程序返回受害者正在等待的磁盘块时）或永远不会发生（例如，如果受害者正在等待来自控制台的输入，但用户没有键入任何输入）。\n注：上节中说到kill的工作方式，kill设置p-\u0026gt;killed，如果遇到进程正在休眠，则会唤醒它，此后在usertrap中检测p-\u0026gt;killed，并使进程退出\n而如果像上面说的，在检查p-\u0026gt;killed之后调用sleep之前唤醒受害者进程，那么接下来执行sleep就会导致进程无法进入内核，无法在usertrap中退出，而必须等待所需事件的发生再次唤醒\n一个实际的操作系统将在固定时间内使用空闲列表找到自由的proc结构体，而不是allocproc中的线性时间搜索；xv6使用线性扫描是为了简单起见。\n正课 线程切换过程中锁的限制 在XV6中，任何时候调用switch函数都会从一个线程切换到另一个线程，通常是在用户进程的内核线程和调度器线程之间切换。\n在调用switch函数之前，总是会先获取线程对应的用户进程的锁。\n所以过程是这样，一个进程先获取自己的锁，然后调用switch函数切换到调度器线程，调度器线程再释放进程锁。\n实际上的代码顺序更像这样：\n一个进程出于某种原因想要进入休眠状态，比如说出让CPU或者等待数据，它会先获取自己的锁； 之后进程将自己的状态从RUNNING设置为RUNNABLE； 之后进程调用switch函数，其实是调用sched函数在sched函数中再调用的switch函数； switch函数将当前的线程切换到调度器线程； 调度器线程之前也调用了switch函数，现在恢复执行会从自己的switch函数返回； 返回之后，调度器线程会释放刚刚出让了CPU的进程的锁 在第1步中获取进程的锁的原因是，这样可以阻止其他CPU核的调度器线程在当前进程完成切换前，发现进程是RUNNABLE的状态并尝试运行它。\n为什么要阻止呢？因为其他每一个CPU核都有一个调度器线程在遍历进程表单，如果没有在进程切换的最开始就获取进程的锁的话，其他CPU核就有可能在当前进程还在运行时，认为该进程是RUNNABLE并运行它。\n而两个CPU核使用同一个栈运行同一个线程会使得系统立即崩溃。\n所以，在进程切换的最开始，进程先获取自己的锁，并且直到调用switch函数时也不释放锁。\n而另一个线程，也就是调度器线程会在进程的线程完全停止使用自己的栈之后，再释放进程的锁。\n释放锁之后，就可以由其他的CPU核再来运行进程的线程，因为这些线程现在已经不在运行了。\n以上是线程切换中非常重要的知识点。\n我们之后会用到它，因为这是Sleep\u0026amp;Wakeup设计中众多限制条件之一。\n学生提问：当我们有多个CPU核时，它们能看到同样的锁对象的唯一原因只可能是它们有一个共享的物理内存系统，对吧？\nRobert教授：是的。如果你有两个电脑，那么它们不会共享内存，并且我们就不会有这些问题。现在的处理器上，总是有多个CPU核，它们共享了相同的内存系统。\n在线程切换的过程中，还有一点我之前没有提过。\nXV6中，不允许进程在执行switch函数的过程中，持有任何其他的锁。\n所以，进程在调用switch函数的过程中，必须要持有p-\u0026gt;lock（注，也就是进程对应的proc结构体中的锁），但是同时又不能持有任何其他的锁。\n这也是包含了Sleep在内的很多设计的限制条件之一。\n接下来让我解释一下背后的原因，首先构建一个不满足这个限制条件的场景：\n我们有进程P1，P1的内核线程持有了p-\u0026gt;lock以外的其他锁，这些锁可能是在使用磁盘，UART，console过程中持有的。\n之后内核线程在持有锁的时候，通过调用switch/yield/sched函数出让CPU，这会导致进程P1持有了锁，但是进程P1又不在运行。\n假设我们在一个只有一个CPU核的机器上，进程P1调用了switch函数将CPU控制转给了调度器线程，调度器线程发现还有一个进程P2的内核线程正在等待被运行，所以调度器线程会切换到运行进程P2。\n假设P2也想使用磁盘，UART或者console，它会对P1持有的锁调用acquire，这是对于同一个锁的第二个acquire调用。\n当然这个锁现在已经被P1持有了，所以这里的acquire并不能获取锁。\n假设这里是spinlock，那么进程P2会在一个循环里不停的“旋转”并等待锁被释放。\n但是很明显进程P2的acquire不会返回，所以即使进程P2稍后愿意出让CPU，P2也没机会这么做。\n之所以没机会是因为P2对于锁的acquire调用在直到锁释放之前都不会返回，而唯一锁能被释放的方式就是进程P1恢复执行并在稍后release锁，但是这一步又还没有发生，因为进程P1通过调用switch函数切换到了P2，而P2又在不停的“旋转”并等待锁被释放。这是一种死锁，它会导致系统停止运行。\n虽然我刚刚的描述是基于机器上只有一个CPU核，但是你可以通过多个锁在多个CPU核的机器上构建类似的死锁场景。\n所以，我们在XV6中禁止在调用switch时持有除进程自身锁（注，也就是p-\u0026gt;lock）以外的其他锁。\n学生提问：难道定时器中断不会将CPU控制切换回进程P1从而解决死锁的问题吗？\nRobert教授：首先，所有的进程切换过程都发生在内核中，所有的acquire，switch，release都发生在内核代码而不是用户代码。实际上XV6允许在执行内核代码时触发中断，如果你查看trap.c中的代码你可以发现，如果XV6正在执行内核代码时发生了定时器中断，中断处理程序会调用yield函数并出让CPU。\n但是在之前的课程中我们讲过acquire函数在等待锁之前会关闭中断，否则的话可能会引起死锁（注，详见10.8），\n所以我们不能在等待锁的时候处理中断。所以如果你查看XV6中的acquire函数，你可以发现函数中第一件事情就是关闭中断，之后再“自旋”等待锁释放。你或许会想，为什么不能先“自旋”等待锁释放，再关闭中断？因为这样会有一个短暂的时间段锁被持有了但是中断没有关闭，在这个时间段内的设备的中断处理程序可能会引起死锁。\n所以不幸的是，当我们在自旋等待锁释放时会关闭中断，进而阻止了定时器中断并且阻止了进程P2将CPU出让回给进程P1。嗯，这是个好问题。\n学生提问：能重复一下死锁是如何避免的吗？\nRobert教授：哦，在XV6中，死锁是通过禁止在线程切换的时候加锁来避免的。\nXV6禁止在调用switch函数时，获取除了p-\u0026gt;lock以外的其他锁。如果你查看sched函数的代码（注，详见11.6），里面包含了一些检查代码来确保除了p-\u0026gt;lock以外线程不持有其他锁。所以上面会产生死锁的代码在XV6中是不合法的并被禁止的。\n将这里描述的对于锁的两个限制条件记住，因为我们后面讨论Sleep\u0026amp;Wakeup如何工作时会再次使用它们。\n在多核系统中，进程或者线程为了同步和共享资源，通常使用锁（如自旋锁）。但是在锁的操作中，如果处理不当，可能会引发死锁。死锁发生在多个进程/线程因为争抢资源（锁）而互相等待，导致系统无法前进。\n当一个进程/线程试图获取某个锁时（acquire），它可能会进入“自旋等待”状态，直到锁被释放。如果此时中断没有被关闭，可能会发生以下情况：\n假设某个进程（P1）已经持有锁，并正在执行一些关键代码，而另一个进程（P2）正在尝试获取同一个锁，但因为锁已经被 P1 持有，所以 P2 进入自旋等待状态。在这种情况下，如果不关闭中断，可能会发生以下情景：\n中断触发上下文切换：如果在 P2 自旋等待的过程中发生了定时器中断，操作系统可能会在 P2 等待锁时尝试进行上下文切换。这意味着 CPU 控制可能会切换到另一个进程（例如 P1），但这个切换可能会导致问题。 中断处理程序持有锁：假设中断处理程序需要持有同一个锁，但锁已经被 P1 持有，这可能会导致死锁。如果中断处理程序没有获取锁而被阻塞，并且 P1 继续等待另一个资源或中断处理结束，那么系统会陷入死锁状态。 设备中断与并发操作的冲突：如果在 P1 正在持有锁时，设备中断处理程序试图获取该锁，并在中断上下文中运行某些代码，那么设备中断的处理将被阻塞。这样，既无法完成设备的中断响应，又可能导致进程无法正常释放锁，进一步加剧系统的死锁风险。 为了避免这种情况，acquire 函数在等待锁之前会关闭中断。这样可以确保在进程/线程自旋等待锁的时候，不会被中断打断，从而防止中断处理程序引发的竞争条件或死锁。具体来说，关闭中断有以下好处：\n避免中断处理程序的竞争：当一个进程关闭中断并开始等待锁时，确保了 CPU 不会因为中断处理而切换到另一个进程。这样，持有锁的进程（如 P1）可以继续执行，直到释放锁，而不会因为中断导致死锁。 防止并发修改状态：锁保护的是共享数据的访问，关闭中断后，确保了其他硬件中断（比如定时器、设备中断）不会打断当前进程对锁和共享数据的操作。中断处理程序不能抢占当前进程，数据一致性得以保障。 如果我们在自旋等待锁释放时才关闭中断（即：先“自旋”再关闭中断），会存在一个竞态条件。在这个短暂的时间段内，虽然锁还没有被成功获取，但是中断已经没有关闭。这种情况下，设备中断可能会触发，并执行某些需要持有锁的操作，导致中断处理程序和进程竞争同一个锁，从而引发死锁。\nSleep\u0026amp;Wakeup接口 接下来看一下通过Sleep\u0026amp;Wakeup实现Coordination。\n锁可以使得线程本身不必关心其他线程的具体实现。\n我们为共享的数据增加锁，这样就不用担心其他线程也使用了相同的数据，因为锁可以确保对于数据的操作是依次发生的。\n当你在写一个线程的代码时，有些场景需要等待一些特定的事件，或者不同的线程之间需要交互。\n假设我们有一个Pipe，并且我正在从Pipe中读数据。但是Pipe当前又没有数据，所以我需要等待一个Pipe非空的事件。\n类似的，假设我在读取磁盘，我会告诉磁盘控制器请读取磁盘上的特定块。这或许要花费较长的时间，尤其当磁碟需要旋转时\n（通常是毫秒级别），磁盘才能完成读取。而执行读磁盘的进程需要等待读磁盘结束的事件。\n类似的，一个Unix进程可以调用wait函数。这个会使得调用进程等待任何一个子进程退出。所以这里父进程有意的在等待另一个进程产生的事件。\n以上就是进程需要等待特定事件的一些例子。\n特定事件可能来自于I/O，也可能来自于另一个进程，并且它描述了某件事情已经发生。\nCoordination是帮助我们解决这些问题并帮助我们实现这些需求的工具。Coordination是非常基础的工具，就像锁一样，在实现线程代码时它会一直出现。\n我们怎么能让进程或者线程等待一些特定的事件呢？一种非常直观的方法是通过循环实现busy-wait。\n假设我们想从一个Pipe读取数据，我们就写一个循环一直等待Pipe的buffer不为空。\n这个循环会一直运行直到其他的线程向Pipe的buffer写了数据。之后循环会结束，我们就可以从Pipe中读取数据并返回。\n实际中会有这样的代码。\n如果你知道你要等待的事件极有可能在0.1微秒内发生，通过循环等待或许是最好的实现方式。\n通常来说在操作设备硬件的代码中会采用这样的等待方式，如果你要求一个硬件完成一个任务，并且你知道硬件总是能非常快的完成任务，这时通过一个类似的循环等待或许是最正确的方式。\n另一方面，事件可能需要数个毫秒甚至你都不知道事件要多久才能发生，或许要10分钟其他的进程才能向Pipe写入数据，那么我们就不想在这一直循环并且浪费本可以用来完成其他任务的CPU时间。\n这时我们想要通过类似switch函数调用的方式出让CPU，并在我们关心的事件发生时重新获取CPU。\nCoordination就是有关出让CPU，直到等待的事件发生再恢复执行。\n人们发明了很多不同的Coordination的实现方式，但是与许多Unix风格操作系统一样，XV6使用的是Sleep\u0026amp;Wakeup这种方式。\n介绍完背景了，接下来我们看一下XV6的代码。\n为了准备这节课，我重写了UART的驱动代码，XV6通过这里的驱动代码从console中读写字符。\n首先是uartwrite函数。\n当shell需要输出时会调用write系统调用最终走到uartwrite函数中，这个函数会在循环中将buf中的字符一个一个的向UART硬件写入。\n这是一种经典的设备驱动实现风格，你可以在很多设备驱动中看到类似的代码。\nUART硬件一次只能接受一个字符的传输，而通常来说会有很多字符需要写到UART硬件。你可以向UART硬件写入一个字符，并等待UART硬件说：好的我完成了传输上一个字符并且准备好了传输下一个字符，之后驱动程序才可以写入下一个字符。\n因为这里的硬件可能会非常慢，或许每秒只能传输1000个字符，所以我们在两个字符之间的等待时间可能会很长。\n而1毫秒在现在计算机上是一个非常非常长的时间，它可能包含了数百万条指令时间，所以我们不想通过循环来等待UART完成字符传输，我们想通过一个更好的方式来等待。\n如大多数操作系统一样，XV6也的确存在更好的等待方式。\nUART硬件会在完成传输一个字符后，触发一个中断。\n所以UART驱动中除了uartwrite函数外，还有名为uartintr的中断处理程序。这个中断处理程序会在UART硬件触发中断时由trap.c代码调用。\n中断处理程序会在最开始读取UART对应的memory mapped register，并检查其中表明传输完成的相应的标志位，也就是LSR_TX_IDLE标志位。\n如果这个标志位为1，代码会将tx_done设置为1，并调用wakeup函数。\n这个函数会使得uartwrite中的sleep函数恢复执行，并尝试发送一个新的字符。\n所以这里的机制是，如果一个线程需要等待某些事件，比如说等待UART硬件愿意接收一个新的字符，线程调用sleep函数并等待一个特定的条件。\n当特定的条件满足时，代码会调用wakeup函数。\n这里的sleep函数和wakeup函数是成对出现的。\n我们之后会看sleep函数的具体实现，它会做很多事情最后再调用switch函数来出让CPU。\n这里有件事情需要注意，sleep和wakeup函数需要通过某种方式链接到一起。\n也就是说，如果我们调用wakeup函数，我们只想唤醒正在等待刚刚发生的特定事件的线程。\n所以，sleep函数和wakeup函数都带有一个叫做sleep channel的参数。\n我们在调用wakeup的时候，需要传入与调用sleep函数相同的sleep channel。\n不过sleep和wakeup函数只是接收表示了sleep channel的64bit数值，它们并不关心这个数值代表什么。\n当我们调用sleep函数时，我们通过一个sleep channel表明我们等待的特定事件，当调用wakeup时我们希望能传入相同的数值来表明想唤醒哪个线程。\n学生提问：进程会在写入每个字符时候都被唤醒一次吗？\nRobert教授：在这个我出于演示目的而特别改过的UART驱动中，传输每个字符都会有一个中断，所以你是对的，对于buffer中的每个字符，我们都会等待UART可以接收下一个字符，之后写入一个字符，将tx_done设置为0，回到循环的最开始并再次调用sleep函数进行睡眠状态，直到tx_done为1。当UART传输完了这个字符，uartintr函数会将tx_done设置为1，并唤醒uartwrite所在的线程。所以对于每个字符都有调用一次sleep和wakeup，并占用一次循环。\nUART实际上支持一次传输4或者16个字符，所以一个更有效的驱动会在每一次循环都传输16个字符给UART，并且中断也是每16个字符触发一次。更高速的设备，例如以太网卡通常会更多个字节触发一次中断。\n以上就是接口的演示。\nSleep\u0026amp;wakeup的一个优点是它们可以很灵活，它们不关心代码正在执行什么操作，你不用告诉sleep函数你在等待什么事件，你也不用告诉wakeup函数发生了什么事件，你只需要匹配好64bit的sleep channel就行。\n不过，对于sleep函数，有一个有趣的参数，我们需要将一个锁作为第二个参数传入，这背后是一个大的故事，我后面会介绍背后的原因。\n总的来说，不太可能设计一个sleep函数并完全忽略需要等待的事件。\n所以很难写一个通用的sleep函数，只是睡眠并等待一些特定的事件，并且这也很危险，因为可能会导致lost wakeup，而几乎所有的Coordination机制都需要处理lost wakeup的问题。\n在sleep接口中，我们需要传入一个锁是一种稍微丑陋的实现，我在稍后会再介绍。\nLost wakeup 在解释sleep函数为什么需要一个锁使用作为参数传入之前，我们先来看看假设我们有了一个更简单的不带锁作为参数的sleep函数，会有什么样的结果。这里的结果就是lost wakeup。\n假设sleep只是接收任意的sleep channel作为唯一的参数。\n它其实不能正常工作，我们称这个sleep实现为broken_sleep。\n你可以想象一个sleep函数内会将进程的状态设置为SLEEPING，表明当前进程不想再运行，而是正在等待一个特定的事件。\n如果你们看过了XV6的实现，你们可以发现sleep函数中还会做很多其他操作。\n我们需要记录特定的sleep channel值，这样之后的wakeup函数才能发现是当前进程正在等待wakeup对应的事件。\n最后再调用switch函数出让CPU。\n如果sleep函数只做了这些操作，那么很明显sleep函数会出问题，我们至少还应该在这里获取进程的锁。\n之后是wakeup函数。\n我们希望唤醒所有正在等待特定sleep channel的线程。\n所以wakeup函数中会查询进程表单中的所有进程，如果进程的状态是SLEEPING并且进程对应的channel是当前wakeup的参数，那么将进程的状态设置为RUNNABLE。\n在一些平行宇宙中，sleep\u0026amp;wakeup或许就是这么简单。\n在我回到XV6代码之前，让我演示一下如何在UART驱动中使用刚刚介绍的sleep和wakeup函数。\n首先是定义done标志位。之后是定义uartwrite函数。\n在函数中，对于buffer内的每一个字符，检查done标志位，如果标志位为0，就调用sleep函数并传入tx_channel。\n之后将字符传递给UART并将done设置为0。\n之后是中断处理函数uartintr。\n函数中首先将done标志位设置为1，并调用wakeup。\n以上就是使用broken_sleep的方式。这里缺失的是锁。\n这里uartwrite和uartintr两个函数需要使用锁来协调工作。\n第一个原因是done标志位，任何时候我们有了共享的数据，我们需要为这个数据加上锁。 另一个原因是两个函数都需要访问UART硬件，通常来说让两个线程并发的访问memory mapped register是错误的行为。 所以我们需要在两个函数中加锁来避免对于done标志位和硬件的竞争访问。\n现在的问题是，我们该在哪个位置加锁？在中断处理程序中较为简单，我们在最开始加锁，在最后解锁。\n难的是如何在uartwrite函数中加锁。一种可能是，每次发送一个字符的过程中持有锁，所以在每一次遍历buffer的起始和结束位置加锁和解锁。\n为什么这样肯定不能工作？一个原因是，我们能从while not done的循环退出的唯一可能是中断处理程序将done设置为1。\n但是如果我们为整个代码段都加锁的话，中断处理程序就不能获取锁了，中断程序会不停“自旋”并等待锁释放。\n而锁被uartwrite持有，在done设置为1之前不会释放。\n而done只有在中断处理程序获取锁之后才可能设置为1。\n所以我们不能在发送每个字符的整个处理流程都加锁。\n上面加锁方式的问题是，uartwrite在期望中断处理程序执行的同时又持有了锁。\n而我们唯一期望中断处理程序执行的位置就是sleep函数执行期间，其他的时候uartwrite持有锁是没有问题的。\n所以另一种实现可能是，在传输字符的最开始获取锁，因为我们需要保护共享变量done，但是在调用sleep函数之前释放锁。\n这样中断处理程序就有可能运行并且设置done标志位为1。\n之后在sleep函数返回时，再次获取锁。\n让我来修改代码，并看看相应的运行结果是什么。现有的代码中，uartwrite在最开始获取了锁，并在最后释放了锁。\n中断处理程序也在最开始获取锁，之后释放锁。\n接下来，我们会探索为什么只接收一个参数的broken_sleep在这不能工作。\n为了让锁能正常工作，我们需要在调用broken_sleep函数之前释放uart_tx_lock，并在broken_sleep返回时重新获取锁。\nbroken_sleep内的代码与之前在白板上演示的是一样的。也就是首先将进程状态设置为SLEEPING，并且保存tx_chan到进程结构体中，最后调用switch函数。\n接下来编译代码并看一下会发生什么。\n在XV6启动的时候会打印“init starting”，这里看来输出了一些字符之后就hang住了。如果我输入任意字符，剩下的字符就能输出。\n这里的问题必然与之前修改的代码相关。在前面的代码中，sleep之前释放了锁，但是在释放锁和broken_sleep之间可能会发生中断。\n一旦释放了锁，当前CPU的中断会被重新打开。\n因为这是一个多核机器，所以中断可能发生在任意一个CPU核。\n在上面代码标记的位置，其他CPU核上正在执行UART的中断处理程序，并且正在acquire函数中等待当前锁释放。\n所以一旦锁被释放了，另一个CPU核就会获取锁，并发现UART硬件完成了发送上一个字符，之后会设置tx_done为1，最后再调用wakeup函数，并传入tx_chan。\n目前为止一切都还好，除了一点：现在写线程还在执行并位于release和broken_sleep之间，也就是写线程还没有进入SLEEPING状态，所以中断处理程序中的wakeup并没有唤醒任何进程，因为还没有任何进程在tx_chan上睡眠。\n之后写线程会继续运行，调用broken_sleep，将进程状态设置为SLEEPING，保存sleep channel。\n但是中断已经发生了，wakeup也已经被调用了。所以这次的broken_sleep，没有人会唤醒它，因为wakeup已经发生过了。这就是lost wakeup问题。\n学生提问：是不是总是这样，一旦一个wakeup被丢失了，下一次wakeup时，之前缓存的数据会继续输出？\nRobert教授：这完全取决于实现细节。\n在我们的例子中，实际上出于偶然才会出现当我输入某些内容会导致之前的输出继续的现象。\n这里背后的原因是，我们的代码中，UART只有一个中断处理程序。\n不论是有输入，还是完成了一次输出，都会调用到同一个中断处理程序中。\n所以当我输入某些内容时，会触发输入中断，之后会调用uartintr函数。\n然后在中断处理程序中又会判断LSR_TX_IDLE标志位，并再次调用wakeup，所以刚刚的现象完全是偶然。\n如果出现了lost wakeup问题，并且你足够幸运的话，某些时候它们能自动修复。\n如果UART有不同的接收和发送中断处理程序的话，那么就没办法从lost wakeup恢复。\n学生提问：tx_done标志位的作用是什么？\nRobert教授：这是一种简单的在uartintr和uartwrite函数之间通信的方法。\ntx_done标志位为1表示已经完成了对于前一个字符的传输，并且uartwrite可以传输下一个字符，所以这是用来在中断处理程序和uartwrite之间通信的标志位。\n同一个学生提问：当从sleep函数中唤醒时，不是已经知道是来自UART的中断处理程序调用wakeup的结果吗？这样的话tx_done有些多余。\nRobert教授：我想你的问题也可以描述为：为什么需要通过一个循环while(tx_done == 0)来调用sleep函数？\n这个问题的答案适用于一个更通用的场景：实际中不太可能将sleep和wakeup精确匹配。\n并不是说sleep函数返回了，你等待的事件就一定会发生。\n举个例子，假设我们有两个进程同时想写UART，它们都在uartwrite函数中。\n可能发生这种场景，当一个进程写完一个字符之后，会进入SLEEPING状态并释放锁，而另一个进程可以在这时进入到循环并等待UART空闲下来。\n之后两个进程都进入到SLEEPING状态，当发生中断时UART可以再次接收一个字符，两个进程都会被唤醒，但是只有一个进程应该写入字符，所以我们才需要在sleep外面包一层while循环。\n实际上，你可以在XV6中的每一个sleep函数调用都被一个while循环包着。\n因为事实是，你或许被唤醒了，但是其他人将你等待的事件拿走了，所以你还得继续sleep。这种现象还挺普遍的。\n学生提问：我们只看到了一个lost wakeup，当我们随便输入一个字符，整个剩下的字符都能输出，为什么没有在输出剩下字符的时候再次发生lost wakeup？\nRobert教授：这会发生的。\n我来敲一下cat README，这会输出数千个字符。\n可以看到每过几个字符就会hang一次，需要我再次输入某个字符。\n这个过程我们可以看到很多lost wakeup。\n之前之所以没有出现，是因为lost wakeup需要中断已经在等待获取锁，并且uartwrite位于release和broken_sleep之间，这需要一定的巧合并不总是会发生。\n如何避免Lost wakeup 现在我们的目标是消灭掉lost wakeup。这可以通过消除下面的窗口时间来实现。\n首先我们必须要释放uart_tx_lock锁，因为中断需要获取这个锁，但是我们又不能在释放锁和进程将自己标记为SLEEPING之间留有窗口。\n这样中断处理程序中的wakeup才能看到SLEEPING状态的进程，并将其唤醒，进而我们才可以避免lost wakeup的问题。\n所以，我们应该消除这里的窗口。\n为了实现这个目的，我们需要将sleep函数设计的稍微复杂点。\n这里的解决方法是，即使sleep函数不需要知道你在等待什么事件，它还是需要你知道你在等待什么数据，并且传入一个用来保护你在等待数据的锁。\nsleep函数需要特定的条件才能执行，而sleep自己又不需要知道这个条件是什么。\n在我们的例子中，sleep函数执行的特定条件是tx_done等于1。\n虽然sleep不需要知道tx_done，但是它需要知道保护这个条件的锁，也就是这里的uart_tx_lock。\n在调用sleep的时候，锁还被当前线程持有，之后这个锁被传递给了sleep。\n在接口层面，sleep承诺可以原子性的将进程设置成SLEEPING状态，同时释放锁。\n这样wakeup就不可能看到这样的场景：锁被释放了但是进程还没有进入到SLEEPING状态。\n所以sleep这里将释放锁和设置进程为SLEEPING状态这两个行为合并为一个原子操作。\n所以我们需要有一个锁来保护sleep的条件，并且这个锁需要传递给sleep作为参数。\n更进一步的是，当调用wakeup时，锁必须被持有。\n如果程序员想要写出正确的代码，都必须遵守这些规则来使用sleep和wakeup。\n接下来我们看一下sleep和wakeup如何使用这一小块额外的信息（注，也就是传入给sleep函数的锁）和刚刚提到的规则，来避免lost wakeup。\n首先我们来看一下proc.c中的wakeup函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //Wake up all processes sleeping on chan. //Must be called without any p-\u0026gt;lock. void wakeup(void *chan) { struct proc *p; for(p = proc;p \u0026lt; \u0026amp;proc[NPROC];p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state =SLEEPING \u0026amp;\u0026amp;p-\u0026gt;chan =chan){ p-\u0026gt;state RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); } wakeup函数并不十分出人意料。\n它查看整个进程表单，对于每个进程首先加锁，这点很重要。\n之后查看进程的状态，如果进程当前是SLEEPING并且进程的channel与wakeup传入的channel相同，将进程的状态设置为RUNNABLE。最后再释放进程的锁。\n接下来我们忽略broken_sleep，直接查看带有锁作为参数的sleep函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Atomically release lock and sleep on chan. // Reacquires lock when awakened. void sleep(void *chan, struct spinlock *lk) { struct proc *p = myproc(); // Must acquire p-\u0026gt;lock in order to // change p-\u0026gt;state and then call sched. // Once we hold p-\u0026gt;lock, we can be // guaranteed that we won\u0026#39;t miss any wakeup // (wakeup locks p-\u0026gt;lock), // so it\u0026#39;s okay to release lk. if(lk != \u0026amp;p-\u0026gt;lock){ //DOC: sleeplock0 acquire(\u0026amp;p-\u0026gt;lock); //DOC: sleeplock1 release(lk); } // Go to sleep. p-\u0026gt;chan = chan; p-\u0026gt;state = SLEEPING; sched(); // Tidy up. p-\u0026gt;chan = 0; // Reacquire original lock. if(lk != \u0026amp;p-\u0026gt;lock){ release(\u0026amp;p-\u0026gt;lock); acquire(lk); } } 我们已经知道了sleep函数需要释放作为第二个参数传入的锁，这样中断处理程序才能获取锁。\n函数中第一件事情就是释放这个锁。\n当然在释放锁之后，我们会担心在这个时间点相应的wakeup会被调用并尝试唤醒当前进程，而当前进程还没有进入到SLEEPING状态。\n所以我们不能让wakeup在release锁之后执行。\n为了让它不在release锁之后执行，在release锁之前，sleep会获取即将进入SLEEPING状态的进程的锁。\n如果你还记得的话，wakeup在唤醒一个进程前，需要先获取进程的锁。\n所以在整个时间uartwrite检查条件之前到sleep函数中调用sched函数之间，这个线程一直持有了保护sleep条件的锁或者p-\u0026gt;lock。\n让我回到UART的代码并强调一下这一点。\nuartwrite在最开始获取了sleep的condition lock，并且一直持有condition lock直到调用sleep函数。\n所以它首先获取了condition lock，之后检查condition（注，也就是tx_done等于0），之后在持有condition lock的前提下调用了sleep函数。\n此时wakeup不能做任何事情，wakeup现在甚至都不能被调用直到调用者能持有condition lock。所以现在wakeup必然还没有执行。\nsleep函数在释放condition lock之前，先获取了进程的锁。\n在释放了condition lock之后，wakeup就可以被调用了，但是除非wakeup获取了进程的锁，否则wakeup不能查看进程的状态。\n所以，在sleep函数中释放了condition lock之后，wakeup也还没有执行。\n在持有进程锁的时候，将进程的状态设置为SLEEPING并记录sleep channel，之后再调用sched函数，这个函数中会再调用switch函数（注，详见11.6），此时sleep函数中仍然持有了进程的锁，wakeup仍然不能做任何事情。\n当我们从当前线程切换走时，调度器线程中会释放前一个进程的锁（注，详见11.8）。\n所以在调度器线程释放进程锁之后，wakeup才能终于获取进程的锁，发现它正在SLEEPING状态，并唤醒它。\n这里的效果是由之前定义的一些规则确保的，这些规则包括了：\n调用sleep时需要持有condition lock，这样sleep函数才能知道相应的锁。 sleep函数只有在获取到进程的锁p-\u0026gt;lock之后，才能释放condition lock。 wakeup需要同时持有两个锁才能查看进程。 这样的话，我们就不会再丢失任何一个wakeup，也就是说我们修复了lost wakeup的问题。\nPipe中的sleep和wakeup 前面我们介绍了在UART的驱动中，如何使用sleep和wakeup才能避免lost wakeup。\n前面这个特定的场景中，sleep等待的condition是发生了中断并且硬件准备好了传输下一个字符。\n在一些其他场景，内核代码会调用sleep函数并等待其他的线程完成某些事情。\n这些场景从概念上来说与我们介绍之前的场景没有什么区别，但是感觉上还是有些差异。\n例如，在读写pipe的代码中，如果你查看pipe.c中的piperead函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 int piperead(struct pipe *pi, uint64 addr, int n) { int i; struct proc *pr = myproc(); char ch; acquire(\u0026amp;pi-\u0026gt;lock); while(pi-\u0026gt;nread == pi-\u0026gt;nwrite \u0026amp;\u0026amp; pi-\u0026gt;writeopen){ //DOC: pipe-empty if(pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } sleep(\u0026amp;pi-\u0026gt;nread, \u0026amp;pi-\u0026gt;lock); //DOC: piperead-sleep } for(i = 0; i \u0026lt; n; i++){ //DOC: piperead-copy if(pi-\u0026gt;nread == pi-\u0026gt;nwrite) break; ch = pi-\u0026gt;data[pi-\u0026gt;nread++ % PIPESIZE]; if(copyout(pr-\u0026gt;pagetable, addr + i, \u0026amp;ch, 1) == -1) break; } wakeup(\u0026amp;pi-\u0026gt;nwrite); //DOC: piperead-wakeup release(\u0026amp;pi-\u0026gt;lock); return i; } 这里有很多无关的代码可以忽略。\n当read系统调用最终调用到piperead函数时，pi-\u0026gt;lock会用来保护pipe，这就是sleep函数对应的condition lock。\npiperead需要等待的condition是pipe中有数据，而这个condition就是pi-\u0026gt;nwrite大于pi-\u0026gt;nread，也就是写入pipe的字节数大于被读取的字节数。\n如果这个condition不满足，那么piperead会调用sleep函数，并等待condition发生。\n同时piperead会将condition lock也就是pi-\u0026gt;lock作为参数传递给sleep函数，以确保不会发生lost wakeup。\n之所以会出现lost wakeup，是因为在一个不同的CPU核上可能有另一个线程刚刚调用了pipewrite。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 int pipewrite(struct pipe *pi, uint64 addr, int n) { int i; char ch; struct proc *pr = myproc(); acquire(\u0026amp;pi-\u0026gt;lock); for(i = 0; i \u0026lt; n; i++){ while(pi-\u0026gt;nwrite == pi-\u0026gt;nread + PIPESIZE){ //DOC: pipewrite-full if(pi-\u0026gt;readopen == 0 || pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } wakeup(\u0026amp;pi-\u0026gt;nread); sleep(\u0026amp;pi-\u0026gt;nwrite, \u0026amp;pi-\u0026gt;lock); } if(copyin(pr-\u0026gt;pagetable, \u0026amp;ch, addr + i, 1) == -1) break; pi-\u0026gt;data[pi-\u0026gt;nwrite++ % PIPESIZE] = ch; } wakeup(\u0026amp;pi-\u0026gt;nread); release(\u0026amp;pi-\u0026gt;lock); return i; } pipewrite会向pipe的缓存写数据，并最后在piperead所等待的sleep channel上调用wakeup。\n而我们想要避免这样的风险：在piperead函数检查发现没有字节可以读取，到piperead函数调用sleep函数之间，另一个CPU调用了pipewrite函数。\n因为这样的话，另一个CPU会向pipe写入数据并在piperead进程进入SLEEPING之前调用wakeup，进而产生一次lost wakeup。\n在pipe的代码中，pipewrite和piperead都将sleep包装在一个while循环中。\npiperead中的循环等待pipe的缓存为非空（pipewrite中的循环等待的是pipe的缓存不为full）。\n之所以要将sleep包装在一个循环中，是因为可能有多个进程在读取同一个pipe。\n但是因为pipe中只有一个字节并且总是有一个进程能够先被唤醒。\nsleep函数中最后一件事情就是重新获取condition lock。所以调用sleep函数的时候，需要对condition lock上锁（注，在sleep函数内部会对condition lock解锁），在sleep函数返回时会重新对condition lock上锁。\n这样第一个被唤醒的线程会持有condition lock，而其他的线程在重新对condition lock上锁的时候会在锁的acquire函数中等待。\n那个幸运的进程（注，这里线程和进程描述的有些乱，但是基本意思是一样的，当说到线程时是指进程唯一的内核线程）会从sleep函数中返回，之后通过检查可以发现pi-\u0026gt;nwrite比pi-\u0026gt;nread大1，所以进程可以从piperead的循环中退出，并读取一个字节，之后pipe缓存中就没有数据了。\n之后piperead函数释放锁并返回。接下来，第二个被唤醒的线程，它的sleep函数可以获取condition lock并返回，但是通过检查发现pi-\u0026gt;nwrite等于pi-\u0026gt;nread（注，因为唯一的字节已经被前一个进程读走了），所以这个线程以及其他所有的等待线程都会重新进入sleep函数。\n所以这里也可以看出，几乎所有对于sleep的调用都需要包装在一个循环中，这样从sleep中返回的时候才能够重新检查condition是否还符合。\nsleep和wakeup的规则稍微有点复杂。\n因为你需要向sleep展示你正在等待什么数据，你需要传入锁并遵循一些规则，某些时候这些规则还挺烦人的。\n另一方面sleep和wakeup又足够灵活，因为它们并不需要理解对应的condition，只是需要有个condition和保护这个condition的锁。\n除了sleep\u0026amp;wakeup之外，还有一些其他的更高级的Coordination实现方式。\n例如今天课程的阅读材料中的semaphore，它的接口就没有那么复杂，你不用告诉semaphore有关锁的信息。而semaphore的调用者也不需要担心lost wakeup的问题，在semaphore的内部实现中考虑了lost wakeup问题。\n因为定制了up-down计数器，所以semaphore可以在不向接口泄露数据的同时（注，也就是不需要向接口传递condition lock），处理lost wakeup问题。\nsemaphore某种程度来说更简单，尽管它也没那么通用，如果你不是在等待一个计数器，semaphore也就没有那么有用了。这也就是为什么我说sleep和wakeup更通用的原因。\nexit系统调用 接下来，我想讨论一下XV6面临的一个与Sleep\u0026amp;Wakeup相关的挑战，也就是如何关闭一个进程。\n每个进程最终都需要退出，我们需要清除进程的状态，释放栈。\n在XV6中，一个进程如果退出的话，我们需要释放用户内存，释放page table，释放trapframe对象，将进程在进程表单中标为REUSABLE，这些都是典型的清理步骤。\n当进程退出或者被杀掉时，有许多东西都需要被释放。\n这里会产生的两大问题：\n首先我们不能直接单方面的摧毁另一个线程，因为：另一个线程可能正在另一个CPU核上运行，并使用着自己的栈；也可能另一个线程正在内核中持有了锁；也可能另一个线程正在更新一个复杂的内核数据，如果我们直接就把线程杀掉了，我们可能在线程完成更新复杂的内核数据过程中就把线程杀掉了。我们不能让这里的任何一件事情发生。 另一个问题是，即使一个线程调用了exit系统调用，并且是自己决定要退出。它仍然持有了运行代码所需要的一些资源，例如它的栈，以及它在进程表单中的位置。当它还在执行代码，它就不能释放正在使用的资源。所以我们需要一种方法让线程能释放最后几个对于运行代码来说关键的资源。 XV6有两个函数与关闭线程进程相关。\n第一个是exit，第二个是kill。\n让我们先来看位于proc.c中的exit函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 // Exit the current process. Does not return. // An exited process remains in the zombie state // until its parent calls wait(). void exit(int status) { struct proc *p = myproc(); if(p == initproc) panic(\u0026#34;init exiting\u0026#34;); // Close all open files. for(int fd = 0; fd \u0026lt; NOFILE; fd++){ if(p-\u0026gt;ofile[fd]){ struct file *f = p-\u0026gt;ofile[fd]; fileclose(f); p-\u0026gt;ofile[fd] = 0; } } begin_op(); iput(p-\u0026gt;cwd); end_op(); p-\u0026gt;cwd = 0; // we might re-parent a child to init. we can\u0026#39;t be precise about // waking up init, since we can\u0026#39;t acquire its lock once we\u0026#39;ve // acquired any other proc lock. so wake up init whether that\u0026#39;s // necessary or not. init may miss this wakeup, but that seems // harmless. acquire(\u0026amp;initproc-\u0026gt;lock); wakeup1(initproc); release(\u0026amp;initproc-\u0026gt;lock); // grab a copy of p-\u0026gt;parent, to ensure that we unlock the same // parent we locked. in case our parent gives us away to init while // we\u0026#39;re waiting for the parent lock. we may then race with an // exiting parent, but the result will be a harmless spurious wakeup // to a dead or wrong process; proc structs are never re-allocated // as anything else. acquire(\u0026amp;p-\u0026gt;lock); struct proc *original_parent = p-\u0026gt;parent; release(\u0026amp;p-\u0026gt;lock); // we need the parent\u0026#39;s lock in order to wake it up from wait(). // the parent-then-child rule says we have to lock it first. acquire(\u0026amp;original_parent-\u0026gt;lock); acquire(\u0026amp;p-\u0026gt;lock); // Give any children to init. reparent(p); // Parent might be sleeping in wait(). wakeup1(original_parent); p-\u0026gt;xstate = status; p-\u0026gt;state = ZOMBIE; release(\u0026amp;original_parent-\u0026gt;lock); // Jump into the scheduler, never to return. sched(); panic(\u0026#34;zombie exit\u0026#34;); } 这就是exit系统调用的内容。\n从exit接口的整体来看，在最后它会释放进程的内存和page table，关闭已经打开的文件，同时我们也知道父进程会从wait系统调用中唤醒，所以exit最终会导致父进程被唤醒。这些都是我们预期可以从exit代码中看到的内容。\n从上面的代码中，首先exit函数关闭了所有已打开的文件。\n这里可能会很复杂，因为关闭文件系统中的文件涉及到引用计数，虽然我们还没学到但是这里需要大量的工作。\n不管怎样，一个进程调用exit系统调用时，会关闭所有自己拥有的文件。\n接下来是类似的处理，进程有一个对于当前目录的记录，这个记录会随着你执行cd指令而改变。\n在exit过程中也需要将对这个目录的引用释放给文件系统。\n如果一个进程要退出，但是它又有自己的子进程，接下来需要设置这些子进程的父进程为init进程。\n我们接下来会看到，每一个正在exit的进程，都有一个父进程中的对应的wait系统调用。\n父进程中的wait系统调用会完成进程退出最后的几个步骤。\n所以如果父进程退出了，那么子进程就不再有父进程，当它们要退出时就没有对应的父进程的wait。\n所以在exit函数中，会为即将exit进程的子进程重新指定父进程为init进程，也就是PID为1的进程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // we need the parent\u0026#39;s lock in order to wake it up from wait(). // the parent-then-child rule says we have to lock it first. acquire(\u0026amp;original_parent-\u0026gt;lock); acquire(\u0026amp;p-\u0026gt;lock); // Give any children to init. reparent(p); // Parent might be sleeping in wait(). wakeup1(original_parent); p-\u0026gt;xstate = status; p-\u0026gt;state = ZOMBIE; release(\u0026amp;original_parent-\u0026gt;lock); // Jump into the scheduler, never to return. sched(); panic(\u0026#34;zombie exit\u0026#34;); } 之后，我们需要通过调用wakeup函数唤醒当前进程的父进程，当前进程的父进程或许正在等待当前进程退出。\n接下来，进程的状态被设置为ZOMBIE。\n现在进程还没有完全释放它的资源，所以它还不能被重用。\n所谓的进程重用是指，我们期望在最后，进程的所有状态都可以被一些其他无关的fork系统调用复用，但是目前我们还没有到那一步。\n现在我们还没有结束，因为我们还没有释放进程资源。我们在还没有完全释放所有资源的时候，通过调用sched函数进入到调度器线程。\n到目前位置，进程的状态是ZOMBIE，并且进程不会再运行，因为调度器只会运行RUNNABLE进程。\n同时进程资源也并没有完全释放，如果释放了进程的状态应该是UNUSED。\n但是可以肯定的是进程不会再运行了，因为它的状态是ZOMBIE。所以调度器线程会决定运行其他的进程。\nwait系统调用 通过Unix的exit和wait系统调用的说明，我们可以知道如果一个进程exit了，并且它的父进程调用了wait系统调用，父进程的wait会返回。\nwait函数的返回表明当前进程的一个子进程退出了。\n接下来看一下wait系统调用的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 // Wait for a child process to exit and return its pid. // Return -1 if this process has no children. int wait(uint64 addr) { struct proc *np; int havekids, pid; struct proc *p = myproc(); // hold p-\u0026gt;lock for the whole time to avoid lost // wakeups from a child\u0026#39;s exit(). acquire(\u0026amp;p-\u0026gt;lock); for(;;){ // Scan through table looking for exited children. havekids = 0; for(np = proc; np \u0026lt; \u0026amp;proc[NPROC]; np++){ // this code uses np-\u0026gt;parent without holding np-\u0026gt;lock. // acquiring the lock first would cause a deadlock, // since np might be an ancestor, and we already hold p-\u0026gt;lock. if(np-\u0026gt;parent == p){ // np-\u0026gt;parent can\u0026#39;t change between the check and the acquire() // because only the parent changes it, and we\u0026#39;re the parent. acquire(\u0026amp;np-\u0026gt;lock); havekids = 1; if(np-\u0026gt;state == ZOMBIE){ // Found one. pid = np-\u0026gt;pid; if(addr != 0 \u0026amp;\u0026amp; copyout(p-\u0026gt;pagetable, addr, (char *)\u0026amp;np-\u0026gt;xstate, sizeof(np-\u0026gt;xstate)) \u0026lt; 0) { release(\u0026amp;np-\u0026gt;lock); release(\u0026amp;p-\u0026gt;lock); return -1; } freeproc(np); release(\u0026amp;np-\u0026gt;lock); release(\u0026amp;p-\u0026gt;lock); return pid; } release(\u0026amp;np-\u0026gt;lock); } } // No point waiting if we don\u0026#39;t have any children. if(!havekids || p-\u0026gt;killed){ release(\u0026amp;p-\u0026gt;lock); return -1; } // Wait for a child to exit. sleep(p, \u0026amp;p-\u0026gt;lock); //DOC: wait-sleep } } 它里面包含了一个大的循环。\n当一个进程调用了wait系统调用，它会扫描进程表单，找到父进程是自己且状态是ZOMBIE的进程。\n从上一节可以知道，这些进程已经在exit函数中几乎要执行完了。\n之后由父进程调用的freeproc函数，来完成释放进程资源的最后几个步骤。\n看一下freeproc的实现，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // free a proc structure and the data hanging from it, // including user pages. // p-\u0026gt;lock must be held. static void freeproc(struct proc *p) { if(p-\u0026gt;trapframe) kfree((void*)p-\u0026gt;trapframe); p-\u0026gt;trapframe = 0; if(p-\u0026gt;pagetable) proc_freepagetable(p-\u0026gt;pagetable, p-\u0026gt;sz); p-\u0026gt;pagetable = 0; p-\u0026gt;sz = 0; p-\u0026gt;pid = 0; p-\u0026gt;parent = 0; p-\u0026gt;name[0] = 0; p-\u0026gt;chan = 0; p-\u0026gt;killed = 0; p-\u0026gt;xstate = 0; p-\u0026gt;state = UNUSED; } 这是关闭一个进程的最后一些步骤。\n如果由正在退出的进程自己在exit函数中执行这些步骤，将会非常奇怪。\n这里释放了trapframe，释放了page table。如果我们需要释放进程内核栈，那么也应该在这里释放。\n但是因为内核栈的guard page，我们没有必要再释放一次内核栈。\n不管怎样，当进程还在exit函数中运行时，任何这些资源在exit函数中释放都会很难受，所以这些资源都是由父进程释放的。\nwait不仅是为了父进程方便的知道子进程退出，wait实际上也是进程退出的一个重要组成部分。\n在Unix中，对于每一个退出的进程，都需要有一个对应的wait系统调用，这就是为什么当一个进程退出时，它的子进程需要变成init进程的子进程。\ninit进程的工作就是在一个循环中不停调用wait，因为每个进程都需要对应一个wait，这样它的父进程才能调用freeproc函数，并清理进程的资源。\n当父进程完成了清理进程的所有资源，子进程的状态会被设置成UNUSED。\n之后，fork系统调用才能重用进程在进程表单的位置。\n学生提问：在exit系统调用中，为什么需要在重新设置父进程之前，先获取当前进程的父进程？\nRobert教授：这里其实就是在防止一个进程和它的父进程同时退出。\n通常情况下，一个进程exit，它的父进程正在wait，一切都正常。\n但是也可能一个进程和它的父进程同时exit。\n所以当子进程尝试唤醒父进程，并告诉它自己退出了时，父进程也在退出。\n它应该是处理这种父进程和子进程同时退出的情况。\n如果不是这种情况的话，一切都会非常直观，子进程会在后面通过wakeup函数唤醒父进程。\n学生提问：为什么我们在唤醒父进程之后才将进程的状态设置为ZOMBIE？难道我们不应该在之前就设置吗？\nRobert教授：正在退出的进程会先获取自己进程的锁，同时，因为父进程的wait系统调用中也需要获取子进程的锁，所以父进程并不能查看正在执行exit函数的进程的状态。\n这意味着，正在退出的进程获取自己的锁到它调用sched进入到调度器线程之间（注，因为调度器线程会释放进程的锁），父进程并不能看到这之间代码引起的中间状态。\n所以这之间的代码顺序并不重要。\n大部分时候，如果没有持有锁，exit中任何代码顺序都不能工作。\n因为有了锁，代码的顺序就不再重要，因为父进程也看不到进程状态。\n所以，直到子进程exit的最后，它都没有释放所有的资源，因为它还在运行的过程中，所以不能释放这些资源。\n相应的其他的进程，也就是父进程，释放了运行子进程代码所需要的资源。这样的设计可以让我们极大的精简exit的实现。\nkill系统调用 Unix中的一个进程可以将另一个进程的ID传递给kill系统调用，并让另一个进程停止运行。\n如果我们不够小心的话，kill一个还在内核执行代码的进程，会有一些我几分钟前介绍过的风险，比如我们想要杀掉的进程的内核线程还在更新一些数据，比如说更新文件系统，创建一个文件。\n如果这样的话，我们不能就这样杀掉进程，因为这样会使得一些需要多步完成的操作只执行了一部分。\n所以kill系统调用不能就直接停止目标进程的运行。\n实际上，在XV6和其他的Unix系统中，kill系统调用基本上不做任何事情。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // Kill the process with the given pid. // The victim won\u0026#39;t exit until it tries to return // to user space (see usertrap() in trap.c). int kill(int pid) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;pid == pid){ p-\u0026gt;killed = 1; if(p-\u0026gt;state == SLEEPING){ // Wake process from sleep(). p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); return 0; } release(\u0026amp;p-\u0026gt;lock); } return -1; } 它先扫描进程表单，找到目标进程。\n然后只是将进程的proc结构体中killed标志位设置为1。\n如果进程正在SLEEPING状态，将其设置为RUNNABLE。\n这里只是将killed标志位设置为1，并没有停止进程的运行。\n所以kill系统调用本身还是很温和的。\n而目标进程运行到内核代码中能安全停止运行的位置时，会检查自己的killed标志位，如果设置为1，目标进程会自愿的执行exit系统调用。\n你可以在trap.c中看到所有可以安全停止运行的位置，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // // handle an interrupt, exception, or system call from user space. // called from trampoline.S // void usertrap(void) { int which_dev = 0; if((r_sstatus() \u0026amp; SSTATUS_SPP) != 0) panic(\u0026#34;usertrap: not from user mode\u0026#34;); // send interrupts and exceptions to kerneltrap(), // since we\u0026#39;re now in the kernel. w_stvec((uint64)kernelvec); struct proc *p = myproc(); // save user program counter. p-\u0026gt;trapframe-\u0026gt;epc = r_sepc(); if(r_scause() == 8){ // system call if(p-\u0026gt;killed) exit(-1); // sepc points to the ecall instruction, // but we want to return to the next instruction. p-\u0026gt;trapframe-\u0026gt;epc += 4; // an interrupt will change sstatus \u0026amp;c registers, // so don\u0026#39;t enable until done with those registers. intr_on(); syscall(); } else if((which_dev = devintr()) != 0){ // ok } else { printf(\u0026#34;usertrap(): unexpected scause %p pid=%d\\n\u0026#34;, r_scause(), p-\u0026gt;pid); printf(\u0026#34; sepc=%p stval=%p\\n\u0026#34;, r_sepc(), r_stval()); p-\u0026gt;killed = 1; } if(p-\u0026gt;killed) exit(-1); 在usertrap函数中（注，详见6.6），在执行系统调用之前，如果进程已经被kill了，进程会自己调用exit。\n在这个内核代码位置，代码并没有持有任何锁，也不在执行任何操作的过程中，所以进程通过exit退出是完全安全的。\n类似的，在usertrap函数的最后，也有类似的代码。\n在执行完系统调用之后，进程会再次检查自己是否已经被kill了。\n即使进程是被中断打断，这里的检查也会被执行。\n例如当一个定时器中断打断了进程的运行，我们可以通过检查发现进程是killed状态，之后进程会调用exit退出。\n所以kill系统调用并不是真正的立即停止进程的运行，它更像是这样：如果进程在用户空间，那么下一次它执行系统调用它就会退出，又或者目标进程正在执行用户代码，当时下一次定时器中断或者其他中断触发了，进程才会退出。\n所以从一个进程调用kill，到另一个进程真正退出，中间可能有很明显的延时。\n这里有个很直观问题：如果进程不在用户空间执行，而是正在执行系统调用的过程中，然后它被kill了，我们需要做什么特别的操作吗？\n之所以会提出这个问题，是因为进程可能正在从console读取即将输入的字符，而你可能要明天才会输入一个字符，所以当你kill一个进程时，最好进程不是等到明天才退出。\n出于这个原因，在XV6的很多位置中，如果进程在SLEEPING状态时被kill了，进程会实际的退出。\n首先要看的是kill函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int kill(int pid) { struct proc *p; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++){ acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;pid == pid){ p-\u0026gt;killed = 1; if(p-\u0026gt;state == SLEEPING){ // Wake process from sleep(). p-\u0026gt;state = RUNNABLE; } release(\u0026amp;p-\u0026gt;lock); return 0; } release(\u0026amp;p-\u0026gt;lock); } return -1; } 可以看到如果目标进程是SLEEPING状态，kill函数会将其状态设置为RUNNABLE，这意味着，即使进程之前调用了sleep并进入到SLEEPING状态，调度器现在会重新运行进程，并且进程会从sleep中返回。\n让我们来查看一下这在哪生效的。\n在pipe.c的piperead函数中，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 int piperead(struct pipe *pi, uint64 addr, int n) { int i; struct proc *pr = myproc(); char ch; acquire(\u0026amp;pi-\u0026gt;lock); while(pi-\u0026gt;nread == pi-\u0026gt;nwrite \u0026amp;\u0026amp; pi-\u0026gt;writeopen){ //DOC: pipe-empty if(pr-\u0026gt;killed){ release(\u0026amp;pi-\u0026gt;lock); return -1; } sleep(\u0026amp;pi-\u0026gt;nread, \u0026amp;pi-\u0026gt;lock); //DOC: piperead-sleep } for(i = 0; i \u0026lt; n; i++){ //DOC: piperead-copy if(pi-\u0026gt;nread == pi-\u0026gt;nwrite) break; ch = pi-\u0026gt;data[pi-\u0026gt;nread++ % PIPESIZE]; if(copyout(pr-\u0026gt;pagetable, addr + i, \u0026amp;ch, 1) == -1) break; } wakeup(\u0026amp;pi-\u0026gt;nwrite); //DOC: piperead-wakeup release(\u0026amp;pi-\u0026gt;lock); return i; } 如果一个进程正在sleep状态等待从pipe中读取数据，然后它被kill了。\nkill函数会将其设置为RUNNABLE，之后进程会从sleep中返回，返回到循环的最开始。\npipe中大概率还是没有数据，之后在piperead中，会判断进程是否被kill了（注，if(pr-\u0026gt;killed)）。\n如果进程被kill了，那么接下来piperead会返回-1，并且返回到usertrap函数的syscall位置，因为piperead就是一种系统调用的实现。\n之后在usertrap函数中会检查p-\u0026gt;killed，并调用exit。\n所以对于SLEEPING状态的进程，如果它被kill了，它会被直接唤醒，包装了sleep的循环会检查进程的killed标志位，最后再调用exit。\n同时还有一些情况，如果进程在SLEEPING状态中被kill了并不能直接退出。\n例如，一个进程正在更新一个文件系统并创建一个文件的过程中，进程不适宜在这个时间点退出，因为我们想要完成文件系统的操作，之后进程才能退出。\n我会向你展示一个磁盘驱动中的sleep循环，这个循环中就没有检查进程的killed标志位。\n下面就是virtio_disk.c文件中的一段代码：\n1 2 3 4 //Wait for virtio_disk_intr()to say request has finished. while(b-\u0026gt;disk == 1){ sleep(b, \u0026amp;disk.vdisk_lock); } 这里一个进程正在等待磁盘的读取结束，这里没有检查进程的killed标志位。\n因为现在可能正在创建文件的过程中，而这个过程涉及到多次读写磁盘。\n我们希望完成所有的文件系统操作，完成整个系统调用，之后再检查p-\u0026gt;killed并退出。\n学生提问：为什么一个进程允许kill另一个进程？这样一个进程不是能杀掉所有其他进程吗？\nRobert教授：如果你在MIT的分时复用计算机Athena上这么做的话，他们可能会开除你。\n在XV6中允许这么做是因为，XV6这是个教学用的操作系统，任何与权限相关的内容在XV6中都不存在。\n在Linux或者真正的操作系统中，每个进程都有一个user id或多或少的对应了执行进程的用户，一些系统调用使用进程的user id来检查进程允许做的操作。\n所以在Linux中会有额外的检查，调用kill的进程必须与被kill的进程有相同的user id，否则的话，kill操作不被允许。\n所以，在一个分时复用的计算机上，我们会有多个用户，我们不会想要用户kill其他人的进程，这样一套机制可以防止用户误删别人的进程。\n学生提问：init进程会退出吗？\nRobert教授：让我来看看。\n是的，如果fork失败了，init进程也会退出。不过，这个问题的真正的答案是，不会退出。init进程的目标就是不退出，它就是在一个循环中不停的调用wait。如果init进程退出了，我认为这是一个Fatal级别的错误，然后系统会崩溃。在exit函数的最开始就会有如下检查\n1 2 3 4 5 6 7 void exit(int status) { struct proc *p = myproc(); if(p == initproc) panic(\u0026#34;init exiting\u0026#34;); 如果调用exit的进程是init进程，那么会触发panic。\n因为如果没有init进程的话，系统最终还是会停止运行。\n如果没有init进程的话就没有人会为退出的进程调用wait系统调用，也就没有人完成进程资源的释放工作，我们最终会用光所有的进程，并引起一些其他的错误，所以我们必须要有init进程。所以这个问题的真正答案是init进程不允许退出。\n学生提问：这节课可能没有怎么讲到，但是如果关闭一个操作系统会发生什么？\nRobert教授：这个过程非常复杂，并且依赖于你运行的是什么系统。\n因为文件系统是持久化的，它能在多次重启之间保持数据，我们需要保持文件系统的良好状态，如果我们正在更新文件系统的过程中，例如创建文件，然后我们想关闭操作系统，断电之类的。\n我们需要一个策略来确保即使我们正在一个复杂的更新文件系统的过程中，我们并不会破坏磁盘上的文件系统数据。\n文件系统其实就是一个位于磁盘的数据结构。\n所以这里涉及到了很多的机制来确保如果你关闭操作系统或者因为断电之类，我们可以恢复磁盘上的文件系统。\n其他的，你是否需要做一些特殊的操作来关闭系统，取决于你正在运行什么进程。\n如果你正在运行一些重要的服务器，例如数据库服务器，并且许多其他计算机依赖这个数据库并通过网络使用它。\n那谁知道呢？答案或许是你不能就这么直接关闭操作系统，因为你正在提供一个对于其他计算机来说非常关键的服务。\n如果你的计算机并没有在做任何事情，那么你可以直接关闭它。\n或许对于你的问题来说，如果你想关闭一个计算机，确保文件系统是正确的，之后停止执行指令，之后就可以关闭计算机了。\n","date":"2024-09-22T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08113sleep-wake-up/bg_hu_71b39a936deda7f9.jpg","permalink":"https://echudet.github.io/p/mit6s08113sleep-wake-up/","title":"MIT6S081：13Sleep \u0026 Wake up"},{"content":"Lecture 11 Thread switching 预习 多路复用 Xv6通过在两种情况下将每个CPU从一个进程切换到另一个进程来实现多路复用（Multiplexing）。\n第一：当进程等待设备或管道I/O完成，或等待子进程退出，或在sleep系统调用中等待时，xv6使用睡眠（sleep）和唤醒（wakeup）机制切换。\n第二：xv6周期性地强制切换以处理长时间计算而不睡眠的进程。\n这种多路复用产生了每个进程都有自己的CPU的错觉，就像xv6使用内存分配器和硬件页表来产生每个进程都有自己内存的错觉一样。\n什么是多路复用 多路复用（Multiplexing） 是一种在通信或计算领域的技术，允许多个信号或数据流通过同一个资源传输或处理。\n它的主要目的是提高资源的利用效率，使得多个信号能够共享有限的硬件或通信通道，而不会相互干扰。\n在多路复用中，多个独立的数据流通过某种机制被组合在一起，在同一个信道或设备上传输或处理。\n多路复用通常在网络通信、操作系统资源管理等场景中广泛使用。\n多路复用的挑战 实现多路复用带来了一些挑战。\n首先，如何从一个进程切换到另一个进程？尽管上下文切换的思想很简单，但它的实现是xv6中最不透明的代码之一。\n第二，如何以对用户进程透明的方式强制切换？Xv6使用标准技术，通过定时器中断驱动上下文切换。\n第三，许多CPU可能同时在进程之间切换，使用一个用锁方案来避免争用是很有必要的。\n第四，进程退出时必须释放进程的内存以及其他资源，但它不能自己完成所有这一切，因为（例如）它不能在仍然使用自己内核栈的情况下释放它。\n第五，多核机器的每个核心必须记住它正在执行哪个进程，以便系统调用正确影响对应进程的内核状态。\n最后，sleep允许一个进程放弃CPU，wakeup允许另一个进程唤醒第一个进程。需要小心避免导致唤醒通知丢失的竞争。\nXv6试图尽可能简单地解决这些问题，但结果代码很复杂。\n多路复用进程与线程的区别 属性 进程 线程 地址空间 独立地址空间 共享同一进程的地址空间 资源 独立资源，如内存、文件描述符等 共享同一进程的资源 开销 创建、销毁、切换开销大 创建、销毁、切换开销小 通信 进程间通信（如管道、消息队列等）较复杂 线程之间的通信相对简单和高效 容错性 进程崩溃不会影响其他进程 线程崩溃可能影响整个进程 代码：上下文切换 图7.1概述了从一个用户进程（旧进程）切换到另一个用户进程（新进程）所涉及的步骤\n一个到旧进程内核线程的用户-内核转换（系统调用或中断）\n一个到当前CPU调度程序线程的上下文切换\n一个到新进程内核线程的上下文切换\n一个返回到用户级进程的陷阱\n调度程序在旧进程的内核栈上执行是不安全的：其他一些核心可能会唤醒进程并运行它，而在两个不同的核心上使用同一个栈将是一场灾难，因此xv6调度程序在每个CPU上都有一个专用线程（保存寄存器和栈）。\n这段话的核心问题在于多核系统中的进程调度，特别是在进程切换时如何安全地管理内核栈和寄存器状态。\n在多核系统中，操作系统需要在多个 CPU 核上管理多个进程，并根据调度策略在进程之间切换执行。调度程序负责管理哪些进程在何时运行，以及如何在不同进程之间切换。\n每个进程在运行时都有用户态和内核态两种状态。当进程从用户态进入内核态时（例如，处理系统调用或中断），进程的状态需要保存在内核栈中。操作系统使用内核栈来保存进程当前的状态（如寄存器的值、程序计数器等），以便在切换回来时可以继续执行。\n这段话提到的一个重要问题是：在多核系统中，如果多个 CPU 核同时使用同一个内核栈，会导致严重的冲突和错误。这是因为：\n内核栈是每个进程在内核态运行时保存状态的地方。当进程从用户态切换到内核态（例如处理系统调用或中断）时，操作系统会将进程的寄存器、程序计数器等信息保存在这个内核栈中。 如果某个进程被唤醒并在一个 CPU 核上运行，而调度程序正在另一个 CPU 核上切换该进程，这两个 CPU 核将会同时访问同一个内核栈，导致冲突。由于栈是一个线性结构，两个 CPU 同时操作会导致数据混乱和系统崩溃。 因此，如果调度程序在旧的进程的内核栈上执行，而此时该进程在另一个 CPU 核上被唤醒并运行，就会导致两个 CPU 共享同一个栈，进而导致系统崩溃。\n为了避免这个问题，xv6 操作系统的设计为每个 CPU 核提供了一个专用线程，并为这个线程分配了一个独立的栈。这个线程称为调度程序线程（scheduler thread）。\n调度程序线程负责在每个 CPU 上执行调度任务。当操作系统决定切换进程时，调度程序线程在它自己的栈上执行，不会干扰其他正在运行的进程。 这种设计确保调度程序不依赖于任何特定的进程栈，而是有自己独立的栈空间。这样即使一个进程在多个 CPU 核之间切换，也不会出现多个 CPU 同时访问同一个栈的情况。 在内核态，进程和调度程序之间需要频繁切换，特别是在多核系统中。这种切换通常会通过保存当前进程的状态（寄存器、程序计数器等）到内核栈上，然后调度程序在线程的栈上执行，决定下一个运行的进程。切换过程如下：\n进程进入内核态：当某个进程由于系统调用或中断进入内核态时，它的状态（如寄存器内容、程序计数器）会保存在它的内核栈上。 调度程序介入：调度程序线程运行在每个 CPU 上，并有自己的内核栈。它从当前进程的内核栈中恢复保存的状态，执行调度逻辑，选择下一个要运行的进程。 切换到下一个进程：调度程序将切换到下一个进程，恢复该进程的状态，并从这个进程的内核栈中恢复执行。 避免共享内核栈的并发问题\n问题：在多核系统中，多个 CPU 核可能会同时运行调度程序和进程。如果它们共享同一个内核栈，会导致栈冲突和系统崩溃。 解决方案：xv6 为每个 CPU 提供了一个独立的调度程序线程和栈，以确保调度程序和进程不会同时使用同一个栈。 内核线程和调度程序的切换机制：调度程序在独立的栈上执行，避免了在多核环境中不同核心共享同一个栈的风险。 在多核系统中，每个 CPU 都有自己的专用内核栈，这个栈用于调度程序的执行。调度程序负责在进程之间切换，也就是当某个进程被暂停时，调度程序会选择下一个要运行的进程。\n调度程序的内核栈主要有以下作用：\n独立于进程的栈：调度程序有自己独立的栈，不依赖任何进程的栈。这样，当调度程序在多个 CPU 核上运行时，它们可以独立地管理每个 CPU 的调度过程，不会与正在运行的进程发生冲突。 保存调度程序的状态：在执行进程切换时，调度程序需要保存当前的 CPU 状态、寄存器、调度算法的变量等信息。这些信息会保存在调度程序自己的内核栈中，而不会影响进程的内核栈。 执行进程调度逻辑：当调度程序决定暂停当前进程并切换到另一个进程时，它使用自己的内核栈来处理这个操作。调度程序在选择下一个进程时，所有与调度相关的局部变量和函数调用都存储在调度程序自己的栈中。 这种设计保证了在多核环境下安全、高效地切换进程，并且避免了内核栈冲突。\n在本节中，我们将研究在内核线程和调度程序线程之间切换的机制。\n从一个线程切换到另一个线程需要保存旧线程的CPU寄存器，并恢复新线程先前保存的寄存器；栈指针和程序计数器被保存和恢复的事实意味着CPU将切换栈和执行中的代码。\n函数swtch为内核线程切换执行保存和恢复操作。\nswtch对线程没有直接的了解；它只是保存和恢复寄存器集，称为上下文（contexts）。\n当某个进程要放弃CPU时，该进程的内核线程调用swtch来保存自己的上下文并返回到调度程序的上下文。\n每个上下文都包含在一个struct context（kernel/proc.h:2）中，这个结构体本身包含在一个进程的struct proc或一个CPU的struct cpu中。\nSwtch接受两个参数：struct context *old和struct context *new。\n它将当前寄存器保存在old中，从new中加载寄存器，然后返回。\n让我们跟随一个进程通过swtch进入调度程序。\n我们在第4章中看到，中断结束时的一种可能性是usertrap调用了yield。\n依次地：Yield调用sched，sched调用swtch将当前上下文保存在p-\u0026gt;context中，并切换到先前保存在cpu-\u0026gt;scheduler（kernel/proc.c:517）中的调度程序上下文。\n注：当前版本的XV6中调度程序上下文是cpu-\u0026gt;context\nSwtch（kernel/swtch.S:3）只保存被调用方保存的寄存器（callee-saved registers）；调用方保存的寄存器（caller-saved registers）通过调用C代码保存在栈上（如果需要）。\nSwtch知道struct context中每个寄存器字段的偏移量。它不保存程序计数器。\n但swtch保存ra寄存器，该寄存器保存调用swtch的返回地址。\n现在，swtch从新进程的上下文中恢复寄存器，该上下文保存前一个swtch保存的寄存器值。\n当swtch返回时，它返回到由ra寄存器指定的指令，即新线程以前调用swtch的指令。\n另外，它在新线程的栈上返回。\n以cc切换到ls为例，且ls此前运行过\nXV6将cc程序的内核线程的内核寄存器保存在一个context对象中\n因为要切换到ls程序的内核线程，那么ls 程序现在的状态必然是RUNABLE ，表明ls程序之前运行了一半。这同时也意味着：\na. ls程序的用户空间状态已经保存在了对应的trapframe中\nb. ls程序的内核线程对应的内核寄存器已经保存在对应的context对象中\n所以接下来，XV6会恢复ls程序的内核线程的context对象，也就是恢复内核线程的寄存器。\n之后ls会继续在它的内核线程栈上，完成它的中断处理程序\n恢复ls程序的trapframe中的用户进程状态，返回到用户空间的ls程序中\n最后恢复执行ls\n进程的上下文信息（内核态的寄存器、程序计数器等）保存在每个进程的 context 对象中，而不是在 CPU 专属的调度程序栈中。 CPU 专属的调度程序栈 仅用于调度程序在进程切换期间的执行，不保存具体进程的状态。 调度程序执行进程切换时，会切换到 CPU 专属的栈，而当新的进程开始运行时，它会切换回到该进程的内核栈，继续执行。 在我们的示例中，sched调用swtch切换到cpu-\u0026gt;scheduler，即每个CPU的调度程序上下文。\n调度程序上下文之前通过scheduler对swtch（kernel/proc.c:475）的调用进行了保存。\n当我们追踪swtch到返回时，他返回到scheduler而不是sched，并且它的栈指针指向当前CPU的调用程序栈（scheduler stack）。\n代码：调度 上一节介绍了swtch的底层细节；\n现在，让我们以swtch为给定对象，检查从一个进程的内核线程通过调度程序切换到另一个进程的情况。\n调度器（scheduler）以每个CPU上一个特殊线程的形式存在，每个线程都运行scheduler函数。\n此函数负责选择下一个要运行的进程。\n想要放弃CPU的进程必须先获得自己的进程锁p-\u0026gt;lock，并释放它持有的任何其他锁，更新自己的状态（p-\u0026gt;state），然后调用sched。\nYield（kernel/proc.c:515）遵循这个约定，sleep和exit也遵循这个约定，我们将在后面进行研究。\nSched对这些条件再次进行检查（kernel/proc.c:499-504），并检查这些条件的隐含条件：由于锁被持有，中断应该被禁用。\n最后，sched调用swtch将当前上下文保存在p-\u0026gt;context中，并切换到cpu-\u0026gt;scheduler中的调度程序上下文。\nSwtch在调度程序的栈上返回，就像是scheduler的swtch返回一样。\nscheduler继续for循环，找到要运行的进程，切换到该进程，重复循环。\nxv6在对swtch的调用中持有p-\u0026gt;lock：swtch的调用者必须已经持有了锁，并且锁的控制权传递给切换到的代码。\n这种约定在锁上是不寻常的；通常，获取锁的线程还负责释放锁，这使得对正确性进行推理更加容易。\n对于上下文切换，有必要打破这个惯例，因为p-\u0026gt;lock保护进程state和context字段上的不变量，而这些不变量在swtch中执行时不成立。\n如果在swtch期间没有保持p-\u0026gt;lock，可能会出现一个问题：在yield将其状态设置为RUNNABLE之后，但在swtch使其停止使用自己的内核栈之前，另一个CPU可能会决定运行该进程。结果将是两个CPU在同一栈上运行，这不可能是正确的。\n内核线程总是在sched中放弃其CPU，并总是切换到调度程序中的同一位置，而调度程序（几乎）总是切换到以前调用sched的某个内核线程。\n因此，如果要打印xv6切换线程处的行号，将观察到以下简单模式：（kernel/proc.c:475），（kernel/proc.c:509），（kernel/proc.c:475），（kernel/proc.c:509）等等。\n在两个线程之间进行这种样式化切换的过程有时被称为协程（coroutines）；在本例中，sched和scheduler是彼此的协同程序。\n“这种样式化切换”是指在 xv6 中，上下文切换时，程序总是以一种有规律的模式在两个地方来回切换：\n调度程序的 sched 函数负责放弃当前进程的 CPU，保存进程状态，并切换到调度程序。 调度程序再选择下一个要运行的进程，切换回它之前调用 sched 的进程。 这种切换的模式是循环往复的，比如代码总是从 sched 返回到 scheduler 的特定行号，再从 scheduler 切换到某个进程的特定行号。这个有规律的模式就是所谓的“样式化切换”。\n总结这段话的核心内容：\n进程锁和上下文切换：进程在调用 sched 时需要先持有自己的 p-\u0026gt;lock，这是为了防止在 swtch 期间其他 CPU 并发地操作同一个进程。 swtch 的工作原理：sched 调用 swtch 将当前进程的状态保存到 p-\u0026gt;context，然后切换到调度程序的上下文。调度程序随后选择下一个要运行的进程。 锁的特殊约定：xv6 打破了通常的“获取锁的线程负责释放锁”的惯例，sched 持有的锁传递给调度程序，以确保状态安全切换。 协程（coroutines）模式：调度程序和 sched 之间的这种循环切换，被称为协程，它们互相配合着切换工作，形成协作运行的模式。 传统的函数调用与返回：\n当你调用一个函数时，程序会暂停当前的执行流程，进入被调用的函数。 当被调用的函数执行 return 后，程序回到调用点继续执行。 但 函数调用是“单向的”：调用者调用被调用者，被调用者完成后返回调用者。调用者没有暂停或保存自己的状态等待被调用。 协程的关键区别：\n协程可以“暂停”和“恢复”：协程允许在执行过程中暂停，并且可以在某个时刻恢复执行。当协程暂停时，它的执行上下文（包括局部变量、程序计数器等）被保存下来，当它被重新调用时，会从暂停的地方继续执行，而不是从调用点继续像函数返回那样。 协同执行：协程允许多个函数互相让出控制权，并在彼此之间切换。这与传统的函数调用不同，传统的调用只是单向的，而协程可以在不同协程之间来回切换，形成一种“合作式多任务”的执行方式。 函数调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 void funcA() { printf(\u0026#34;In funcA\\n\u0026#34;); funcB(); printf(\u0026#34;Back to funcA\\n\u0026#34;); } void funcB() { printf(\u0026#34;In funcB\\n\u0026#34;); } int main() { funcA(); } 协程（伪代码示例）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ccodepreviewrunvoid coroutineA() { printf(\u0026#34;In coroutineA\\n\u0026#34;); yield(); // 暂停并交出控制权 printf(\u0026#34;Back to coroutineA\\n\u0026#34;); } void coroutineB() { printf(\u0026#34;In coroutineB\\n\u0026#34;); yield(); // 暂停并交出控制权 } int main() { startCoroutine(coroutineA); startCoroutine(coroutineB); } 整个过程的执行类似于进程 A -\u0026gt; sched -\u0026gt; scheduler（-\u0026gt;swtch） -\u0026gt; 进程 B -\u0026gt; sched -\u0026gt; scheduler -\u0026gt; 进程 C -\u0026gt; sched -\u0026gt; \u0026hellip;\n关键点：\nsched 负责保存当前进程的状态并交出控制权。 scheduler 负责选择下一个进程，并通过 swtch 恢复这个进程的状态。 这个过程不断循环，类似于协程的交替执行，每个进程在被切换出去后暂停，再被调度回来时继续执行。 存在一种情况使得调度程序对swtch的调用没有以sched结束。\n一个新进程第一次被调度时，它从forkret（kernel/proc.c:527）开始。\nForkret存在以释放p-\u0026gt;lock；否则，新进程可以从usertrapret开始。\nscheduler（kernel/proc.c:457）运行一个简单的循环：找到要运行的进程，运行它直到它让步，然后重复循环。\nscheduler在进程表上循环查找可运行的进程，该进程具有p-\u0026gt;state == RUNNABLE。\n一旦找到一个进程，它将设置CPU当前进程变量c-\u0026gt;proc，将该进程标记为RUNINING，然后调用swtch开始运行它（kernel/proc.c:470-475）。\n考虑调度代码结构的一种方法是，它为每个进程强制维持一个不变量的集合，并在这些不变量不成立时持有p-\u0026gt;lock。\n其中一个不变量是：如果进程是RUNNING状态，计时器中断的yield必须能够安全地从进程中切换出去；这意味着CPU寄存器必须保存进程的寄存器值（即swtch没有将它们移动到context中），并且c-\u0026gt;proc必须指向进程。\n另一个不变量是：如果进程是RUNNABLE状态，空闲CPU的调度程序必须安全地运行它；这意味着p-\u0026gt;context必须保存进程的寄存器（即，它们实际上不在实际寄存器中），没有CPU在进程的内核栈上执行，并且没有CPU的c-\u0026gt;proc引用进程。请注意，在保持p-\u0026gt;lock时，这些属性通常不成立。\n这段话提到的两个不变量是：\n第一个不变量（对于 RUNNING 状态的进程）：\n如果进程处于 RUNNING 状态，计时器中断发生时，操作系统的 yield 必须能够安全地切换到调度程序。这意味着，CPU 寄存器必须保持进程的寄存器值，并且 c-\u0026gt;proc 必须指向该进程。也就是说，寄存器状态还没有被保存到 p-\u0026gt;context 中。 第二个不变量（对于 RUNNABLE 状态的进程）：\n如果进程处于RUNNABLE状态，空闲的 CPU 调度程序必须能够\n安全地运行该进程。这意味着：p-\u0026gt;context 必须保存该进程的寄存器状态，因为它已经不再在 CPU 的实际寄存器中。\n没有 CPU 正在使用该进程的内核栈，确保没有其他 CPU 在执行该进程。\n没有 CPU 的 c-\u0026gt;proc（当前正在运行的进程指针）引用该进程，说明没有任何 CPU 正在运行它。\n为什么叫做不变量？\n不变量 是指程序在运行时始终需要保持成立的条件，不论程序执行的状态如何。在这里，这两个不变量必须始终保持成立，以确保在上下文切换时，进程状态能够正确保存和恢复，从而保证系统的稳定性和安全性。 “安全地从进程中切换出去”和“空闲 CPU 的调度程序必须安全地运行它”是什么意思？\n“安全地从进程中切换出去”： 当一个正在运行的进程（RUNNING 状态）被计时器中断触发时，操作系统可能决定切换到另一个进程。这时，系统必须能够安全地切换，意味着当前进程的寄存器状态必须妥善保存，否则在切换回来时进程可能会丢失当前状态。因此，CPU 寄存器必须保持进程的寄存器值，不能在 swtch 中过早地将其保存到 p-\u0026gt;context 中。 简单来说，这意味着在切换到调度程序前，系统必须确保当前进程的所有运行状态都已正确保存，以便下次切换回来时能够恢复。 “空闲 CPU 的调度程序必须安全地运行它”： 当某个 CPU 处于空闲状态，调度程序会从可运行的进程队列中选择一个RUNNABLE状态的进程并运行它。为了确保安全运行，系统必须确保： p-\u0026gt;context 中包含了进程的寄存器状态，因为进程的状态已经被保存，CPU 寄存器中不再有它的内容。 没有其他 CPU 在使用该进程的内核栈，这意味着该进程没有被任何其他 CPU 执行，从而避免栈冲突。 没有 CPU 的 c-\u0026gt;proc 引用该进程，表明它确实处于可运行状态，等待被分配 CPU。 为什么说“保持 p-\u0026gt;lock 时，这些属性通常不成立”，“这些属性”又是什么？\n当进程处于 RUNNING 状态时，计时器中断发生时，CPU 寄存器必须保持进程的寄存器值，c-\u0026gt;proc 必须指向该进程。 当进程处于 RUNNABLE 状态时，p-\u0026gt;context 必须保存进程的寄存器状态，没有 CPU 在进程的内核栈上运行，没有 CPU 的 c-\u0026gt;proc 指向它。 为什么保持 p-\u0026gt;lock 时，这些属性通常不成立：\n在持有p-\u0026gt;lock的过程中，系统可能正在进行上下文切换，此时进程的状态尚未完全确定。例如： 当进程正从 RUNNING 状态切换到 RUNNABLE 状态时，进程的寄存器可能还没有完全保存到 p-\u0026gt;context 中。 或者，进程的状态正在更新，可能还存在 CPU 在该进程的内核栈上运行，导致暂时的不一致。 因此，在上下文切换的过程中，这些不变量可能会短暂地被打破。这就是为什么在 sched 执行过程中，必须持有 p-\u0026gt;lock，确保在完成整个上下文切换前，其他 CPU 不会并发地修改该进程的状态。\n维护上述不变量是xv6经常在一个线程中获取p-\u0026gt;lock并在另一个线程中释放它的原因，例如在yield中获取并在scheduler中释放。\n一旦yield开始修改一个RUNNING进程的状态为RUNNABLE，锁必须保持被持有状态，直到不变量恢复：最早的正确释放点是scheduler（在其自身栈上运行）清除c-\u0026gt;proc之后。类似地，一旦scheduler开始将RUNNABLE进程转换为RUNNING，在内核线程完全运行之前（在swtch之后，例如在yield中）绝不能释放锁。\n在上下文切换和调度过程中，锁的释放时机非常关键。锁的作用是确保进程的状态在切换时不会被多个 CPU 并发修改或破坏，尤其是维护那些关键的不变量。具体来说，锁必须在整个上下文切换过程中保持，以防止状态不一致。下面详细解释锁的释放时机：\nyield 中获取锁的时机：\n当一个 RUNNING 状态的进程决定放弃 CPU（例如通过 yield），它需要先获取自己的锁（p-\u0026gt;lock）。\n获取锁后，进程的状态开始从 RUNNING 改为 RUNNABLE。这个过程非常关键，因为它影响到进程的执行状态，以及 CPU 寄存器中的内容如何保存。\n在这个状态转换过程中，锁必须保持，直到确认状态安全地转换为 RUNNABLE，并且不会有其他 CPU 并发地访问或修改该进程的状态。\n锁在 scheduler 中释放的时机：\n一旦 yield 把进程的状态标记为 RUNNABLE，接下来调度程序（scheduler）会从可运行的进程列表中选择下一个进程，并进行上下文切换。\n但是，在调度过程中，锁不能立即释放，因为此时进程的状态（p-\u0026gt;state 和 p-\u0026gt;context）还没有完全稳定。特别是，调度器还没有完成对 c-\u0026gt;proc 的清除（c-\u0026gt;proc 表示当前 CPU 上正在运行的进程）。具体来说：\n调度器会在其自身的栈上运行，选择下一个要执行的进程。 锁的释放时机：调度器必须等到它完全切换到另一个进程后，并且确保当前进程的 c-\u0026gt;proc 被清除（即当前 CPU 不再引用该进程）后，才可以释放 p-\u0026gt;lock。\n为何不能提前释放锁？\n如果在 yield 修改状态后、上下文切换前提前释放锁，可能会导致系统中的其他 CPU 错误地认为这个进程已经处于稳定的 RUNNABLE 状态，并尝试同时调度和运行它。这会导致多个 CPU 操作同一个内核栈，从而导致数据冲突和不一致性。\n锁必须确保状态转换和上下文切换在完成后，才释放锁给其他调度程序或 CPU，以免破坏不变量。\n调度器把 RUNNABLE 转换为 RUNNING 时的锁释放：\n当调度器选择一个 RUNNABLE 的进程运行时，它会将该进程的状态从 RUNNABLE 转为 RUNNING。\n在这之前，调度器会持有该进程的锁，确保没有其他 CPU 并发修改它。\n在上下文切换完成（即 swtch 切换到该进程）后，调度器必须继续持有锁，直到切换完成，并且进程真正开始运行。\n一旦切换到该进程，锁可以安全地释放，因为该进程已经完全运行，状态已经稳定。\n总结锁的释放时机：\nyield 后：锁不能立即释放，必须保持到调度器清理完 c-\u0026gt;proc，并安全地将控制权交给另一个进程。 scheduler 选择下一个进程后：当调度器把进程从 RUNNABLE 转换为 RUNNING 时，必须确保切换完成，才能释放锁。 总的来说，锁的释放时机是确保状态和上下文切换完成，进程状态稳定后，才能释放锁，否则可能导致状态混乱和并发冲突。\np-\u0026gt;lock还保护其他东西：exit和wait之间的相互作用，避免丢失wakeup的机制（参见第7.5节），以及避免一个进程退出和其他进程读写其状态之间的争用（例如，exit系统调用查看p-\u0026gt;pid并设置p-\u0026gt;killed(*kernel/proc.c*:611)）。\n为了清晰起见，也许为了性能起见，有必要考虑一下p-\u0026gt;lock的不同功能是否可以拆分。\n代码：mycpu和myproc Xv6通常需要指向当前进程的proc结构体的指针。\n在单处理器系统上，可以有一个指向当前proc的全局变量。\n但这不能用于多核系统，因为每个核执行的进程不同。\n解决这个问题的方法是基于每个核心都有自己的寄存器集，从而使用其中一个寄存器来帮助查找每个核心的信息。\nXv6为每个CPU维护一个struct cpu，它记录当前在该CPU上运行的进程（如果有的话），为CPU的调度线程保存寄存器，以及管理中断禁用所需的嵌套自旋锁的计数。\n函数mycpu (kernel/proc.c:60)返回一个指向当前CPU的struct cpu的指针。\nRISC-V给它的CPU编号，给每个CPU一个hartid。Xv6确保每个CPU的hartid在内核中存储在该CPU的tp寄存器中。\n这允许mycpu使用tp对一个cpu结构体数组（即cpus数组，kernel/proc.c:9）进行索引，以找到正确的那个。\n确保CPU的tp始终保存CPU的hartid有点麻烦。\nmstart在CPU启动次序的早期设置tp寄存器，此时仍处于机器模式（kernel/start.c:46）。\n因为用户进程可能会修改tp，usertrapret在蹦床页面（trampoline page）中保存tp。\n最后，uservec在从用户空间（kernel/trampoline.S:70）进入内核时恢复保存的tp。\n编译器保证永远不会使用tp寄存器。\n如果RISC-V允许xv6直接读取当前hartid会更方便，但这只允许在机器模式下，而不允许在管理模式下。\ncpuid和mycpu的返回值很脆弱：如果定时器中断并导致线程让步（yield），然后移动到另一个CPU，以前返回的值将不再正确。\nxv6要求调用者禁用中断，并且只有在使用完返回的struct cpu后才重新启用。\n函数myproc (kernel/proc.c:68)返回当前CPU上运行进程struct proc的指针。\nmyproc禁用中断，调用mycpu，从struct cpu中取出当前进程指针（c-\u0026gt;proc），然后启用中断。\n即使启用中断，myproc的返回值也可以安全使用：如果计时器中断将调用进程移动到另一个CPU，其struct proc指针不会改变。\n虽然在多核环境下多个 CPU 是并发执行的，但每次调用这些函数时，都会针对一个特定的 CPU 上的线程运行。\nXv6 操作系统在多核系统上无法使用单一的全局变量来指向当前运行的进程。为了区分每个处理器核上执行的进程，Xv6 为每个 CPU 维护了一个 struct cpu，记录了当前 CPU 上正在运行的进程，以及其他 CPU 状态信息。为了能够快速访问每个核心的 struct cpu，Xv6 使用了 RISC-V 的 tp 寄存器，该寄存器保存每个 CPU 的 hartid（硬件线程 ID）。这使得 Xv6 能通过 tp 对应 cpus 数组进行索引，从而获取每个核的 struct cpu。\nmycpu() 函数返回当前 CPU 的 struct cpu 指针。它通过 tp 寄存器找到对应的 CPU 结构体。 myproc() 函数返回当前 CPU 上运行的进程的 struct proc 指针。它通过禁用中断，调用 mycpu() 函数从当前 CPU 的 struct cpu 中取出正在运行的进程指针。 为了避免在中断或线程迁移（例如由于计时器中断导致线程切换到另一个 CPU）时返回值不再有效，Xv6 要求在获取这些信息时禁用中断，并在使用完后再启用。\n正课 线程概述 今天的课程会讨论线程以及XV6如何实现线程切换。\n今天这节课与之前介绍的系统调用，Interrupt，page table和锁的课程一样，都是有关XV6底层实现的课程。\n为什么计算机需要运行多线程？可以归结为以下原因：\n首先，人们希望他们的计算机在同一时间不是只执行一个任务。\n有可能计算机需要执行分时复用的任务，例如MIT的公共计算机系统Athena允许多个用户同时登陆一台计算机，并运行各自的进程。甚至在一个单用户的计算机或者在你的iphone上，你会运行多个进程，并期望计算机完成所有的任务而不仅仅只是一个任务。\n其次，多线程可以让程序的结构变得简单。\n线程在有些场合可以帮助程序员将代码以简单优雅的方式进行组织，并减少复杂度。实际上在第一个lab中prime number部分，通过多个进程可以更简单，方便，优雅的组织代码。\n最后，使用多线程可以通过并行运算，在拥有多核CPU的计算机上获得更快的处理速度。\n常见的方式是将程序进行拆分，并通过线程在不同的CPU核上运行程序的不同部分。如果你足够幸运的话，你可以将你的程序拆分并在4个CPU核上通过4个线程运行你的程序，同时你也可以获取4倍的程序运行速度。你可以认为XV6就是一个多CPU并行运算的程序。\n所以，线程可以认为是一种在有多个任务时简化编程的抽象。\n一个线程可以认为是串行执行代码的单元。\n如果你写了一个程序只是按顺序执行代码，那么你可以认为这个程序就是个单线程程序，这是对于线程的一种宽松的定义。\n虽然人们对于线程有很多不同的定义，在这里，我们认为线程就是单个串行执行代码的单元，它只占用一个CPU并且以普通的方式一个接一个的执行指令。\n除此之外，线程还具有状态，我们可以随时保存线程的状态并暂停线程的运行，并在之后通过恢复状态来恢复线程的运行。\n线程的状态包含了三个部分：\n程序计数器（Program Counter），它表示当前线程执行指令的位置。 保存变量的寄存器。 程序的Stack（注，详见5.5）。通常来说每个线程都有属于自己的Stack，Stack记录了函数调用的记录，并反映了当前线程的执行点。 操作系统中线程系统的工作就是管理多个线程的运行。\n我们可能会启动成百上千个线程，而线程系统的工作就是弄清楚如何管理这些线程并让它们都能运行。\n多线程的并行运行主要有两个策略：\n第一个策略是在多核处理器上使用多个CPU，每个CPU都可以运行一个线程，如果你有4个CPU，那么每个CPU可以运行一个线程。\n每个线程自动的根据所在CPU就有了程序计数器和寄存器。但是如果你只有4个CPU，却有上千个线程，每个CPU只运行一个线程就不能解决这里的问题了。\n第二个策略是一个CPU在多个线程之间来回切换。\n假设我只有一个CPU，但是有1000个线程，我们接下来将会看到XV6是如何实现线程切换使得XV6能够先运行一个线程，之后将线程的状态保存，再切换至运行第二个线程，然后再是第三个线程，依次类推直到每个线程都运行了一会，再回来重新执行第一个线程。\n实际上，与大多数其他操作系统一样，XV6结合了这两种策略。\n首先线程会运行在所有可用的CPU核上，其次每个CPU核会在多个线程之间切换，因为通常来说，线程数会远远多于CPU的核数。\n不同线程系统之间的一个主要的区别就是，线程之间是否会共享内存。\n一种可能是你有一个地址空间，多个线程都在这一个地址空间内运行，并且它们可以看到彼此的更新。\n比如说共享一个地址空间的线程修改了一个变量，共享地址空间的另一个线程可以看到变量的修改。\n所以当多个线程运行在一个共享地址空间时，我们需要用到上节课讲到的锁。\nXV6内核共享了内存，并且XV6支持内核线程的概念，对于每个用户进程都有一个内核线程来执行来自用户进程的系统调用。\n所有的内核线程都共享了内核内存，所以XV6的内核线程的确会共享内存。\n另一方面，XV6还有另外一种线程。\n每一个用户进程都有独立的内存地址空间（注，详见4.2），并且包含了一个线程，这个线程控制了用户进程代码指令的执行。\n所以XV6中的用户线程之间没有共享内存，你可以有多个用户进程，但是每个用户进程都是拥有一个线程的独立地址空间。XV6中的进程不会共享内存。\n在一些其他更加复杂的系统中，例如Linux，允许在一个用户进程中包含多个线程，进程中的多个线程共享进程的地址空间。\n当你想要实现一个运行在多个CPU核上的用户进程时，你就可以在用户进程中创建多个线程。\nLinux中也用到了很多我们今天会介绍的技术，但是在Linux中跟踪每个进程的多个线程比XV6中每个进程只有一个线程要复杂的多。\n还有一些其他的方式可以支持在一台计算机上交织的运行多个任务，我们不会讨论它们，但是如果你感兴趣的话，你可以去搜索event-driven programming或者state machine，这些是在一台计算机上不使用线程但又能运行多个任务的技术。\n在所有的支持多任务的方法中，线程技术并不是非常有效的方法，但是线程通常是最方便，对程序员最友好的，并且可以用来支持大量不同任务的方法。\nXV6线程调度 实现内核中的线程系统存在以下挑战：\n第一个是如何实现线程间的切换。\n这里停止一个线程的运行并启动另一个线程的过程通常被称为线程调度（Scheduling）。我们将会看到XV6为每个CPU核都创建了一个线程调度器（Scheduler）。\n第二个挑战是，当你想要实际实现从一个线程切换到另一个线程时，你需要保存并恢复线程的状态。\n所以需要决定线程的哪些信息是必须保存的，并且在哪保存它们。\n最后一个挑战是如何处理运算密集型线程（compute bound thread）。\n对于线程切换，很多直观的实现是由线程自己自愿的保存自己的状态，再让其他的线程运行。但是如果我们有一些程序正在执行一些可能要花费数小时的长时间计算任务，这样的线程并不能自愿的出让CPU给其他的线程运行。所以这里需要能从长时间运行的运算密集型线程撤回对于CPU的控制，将其放置于一边，稍后再运行它。\n接下来，将首先介绍如何处理运算密集型线程。\n这里的具体实现，就是利用定时器中断。\n在每个CPU核上，都存在一个硬件设备，它会定时产生中断。\nXV6与其他所有的操作系统一样，将这个中断传输到了内核中。\n所以即使我们正在用户空间计算π的前100万位，定时器中断仍然能在例如每隔10ms的某个时间触发，并将程序运行的控制权从用户空间代码切换到内核中的中断处理程序（注，因为中断处理程序优先级更高）。\n哪怕这些用户空间进程并不配合工作（注，也就是用户空间进程一直占用CPU），内核也可以从用户空间进程获取CPU控制权。\n位于内核的定时器中断处理程序，会自愿的将CPU出让（yield）给线程调度器，并告诉线程调度器说，你可以让一些其他的线程运行了。\n这里的出让其实也是一种线程切换，它会保存当前线程的状态，并在稍后恢复。\n这里的基本流程是，定时器中断将CPU控制权给到内核，内核再自愿的出让CPU。\n这样的处理流程被称为pre-emptive scheduling。\npre-emptive的意思是，即使用户代码本身没有出让CPU，定时器中断仍然会将CPU的控制权拿走，并出让给线程调度器。与之相反的是voluntary scheduling。\n在XV6和其他的操作系统中，线程调度是这么实现的：定时器中断会强制的将CPU控制权从用户进程给到内核，这里是pre-emptive scheduling，之后内核会代表用户进程（注，实际是内核中用户进程对应的内核线程会代表用户进程出让CPU），使用voluntary scheduling。\n在执行线程调度的时候，操作系统需要能区分几类线程：\n当前在CPU上运行的线程 一旦CPU有空闲时间就想要运行在CPU上的线程 以及不想运行在CPU上的线程，因为这些线程可能在等待I/O或者其他事件 这里不同的线程是由状态区分，但是实际上线程的完整状态会要复杂的多（注，线程的完整状态包含了程序计数器，寄存器，栈等等）。下面是我们将会看到的一些线程状态：\nRUNNING，线程当前正在某个CPU上运行 RUNABLE，线程还没有在某个CPU上运行，但是一旦有空闲的CPU就可以运行 SLEEPING，这个状态意味着线程在等待一些I/O事件，它只会在I/O事件发生了之后运行 这节课，我们主要关注RUNNING和RUNABLE这两类线程。\n前面介绍的定时器中断或者说pre-emptive scheduling，实际上就是将一个RUNNING线程转换成一个RUNABLE线程。\n通过出让CPU，pre-emptive scheduling将一个正在运行的线程转换成了一个当前不在运行但随时可以再运行的线程。\n因为当定时器中断触发时，这个线程还在好好的运行着。\n对于RUNNING状态下的线程，它的程序计数器和寄存器位于正在运行它的CPU硬件中。\n而RUNABLE线程，因为并没有CPU与之关联，所以对于每一个RUNABLE线程，当我们将它从RUNNING转变成RUNABLE时，我们需要将它还在RUNNING时位于CPU的状态拷贝到内存中的某个位置，注意这里不是从内存中的某处进行拷贝，而是从CPU中的寄存器拷贝。\n我们需要拷贝的信息就是程序计数器（Program Counter）和寄存器。\n当线程调度器决定要运行一个RUNABLE线程时，这里涉及了很多步骤，但是其中一步是将之前保存的程序计数器和寄存器拷贝回调度器对应的CPU中。\nXV6线程切换（一） 我们或许会运行多个用户空间进程，例如C compiler（CC），LS，Shell，它们或许会，也或许不会想要同时运行。\n在用户空间，每个进程有自己的内存，每个进程都包含了一个用户程序栈（user stack），并且当进程运行的时候，它在RISC-V处理器中会有程序计数器和寄存器。\n当用户程序在运行时，实际上是用户进程中的一个用户线程在运行。\n如果程序执行了一个系统调用或者因为响应中断走到了内核中，那么相应的用户空间状态会被保存在程序的trapframe中（注，详见lec06），同时属于这个用户程序的内核线程被激活。\n所以首先，用户的程序计数器，寄存器等等被保存到了trapframe中，之后CPU被切换到内核栈上运行，实际上会走到trampoline和usertrap代码中（注，详见lec06）。\n之后内核会运行一段时间处理系统调用或者执行中断处理程序。在处理完成之后，如果需要返回到用户空间，trapframe中保存的用户进程状态会被恢复。\n除了系统调用，用户进程也有可能是因为CPU需要响应类似于定时器中断走到了内核空间。\n上一节提到的pre-emptive scheduling，会通过定时器中断将CPU运行切换到另一个用户进程。\n在定时器中断程序中，如果XV6内核决定从一个用户进程切换到另一个用户进程，那么首先在内核中第一个进程的内核线程会被切换到第二个进程的内核线程。\n之后再在第二个进程的内核线程中返回到用户空间的第二个进程，这里返回也是通过恢复trapframe中保存的用户进程状态完成。\n当XV6从CC程序的内核线程切换到LS程序的内核线程时：\nXV6会首先会将CC程序的内核线程的内核寄存器保存在一个context对象中。 类似的，因为要切换到LS程序的内核线程，那么LS程序现在的状态必然是RUNABLE，表明LS程序之前运行了一半。这同时也意味着LS程序的用户空间状态已经保存在了对应的trapframe中，更重要的是，LS程序的内核线程对应的内核寄存器也已经保存在对应的context对象中。所以接下来，XV6会恢复LS程序的内核线程的context对象，也就是恢复内核线程的寄存器。 之后LS会继续在它的内核线程栈上，完成它的中断处理程序（注，假设之前LS程序也是通过定时器中断触发的pre-emptive scheduling进入的内核）。 然后通过恢复LS程序的trapframe中的用户进程状态，返回到用户空间的LS程序中。 最后恢复执行LS。 这里核心点在于，在XV6中，任何时候都需要经历：\n从一个用户进程切换到另一个用户进程，都需要从第一个用户进程接入到内核中，保存用户进程的状态并运行第一个用户进程的内核线程。 再从第一个用户进程的内核线程切换到第二个用户进程的内核线程。 之后，第二个用户进程的内核线程暂停自己，并恢复第二个用户进程的用户寄存器。 最后返回到第二个用户进程继续执行。 XV6线程切换（二） 实际的线程切换流程会复杂的多。\n假设我们有进程P1正在运行，进程P2是RUNABLE当前并不在运行。\n假设在XV6中我们有2个CPU核，这意味着在硬件层面我们有CPU0和CPU1。\n我们从一个正在运行的用户空间进程切换到另一个RUNABLE但是还没有运行的用户空间进程的更完整的故事是：\n首先与我之前介绍的一样，一个定时器中断强迫CPU从用户空间进程切换到内核，trampoline代码将用户寄存器保存于用户进程对应的trapframe对象中； 之后在内核中运行usertrap，来实际执行相应的中断处理程序。这时，CPU正在进程P1的内核线程和内核栈上，执行内核中普通的C代码； 假设进程P1对应的内核线程决定它想出让CPU，它会做很多工作，这个我们稍后会看，但是最后它会调用swtch函数（译注：switch 是C 语言关键字，因此这个函数命名为swtch 来避免冲突），这是整个线程切换的核心函数之一； swtch函数会保存用户进程P1对应内核线程的寄存器至context对象。所以目前为止有两类寄存器：用户寄存器存在trapframe中，内核线程的寄存器存在context中。 但是，实际上swtch函数并不是直接从一个内核线程切换到另一个内核线程。\nXV6中，一个CPU上运行的内核线程可以直接切换到的是这个CPU对应的调度器线程。\n所以如果我们运行在CPU0，swtch函数会恢复之前为CPU0的调度器线程保存的寄存器和stack pointer，之后就在调度器线程的context下执行schedulder函数中（注，后面代码分析有介绍）。\n在schedulder函数中会做一些清理工作，例如将进程P1设置成RUNABLE状态。\n之后再通过进程表单找到下一个RUNABLE进程。\n假设找到的下一个进程是P2（虽然也有可能找到的还是P1），schedulder函数会再次调用swtch函数，完成下面步骤：\n先保存自己的寄存器到调度器线程的context对象 找到进程P2之前保存的context，恢复其中的寄存器 因为进程P2在进入RUNABLE状态之前，如刚刚介绍的进程P1一样，必然也调用了swtch函数。所以之前的swtch函数会被恢复，并返回到进程P2所在的系统调用或者中断处理程序中（注，因为P2进程之前调用swtch函数必然在系统调用或者中断处理程序中）。 不论是系统调用也好中断处理程序也好，在从用户空间进入到内核空间时会保存用户寄存器到trapframe对象。所以当内核程序执行完成之后，trapframe中的用户寄存器会被恢复。 最后用户进程P2就恢复运行了。 每一个CPU都有一个完全不同的调度器线程。\n调度器线程也是一种内核线程，它也有自己的context对象。\n任何运行在CPU1上的进程，当它决定出让CPU，它都会切换到CPU1对应的调度器线程，并由调度器线程切换到下一个进程。\n学生提问：context保存在哪？\nRobert教授：\n每一个内核线程都有一个context对象。但是内核线程实际上有两类。\n每一个用户进程有一个对应的内核线程，它的context对象保存在用户进程对应的proc结构体中。\n每一个调度器线程，它也有自己的context对象，但是它却没有对应的进程和proc结构体，所以调度器线程的context对象保存在cpu结构体中。\n在内核中，有一个cpu结构体的数组，每个cpu结构体对应一个CPU核，每个结构体中都有一个context字段。\n学生提问：为什么不能将context对象保存在进程对应的trapframe中？\nRobert教授：context可以保存在trapframe中，因为每一个进程都只有一个内核线程对应的一组寄存器，我们可以将这些寄存器保存在任何一个与进程一一对应的数据结构中。\n对于每个进程来说，有一个proc结构体，有一个trapframe结构体，所以我们可以将context保存于trapframe中。\n但是或许出于简化代码或者让代码更清晰的目的，trapframe还是只包含进入和离开内核时的数据。\n而context结构体中包含的是在内核线程和调度器线程之间切换时，需要保存和恢复的数据。\n学生提问：出让CPU是由用户发起的还是由内核发起的？\nRobert教授：对于XV6来说，并不会直接让用户线程出让CPU或者完成线程切换，而是由内核在合适的时间点做决定。\n有的时候你可以猜到特定的系统调用会导致出让CPU，例如一个用户进程读取pipe，而它知道pipe中并不能读到任何数据，这时你可以预测读取会被阻塞，而内核在等待数据的过程中会运行其他的进程。\n内核会在两个场景下出让CPU。\n当定时器中断触发了，内核总是会让当前进程出让CPU，因为我们需要在定时器中断间隔的时间点上交织执行所有想要运行的进程。\n另一种场景就是任何时候一个进程调用了系统调用并等待I/O，例如等待你敲入下一个按键，在你还没有按下按键时，等待I/O的机制会触发出让CPU。\n学生提问：用户进程调用sleep函数是不是会调用某个系统调用，然后将用户进程的信息保存在trapframe，然后触发进程切换，这时就不是定时器中断决定，而是用户进程自己决定了吧？\nRobert教授：如果进程执行了read系统调用，然后进入到了内核中。\n而read系统调用要求进程等待磁盘，这时系统调用代码会调用sleep，而sleep最后会调用swtch函数。\nswtch函数会保存内核线程的寄存器到进程的context中，然后切换到对应CPU的调度器线程，再让其他的线程运行。\n这样在当前线程等待磁盘读取结束时，其他线程还能运行。\n所以，这里的流程除了没有定时器中断，其他都一样，只是这里是因为一个系统调用需要等待I/O（注，感觉答非所问）\n学生提问：每一个CPU的调度器线程有自己的栈吗？\nRobert教授：是的，每一个调度器线程都有自己独立的栈。\n实际上调度器线程的所有内容，包括栈和context，与用户进程不一样，都是在系统启动时就设置好了。\n如果你查看XV6的start.s（注：是entry.S和start.c）文件，你就可以看到为每个CPU核设置好调度器线程。\n这里有一个术语需要解释一下。\n当人们在说context switching，他们通常说的是从一个线程切换到另一个线程，因为在切换的过程中需要先保存前一个线程的寄存器，然后再恢复之前保存的后一个线程的寄存器，这些寄存器都是保存在context对象中。\n在有些时候，context switching也指从一个用户进程切换到另一个用户进程的完整过程。\n偶尔你也会看到context switching是指从用户空间和内核空间之间的切换。\n对于我们这节课来说，context switching主要是指一个内核线程和调度器线程之间的切换。\n这里有一些有用的信息可以记住。\n每一个CPU核在一个时间只会做一件事情，每个CPU核在一个时间只会运行一个线程，它要么是运行用户进程的线程，要么是运行内核线程，要么是运行这个CPU核对应的调度器线程。\n所以在任何一个时间点，CPU核并没有做多件事情，而是只做一件事情。\n线程的切换创造了多个线程同时运行在一个CPU上的假象。\n类似的每一个线程要么是只运行在一个CPU核上，要么它的状态被保存在context中。\n线程永远不会运行在多个CPU核上，线程要么运行在一个CPU核上，要么就没有运行。\n在XV6的代码中，context对象总是由swtch函数产生，所以context总是保存了内核线程在执行swtch函数时的状态。\n当我们在恢复一个内核线程时，对于刚恢复的线程所做的第一件事情就是从之前的swtch函数中返回。\n学生提问：我们这里一直在说线程，但是从我看来XV6的实现中，一个进程就只有一个线程，有没有可能一个进程有多个线程？\nRobert教授：我们这里的用词的确有点让人混淆。\n在XV6中，一个进程要么在用户空间执行指令，要么是在内核空间执行指令，要么它的状态被保存在context和trapframe中，并且没有执行任何指令。\n这里该怎么称呼它呢？你可以根据自己的喜好来称呼它，对于我来说，每个进程有两个线程，一个用户空间线程，一个内核空间线程，并且存在限制使得一个进程要么运行在用户空间线程，要么为了执行系统调用或者响应中断而运行在内核空间线程 ，但是永远也不会两者同时运行。\nXV6进程切换示例程序 我们先来看一下proc.h中的proc结构体，从结构体中我们可以看到很多之前介绍的内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Per-process state struct proc { struct spinlock lock; // p-\u0026gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent\u0026#39;s wait int pid; // Process ID // these are private to the process, so p-\u0026gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) }; 首先是保存了用户空间线程寄存器的trapframe字段 其次是保存了内核线程寄存器的context字段 还有保存了当前进程的内核栈的kstack字段，这是进程在内核中执行时保存函数调用的位置 state字段保存了当前进程状态，要么是RUNNING，要么是RUNABLE，要么是SLEEPING等等 lock字段保护了很多数据，目前来说至少保护了对于state字段的更新。举个例子，因为有锁的保护，两个CPU的调度器线程不会同时拉取同一个RUNABLE进程并运行它 接下来会运行一个简单的演示程序，在这个程序中我们会从一个进程切换到另一个。\n这个程序中会创建两个进程，两个进程会一直运行。\n代码首先通过fork创建了一个子进程，然后两个进程都会进入一个死循环，并每隔一段时间生成一个输出表明程序还在运行。\n但是它们都不会很频繁的打印输出（注，每隔1000000次循环才打印一个输出），并且它们也不会主动出让CPU（注，因为每个进程都执行的是没有sleep的死循环）。\n所以我们这里有了两个运算密集型进程，并且因为我们接下来启动的XV6只有一个CPU核，它们都运行在同一个CPU上。\n为了让这两个进程都能运行，有必要让两个进程之间能相互切换。\n接下来运行spin程序，\n可以看到一直有字符在输出，一个进程在输出“/”，另一个进程在输出\u0026quot;\u0026quot;。\n从输出看，虽然现在XV6只有一个CPU核，但是每隔一会，XV6就在两个进程之间切换。\n“/”输出了一会之后，定时器中断将CPU切换到另一个进程运行然后又输出“\\”一会。所以可以看到定时器中断在起作用。\n接下来，在trap.c的devintr函数中的207行设置一个断点，这一行会识别出当前是在响应定时器中断。\n之后在gdb中continue。立刻会停在中断的位置，因为定时器中断还是挺频繁的。现在我们可以确认我们在usertrap函数中，并且usertrap函数通过调用devintr函数来处理这里的中断（注，从下图的栈输出可以看出）。\n因为devintr函数处理定时器中断的代码基本没有内容，接下来我在gdb中输入finish来从devintr函数中返回到usertrap函数。\n当我们返回到usertrap函数时，虽然我们刚刚从devintr函数中返回，但是我们期望运行到下面的yield函数。所以我们期望devintr函数返回2。\n可以从gdb中看到devintr的确返回的是2。\n在yield函数中，当前进程会出让CPU并让另一个进程运行。\n现在让我们看一下当定时器中断发生的时候，用户空间进程正在执行什么内容。\n我在gdb中输入print p来打印名称为p的变量。变量p包含了当前进程的proc结构体。\n学生提问：怎么区分不同进程的内核线程？\nRobert教授：每一个进程都有一个独立的内核线程。实际上有两件事情可以区分不同进程的内核线程。\n其中一件是，每个进程都有不同的内核栈，它由proc结构体中的kstack字段所指向；\n另一件就是，任何内核代码都可以通过调用myproc函数来获取当前CPU正在运行的进程。\n内核线程可以通过调用这个函数知道自己属于哪个用户进程。\nmyproc函数会使用tp寄存器来获取当前的CPU核的ID，并使用这个ID在一个保存了所有CPU上运行的进程的结构体数组中，找到对应的proc结构体。这就是不同的内核线程区分自己的方法。\n首先会打印p-\u0026gt;name来获取进程的名称，\n当前进程是spin程序，如预期一样。\n当前的进程ID是3，进程切换之后，我们预期进程ID会不一样。\n我们还可以通过打印变量p的trapframe字段获取表示用户空间状态的32个寄存器，这些都是我们在Lec06中学过的内容。这里面最有意思的可能是trapframe中保存的用户程序计数器。\n可以查看spin.asm文件来确定对应地址的指令。\n可以看到定时器中断触发时，用户进程正在执行死循环的加1，这符合我们的预期。\n学生提问：看起来所有的CPU核要能完成线程切换都需要有一个定时器中断，那如果硬件定时器出现故障了怎么办？\nRobert教授：是的，总是需要有一个定时器中断。\n用户进程的pre-emptive scheduling能工作的原因是，用户进程运行时，中断总是打开的。\nXV6会确保返回到用户空间时，中断是打开的。\n这意味着当代码在用户空间执行时，定时器中断总是能发生。\n在内核中会更加复杂点，因为内核中偶尔会关闭中断，比如当获取锁的时候，中断会被关闭，只有当锁被释放之后中断才会重新打开，所以如果内核中有一些bug导致内核关闭中断之后再也没有打开中断，同时内核中的代码永远也不会释放CPU，那么定时器中断不会发生。\n但是因为XV6是我们写的，所以它总是会重新打开中断。XV6中的代码如果关闭了中断，它要么过会会重新打开中断，然后内核中定时器中断可以发生并且我们可以从这个内核线程切换走，要么代码会返回到用户空间。我们相信XV6中不会有关闭中断然后还死循环的代码。\n同一个学生提问：我的问题是，定时器中断是来自于某个硬件，如果硬件出现故障了呢？\nRobert教授：那你的电脑坏了，你要买个新电脑了。\n这个问题是可能发生的，因为电脑中有上亿的晶体管，有的时候电脑会有问题，但是这超出了内核的管理范围了。\n所以我们假设计算机可以正常工作。\n有的时候软件会尝试弥补硬件的错误，比如通过网络传输packet，总是会带上checksum，这样如果某个网络设备故障导致某个bit反转了，可以通过checksum发现这个问题。\n但是对于计算机内部的问题，人们倾向于不用软件来尝试弥补硬件的错误。\n学生提问：当一个线程结束执行了，比如说在用户空间通过exit系统调用结束线程，同时也会关闭进程的内核线程。那么线程结束之后和下一个定时器中断之间这段时间，CPU仍然会被这个线程占有吗？还是说我们在结束线程的时候会启动一个新的线程？\nRobert教授：exit系统调用会出让CPU。\n尽管我们这节课主要是基于定时器中断来讨论，但是实际上XV6切换线程的绝大部分场景都不是因为定时器中断，比如说一些系统调用在等待一些事件并决定让出CPU。\nexit系统调用会做各种操作然后调用yield函数来出让CPU，这里的出让并不依赖定时器中断。\nXV6线程切换\u0026ndash;yield/sched函数 回到devintr函数返回到usertrap函数中的位置。\n在gdb里面输入几次step走到yield函数的调用。\nyield函数是整个线程切换的第一步，下面是yield函数的内容：\n1 2 3 4 5 6 7 8 9 10 // Give up the CPU for one scheduling round. void yield(void) { struct proc *p = myproc(); acquire(\u0026amp;p-\u0026gt;lock); p-\u0026gt;state = RUNNABLE; sched(); release(\u0026amp;p-\u0026gt;lock); } yield函数只做了几件事情，它首先获取了进程的锁。\n实际上，在锁释放之前，进程的状态会变得不一致，例如，yield将要将进程的状态改为RUNABLE，表明进程并没有在运行，但是实际上这个进程还在运行，代码正在当前进程的内核线程中运行。\n所以这里加锁的目的之一就是：即使我们将进程的状态改为了RUNABLE，其他的CPU核的调度器线程也不可能看到进程的状态为RUNABLE并尝试运行它。\n否则的话，进程就会在两个CPU核上运行了，而一个进程只有一个栈，这意味着两个CPU核在同一个栈上运行代码（注，因为XV6中一个用户进程只有一个用户线程）。\n接下来yield函数中将进程的状态改为RUNABLE。\n这里的意思是，当前进程要出让CPU，并切换到调度器线程。\n当前进程的状态是RUNABLE意味着它还会再次运行，因为毕竟现在是一个定时器中断打断了当前正在运行的进程。\n之后yield函数中调用了位于proc.c文件中的sched函数。\n我们进入到sched函数中，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // Switch to scheduler. Must hold only p-\u0026gt;lock // and have changed proc-\u0026gt;state. Saves and restores // intena because intena is a property of this // kernel thread, not this CPU. It should // be proc-\u0026gt;intena and proc-\u0026gt;noff, but that would // break in the few places where a lock is held but // there\u0026#39;s no process. void sched(void) { int intena; struct proc *p = myproc(); if(!holding(\u0026amp;p-\u0026gt;lock)) panic(\u0026#34;sched p-\u0026gt;lock\u0026#34;); if(mycpu()-\u0026gt;noff != 1) panic(\u0026#34;sched locks\u0026#34;); if(p-\u0026gt;state == RUNNING) panic(\u0026#34;sched running\u0026#34;); if(intr_get()) panic(\u0026#34;sched interruptible\u0026#34;); intena = mycpu()-\u0026gt;intena; swtch(\u0026amp;p-\u0026gt;context, \u0026amp;mycpu()-\u0026gt;context); mycpu()-\u0026gt;intena = intena; } 可以看出，sched函数基本没有干任何事情，只是做了一些合理性检查，如果发现异常就panic。\n为什么会有这么多检查？因为这里的XV6代码已经有很多年的历史了，这些代码经历过各种各样的bug，相应的这里就有各种各样的合理性检查和panic来避免可能的bug。\n我将跳过所有的检查，直接走到位于底部的swtch函数。\nXV6线程切换\u0026ndash;switch函数\nswtch函数会将当前的内核线程的寄存器保存到p-\u0026gt;context中。\nswtch函数的另一个参数c-\u0026gt;context，c表示当前CPU的结构体。\nCPU结构体中的context保存了当前CPU核的调度器线程的寄存器。\n所以swtch函数在保存完当前内核线程的内核寄存器之后，就会恢复当前CPU核的调度器线程的寄存器，并继续执行当前CPU核的调度器线程。\n接下来，我们快速的看一下我们将要切换到的context（注，也就是调度器线程的context）。\n因为我们只有一个CPU核，这里我在gdb中print cpus[0].context\n这里看到的就是之前保存的当前CPU核的调度器线程的寄存器。\n在这些寄存器中，最有趣的就是ra（Return Address）寄存器，因为ra寄存器保存的是当前函数的返回地址，所以调度器线程中的代码会返回到ra寄存器中的地址。\n通过查看kernel.asm，我们可以知道这个地址的内容是什么。也可以在gdb中输入“x/i 0x80001f2e”进行查看。\n输出中包含了地址中的指令和指令所在的函数名。\n所以我们将要返回到scheduler函数中。\n因为我们接下来要调用swtch函数，让我们来看看swtch函数的内容。swtch函数位于switch.s文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Context switch # # void swtch(struct context *old, struct context *new); # # Save current registers in old. Load from new.\t.globl swtch swtch: sd ra, 0(a0) sd sp, 8(a0) sd s0, 16(a0) sd s1, 24(a0) sd s2, 32(a0) sd s3, 40(a0) sd s4, 48(a0) sd s5, 56(a0) sd s6, 64(a0) sd s7, 72(a0) sd s8, 80(a0) sd s9, 88(a0) sd s10, 96(a0) sd s11, 104(a0) ld ra, 0(a1) ld sp, 8(a1) ld s0, 16(a1) ld s1, 24(a1) ld s2, 32(a1) ld s3, 40(a1) ld s4, 48(a1) ld s5, 56(a1) ld s6, 64(a1) ld s7, 72(a1) ld s8, 80(a1) ld s9, 88(a1) ld s10, 96(a1) ld s11, 104(a1) ret 首先，ra寄存器被保存在了a0寄存器指向的地址。\na0寄存器对应了swtch函数的第一个参数，从前面可以看出这是当前线程的context对象地址 ；\na1寄存器对应了swtch函数的第二个参数，从前面可以看出这是即将要切换到的调度器线程的context对象地址。\n所以函数中上半部分是将当前的寄存器保存在当前线程对应的context对象中，函数的下半部分是将调度器线程的寄存器，也就是我们将要切换到的线程的寄存器恢复到CPU的寄存器中。\n之后函数就返回了。所以调度器线程的ra寄存器的内容才显得有趣，因为它指向的是swtch函数返回的地址，也就是scheduler函数。\nswtch函数的上半部分保存了ra，sp等等寄存器，但是并没有保存程序计数器pc（Program Counter），因为程序计数器不管怎样都会随着函数调用更新。\n程序计数器并没有有效信息，我们现在知道我们在swtch函数中执行，所以保存程序计数器并没有意义。\n但是我们关心的是我们是从哪调用进到swtch函数的，因为当我们通过switch恢复执行当前线程并且从swtch函数返回时，我们希望能够从调用点继续执行。\nra寄存器保存了swtch函数的调用点，所以这里保存的是ra寄存器。\n我们可以打印ra寄存器，如你们所预期的一样，它指向了sched函数。\n为什么RISC-V中有32个寄存器，但是swtch函数中只保存并恢复了14个寄存器？\n因为switch是按照一个普通函数来调用的，对于有些寄存器，swtch函数的调用者默认swtch函数会做修改，所以调用者已经在自己的栈上保存了这些寄存器，当函数返回时，这些寄存器会自动恢复。所以swtch函数里只需要保存Callee Saved Register就行。\n因为swtch函数是从C代码调用的，所以我们知道Caller Saved Register会被C编译器保存在当前的栈上。\nCaller Saved Register大概有15-18个，而我们在swtch函数中只需要处理C编译器不会保存，但是对于swtch函数又有用的一些寄存器。\n最后我想看的是sp（Stack Pointer）寄存器。\n从它的值很难看出它的意义是什么。它实际是当前进程的内核栈地址，它由虚拟内存系统映射在了一个高地址。\n现在，我们保存了当前的寄存器，并从调度器线程的context对象恢复了寄存器，我直接跳到swtch函数的最后，也就是ret指令的位置。\n在我们实际返回之前，我们再来打印一些有趣的寄存器。\n首先sp寄存器有了一个不同的值，sp寄存器的值现在在内存中的stack0区域中。\n这个区域实际上是在启动顺序中非常非常早的一个位置，start.s在这个区域创建了栈，这样才可以调用第一个C函数。\n所以调度器线程运行在CPU对应的bootstack上。\n其次是ra寄存器，现在指向了scheduler函数，因为我们恢复了调度器线程的context对象中的内容。\n现在，我们其实已经在调度器线程中了，这里寄存器的值与上次打印的已经完全不一样了。\n虽然我们还在swtch函数中，但是现在我们实际上位于调度器线程调用的swtch函数中。\n调度器线程在启动过程中调用的也是swtch函数。\n接下来通过执行ret指令，我们就可以返回到调度器线程中。\n学生提问：我不知道我们使用的RISC-V处理器是不是有一些其他的状态？但是我知道一些Intel的X86芯片有floating point unit state等其他的状态，我们需要处理这些状态吗？\nRobert教授：你的观点非常对。在一些其他处理器例如X86中，线程切换的细节略有不同，因为不同的处理器有不同的状态。\n所以我们这里介绍的代码非常依赖RISC-V。\n其他处理器的线程切换流程可能看起来会非常的不一样，比如说可能要保存floating point寄存器。我不知道RISC-V如何处理浮点数，但是XV6内核并没有使用浮点数，所以不必担心。但是是的，线程切换与处理器非常相关。\n学生提问：为什么swtch函数要用汇编来实现，而不是C语言？\nRobert教授：C语言中很难与寄存器交互。\n可以肯定的是C语言中没有方法能更改sp、ra寄存器。\n所以在普通的C语言中很难完成寄存器的存储和加载，唯一的方法就是在C中嵌套汇编语言。\n所以我们也可以在C函数中内嵌switch中的指令，但是这跟我们直接定义一个汇编函数是一样的。\n或者说swtch函数中的操作是在C语言的层级之下，所以并不能使用C语言。\nXV6线程切换\u0026ndash;scheduler 来看一下scheduler的完整代码，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // Per-CPU process scheduler. // Each CPU calls scheduler() after setting itself up. // Scheduler never returns. It loops, doing: // - choose a process to run. // - swtch to start running that process. // - eventually that process transfers control // via swtch back to the scheduler. void scheduler(void) { struct proc *p; struct cpu *c = mycpu(); c-\u0026gt;proc = 0; for(;;){ // Avoid deadlock by ensuring that devices can interrupt. intr_on(); int nproc = 0; for(p = proc; p \u0026lt; \u0026amp;proc[NPROC]; p++) { acquire(\u0026amp;p-\u0026gt;lock); if(p-\u0026gt;state != UNUSED) { nproc++; } if(p-\u0026gt;state == RUNNABLE) { // Switch to chosen process. It is the process\u0026#39;s job // to release its lock and then reacquire it // before jumping back to us. p-\u0026gt;state = RUNNING; c-\u0026gt;proc = p; swtch(\u0026amp;c-\u0026gt;context, \u0026amp;p-\u0026gt;context); // Process is done running for now. // It should have changed its p-\u0026gt;state before coming back. c-\u0026gt;proc = 0; } release(\u0026amp;p-\u0026gt;lock); } if(nproc \u0026lt;= 2) { // only init and sh exist intr_on(); asm volatile(\u0026#34;wfi\u0026#34;); } } } 现在我们正运行在CPU拥有的调度器线程中，并且我们正好在之前调用swtch函数的返回状态。\n之前调度器线程调用switch是因为想要运行pid为3的进程，也就是刚刚被中断的spin程序。\n虽然pid为3的spin进程也调用了swtch函数，但是那个switch并不是当前返回的这个switch。\nspin进程调用的swtch函数还没有返回，而是保存在了pid为3的栈和context对象中。现在返回的是之前调度器线程对于swtch函数的调用。\n在scheduler函数中，因为我们已经停止了spin进程的运行，所以我们需要抹去对于spin进程的记录。\n我们接下来将c-\u0026gt;proc设置为0（c-\u0026gt;proc = 0;）。\n因为我们现在并没有在这个CPU核上运行这个进程，为了不让任何人感到困惑，我们这里将CPU核运行的进程对象设置为0。\n之前在yield函数中获取了进程的锁，因为yield不想进程完全进入到Sleep状态之前，任何其他的CPU核的调度器线程看到这个进程并运行它。\n而现在我们完成了从spin进程切换走，所以现在可以释放锁了。\n这就是release(\u0026amp;p-\u0026gt;lock)的意义。\n现在，我们仍然在scheduler函数中，但是其他的CPU核可以找到spin进程，并且因为spin进程是RUNABLE状态，其他的CPU可以运行它。\n这没有问题，因为我们已经完整的保存了spin进程的寄存器，并且我们不在spin进程的栈上运行程序，而是在当前CPU核的调度器线程栈上运行程序，所以其他的CPU核运行spin程序并没有问题。\n但是因为启动QEMU时我们只指定了一个核，所以在我们现在的演示中并没有其他的CPU核来运行spin程序。\n接下来我将简单介绍一下p-\u0026gt;lock。从调度的角度来说，这里的锁完成了两件事情。\n首先，出让CPU涉及到很多步骤，我们需要将进程的状态从RUNNING改成RUNABLE，我们需要将进程的寄存器保存在context对象中，并且我们还需要停止使用当前进程的栈。\n所以这里至少有三个步骤，而这三个步骤需要花费一些时间。\n所以锁的第一个工作就是在这三个步骤完成之前，阻止任何一个其他核的调度器线程看到当前进程。\n锁这里确保了三个步骤的原子性。从CPU核的角度来说，三个步骤要么全发生，要么全不发生。\n第二，当我们开始要运行一个进程时，p-\u0026gt;lock也有类似的保护功能。\n当我们要运行一个进程时，我们需要将进程的状态设置为RUNNING，我们需要将进程的context移到RISC-V的寄存器中。\n但是，如果在这个过程中，发生了中断，从中断的角度来说进程将会处于一个奇怪的状态。\n比如说进程的状态是RUNNING，但是又还没有将所有的寄存器从context对象拷贝到RISC-V寄存器中。\n所以，如果这时候有了一个定时器中断将会是个灾难，因为我们可能在寄存器完全恢复之前，从这个进程中切换走。\n而从这个进程切换走的过程中，将会保存不完整的RISC-V寄存器到进程的context对象中。\n所以我们希望启动一个进程的过程也具有原子性。\n在这种情况下，切换到一个进程的过程中，也需要获取进程的锁以确保其他的CPU核不能看到这个进程。\n同时在切换到进程的过程中，还需要关闭中断，这样可以避免定时器中断看到还在切换过程中的进程。\n现在我们在scheduler函数的循环中，代码会检查所有的进程并找到一个来运行。\n现在我们知道还有另一个进程，因为我们之前fork了另一个spin进程。\n这里我跳过进程检查，直接在找到RUNABLE进程的位置设置一个断点。\n在代码的468行，获取了进程的锁，所以现在我们可以进行切换到进程的各种步骤。\n在代码的473行，进程的状态被设置成了RUNNING。\n代码的474行将找到的RUNABLE进程记录为当前CPU执行的进程。\n代码的475行，又调用了swtch函数来保存调度器线程的寄存器，并恢复目标进程的寄存器（注，实际上恢复的是目标进程的内核线程）。\n我们可以打印新的进程的名字来查看新的进程。可以看到进程名还是spin，但是pid已经变成了4，而前一个进程的pid是3。\n我们还可以查看目标进程的context对象，其中ra寄存器的内容就是我们要切换到的目标线程的代码位置。\n虽然我们在代码475行调用的是swtch函数，但是我们前面已经看过了swtch函数会返回到即将恢复的ra寄存器地址，所以我们真正关心的就是ra指向的地址。\n通过打印这个地址的内容，可以看到swtch函数会返回到sched函数中。\n这完全在意料之中，因为可以预期的是，将要切换到的进程之前是被定时器中断通过sched函数挂起的，并且之前在sched函数中又调用了swtch函数。\n在swtch函数的最开始，我们仍然在调度器线程中，但是这一次是从调度器线程切换到目标进程的内核线程。\n所以从swtch函数内部将会返回到目标进程的内核线程的sched函数，通过打印backtrace，我们可以看到，之前有一个usertrap的调用，这必然是之前因为定时器中断而出现的调用。\n之后在中断处理函数中还调用了yield和sched函数，正如我们之前看到的一样。\n但是，这里调用yield和sched函数是在pid为4的进程调用的，而不是我们刚刚看的pid为3的进程。\n学生提问：如果不是因为定时器中断发生的切换，我们是不是可以期望ra寄存器指向其他位置，例如sleep函数？\nRobert教授：是的，我们之前看到了代码执行到这里会包含一些系统调用相关的函数。\n你基本上回答了自己的问题，如果我们因为定时器中断之外的原因而停止了执行当前的进程，switch会返回到一些系统调用的代码中，而不是我们这里看到sched函数。\n我记得sleep最后也调用了sched函数，虽然bracktrace可能看起来会不一样，但是还是会包含sched。\n所以我这里只介绍了一种进程间切换的方法，也就是因为定时器中断而发生切换。\n但是还有其他的可能会触发进程切换，例如等待I/O或者等待另一个进程向pipe写数据。\n这里有件事情需要注意，调度器线程调用了swtch函数，但是我们从swtch函数返回时，实际上是返回到了对于switch的另一个调用，而不是调度器线程中的调用。\n我们返回到的是pid为4的进程在很久之前对于switch的调用。这里可能会有点让人困惑，但是这就是线程切换的核心。\n另一件需要注意的事情是，swtch函数是线程切换的核心，但是swtch函数中只有保存寄存器，再加载寄存器的操作。\n线程除了寄存器以外的还有很多其他状态，它有变量，堆中的数据等等，但是所有的这些数据都在内存中，并且会保持不变。\n我们没有改变线程的任何栈或者堆数据。\n所以线程切换的过程中，处理器中的寄存器是唯一的不稳定状态，且需要保存并恢复。\n而所有其他在内存中的数据会保存在内存中不被改变，所以不用特意保存并恢复。\n我们只是保存并恢复了处理器中的寄存器，因为我们想在新的线程中也使用相同的一组寄存器。\nXV6线程第一次调用switch函数 学生提问Linux内一个进程多个线程的实现方式，因为在XV6中，一个进程只有一个用户线程\n学生提问：操作系统都带了线程的实现，如果想要在多个CPU上运行一个进程内的多个线程，那需要通过操作系统来处理而不是用户空间代码，是吧？那这里的线程切换是怎么工作的？是每个线程都与进程一样了吗？操作系统还会遍历所有存在的线程吗？比如说我们有8个核，每个CPU核都会在多个进程的更多个线程之间切换。同时我们也不想只在一个CPU核上切换一个进程的多个线程，是吧？\nRobert教授：Linux是支持一个进程包含多个线程，Linux的实现比较复杂，或许最简单的解释方式是：几乎可以认为Linux中的每个线程都是一个完整的进程。\nLinux中，我们平常说一个进程中的多个线程，本质上是共享同一块内存的多个独立进程。\n所以Linux中一个进程的多个线程仍然是通过一个内存地址空间执行代码。\n如果你在一个进程创建了2个线程，那基本上是2个进程共享一个地址空间。\n之后，调度就与XV6是一致的，也就是针对每个进程进行调度。\n学生提问：用户可以指定将线程绑定在某个CPU上吗？操作系统如何确保一个进程的多个线程不会运行在同一个CPU核上？要不然就违背了多线程的初衷了。\nRobert教授：这里其实与XV6非常相似，假设有4个CPU核，Linux会找到4件事情运行在这4个核上。\n如果并没有太多正在运行的程序的话，或许会将一个进程的4个线程运行在4个核上。\n或者如果有100个用户登录在Athena机器上，内核会随机为每个CPU核找到一些事情做。\n如果你想做一些精细的测试，有一些方法可以将线程绑定在CPU核上，但正常情况下人们不会这么做。\n学生提问：所以说一个进程中的多个线程会有相同的page table？\nRobert教授：是的，如果你在Linux上，你为一个进程创建了2个线程，我不确定它们是不是共享同一个的page table，还是说它们是不同的page table，但是内容是相同的。\n学生提问：有没有原因说这里的page table要是分开的？\nRobert教授：我不知道Linux究竟用了哪种方法。\n以下是线程第一次调用switch的过程\n学生提问：当调用swtch函数的时候，实际上是从一个线程对于switch的调用切换到了另一个线程对于switch的调用。所以线程第一次调用swtch函数时，需要伪造一个“另一个线程”对于switch的调用，是吧？因为也不能通过swtch函数随机跳到其他代码去。\nRobert教授：是的。我们来看一下第一次调用switch时，“另一个”调用swtch函数的线程的context对象。\nproc.c文件中的allocproc函数会被启动时的第一个进程和fork调用，allocproc会设置好新进程的context，如下所示：\n实际上大部分寄存器的内容都无所谓。\n但是ra很重要，因为这是进程的第一个switch调用会返回的位置。\n同时因为进程需要有自己的栈，所以ra和sp都被设置了。\n这里设置的forkret函数就是进程的第一次调用swtch函数会切换到的“另一个”线程位置。\n学生提问：所以当swtch函数返回时，CPU会执行forkret中的指令，就像forkret刚刚调用了swtch函数并且返回了一样？\nRobert教授：是的，从switch返回就直接跳到了forkret的最开始位置。\n学生提问：我们会在其他场合调用forkret吗？还是说它只会用在这？\nRobert教授：是的，它只会在启动进程的时候以这种奇怪的方式运行。下面是forkret函数的代码，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // A fork child\u0026#39;s very first scheduling by scheduler() // will swtch to forkret. void forkret(void) { static int first = 1; // Still holding p-\u0026gt;lock from scheduler. release(\u0026amp;myproc()-\u0026gt;lock); if (first) { // File system initialization must be run in the context of a // regular process (e.g., because it calls sleep), and thus cannot // be run from main(). first = 0; fsinit(ROOTDEV); } usertrapret(); } 从代码中看，它的工作其实就是释放调度器之前获取的锁。\n函数最后的usertrapret函数其实也是一个假的函数，它会使得程序表现的看起来像是从trap中返回，但是对应的trapframe其实也是假的，这样才能跳到用户的第一个指令中。\n学生提问：与之前的context对象类似的是，对于trapframe也不用初始化任何寄存器，因为我们要去的是程序的最开始，所以不需要做任何假设，对吧？\nRobert教授：我认为程序计数器还是要被初始化为0的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Set up first user process. void userinit(void) { struct proc *p; p = allocproc(); initproc = p; // allocate one user page and copy init\u0026#39;s instructions // and data into it. uvminit(p-\u0026gt;pagetable, initcode, sizeof(initcode)); p-\u0026gt;sz = PGSIZE; // prepare for the very first \u0026#34;return\u0026#34; from kernel to user. p-\u0026gt;trapframe-\u0026gt;epc = 0; // user program counter p-\u0026gt;trapframe-\u0026gt;sp = PGSIZE; // user stack pointer safestrcpy(p-\u0026gt;name, \u0026#34;initcode\u0026#34;, sizeof(p-\u0026gt;name)); p-\u0026gt;cwd = namei(\u0026#34;/\u0026#34;); p-\u0026gt;state = RUNNABLE; release(\u0026amp;p-\u0026gt;lock); } 因为fork拷贝的进程会同时拷贝父进程的程序计数器，所以我们唯一不是通过fork创建进程的场景就是创建第一个进程的时候。这时需要设置程序计数器为0。\n学生提问：在fortret函数中，if(first)是什么意思？\nRobert教授：文件系统需要被初始化，具体来说，需要从磁盘读取一些数据来确保文件系统的运行，比如说文件系统究竟有多大，各种各样的东西在文件系统的哪个位置，同时还需要有crash recovery log。\n完成任何文件系统的操作都需要等待磁盘操作结束，但是XV6只能在进程的context下执行文件系统操作，比如等待I/O。所以初始化文件系统需要等到我们有了一个进程才能进行。而这一步是在第一次调用forkret时完成的，所以在forkret中才有了if(first)判断。\n","date":"2024-09-21T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08111thread-switching/bg_hu_414a7d19571441a3.jpg","permalink":"https://echudet.github.io/p/mit6s08111thread-switching/","title":"MIT6S081：11Thread switching"},{"content":"Lecture 10 Multiprocessors and locking 预习 为什么要用锁 大多数内核，包括xv6，交错执行多个活动。交错的一个来源是多处理器硬件：计算机的多个CPU之间独立执行，如xv6的RISC-V。多个处理器共享物理内存，xv6利用共享（sharing）来维护所有CPU进行读写的数据结构。\n这种共享增加了一种可能性，即一个CPU读取数据结构，而另一个CPU正在更新它，甚至多个CPU同时更新相同的数据；如果不仔细设计，这种并行访问可能会产生不正确的结果或损坏数据结构。\n即使在单处理器上，内核也可能在许多线程之间切换CPU，导致它们的执行交错。\n最后，如果中断发生在错误的时间，设备中断处理程序修改与某些可中断代码相同的数据，可能导致数据损坏。\n单词并发（concurrency）是指由于多处理器并行、线程切换或中断，多个指令流交错的情况。\n内核中充满了并发访问数据（concurrently-accessed data）。例如，两个CPU可以同时调用kalloc，从而从空闲列表的头部弹出。\n以并发下的正确性为目标的策略和支持它们的抽象称为并发控制技术（concurrency control techniques）。\n锁的作用 锁提供了互斥，确保一次只有一个CPU可以持有锁。如果程序员将每个共享数据项关联一个锁，并且代码在使用一个数据项时总是持有相关联的锁，那么该项一次将只被一个CPU使用。在这种情况下，我们说锁保护数据项。尽管锁是一种易于理解的并发控制机制，但锁的缺点是它们会扼杀性能，因为它们会串行化并发操作。\n竞态条件 考虑两个进程在两个不同的CPU上调用wait。wait释放了子进程的内存。因此，在每个CPU上，内核将调用kfree来释放子进程的页面。内核分配器维护一个链接列表：kalloc() 从空闲页面列表中取出（pop）一个内存页面；kfree()将一个内存页面添加（push）到空闲列表上。为了获得最佳性能，我们可能希望两个父进程的kfree可以并行执行，而不必等待另一个进程，但是考虑到xv6的kfree实现，这将导致错误。\n链表位于两个CPU共享的内存中，这两个CPU使用load和store指令操作链表。（实际上，每个处理器都有cache，但从概念上讲，多处理器系统的行为就像所有CPU共享一块单独的内存一样）如果没有并发请求，您可能以如下方式实现列表push操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct element { int data; struct element *next; }; struct element *list = 0; void push(int data) { struct element *l; l = malloc(sizeof *l); l-\u0026gt;data = data; l-\u0026gt;next = list; list = l; } 如果存在隔离性，那么这个实现是正确的。但是，如果多个副本并发执行，代码就会出错。如果两个CPU同时执行push，如图6.1所示，两个CPU都可能在执行第16行之前执行第15行，这会导致如图6.2所示的不正确的结果。然后会有两个类型为element的列表元素使用next指针设置为list的前一个值。当两次执行位于第16行的对list的赋值时，第二次赋值将覆盖第一次赋值；第一次赋值中涉及的元素将丢失。\n竞态条件是指多个进程读写某些共享数据（至少有一个访问是写入）的情况。竞争通常包含bug，要么丢失更新（如果访问是写入的），要么读取未完成更新的数据结构。竞争的结果取决于进程在处理器运行的确切时机以及内存系统如何排序它们的内存操作，这可能会使竞争引起的错误难以复现和调试。例如，在调试push时添加printf语句可能会改变执行的时间，从而使竞争消失。\n避免竞争的通常方法是使用锁。锁确保互斥，这样一次只有一个CPU可以执行push中敏感的代码行；这使得上述情况不可能发生。\n上面代码的正确上锁版本只添加了几行（用黄色突出显示）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct element { int data; struct element *next; }; struct element *list = 0; struct lock listlock; void push(int data) { struct element *l; l = malloc(sizeof *l); l-\u0026gt;data = data; acquire(\u0026amp;listlock); l-\u0026gt;next = list; list = l; release(\u0026amp;listlock); } acquire和release之间的指令序列通常被称为临界区域（critical section）。锁的作用通常被称为保护list。\n当我们说锁保护数据时，我们实际上是指锁保护适用于数据的某些不变量集合。不变量是跨操作维护的数据结构的属性。\n通常，操作的正确行为取决于操作开始时不变量是否为真。操作可能暂时违反不变量，但必须在完成之前重新建立它们。\n例如，在链表的例子中，不变量是list指向列表中的第一个元素，以及每个元素的next字段指向下一个元素。push的实现暂时违反了这个不变量：在第17行，l-\u0026gt;next指向list（注：则此时list不再指向列表中的第一个元素，即违反了不变量），但是list还没有指向l（在第18行重新建立）。我们上面检查的竞态条件发生了，因为第二个CPU执行了依赖于列表不变量的代码，而这些代码（暂时）被违反了。正确使用锁可以确保每次只有一个CPU可以对临界区域中的数据结构进行操作，因此当数据结构的不变量不成立时，将没有其他CPU对数据结构执行操作。\n不变量（invariant） 是在程序或数据结构的某些时刻始终必须保持为真的条件或规则。它们是系统运行过程中所必须遵循的“永恒真理”，确保数据结构处于正确的状态。如果不变量被违反，程序的行为可能会出现错误。\n不变量确保了数据结构的正确性，而在并发环境下（多CPU同时操作），如果没有锁的保护，某个CPU在不变量被暂时打破时访问数据结构，就可能导致不一致的情况。\n因此，锁的作用是确保数据结构的修改过程能够独占，防止其他CPU在不变量暂时不成立时访问。\n您可以将锁视为串行化（serializing）并发的临界区域，以便同时只有一个进程在运行这部分代码，从而维护不变量（假设临界区域设定了正确的隔离性）。您还可以将由同一锁保护的临界区域视为彼此之间的原子，即彼此之间只能看到之前临界区域的完整更改集，而永远看不到部分完成的更新。\n尽管正确使用锁可以改正不正确的代码，但锁限制了性能。\n如果两个进程并发调用kfree，锁将串行化这两个调用，我们在不同的CPU上运行它们没有任何好处。如果多个进程同时想要相同的锁或者锁经历了争用，则称之为发生冲突（conflict）。内核设计中的一个主要挑战是避免锁争用。\n锁的位置对性能也很重要。例如，在push中把acquire的位置提前也是正确的：将acquire移动到第13行之前完全没问题。但这样对malloc的调用也会被串行化，从而降低了性能。\n代码：Locks Xv6有两种类型的锁：自旋锁（spinlocks）和睡眠锁（sleep-locks）。\n从自旋锁（注：自旋，即循环等待）开始。\nXv6将自旋锁表示为struct spinlock。结构体中的重要字段是locked，当锁可用时为零，当它被持有时为非零。\n从逻辑上讲，xv6应该通过执行以下代码来获取锁\n1 2 3 4 5 6 7 8 9 10 void acquire(struct spinlock* lk) // does not work! { for(;;) { if(lk-\u0026gt;locked == 0) { lk-\u0026gt;locked = 1; break; } } } 不幸的是，这种实现不能保证多处理器上的互斥。\n可能会发生两个CPU同时到达第5行，看到lk-\u0026gt;locked为零，然后都通过执行第6行占有锁。此时就有两个不同的CPU持有锁，从而违反了互斥属性。我们需要的是一种方法，使第5行和第6行作为原子（即不可分割）步骤执行。\n因为锁被广泛使用，多核处理器通常提供实现第5行和第6行的原子版本的指令。\n在RISC-V上，这条指令是amoswap r, a。amoswap读取内存地址a处的值，将寄存器r的内容写入该地址，并将其读取的值放入r中。\n也就是说，它交换寄存器和指定内存地址的内容。\n它原子地执行这个指令序列，使用特殊的硬件来防止任何其他CPU在读取和写入之间使用内存地址。\nXv6的acquire使用可移植的C库调用归结为amoswap的指令__sync_lock_test_and_set；返回值是lk-\u0026gt;locked的旧（交换了的）内容。\nacquire函数将swap包装在一个循环中，直到它获得了锁前一直重试（自旋）。\n每次迭代将1与lk-\u0026gt;locked进行swap操作，并检查lk-\u0026gt;locked之前的值。\n如果之前为0，swap已经把lk-\u0026gt;locked设置为1，那么我们就获得了锁；如果前一个值是1，那么另一个CPU持有锁，我们原子地将1与lk-\u0026gt;locked进行swap的事实并没有改变它的值。\n获取锁后，用于调试，acquire将记录下来获取锁的CPU。lk-\u0026gt;cpu字段受锁保护，只能在保持锁时更改。\n函数release与acquire相反：它清除lk-\u0026gt;cpu字段，然后释放锁。\n从概念上讲，release只需要将0分配给lk-\u0026gt;locked。C标准允许编译器用多个存储指令实现赋值，因此对于并发代码，C赋值可能是非原子的。\n因此release使用执行原子赋值的C库函数__sync_lock_release。该函数也可以归结为RISC-V的amoswap指令。\n代码：使用锁 Xv6在许多地方使用锁来避免竞争条件（race conditions）。\n如上所述，kalloc(kernel/kalloc.c:69)和kfree(kernel/kalloc.c:47)就是一个很好的例子。\n使用锁的一个困难部分是决定要使用多少锁，以及每个锁应该保护哪些数据和不变量。\n有几个基本原则。\n首先，任何时候可以被一个CPU写入，同时又可以被另一个CPU读写的变量，都应该使用锁来防止两个操作重叠。\n其次，请记住锁保护不变量（invariants）：如果一个不变量涉及多个内存位置，通常所有这些位置都需要由一个锁来保护，以确保不变量不被改变。\n为了提高效率，不要向太多地方上锁是很重要的，因为锁会降低并行性。\n如果并行性不重要，那么可以安排只拥有一个线程，而不用担心锁。\n一个简单的内核可以在多处理器上做到这一点，方法是拥有一个锁，这个锁必须在进入内核时获得，并在退出内核时释放（尽管如管道读取或wait的系统调用会带来问题）。\n许多单处理器操作系统已经被转换为使用这种方法在多处理器上运行，有时被称为“大内核锁（big kernel lock）”。\n但是这种方法牺牲了并行性：一次只能有一个CPU运行在内核中。如果内核做一些繁重的计算，使用一组更细粒度的锁的集合会更有效率，这样内核就可以同时在多个处理器上执行。\n作为粗粒度锁的一个例子，xv6的kalloc.c分配器有一个由单个锁保护的空闲列表。\n如果不同CPU上的多个进程试图同时分配页面，每个进程在获得锁之前将必须在acquire中自旋等待。自旋会降低性能，因为它只是无用的等待。\n如果对锁的争夺浪费了很大一部分CPU时间，也许可以通过改变分配器的设计来提高性能，使其拥有多个空闲列表，每个列表都有自己的锁，以允许真正的并行分配。\n作为细粒度锁定的一个例子，xv6对每个文件都有一个单独的锁，这样操作不同文件的进程通常可以不需等待彼此的锁而继续进行。\n文件锁的粒度可以进一步细化，以允许进程同时写入同一个文件的不同区域。最终的锁粒度决策需要由性能测试和复杂性考量来驱动。\n作为预览，表6.3列出了xv6中的所有锁。\n锁 描述 bcache.lock 保护块缓冲区缓存项（block buffer cache entries）的分配 cons.lock 串行化对控制台硬件的访问，避免混合输出 ftable.lock 串行化文件表中文件结构体的分配 icache.lock 保护索引结点缓存项（inode cache entries）的分配 vdisk_lock 串行化对磁盘硬件和DMA描述符队列的访问 kmem.lock 串行化内存分配 log.lock 串行化事务日志操作 管道的pi-\u0026gt;lock 串行化每个管道的操作 pid_lock 串行化next_pid的增量 进程的p-\u0026gt;lock 串行化进程状态的改变 tickslock 串行化时钟计数操作 索引结点的 ip-\u0026gt;lock 串行化索引结点及其内容的操作 缓冲区的b-\u0026gt;lock 串行化每个块缓冲区的操作 Figure 6.3: Locks in xv6\n死锁和锁排序 如果在内核中执行的代码路径必须同时持有数个锁，那么所有代码路径以相同的顺序获取这些锁是很重要的。\n如果它们不这样做，就有死锁的风险。\n假设xv6中的两个代码路径需要锁A和B，但是代码路径1按照先A后B的顺序获取锁，另一个路径按照先B后A的顺序获取锁。假设线程T1执行代码路径1并获取锁A，线程T2执行代码路径2并获取锁B。接下来T1将尝试获取锁B，T2将尝试获取锁A。\n两个获取都将无限期阻塞，因为在这两种情况下，另一个线程都持有所需的锁，并且不会释放它，直到它的获取返回。\n为了避免这种死锁，所有代码路径必须以相同的顺序获取锁。全局锁获取顺序的需求意味着锁实际上是每个函数规范的一部分：调用者必须以一种使锁按照约定顺序被获取的方式调用函数。\n由于sleep的工作方式（见第7章），Xv6有许多包含每个进程的锁（每个struct proc中的锁）在内的长度为2的锁顺序链。\n例如，consoleintr (*kernel/console.c*:138)是处理键入字符的中断例程。当换行符到达时，任何等待控制台输入的进程都应该被唤醒。为此，consoleintr在调用wakeup时持有cons.lock，wakeup获取等待进程的锁以唤醒它。\n因此，全局避免死锁的锁顺序包括必须在任何进程锁之前获取cons.lock的规则。\n文件系统代码包含xv6最长的锁链。例如，创建一个文件需要同时持有目录上的锁、新文件inode上的锁、磁盘块缓冲区上的锁、磁盘驱动程序的vdisk_lock和调用进程的p-\u0026gt;lock。为了避免死锁，文件系统代码总是按照前一句中提到的顺序获取锁。\n遵守全局死锁避免的顺序可能会出人意料地困难。\n有时锁顺序与逻辑程序结构相冲突，例如，也许代码模块M1调用模块M2，但是锁顺序要求在M1中的锁之前获取M2中的锁。\n有时锁的身份是事先不知道的，也许是因为必须持有一个锁才能发现下一个要获取的锁的身份。\n这种情况在文件系统中出现，因为它在路径名称中查找连续的组件，也在wait和exit代码中出现，因为它们在进程表中寻找子进程。\n最后，死锁的危险通常是对细粒度锁定方案的限制，因为更多的锁通常意味着更多的死锁可能性。避免死锁的需求通常是内核实现中的一个主要因素。\n锁和中断处理函数 一些xv6自旋锁保护线程和中断处理程序共用的数据。\n例如，clockintr定时器中断处理程序在增加ticks(kernel/trap.c:163)的同时内核线程可能在sys_sleep(kernel/sysproc.c:64)中读取ticks。锁tickslock串行化这两个访问。\n自旋锁和中断的交互引发了潜在的危险。\n假设sys_sleep持有tickslock，并且它的CPU被计时器中断中断。clockintr会尝试获取tickslock，意识到它被持有后等待释放。在这种情况下，tickslock永远不会被释放：只有sys_sleep可以释放它，但是sys_sleep直到clockintr返回前不能继续运行。所以CPU会死锁，任何需要锁的代码也会冻结。\n为了避免这种情况，如果一个自旋锁被中断处理程序所使用，那么CPU必须保证在启用中断的情况下永远不能持有该锁。\nXv6更保守：当CPU获取任何锁时，xv6总是禁用该CPU上的中断。中断仍然可能发生在其他CPU上，此时中断的acquire可以等待线程释放自旋锁；由于不在同一CPU上，不会造成死锁。\n当CPU未持有自旋锁时，xv6重新启用中断；它必须做一些记录来处理嵌套的临界区域。\nacquire调用push_off (kernel/spinlock.c:89) 并且release调用pop_off (kernel/spinlock.c:100)来跟踪当前CPU上锁的嵌套级别。当计数达到零时，pop_off恢复最外层临界区域开始时存在的中断使能状态。intr_off和intr_on函数执行RISC-V指令分别用来禁用和启用中断。\n严格的在设置lk-\u0026gt;locked (kernel/spinlock.c:28)之前让acquire调用push_off是很重要的。\n如果两者颠倒，会存在一个既持有锁又启用了中断的短暂窗口期，不幸的话定时器中断会使系统死锁。同样，只有在释放锁之后，release才调用pop_off也是很重要的(kernel/spinlock.c:66)。\n指令和内存访问排序 人们很自然地会想到程序是按照源代码语句出现的顺序执行的。\n然而，许多编译器和中央处理器为了获得更高的性能而不按顺序执行代码。\n如果一条指令需要许多周期才能完成，中央处理器可能会提前发出指令，这样它就可以与其他指令重叠，避免中央处理器停顿。\n例如，中央处理器可能会注意到在顺序指令序列A和B中彼此不存在依赖。CPU也许首先启动指令B，或者是因为它的输入先于A的输入准备就绪，或者是为了重叠执行A和B。编译器可以执行类似的重新排序，方法是在源代码中一条语句的指令发出之前，先发出另一条语句的指令。\n编译器和CPU在重新排序时需要遵循一定规则，以确保它们不会改变正确编写的串行代码的结果。\n然而，规则确实允许重新排序后改变并发代码的结果，并且很容易导致多处理器上的不正确行为。\nCPU的排序规则称为内存模型（memory model）。\n例如，在push的代码中，如果编译器或CPU将对应于第4行的存储指令移动到第6行release后的某个地方，那将是一场灾难：\n1 2 3 4 5 6 l = malloc(sizeof *l); l-\u0026gt;data = data; acquire(\u0026amp;listlock); l-\u0026gt;next = list; list = l; release(\u0026amp;listlock); 如果发生这样的重新排序，将会有一个窗口期，另一个CPU可以获取锁并查看更新后的list，但却看到一个未初始化的list-\u0026gt;next。\n为了告诉硬件和编译器不要执行这样的重新排序，xv6在acquire(kernel/spinlock.c、:22) 和release(kernel/spinlock.c:47)中都使用了__sync_synchronize()。\n__sync_synchronize()是一个内存障碍：它告诉编译器和CPU不要跨障碍重新排序load或store指令。\n因为xv6在访问共享数据时使用了锁，xv6的acquire和release中的障碍在几乎所有重要的情况下都会强制顺序执行。\n睡眠锁 有时xv6需要长时间保持锁。\n例如，文件系统（第8章）在磁盘上读写文件内容时保持文件锁定，这些磁盘操作可能需要几十毫秒。如果另一个进程想要获取自旋锁，那么长时间保持自旋锁会导致获取进程在自旋时浪费很长时间的CPU。\n自旋锁的另一个缺点是，一个进程在持有自旋锁的同时不能让出（yield）CPU，然而我们希望持有锁的进程等待磁盘I/O的时候其他进程可以使用CPU。\n持有自旋锁时让步是非法的，因为如果第二个线程试图获取自旋锁，就可能导致死锁：因为acquire不会让出CPU，第二个线程的自旋可能会阻止第一个线程运行并释放锁。在持有锁时让步也违反了在持有自旋锁时中断必须关闭的要求。\n因此，我们想要一种锁，它在等待获取锁时让出CPU，并允许在持有锁时让步（以及中断）。\nXv6以睡眠锁（sleep-locks）的形式提供了这种锁。acquiresleep (kernel/sleeplock.c:22) 在等待时让步CPU。\n在更高层次上，睡眠锁有一个被自旋锁保护的锁定字段，acquiresleep对sleep的调用原子地让出CPU并释放自旋锁。结果是其他线程可以在acquiresleep等待时执行。\n因为睡眠锁保持中断使能，所以它们不能用在中断处理程序中。\n因为acquiresleep可能会让出CPU，所以睡眠锁不能在自旋锁临界区域中使用（尽管自旋锁可以在睡眠锁临界区域中使用）。\n因为等待会浪费CPU时间，所以自旋锁最适合短的临界区域；睡眠锁对于冗长的操作效果很好。\n自旋锁：当进程想要获取自旋锁时，如果锁被占用，进程不会暂停或让出CPU，而是不断地尝试（自旋），直到获取锁。这意味着自旋锁要求进程始终保持运行，不让出CPU，且在锁释放之前，系统无法调度其他进程来运行。 睡眠锁：当进程想要获取睡眠锁时，如果锁被占用，进程会暂停（睡眠），并让出CPU，使得其他进程可以被调度来运行。等到锁可用时，系统会重新唤醒这个进程。 当一个进程持有自旋锁时，中断是关闭的，这是为了确保在自旋锁保护的临界区内，系统不会发生上下文切换（即不会暂停这个进程去调度其他进程）。\n真实世界 尽管对并发原语和并行性进行了多年的研究，但使用锁进行编程仍然具有挑战性。\n通常最好将锁隐藏在更高级别的结构中，如同步队列，尽管xv6没有这样做。\n如果您使用锁进行编程，明智的做法是使用试图识别竞争条件（race conditions）的工具，因为很容易错过需要锁的不变量。\n大多数操作系统都支持POSIX线程（Pthread），它允许一个用户进程在不同的CPU上同时运行几个线程。\nPthread支持用户级锁（user-level locks）、障碍（barriers）等。\n支持Pthread需要操作系统的支持。\n例如，应该是这样的情况，如果一个Pthread在系统调用中阻塞，同一进程的另一个Pthread应当能够在该CPU上运行。\n另一个例子是，如果一个线程改变了其进程的地址空间（例如，映射或取消映射内存），内核必须安排运行同一进程下的线程的其他CPU更新其硬件页表，以反映地址空间的变化。\n没有原子指令实现锁是可能的，但是代价昂贵，并且大多数操作系统使用原子指令。\n如果许多CPU试图同时获取相同的锁，可能会付出昂贵的开销。\n如果一个CPU在其本地cache中缓存了一个锁，而另一个CPU必须获取该锁，那么更新保存该锁的cache行的原子指令必须将该行从一个CPU的cache移动到另一个CPU的cache中，并且可能会使cache行的任何其他副本无效。从另一个CPU的cache中获取cache行可能比从本地cache中获取一行的代价要高几个数量级。\n为了避免与锁相关的开销，许多操作系统使用无锁的数据结构和算法。\n例如，可以实现一个像本章开头那样的链表，在列表搜索期间不需要锁，并且使用一个原子指令在一个列表中插入一个条目。\n然而，无锁编程比有锁编程更复杂；例如，人们必须担心指令和内存重新排序。\n有锁编程已经很难了，所以xv6避免了无锁编程的额外复杂性。\n关于POSIX与Pthread 操作系统支持**POSIX线程（Pthread）**时的一些关键机制，尤其是在多核处理器环境下，如何管理多个线程和进程之间的资源共享和同步。\nPthread 是一种标准化的多线程编程接口，支持在同一个用户进程中同时运行多个线程。\n线程是轻量级进程，它们共享同一个进程的地址空间、文件描述符和其他资源，但可以并行执行。在多核处理器上，不同的线程可以分布在不同的CPU上并行执行。\n用户级锁，这些锁是专门用来控制并发的机制，用于防止多个线程同时修改共享数据，导致数据不一致。\n用户级锁的操作由应用程序控制，不涉及内核。\n例如，多个线程共享一个变量时，通过锁来确保一次只有一个线程能够访问或修改这个变量。\n在操作系统支持Pthread的情况下，用户进程可以同时运行多个线程。每个线程可以分布在不同的CPU上并行执行。\n因此，即使一个线程在执行系统调用时阻塞了（无法继续执行），同一个进程中的另一个线程仍然可以继续在该CPU上运行。\n这确保了即使有线程在系统调用中被阻塞，也不会影响同一进程中其他线程的运行。\n所有线程共享同一个进程的地址空间。\n如果一个线程改变了该进程的地址空间，比如映射（增加内存）或取消映射（释放内存），那么操作系统需要确保在其他CPU上运行的该进程的其他线程的硬件页表（CPU用于管理内存映射的机制）也要更新。\n否则，不同CPU上运行的线程可能会看到不一致的内存视图。\n正课 为什么要用锁 这节课更关注在内核和操作系统中使用的锁。\n为什么我们需要锁？\n故事要从应用程序想要使用多个CPU核开始。使用多个CPU核可以带来性能的提升，如果一个应用程序运行在多个CPU核上，并且执行了系统调用，那么内核需要能够处理并行的系统调用。\n如果系统调用并行的运行在多个CPU核上，那么它们可能会并行的访问内核中共享的数据结构。\nXV6有很多共享的数据结构，例如proc、ticks和我们之后会看到的buffer cache等等。\n当并行的访问数据结构时，例如一个核在读取数据，另一个核在写入数据，我们需要使用锁来协调对于共享数据的更新，以确保数据的一致性。\n但是实际的情况有些令人失望，因为我们想要通过并行来获得高性能，我们想要并行的在不同的CPU核上执行系统调用，但是如果这些系统调用使用了共享的数据，我们又需要使用锁，而锁又会使得这些系统调用串行执行，所以最后锁反过来又限制了性能。\n我们处于一个矛盾的处境，出于正确性，我们需要使用锁，但是考虑到性能，锁又是极不好的。\n这就是现实，我们接下来会看看如何改善这个处境。\n但是回到最开始，为什么应用程序一定要使用多个CPU核来提升性能呢？\n这个实际上与过去几十年技术的发展有关。\n大概从2000年开始：\nCPU的时钟频率就没有再增加过了（绿线）。 这样的结果是，CPU的单线程性能达到了一个极限并且也没有再增加过（蓝线）。 但是另一方面，CPU中的晶体管数量在持续的增加 （深红色线）。 所以现在不能通过使用单核来让代码运行的更快，要想运行的更快，唯一的选择就是使用多个CPU核。所以从2000年开始，处理器上核的数量开始在增加（黑线）。 所以现在如果一个应用程序想要提升性能，它不能只依赖单核，必须要依赖于多核。\n这也意味着，如果应用程序与内核交互的较为紧密，那么操作系统也需要高效的在多个CPU核上运行。\n这就是我们对内核并行的运行在多个CPU核上感兴趣的直接原因。\n那为什么要使用锁呢？前面我们已经提到了，是为了确保正确性。\n当一份共享数据同时被读写时，如果没有锁的话，可能会出现race condition，进而导致程序出错。\nrace condition可以有不同的表现形式，并且它可能发生，也可能不发生。\n锁如何避免竞争条件（race condition） 首先你们在脑海里应该有多个CPU核在运行，比如说CPU0在运行指令，CPU1也在运行指令，这两个CPU核都连接到同一个内存上。\n在前面的代码中，数据freelist位于内存中，它里面记录了2个内存page。\n假设两个CPU核在相同的时间调用kfree。\nkfree函数接收一个物理地址pa作为参数，freelist是个单链表，kfree中将pa作为单链表的新的head节点，并更新freelist指向pa（注，也就是将空闲的内存page加在单链表的头部）。\n当两个CPU都调用kfree时，CPU0想要释放一个page，CPU1也想要释放一个page，现在这两个page都需要加到freelist中。\nkfree中首先将对应内存page的变量r指向了当前的freelist（也就是单链表当前的head节点）。\n我们假设CPU0先运行，那么CPU0会将它的变量r的next指向当前的freelist。如果CPU1在同一时间运行，它可能在CPU0运行第二条指令（kmem.freelist = r）之前运行代码。\n所以它也会完成相同的事情，它会将自己的变量r的next指向当前的freelist。现在两个物理page对应的变量r都指向了同一个freelist（注，也就是原来单链表的head节点）。\n剩下的代码也会并行的执行（kmem.freelist = r），这行代码会更新freelist为r。因为我们这里只有一个内存，所以总是有一个CPU会先执行，另一个后执行。我们假设CPU0先执行，那么freelist会等于CPU0的变量r。之后CPU1再执行，它又会将freelist更新为CPU1的变量r。\n这样的结果是，我们丢失了CPU0对应的page。CPU0想要释放的内存page最终没有出现在freelist数据中。\n在代码中，用来解决这里的问题的最常见方法就是使用锁。\n锁就是一个对象，就像其他在内核中的对象一样。有一个结构体叫做lock，它包含了一些字段，这些字段中维护了锁的状态。\n锁有非常直观的API：\nacquire，接收指向lock的指针作为参数。acquire确保了在任何时间，只会有一个进程能够成功的获取锁。 release，也接收指向lock的指针作为参数。在同一时间尝试获取锁的其他进程需要等待，直到持有锁的进程对锁调用release。 锁的acquire和release之间的代码，通常被称为critical section。\n之所以被称为critical section，是因为通常会在这里以原子的方式执行共享数据的更新。\n所以基本上来说，如果在acquire和release之间有多条指令，它们要么会一起执行，要么一条也不会执行。\n现在的程序通常会有许多锁。实际上，XV6中就有很多的锁。\n因为锁序列化了代码的执行。\n如果两个处理器想要进入到同一个critical section中，只会有一个能成功进入，另一个处理器会在第一个处理器从critical section中退出之后再进入。所以这里完全没有并行执行。\n如果内核中只有一把大锁，我们暂时将之称为big kernel lock。\n基本上所有的系统调用都会被这把大锁保护而被序列化。\n系统调用会按照这样的流程处理：一个系统调用获取到了big kernel lock，完成自己的操作，之后释放这个big kernel lock，再返回到用户空间，之后下一个系统调用才能执行。\n这样的话，如果我们有一个应用程序并行的调用多个系统调用，这些系统调用会串行的执行，因为我们只有一把锁。\n所以通常来说，例如XV6的操作系统会有多把锁，这样就能获得某种程度的并发执行。如果两个系统调用使用了两把不同的锁，那么它们就能完全的并行运行。\n首先，并没有强制说一定要使用锁，锁的使用完全是由程序员决定的。如果你想要一段代码具备原子性，那么其实是由程序员决定是否增加锁的acquire和release。\n其次，代码不会自动加锁，程序员自己要确定好是否将锁与数据结构关联，并在适当的位置增加锁的acquire和release。\n什么时候使用锁 锁限制了并发性，也限制了性能。那这带来了一个问题，什么时候才必须要加锁呢？\n一个非常保守同时也是非常简单的规则：如果两个进程访问了一个共享的数据结构，并且其中一个进程会更新共享的数据结构，那么就需要对于这个共享的数据结构加锁。\n这是个保守的规则，如果一个数据结构可以被多个进程访问，其中一个进程会更新这个数据，那么可能会产生race condition，应该使用锁来确保race condition不会发生。\n但是同时，这条规则某种程度上来说又太过严格了。\n如果有两个进程共享一个数据结构，并且其中一个进程会更新这个数据结构，在某些场合不加锁也可以正常工作。\n不加锁的程序通常称为lock-free program，不加锁的目的是为了获得更好的性能和并发度，不过lock-free program比带锁的程序更加复杂一些。\n矛盾的是，有时候这个规则太过严格，而有时候这个规则又太过宽松了。\n除了共享的数据，在一些其他场合也需要锁，例如对于printf，如果我们将一个字符串传递给它，XV6会尝试原子性的将整个字符串输出，而不是与其他进程的printf交织输出。\n尽管这里没有共享的数据结构，但在这里锁仍然很有用处，因为我们想要printf的输出也是序列化的。\n这条规则并不完美，但是它已经是一个足够好的指导准则。\n因为有了race condition，所以需要锁。\n我们之前在kfree函数中构造的race condition是很容易被识别到的，实际上如果你使用race detection工具，就可以立即找到它。但是对于一些更复杂的场景，就不是那么容易探测到race condition。\n那么我们能通过自动的创建锁来自动避免race condition吗？\n如果按照刚刚的简单规则，一旦我们有了一个共享的数据结构，任何操作这个共享数据结构都需要获取锁，那么对于XV6来说，每个结构体都需要自带一个锁，当我们对于结构体做任何操作的时候，会自动获取锁。\n可是如果我们这样做的话，结果就太过严格了，所以不能自动加锁。\n假设我们有一个对于rename的调用，这个调用会将文件从一个目录移到另一个目录，我们现在将文件d1/x移到文件d2/y。\n如果我们按照前面说的，对数据结构自动加锁。\n现在我们有两个目录对象，一个是d1，另一个是d2，那么我们会先对d1加锁，删除x，之后再释放对于d1的锁；之后我们会对d2加锁，增加y，之后再释放d2的锁。这是我们在使用自动加锁之后的一个假设的场景。\n在我们完成了第一步，也就是删除了d1下的x文件，但是还没有执行第二步，也就是创建d2下的y文件时，其他的进程会看到文件完全不存在。\n这明显是个错误的结果，因为文件还存在只是被重命名了，文件在任何一个时间点都是应该存在的。\n所以这里正确的解决方法是，我们在重命名的一开始就对d1和d2加锁，之后删除x再添加y，最后再释放对于d1和d2的锁。\n在这个例子中，我们的操作需要涉及到多个锁，但是直接为每个对象自动分配一个锁会带来错误的结果。\n在这个例子中，锁应该与操作而不是数据关联，所以自动加锁在某些场景下会出问题。\n学生提问：可不可以在访问某个数据结构的时候，就获取所有相关联的数据结构的锁？\nFrans教授：这是一种实现方式。但是这种方式最后会很快演进成big kernel lock，这样你就失去了并发执行的能力，但是你肯定想做得更好。这里就是使用锁的矛盾点了，如果你想要程序简单点，可以通过coarse-grain locking（注，也就是大锁），但是这时你就失去了性能。\n锁的特性和死锁 通常锁有三种作用。\n锁可以避免丢失更新。\n如果你回想我们之前在kalloc.c中的例子，丢失更新是指我们丢失了对于某个内存page在kfree函数中的更新。如果没有锁，在出现race condition的时候，内存page不会被加到freelist中。但是加上锁之后，我们就不会丢失这里的更新。\n锁可以打包多个操作，使它们具有原子性。\n我们之前介绍了加锁解锁之间的区域是critical section，在critical section的所有操作会都会作为一个原子操作执行。\n锁可以维护共享数据结构的不变性。\n共享数据结构如果不被任何进程修改的话是会保持不变的。如果某个进程acquire了锁并且做了一些更新操作，共享数据的不变性暂时会被破坏，但是在release锁之后，数据的不变性又恢复了。你们可以回想一下之前在kfree函数中的freelist数据，所有的free page都在一个单链表上。但是在kfree函数中，这个单链表的head节点会更新。freelist并不太复杂，对于一些更复杂的数据结构可能会更好的帮助你理解锁的作用。\n接下来再来看一下锁可能带来的一些缺点。不恰当的使用锁，可能会带来一些锁特有的问题。最明显的一个例子就是死锁（Deadlock）。\n一个死锁的最简单的场景就是：首先acquire一个锁，然后进入到critical section；在critical section中，再acquire同一个锁；第二个acquire必须要等到第一个acquire状态被release了才能继续执行，但是不继续执行的话又走不到第一个release，所以程序就一直卡在这了。这就是一个死锁。\n这是死锁的一个最简单的例子，XV6会探测这样的死锁，如果XV6看到了同一个进程多次acquire同一个锁，就会触发一个panic。\n当有多个锁的时候，场景会更加有趣。\n假设现在我们有两个CPU，一个是CPU1，另一个是CPU2。CPU1执行rename将文件d1/x移到d2/y，CPU2执行rename将文件d2/a移到d1/b。这里CPU1将文件从d1移到d2，CPU2正好相反将文件从d2移到d1。\n我们假设我们按照参数的顺序来acquire锁，那么CPU1会先获取d1的锁，如果程序是真正的并行运行，CPU2同时也会获取d2的锁。之后CPU1需要获取d2的锁，这里不能成功，因为CPU2现在持有锁，所以CPU1会停在这个位置等待d2的锁释放。而另一个CPU2，接下来会获取d1的锁，它也不能成功，因为CPU1现在持有锁。\n这也是死锁的一个例子，有时候这种场景也被称为deadly embrace。这里的死锁就没那么容易探测了。\n这里的解决方案是，如果你有多个锁，你需要对锁进行排序，所有的操作都必须以相同的顺序获取锁。\n所以对于一个系统设计者，你需要确定对于所有的锁对象的全局的顺序。\n如在这里的例子中我们让d1一直在d2之前，这样我们在rename的时候，总是先获取排序靠前的目录的锁，再获取排序靠后的目录的锁。如果对于所有的锁有了一个全局的排序，这里的死锁就不会出现了。\n不过在设计一个操作系统的时候，定义一个全局的锁的顺序会有些问题。\n如果一个模块m1中方法g调用了另一个模块m2中的方法f，那么m1中的方法g需要知道m2的方法f使用了哪些锁。\n因为如果m2使用了一些锁，那么m1的方法g必须集合f和g中的锁，并形成一个全局的锁的排序，意味着在m2中的锁必须对m1可见，这样m1才能以恰当的方法调用m2。\n但是这样又违背了代码抽象的原则。在完美的情况下，代码抽象要求m1完全不知道m2是如何实现的。\n但是不幸的是，具体实现中，m2内部的锁需要泄露给m1，这样m1才能完成全局锁排序。所以当你设计一些更大的系统时，锁使得代码的模块化更加的复杂了。\n学生提问：有必要对所有的锁进行排序吗？\nFrans教授：在上面的例子中，这取决于f和g是否共用了一些锁。如果你看XV6的代码，你可以看到会有多种锁的排序，因为一些锁与其他的锁没有任何关系，它们永远也不会在同一个操作中被acquire。如果两组锁不可能在同一个操作中被acquire，那么这两组锁的排序是完全独立的。所以没有必要对所有的锁进行一个全局的排序，但是所有的函数需要对共同使用的一些锁进行一个排序。\n锁与性能 我们前面已经看过了两类锁带来的挑战，一个是死锁，另一个是破坏了程序的模块化。\n这一部分来看看第三个挑战，也就是锁与性能之间的权衡。\n基本上来说，如果你想获得更高的性能，你需要拆分数据结构和锁。\n如果你只有一个big kernel lock，那么操作系统只能被一个CPU运行。\n如果你想要性能随着CPU的数量增加而增加，你需要将数据结构和锁进行拆分。\n那怎么拆分呢？通常不会很简单，有的时候还有些困难。比如说，你是否应该为每个目录关联不同的锁？你是否应该为每个inode关联不同的锁？你是否应该为每个进程关联不同的锁？或者是否有更好的方式来拆分数据结构呢？\n如果你重新设计了加锁的规则，你需要确保不破坏内核一直尝试维护的数据不变性。\n如果你拆分了锁，你可能需要重写代码。如果你为了获得更好的性能，重构了部分内核或者程序，将数据结构进行拆分并引入了更多的锁，这涉及到很多工作，你需要确保你能够继续维持数据的不变性，你需要重写代码。\n所以这里就有矛盾点了。我们想要获得更好的性能，那么我们需要有更多的锁，但是这又引入了大量的工作。\n通常来说，开发的流程是：\n先以coarse-grained lock（注，也就是大锁）开始。 再对程序进行测试，来看一下程序是否能使用多核。 如果可以的话，那么工作就结束了，你对于锁的设计足够好了；如果不可以的话，那意味着锁存在竞争，多个进程会尝试获取同一个锁，因此它们将会序列化的执行，性能也上不去，之后你就需要重构程序。 在这个流程中，测试的过程比较重要。有可能模块使用了coarse-grained lock，但是它并没有经常被并行的调用，那么其实就没有必要重构程序，因为重构程序设计到大量的工作，并且也会使得代码变得复杂。\n所以如果不是必要的话，还是不要进行重构。\nXV6中UART模块对于锁的使用 接下来我们看一下XV6的代码，通过代码来理解锁是如何在XV6中工作的。首先查看一下uart.c。\n1 2 3 4 5 6 //the transmit output buffer. struct spinlock uart_tx_lock; #define UART_TX_BUF_SIZE 32 char uart_tx_buf [UART_TX_BUF_SIZE]; int uart_tx_w;//write next to uart_tx_buf[uart_tx_w++] int uart_tx_r;//read next from uart_tx_buf[uar_tx_r++] 所以你可以认为对于UART模块来说，现在是一个coarse-grained lock的设计。\n这个锁保护了UART的的传输缓存；写指针；读指针。当我们传输数据时，写指针会指向传输缓存的下一个空闲槽位，而读指针指向的是下一个需要被传输的槽位。\n这是我们对于并行运算的一个标准设计，它叫做消费者-生产者模式。\n所以现在有了一个缓存，一个写指针和一个读指针。\n读指针的内容需要被显示，写指针接收来自例如printf的数据。\n我们前面已经了解到了锁有多个角色。\n第一个是保护数据结构的特性不变，数据结构有一些不变的特性，例如读指针需要追赶写指针；从读指针到写指针之间的数据是需要被发送到显示端；从写指针到读指针之间的是空闲槽位，锁帮助我们维护了这些特性不变。\n接下来看一下uart.c中的uartputc函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // add a character to the output buffer and tell the // UART to start sending if it isn\u0026#39;t already. // blocks if the output buffer is full. // because it may block, it can\u0026#39;t be called // from interrupts; it\u0026#39;s only suitable for use // by write(). void uartputc(int c) { acquire(\u0026amp;uart_tx_lock); if(panicked){ for(;;) ; } while(1){ if(((uart_tx_w + 1) % UART_TX_BUF_SIZE) == uart_tx_r){ // buffer is full. // wait for uartstart() to open up space in the buffer. sleep(\u0026amp;uart_tx_r, \u0026amp;uart_tx_lock); } else { uart_tx_buf[uart_tx_w] = c; uart_tx_w = (uart_tx_w + 1) % UART_TX_BUF_SIZE; uartstart(); release(\u0026amp;uart_tx_lock); return; } } } 函数首先获得了锁，然后查看当前缓存是否还有空槽位，如果有的话将数据放置于空槽位中；写指针加1；调用uartstart；最后释放锁。\n如果两个进程在同一个时间调用uartputc，那么这里的锁会确保来自于第一个进程的一个字符进入到缓存的第一个槽位，接下来第二个进程的一个字符进入到缓存的第二个槽位。\n这就是锁帮助我们避免race condition的一个简单例子。如果没有锁的话，第二个进程可能会覆盖第一个进程的字符。\n接下来我们看一下uartstart函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // if the UART is idle, and a character is waiting // in the transmit buffer, send it. // caller must hold uart_tx_lock. // called from both the top- and bottom-half. void uartstart() { while(1){ if(uart_tx_w == uart_tx_r){ // transmit buffer is empty. return; } if((ReadReg(LSR) \u0026amp; LSR_TX_IDLE) == 0){ // the UART transmit holding register is full, // so we cannot give it another byte. // it will interrupt when it\u0026#39;s ready for a new byte. return; } int c = uart_tx_buf[uart_tx_r]; uart_tx_r = (uart_tx_r + 1) % UART_TX_BUF_SIZE; // maybe uartputc() is waiting for space in the buffer. wakeup(\u0026amp;uart_tx_r); WriteReg(THR, c); } } 如果uart_tx_w不等于uart_tx_r，那么缓存不为空，说明需要处理缓存中的一些字符。\n锁确保了我们可以在下一个字符写入到缓存之前，处理完缓存中的字符，这样缓存中的数据就不会被覆盖。\n最后，锁确保了一个时间只有一个CPU上的进程可以写入UART的寄存器，THR。\n所以这里锁确保了硬件寄存器只有一个写入者。\n当UART硬件完成传输，会产生一个中断。在前面的代码中我们知道了uartstart的调用者会获得锁以确保不会有多个进程同时向THR寄存器写数据。\n但是UART中断本身也可能与调用printf的进程并行执行。\n如果一个进程调用了printf，它运行在CPU0上；CPU1处理了UART中断，那么CPU1也会调用uartstart。\n因为我们想要确保对于THR寄存器只有一个写入者，同时也确保传输缓存的特性不变（注，这里指的是在uartstart中对于uart_tx_r指针的更新），我们需要在中断处理函数中也获取锁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // handle a uart interrupt, raised because input has // arrived, or the uart is ready for more output, or // both. called from trap.c. void uartintr(void) { // read and process incoming characters. while(1){ int c = uartgetc(); if(c == -1) break; consoleintr(c); } // send buffered characters. acquire(\u0026amp;uart_tx_lock); uartstart(); release(\u0026amp;uart_tx_lock); } 所以，在XV6中，驱动的bottom部分（注，也就是中断处理程序）和驱动的up部分（注，uartputc函数）可以完全的并行运行，所以中断处理程序也需要获取锁。\n学生提问：UART的缓存中，读指针是不是总是会落后于写指针？\nFrans教授：从读指针到写指针之间的字符是要显示的字符，UART会逐次的将读指针指向的字符在显示器上显示，同时printf可能又会将新的字符写入到缓存。读指针总是会落后于写指针直到读指针追上了写指针，这时两个指针相同，并且此时缓存中没有字符需要显示。\n自旋锁的实现（一） 接下来我们看一下如何实现自旋锁。\n锁的特性就是只有一个进程可以获取锁，在任何时间点都不能有超过一个锁的持有者。\n实现锁的主要难点在于锁的acquire接口，在acquire里面有一个死循环，循环中判断锁对象的locked字段是否为0，如果为0那表明当前锁没有持有者，当前对于acquire的调用可以获取锁。\n之后我们通过设置锁对象的locked字段为1来获取锁。最后返回。\n如果锁的locked字段不为0，那么当前对于acquire的调用就不能获取锁，程序会一直spin。\n也就是说，程序在循环中不停的重复执行，直到锁的持有者调用了release并将锁对象的locked设置为0。\n两个进程可能同时读到锁的locked字段为0。\n如果CPU0和CPU1同时到达A语句，它们会同时看到锁的locked字段为0，之后它们会同时走到B语句，这样它们都acquire了锁。这样我们就违背了锁的特性。\n为了解决这里的问题并得到一个正确的锁的实现方式，其实有多种方法，但是最常见的方法是依赖于一个特殊的硬件指令。\n这个特殊的硬件指令会保证一次test-and-set操作的原子性。\n在RISC-V上，这个特殊的指令就是amoswap（atomic memory swap）。\n这个指令接收3个参数，分别是address，寄存器r1，寄存器r2。\n这条指令会先锁定住address，将address中的数据保存在一个临时变量中（tmp），之后将r1中的数据写入到地址中，之后再将保存在临时变量中的数据写入到r2中，最后再对于地址解锁。\n通过这里的加锁，可以确保address中的数据存放于r2，而r1中的数据存放于address中，并且这一系列的指令打包具备原子性。\n大多数的处理器都有这样的硬件指令，因为这是一个实现锁的方便的方式。\n这里我们通过将一个软件锁转变为硬件锁最终实现了原子性。\n不同处理器的具体实现可能会非常不一样，处理器的指令集通常像是一个说明文档，它不会有具体实现的细节，具体的实现依赖于内存系统是如何工作的，比如说：\n多个处理器共用一个内存控制器，内存控制器可以支持这里的操作，比如给一个特定的地址加锁，然后让一个处理器执行2-3个指令，然后再解锁。因为所有的处理器都需要通过这里的内存控制器完成读写，所以内存控制器可以对操作进行排序和加锁。 如果内存位于一个共享的总线上，那么需要总线控制器（bus arbiter）来支持。总线控制器需要以原子的方式执行多个内存操作。 如果处理器有缓存，那么缓存一致性协议会确保对于持有了我们想要更新的数据的cache line只有一个写入者，相应的处理器会对cache line加锁，完成两个操作。 硬件原子操作的实现可以有很多种方法。\n但是基本上都是对于地址加锁，读出数据，写入新数据，然后再返回旧数据（注，也就是实现了atomic swap）。\n接下来我们看一下如何使用这条指令来实现自旋锁。让我们来看一下XV6中的acquire和release的实现。首先我们看一下spinlock.h。\n1 2 3 4 5 6 7 8 // Mutual exclusion lock. struct spinlock { uint locked; // Is the lock held? // For debugging: char *name; // Name of lock. struct cpu *cpu; // The cpu holding the lock. }; 里面有spinlock结构体的定义。内容也比较简单，包含了locked字段表明当前是否上锁，其他两个字段主要是用来输出调试信息，一个是锁的名字，另一个是持有锁的CPU。\n接下来我们看一下spinlock.c文件，先来看一下acquire函数，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // Acquire the lock. // Loops (spins) until the lock is acquired. void acquire(struct spinlock *lk) { push_off(); // disable interrupts to avoid deadlock. if(holding(lk)) panic(\u0026#34;acquire\u0026#34;); // On RISC-V, sync_lock_test_and_set turns into an atomic swap: // a5 = 1 // s1 = \u0026amp;lk-\u0026gt;locked // amoswap.w.aq a5, a5, (s1) while(__sync_lock_test_and_set(\u0026amp;lk-\u0026gt;locked, 1) != 0) ; // Tell the C compiler and the processor to not move loads or stores // past this point, to ensure that the critical section\u0026#39;s memory // references happen strictly after the lock is acquired. // On RISC-V, this emits a fence instruction. __sync_synchronize(); // Record info about lock acquisition for holding() and debugging. lk-\u0026gt;cpu = mycpu(); } 在函数中有一个while循环，这就是我刚刚提到的test-and-set循环。实际上C的标准库已经定义了这些原子操作，所以C标准库中已经有一个函数__sync_lock_test_and_set，它里面的具体行为与我刚刚描述的是一样的。\n因为大部分处理器都有的test-and-set硬件指令，所以这个函数的实现比较直观。\n我们可以通过查看kernel.asm来了解RISC-V具体是如何实现的。\n这里比较复杂，总的来说，一种情况下我们跳出循环，另一种情况我们继续执行循环。C代码就要简单的多。\n如果锁没有被持有，那么锁对象的locked字段会是0，如果locked字段等于0，我们调用test-and-set将1写入locked字段，并且返回locked字段之前的数值0。\n如果返回0，那么意味着没有人持有锁，循环结束。如果locked字段之前是1，那么这里的流程是，先将之前的1读出，然后写入一个新的1，但是这不会改变任何数据，因为locked之前已经是1了。\n之后__sync_lock_test_and_set会返回1，表明锁之前已经被人持有了，这样的话，判断语句不成立，程序会持续循环（spin），直到锁的locked字段被设置回0。\n接下来我们看一下release的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Release the lock. void release(struct spinlock *lk) { if(!holding(lk)) panic(\u0026#34;release\u0026#34;); lk-\u0026gt;cpu = 0; // Tell the C compiler and the CPU to not move loads or stores // past this point, to ensure that all the stores in the critical // section are visible to other CPUs before the lock is released, // and that loads in the critical section occur strictly before // the lock is released. // On RISC-V, this emits a fence instruction. __sync_synchronize(); // Release the lock, equivalent to lk-\u0026gt;locked = 0. // This code doesn\u0026#39;t use a C assignment, since the C standard // implies that an assignment might be implemented with // multiple store instructions. // On RISC-V, sync_lock_release turns into an atomic swap: // s1 = \u0026amp;lk-\u0026gt;locked // amoswap.w zero, zero, (s1) __sync_lock_release(\u0026amp;lk-\u0026gt;locked); pop_off(); } 可以看出release也使用了atomic swap操作，将0写入到了s1。下面是对应的C代码，它基本确保了将lk-\u0026gt;locked中写入0是一个原子操作。\n自旋锁的实现（二） 有关spin lock的实现，有3个细节我想介绍一下。\n首先，为什么release函数中不直接使用一个store指令将锁的locked字段写为0?因为其他的处理器可能会向locked字段写入1，或者写入0。\n这里的问题是，对于很多人包括我自己来说，经常会认为一个store指令是一个原子操作，但实际并不总是这样，这取决于具体的实现。\n例如，对于CPU内的缓存，每一个cache line的大小可能大于一个整数，那么store指令实际的过程将会是：首先会加载cache line，之后再更新cache line。\n所以对于store指令来说，里面包含了两个微指令。\n这样的话就有可能得到错误的结果。所以为了避免理解硬件实现的所有细节，例如整数操作不是原子的，或者向一个64bit的内存值写数据是不是原子的，我们直接使用一个RISC-V提供的确保原子性的指令来将locked字段写为0。\namoswap并不是唯一的原子指令，下图是RISC-V的手册，它列出了所有的原子指令。\n(略)\n第二个细节是，在acquire函数的最开始，会先关闭中断。\n为什么会是这样呢？让我们回到uart.c中。我们先来假设acquire在一开始并没有关闭中断。在uartputc函数中，首先会acquire锁，如果不关闭中断会发生什么呢？\nuartputc函数会acquire锁，UART本质上就是传输字符，当UART完成了字符传输它会做什么？是的，它会产生一个中断之后会运行uartintr函数，在uartintr函数中，会获取同一把锁，但是这把锁正在被uartputc持有。\n如果这里只有一个CPU的话，那这里就是死锁。\n中断处理程序uartintr函数会一直等待锁释放，但是CPU不出让给uartputc执行的话锁又不会释放。在XV6中，这样的场景会触发panic，因为同一个CPU会再次尝试acquire同一个锁。\n所以spinlock需要处理两类并发，一类是不同CPU之间的并发，一类是相同CPU上中断和普通程序之间的并发。\n针对后一种情况，我们需要在acquire中关闭中断。中断会在release的结束位置再次打开，因为在这个位置才能再次安全的接收中断。\n第三个细节就是memory ordering。假设我们先通过将locked字段设置为1来获取锁，之后对x加1，最后再将locked字段设置0来释放锁。\n但是编译器或者处理器可能会重排指令以获得更好的性能。\n对于上面的串行指令流，如果将x\u0026lt;-x+1移到locked\u0026lt;-0之后可以吗？这会改变指令流的正确性吗？\n并不会，因为x和锁完全相互独立，它们之间没有任何关联。\n如果他们还是按照串行的方式执行，x\u0026lt;-x+1移到锁之外也没有问题。所以在一个串行执行的场景下是没有问题的。\n实际中，处理器在执行指令时，实际指令的执行顺序可能会改变。编译器也会做类似的事情，编译器可能会在不改变执行结果的前提下，优化掉一些代码路径并进而改变指令的顺序。\n但是对于并发执行，很明显这将会是一个灾难。如果我们将critical section与加锁解锁放在不同的CPU执行，将会得到完全错误的结果。所以指令重新排序在并发场景是错误的。\n为了禁止，或者说为了告诉编译器和硬件不要这样做，我们需要使用memory fence或者叫做synchronize指令，来确定指令的移动范围。\n对于synchronize指令，任何在它之前的load/store指令，都不能移动到它之后。锁的acquire和release函数都包含了synchronize指令。\n这样前面的例子中，x\u0026lt;-x+1就不会被移到特定的memory synchronization点之外。\n这就是为什么在acquire和release中都有__sync_synchronize函数的调用。\n学生提问：有没有可能在锁acquire之前的一条指令被移到锁release之后？或者说这里会有一个界限不允许这么做？\nFrans教授：在这里的例子中，acquire和release都有自己的界限（注，也就是__sync_synchronize函数的调用点）。所以发生在锁acquire之前的指令不会被移到acquire的__sync_synchronize函数调用之后，这是一个界限。在锁的release函数中有另一个界限。所以在第一个界限之前的指令会一直在这个界限之前，在两个界限之间的指令会保持在两个界限之间，在第二个界限之后的指令会保持在第二个界限之后。\n总结 让我来总结一下这节课的内容。\n首先，锁确保了正确性，但是同时又会降低性能，这是个令人失望的现实，我们是因为并发运行代码才需要使用锁，而锁另一方面又限制了代码的并发运行。\n其次锁会增加编写程序的复杂性，在我们的一些实验中会看到锁，我们需要思考锁为什么在这，它需要保护什么。如果你在程序中使用了并发，那么一般都需要使用锁。如果你想避免锁带来的复杂性，可以遵循以下原则：不到万不得已不要共享数据。如果你不在多个进程之间共享数据，那么race condition就不可能发生，那么你也就不需要使用锁，程序也不会因此变得复杂。但是通常来说如果你有一些共享的数据结构，那么你就需要锁，你可以从coarse-grained lock开始，然后基于测试结果，向fine-grained lock演进。\n最后，使用race detector来找到race condition，如果你将锁的acquire和release放置于错误的位置，那么就算使用了锁还是会有race。\n学生提问：在一个处理器上运行多个线程与在多个处理器上运行多个进程是否一样？\nFrans教授：差不多吧，如果你有多个线程，但是只有一个CPU，那么你还是会想要特定内核代码能够原子执行。所以你还是需要有critical section的概念。你或许不需要锁，但是你还是需要能够对特定的代码打开或者关闭中断。如果你查看一些操作系统的内核代码，通常它们都没有锁的acquire，因为它们假定自己都运行在单个处理器上，但是它们都有开关中断的操作。\n","date":"2024-09-17T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s08110multiprocessors-and-locking/bg_hu_6a2eba60ea755a3c.png","permalink":"https://echudet.github.io/p/mit6s08110multiprocessors-and-locking/","title":"MIT6S081：10Multiprocessors and locking"},{"content":"Lecture 9 Interrupts 课程杂记 内存是如何被真实的操作系统（而不是像XV6这样的教学操作系统）所使用 首先是计算机中总共有多少内存（33048332），如果你再往后看的话，你会发现大部分内存都被使用了（4214604 + 26988148）。但是大部分内存并没有被应用程序所使用，而是被buff/cache用掉了。这在一个操作系统中还挺常见的，因为我们不想让物理内存就在那闲置着，我们想让物理内存被用起来，所以这里大块的内存被用作buff/cache。 大部分操作系统运行时几乎没有任何空闲的内存。这意味着，如果应用程序或者内核需要使用新的内存，那么我们需要丢弃一些已有的内容。\u0026mdash;\u0026ndash;现在的空闲内存（free）或许足够几个page用，但是在某个时间点如果需要大量内存的话，要么是从应用程序，要么是从buffer/cache中，需要撤回已经使用的一部分内存。 VIRT表示的是虚拟内存地址空间的大小，RES是实际使用的内存数量。实际使用的内存数量远小于地址空间的大小。所以，我们上节课讨论的基于虚拟内存和page fault提供的非常酷的功能在这都有使用。 中断对应的场景很简单，就是硬件想要得到操作系统的关注。（例如网卡收到了一个packet，网卡会生成一个中断；用户通过键盘按下了一个按键，键盘会产生一个中断。）\n操作系统需要做的是，保存当前的工作，处理中断，处理完成之后再恢复之前的工作。这里的保存和恢复工作，与我们之前看到的系统调用过程非常相似。（所以系统调用，page fault，中断，都使用相同的机制）\n但是中断又有一些不一样的地方\nasynchronous。当硬件生成中断时，Interrupt handler与当前运行的进程在CPU上没有任何关联。但如果是系统调用的话，系统调用发生在运行进程的context下。\nconcurrency。我们这节课会稍微介绍并发，在下一节课，我们会介绍更多并发相关的内容。对于中断来说，CPU和生成中断的设备是并行的在运行。网卡自己独立的处理来自网络的packet，然后在某个时间点产生中断，但是同时，CPU也在运行。所以我们在CPU和设备之间是真正的并行的，我们必须管理这里的并行。\nprogram device。我们这节课主要关注外部设备，例如网卡，UART，而这些设备需要被编程。每个设备都有一个编程手册，就像RISC-V有一个包含了指令和寄存器的手册一样。设备的编程手册包含了它有什么样的寄存器，它能执行什么样的操作，在读写控制寄存器的时候，设备会如何响应。不过通常来说，设备的手册不如RISC-V的手册清晰，这会使得对于设备的编程会更加复杂。\n中断是从哪里产生的？因为我们主要关心的是外部设备的中断，而不是定时器中断或者软件中断。外设中断来自于主板上的设备。\n主板可以连接以太网卡，MicroUSB，MicroSD等，主板上的各种线路将外设和CPU连接在一起。\n我们在之前的课程已经知道UART0会映射到内核内存地址的某处，类似于读写内存，通过向相应的设备地址执行load/store指令，我们就可以对例如UART的设备进行编程。\n所有的设备都连接到处理器上，处理器上是通过Platform Level Interrupt Control，简称PLIC来处理设备中断。PLIC会管理来自于外设的中断。\n我们有53个不同的来自于设备的中断。这些中断到达PLIC之后，PLIC会路由这些中断。图的右下角是CPU的核，PLIC会将中断路由到某一个CPU的核。如果所有的CPU核都正在处理中断，PLIC会保留中断直到有一个CPU核可以用来处理中断。所以PLIC需要保存一些内部数据来跟踪中断的状态。\n具体流程是：\nPLIC会通知当前有一个待处理的中断 其中一个CPU核会Claim接收中断，这样PLIC就不会把中断发给其他的CPU处理 CPU核处理完中断之后，CPU会通知PLIC PLIC将不再保存中断的信息 PLIC只是分发中断，而内核需要对PLIC进行编程来告诉它中断应该分发到哪。实际上，内核可以对中断优先级进行编程，这里非常的灵活。\n当UART触发中断的时候，所有的CPU核都能收到中断吗？取决于你如何对PLIC进行编程。对于XV6来说，所有的CPU都能收到中断，但是只有一个CPU会Claim相应的中断。\n通常来说，管理设备的代码称为驱动，所有的驱动都在内核中。\nxv6的示范是UART设备的驱动，代码在uart.c文件中。如果我们查看代码的结构，我们可以发现大部分驱动都分为两个部分，bottom/top。\nbottom部分通常是Interrupt handler。当一个中断送到了CPU，并且CPU设置接收这个中断，CPU会调用相应的Interrupt handler。Interrupt handler并不运行在任何特定进程的context中，它只是处理中断。 top部分，是用户进程，或者内核的其他部分调用的接口。对于UART来说，这里有read/write接口，这些接口可以被更高层级的代码调用。 通常情况下，驱动中会有一些队列（或者说buffer），top部分的代码会从队列中读写数据，而Interrupt handler（bottom部分）同时也会向队列中读写数据。这里的队列可以将并行运行的设备和CPU解耦开来。\n通常对于Interrupt handler来说存在一些限制，因为它并没有运行在任何进程的context中，所以进程的page table并不知道该从哪个地址读写数据，也就无法直接从Interrupt handler读写数据。驱动的top部分通常与用户的进程交互，并进行数据的读写。\n在很多操作系统中，驱动代码加起来可能会比内核还要大，主要是因为，对于每个设备，你都需要一个驱动，而设备又很多。\n通常来说，编程是通过memory mapped I/O完成的。\n在SiFive的手册中，设备地址出现在物理地址的特定区间内，这个区间由主板制造商决定。操作系统需要知道这些设备位于物理地址空间的具体/位置，然后再通过普通的load/store指令对这些地址进行编程。load/store指令实际上的工作就是读写设备的控制寄存器。\n例如，对网卡执行store指令时，CPU会修改网卡的某个控制寄存器，进而导致网卡发送一个packet。所以这里的load/store指令不会读写内存，而是会操作设备。并且你需要阅读设备的文档来弄清楚设备的寄存器和相应的行为，有的时候文档很清晰，有的时候文档不是那么清晰。\n例如，0x200_0000对应CLINT，0xC000000对应的是PLIC。在这个图中UART0对应的是0x1001*_*0000，但是在QEMU中，我们的UART0的地址略有不同，因为在QEMU中我们并不是完全的模拟SiFive主板，而是模拟与SiFive主板非常类似的东西。\n以上就是Memory-mapped IO。\n这是一个很简单的芯片，图中表明了芯片拥有的寄存器。例如对于控制寄存器000，如果写它会将数据写入到寄存器中并被传输到其他地方，如果读它可以读出存储在寄存器中的内容。UART可以让你能够通过串口发送数据bit，在线路的另一侧会有另一个UART芯片，能够将数据bit组合成一个个Byte。\n这里还有一些其他可以控制的地方，例如控制寄存器001，可以通过它来控制UART是否产生中断。实际上对于一个寄存器，其中的每个bit都有不同的作用。例如对于寄存器001，也就是IER寄存器，bit0-bit3分别控制了不同的中断。这个文档还有很多内容，但是对于我们这节课来说，上图就足够了。不过即使是这么简单的一个设备，它的文档也有很多页。\n当XV6启动时，Shell会输出提示符“$ ”\n对于“$ ”来说，实际上就是设备会将字符传输给UART的寄存器，UART之后会在发送完字符之后产生一个中断。在QEMU中，模拟的线路的另一端会有另一个UART芯片（模拟的），这个UART芯片连接到了虚拟的Console，它会进一步将“$ ”显示在console上。\n另一方面，对于“ls”，这是用户输入的字符。键盘连接到了UART的输入线路，当你在键盘上按下一个按键，UART芯片会将按键字符通过串口线发送到另一端的UART芯片。另一端的UART芯片先将数据bit合并成一个Byte，之后再产生一个中断，并告诉处理器说这里有一个来自于键盘的字符。之后Interrupt handler会处理来自于UART的字符。\nRISC-V有许多与中断相关的寄存器：\nSIE（Supervisor Interrupt Enable）寄存器。这个寄存器中有一个bit（E）专门针对例如UART的外部设备的中断；有一个bit（S）专门针对软件中断，软件中断可能由一个CPU核触发给另一个CPU核；还有一个bit（T）专门针对定时器中断。我们这节课只关注外部设备的中断。 SSTATUS（Supervisor Status）寄存器。这个寄存器中有一个bit来打开或者关闭中断。每一个CPU核都有独立的SIE和SSTATUS寄存器，除了通过SIE寄存器来单独控制特定的中断，还可以通过SSTATUS寄存器中的一个bit来控制所有的中断。 SIP（Supervisor Interrupt Pending）寄存器。当发生中断时，处理器可以通过查看这个寄存器知道当前是什么类型的中断。 SCAUSE寄存器，这个寄存器我们之前看过很多次。它会表明当前状态的原因是中断。 STVEC寄存器，它会保存当trap，page fault或者中断发生时，CPU运行的用户程序的程序计数器，这样才能在稍后恢复程序的运行。 我们第一个外设是console，这是我们print的输出位置。查看位于console.c的consoleinit函数。这里首先初始化了锁，我们现在还不关心这个锁。然后调用了uartinit，uartinit函数位于uart.c文件。这个函数实际上就是配置好UART芯片使其可以被使用。这里的流程是先关闭中断，之后设置波特率，设置字符长度为8bit，重置FIFO，最后再重新打开中断。以上就是uartinit函数，运行完这个函数之后，原则上UART就可以生成中断了。但是因为我们还没有对PLIC编程，所以中断不能被CPU感知。最终，在main函数中，需要调用plicinit函数。下图是plicinit函数。\nPLIC与外设一样，也占用了一个I/O地址（0xC000_0000）。代码的第一行使能了UART的中断，这里实际上就是设置PLIC会接收哪些中断，进而将中断路由到CPU。类似的，代码的第二行设置PLIC接收来自IO磁盘的中断。\nmain函数中，plicinit之后就是plicinithart函数。plicinit是由0号CPU运行，之后，每个CPU的核都需要调用plicinithart函数表明对于哪些外设中断感兴趣。\n所以在plicinithart函数中，每个CPU的核都表明自己对来自于UART和VIRTIO的中断感兴趣。因为我们忽略中断的优先级，所以我们将优先级设置为0。\n到目前为止，我们有了生成中断的外部设备，我们有了PLIC可以传递中断到单个的CPU。但是CPU自己还没有设置好接收中断，因为我们还没有设置好SSTATUS寄存器。在main函数的最后，程序调用了scheduler函数。scheduler函数主要是运行进程。但是在实际运行进程之前，会执行intr_on函数来使得CPU能接收中断。\nintr_on函数只完成一件事情，就是设置SSTATUS寄存器，打开中断标志位。\n在这个时间点，中断被完全打开了。如果PLIC正好有pending的中断，那么这个CPU核会收到中断。\n以上就是中断的基本设置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //init.c中的main函数，这是系统启动后运行的第一个进程。 int main(void)） { int pid,wpid; if(open(\u0026#34;console\u0026#34;,0 RDWR)\u0026lt;0){ mknod(\u0026#34;console\u0026#34;,CONSOLE,0); open(\u0026#34;console\u0026#34;,0 RDWR); } dup(0);/stdout dup(0);/stderr for(;;){ printf(\u0026#34;init:starting sh\\n\u0026#34;); pid fork(); if(pid 0){ printf(\u0026#34;init:fork failed\\n\u0026#34;); exit(1); } if(pid ==0){ exec(\u0026#34;sh\u0026#34;,argv); printf(\u0026#34;init:exec sh failed\\n\u0026#34;); exit(1); } 首先这个进程的main函数创建了一个代表Console的设备。这里通过mknod操作创建了console设备。因为这是第一个打开的文件，所以这里的文件描述符0。之后通过dup创建stdout和stderr。这里实际上通过复制文件描述符0，得到了另外两个文件描述符1，2。最终文件描述符0，1，2都用来代表Console。\nShell程序首先打开文件描述符0，1，2。之后Shell向文件描述符2打印提示符“$ ”。\n尽管Console背后是UART设备，但是从应用程序来看，它就像是一个普通的文件。Shell程序只是向文件描述符2写了数据，它并不知道文件描述符2对应的是什么。在Unix系统中，设备是由文件表示。\n假设键盘生成了一个中断并且发向了PLIC，PLIC会将中断路由给一个特定的CPU核，并且如果这个CPU核设置了SIE寄存器的E bit（注，针对外部中断的bit位），那么会发生以下事情：\n首先，会清除SIE寄存器相应的bit，这样可以阻止CPU核被其他中断打扰，该CPU核可以专心处理当前中断。处理完成之后，可以再次恢复SIE寄存器相应的bit。 之后，会设置SEPC寄存器为当前的程序计数器。我们假设Shell正在用户空间运行，突然来了一个中断，那么当前Shell的程序计数器会被保存。 之后，要保存当前的mode。在我们的例子里面，因为当前运行的是Shell程序，所以会记录user mode。 再将mode设置为Supervisor mode。 最后将程序计数器的值设置成STVEC的值。（注，STVEC用来保存trap处理程序的地址，详见lec06）在XV6中，STVEC保存的要么是uservec或者kernelvec函数的地址，具体取决于发生中断时程序运行是在用户空间还是内核空间。在我们的例子中，Shell运行在用户空间，所以STVEC保存的是uservec函数的地址。而从之前的课程我们可以知道uservec函数会调用usertrap函数。所以最终，我们在usertrap函数中。我们这节课不会介绍trap过程中的拷贝，恢复过程，因为在之前的课程中已经详细的介绍过了。 在trap.c的devintr函数中，首先会通过SCAUSE寄存器判断当前中断是否是来自于外设的中断。如果是的话，再调用plic_claim函数来获取中断。plic_claim函数位于plic.c文件中。在这个函数中，当前CPU核会告知PLIC，自己要处理中断，PLIC_SCLAIM会将中断号返回，对于UART来说，返回的中断号是10。\n学生提问： UART对于键盘来说很重要，来自于键盘的字符通过UART走到CPU再到我们写的代码。但是我不太理解UART对于Shell输出字符究竟有什么作用？因为在这个场景中，并没有键盘的参与。\nFrans教授：显示设备与UART也是相连的。所以UART连接了两个设备，一个是键盘，另一个是显示设备，也就是Console。QEMU也是通过模拟的UART与Console进行交互，而Console的作用就是将字符在显示器上画出来。\n学生提问：uartinit只被调用了一次，所以才导致了所有的CPU核都共用一个buffer吗？\nFrans教授：因为只有一个UART设备，一个buffer只针对一个UART设备，而这个buffer会被所有的CPU核共享，这样运行在多个CPU核上的多个程序可以同时向Console打印输出，而驱动中是通过锁来确保多个CPU核上的程序串行的向Console打印输出。\n学生提问：我们之所以需要锁是因为有多个CPU核，但是却只有一个Console，对吧？\nFrans教授：是的，如我们之前说的驱动的top和bottom部分可以并行的运行。所以一个CPU核可以执行uartputc函数，而另个一CPU核可以执行uartintr函数，我们需要确保它们是串行执行的，而锁确保了这一点。\n学生提问：那是不是意味着，某个时间，其他所有的CPU核都需要等待某一个CPU核的处理？\nFrans教授：这里并不是死锁。其他的CPU核还是可以在等待的时候运行别的进程。\n与中断相关的并发，并发加大了中断编程的难度。\n设备与CPU是并行运行的。例如当UART向Console发送字符的时候，CPU会返回执行Shell，而Shell可能会再执行一次系统调用，向buffer中写入另一个字符，这些都是在并行的执行。这里的并行称为producer-consumer并行。 中断会停止当前运行的程序。例如，Shell正在运行第212个指令，突然来了个中断，Shell的执行会立即停止。对于用户空间代码，这并不是一个大的问题，因为当我们从中断中返回时，我们会恢复用户空间代码，并继续执行执行停止的指令。我们已经在trap和page fault中看过了这部分内容。但是当内核被中断打断时，事情就不一样了。所以，代码运行在kernel mode也会被中断，这意味着即使是内核代码，也不是直接串行运行的。在两个内核指令之间，取决于中断是否打开，可能会被中断打断执行。对于一些代码来说，如果不能在执行期间被中断，这时内核需要临时关闭中断，来确保这段代码的原子性。 驱动的top和bottom部分是并行运行的。例如，Shell会在传输完提示符“$”之后再调用write系统调用传输空格字符，代码会走到UART驱动的top部分（注，uartputc函数），将空格写入到buffer中。但是同时在另一个CPU核，可能会收到来自于UART的中断，进而执行UART驱动的bottom部分，查看相同的buffer。所以一个驱动的top和bottom部分可以并行的在不同的CPU上运行。这里我们通过lock来管理并行。因为这里有共享的数据，我们想要buffer在一个时间只被一个CPU核所操作。 这里我将会关注在第一点，也就是producer/consumser并发。这是驱动中的非常常见的典型现象。如你们所见的，在驱动中会有一个buffer，在我们之前的例子中，buffer是32字节大小。并且有两个指针，分别是读指针和写指针。\nproducer可以一直写入数据，直到写指针 + 1等于读指针，因为这时，buffer已经满了。当buffer满了的时候，producer必须停止运行。我们之前在uartputc函数中看过，如果buffer满了，代码会sleep，暂时搁置Shell并运行其他的进程。\nInterrupt handler，也就是uartintr函数，在这个场景下是consumer，每当有一个中断，并且读指针落后于写指针，uartintr函数就会从读指针中读取一个字符再通过UART设备发送，并且将读指针加1。当读指针追上写指针，也就是两个指针相等的时候，buffer为空，这时就不用做任何操作。\n学生提问：这里的buffer对于所有的CPU核都是共享的吗？\nFrans教授：这里的buffer存在于内存中，并且只有一份，所以，所有的CPU核都并行的与这一份数据交互。所以我们才需要lock。\n学生提问：对于uartputc中的sleep，它怎么知道应该让Shell去sleep？\nFrans教授： sleep会将当前在运行的进程存放于sleep数据中。它传入的参数是需要等待的信号，在这个例子中传入的是uart_tx_r的地址。在uartstart函数中，一旦buffer中有了空间，会调用与sleep对应的函数wakeup，传入的也是uart_tx_r的地址。任何等待在这个地址的进程都会被唤醒。有时候这种机制被称为conditional synchronization。\n在UART的另一侧，会有类似的事情发生，有时Shell会调用read从键盘中读取字符。 在read系统调用的底层，会调用fileread函数。在这个函数中，如果读取的文件类型是设备，会调用相应设备的read函数。\n在我们的例子中，read函数就是console.c文件中的consoleread函数。\n这里与UART类似，也有一个buffer，包含了128个字符。其他的基本一样，也有producer和consumser。但是在这个场景下Shell变成了consumser，因为Shell是从buffer中读取数据。而键盘是producer，它将数据写入到buffer中。\n从consoleread函数中可以看出，当读指针和写指针一样时，说明buffer为空，进程会sleep。所以Shell在打印完“$ ”之后，如果键盘没有输入，Shell进程会sleep，直到键盘有一个字符输入。所以在某个时间点，假设用户通过键盘输入了“l”，这会导致“l”被发送到主板上的UART芯片，产生中断之后再被PLIC路由到某个CPU核，之后会触发devintr函数，devintr可以发现这是一个UART中断，然后通过uartgetc函数获取到相应的字符，之后再将字符传递给consoleintr函数。\n默认情况下，字符会通过consputc，输出到console上给用户查看。之后，字符被存放在buffer中。在遇到换行符的时候，唤醒之前sleep的进程，也就是Shell，再从buffer中将数据读出。\n所以这里也是通过buffer将consumer和producer之间解耦，这样它们才能按照自己的速度，独立的并行运行。如果某一个运行的过快了，那么buffer要么是满的要么是空的，consumer和producer其中一个会sleep并等待另一个追上来。\n当Unix刚被开发出来的时候，Interrupt处理还是很快的。这使得硬件可以很简单，当外设有数据需要处理时，硬件可以中断CPU的执行，并让CPU处理硬件的数据。\n而现在，中断相对处理器来说变慢了。从前面的介绍可以看出来这一点，需要很多步骤才能真正的处理中断数据。如果一个设备在高速的产生中断，处理器将会很难跟上。所以如果查看现在的设备，可以发现，现在的设备相比之前做了更多的工作。所以在产生中断之前，设备上会执行大量的操作，这样可以减轻CPU的处理负担。所以现在硬件变得更加复杂。\n如果你有一个高性能的设备，例如你有一个千兆网卡，这个网卡收到了大量的小包，网卡每秒可以生成1.5Mpps，这意味着每一个微秒，CPU都需要处理一个中断，这就超过了CPU的处理能力。那么当网卡收到大量包，并且处理器不能处理这么多中断的时候该怎么办呢？\n这里的解决方法就是使用polling。除了依赖Interrupt，CPU可以一直读取外设的控制寄存器，来检查是否有数据。对于UART来说，我们可以一直读取RHR寄存器，来检查是否有数据。现在，CPU不停的在轮询设备，直到设备有了数据。\n这种方法浪费了CPU cycles，当我们在使用CPU不停的检查寄存器的内容时，我们并没有用CPU来运行任何程序。在我们之前的例子中，如果没有数据，内核会让Shell进程sleep，这样可以运行另一个进程。\n所以，对于一个慢设备，你肯定不想一直轮询它来得到数据。我们想要在没有数据的时候切换出来运行一些其他程序。但是如果是一个快设备，那么Interrupt的overhead也会很高，那么我们在polling设备的时候，是经常能拿到数据的，这样可以节省进出中断的代价。\n所以对于一个高性能的网卡，如果有大量的包要传入，那么应该用polling。对于一些精心设计的驱动，它们会在polling和Interrupt之间动态切换（注，也就是网卡的NAPI）。\n内存在真实操作系统中的使用 在真实操作系统中，内存管理是一项复杂的任务。操作系统不仅要为应用程序分配内存，还需要管理内存的使用以确保系统高效运行。大部分内存并不只是简单地分配给应用程序，而是有很多部分被用作缓存或缓冲区（buff/cache）。这样可以确保物理内存不会闲置，而是被充分利用。\n现代操作系统采用了类似于虚拟内存和分页机制（Page Fault）的策略，虚拟内存为每个进程提供了一个独立的地址空间，即使物理内存并不足够。系统通过在需要时从buff/cache或者应用程序中回收内存来满足新的请求。这也是为什么当需要大块内存时，系统必须从已经使用的部分中回收一些内存。\n中断与并发的关系 中断是一种机制，用于设备通知操作系统它们需要处理的情况。例如，网卡接收数据包时，会向操作系统发送中断，或者当用户按下键盘时，键盘产生中断。在并发环境下，中断使得系统能够同时处理多个事件，而不必等待一个进程的结束。\n中断与并发之间的关系体现在设备和CPU之间的真正并行性。设备（如网卡或键盘）与CPU是并行运行的。例如，网卡接收数据包的同时，CPU也在运行其他任务。在设备产生中断时，操作系统需要中断当前的工作，保存状态，处理中断，之后恢复之前的工作。中断处理的这种机制和系统调用、页面错误（Page Fault）等有很大的相似性，但中断具有异步性和并发性，CPU和设备独立地执行任务。\n中断的处理可以分为两部分：Top Half和Bottom Half。\nTop Half：这是中断处理的第一阶段。Top Half负责快速响应中断，执行最紧急的任务，如确认中断来源、读取硬件寄存器等。Top Half的目标是尽可能快地完成，以减少对系统的干扰，因此它的执行时间要尽可能短。 Bottom Half：Bottom Half负责处理那些可以稍后执行的任务。由于Top Half不能处理复杂的任务，这些任务会被推迟到Bottom Half来完成。Bottom Half的处理时间相对较长，但它是在普通进程上下文中执行的，不会阻塞其他中断的处理。 关系：Top Half处理中断的紧急部分，而Bottom Half处理非紧急的部分。Top Half会通过某种机制（如标记位）通知Bottom Half继续处理剩余工作。通过这种分工，系统能迅速响应中断，同时保证复杂任务的处理不会影响系统的性能。\n通过UART的串口通信来具体说明中断机制中的并发关系：\n中断触发与Top Half\n当UART设备接收到一个字符时，它会生成一个中断。 操作系统进入Top Half，从UART的寄存器中读取字符并将其存入内存的缓冲区中。这一步必须快速完成，避免阻塞系统。 Bottom Half处理\n当有空闲时间或者需要时，系统进入Bottom Half，从缓冲区中取出字符，进一步处理或将其传递给用户进程（如Shell）。 在这个例子中并发体现在：\n系统正在执行某个任务（例如，运行一个用户进程），而此时UART设备接收到数据并触发中断，操作系统会暂停当前任务来处理中断。这是中断机制的并发性。 Top Half和Bottom Half之间的协作也是一种并发：Top Half处理了中断的紧急部分，将复杂工作留给Bottom Half。两者通过共享的缓冲区进行数据传递。 此外，Producer-Consumer模式也可以在这个例子中体现出来：\nProducer：UART设备产生的数据被存入缓冲区。 Consumer：用户进程（如Shell）从缓冲区读取数据进行处理。 缓冲区和锁机制可以使Producer和Consumer解耦，使它们能够按照各自的速度运行。这种并发处理通过缓冲区的读写操作来保证系统稳定、高效。\n如材料中提到的，中断处理增加了编程的复杂性，尤其是在并发环境下。设备和CPU是独立并行运行的。对于内核代码来说，中断可能会打断执行，这就要求在一些情况下，内核要临时关闭中断来确保代码执行的原子性。此外，驱动的Top Half和Bottom Half可以在不同的CPU上并行执行，这使得锁和同步机制变得非常重要，以确保不会出现竞态条件。\n总的来说：\n真实操作系统会充分利用内存，使其大部分被用于缓存和缓冲区。 中断机制通过异步处理实现了设备和CPU之间的并发性。 Top Half和Bottom Half分工协作，共同处理中断事件。 通过缓冲区和同步机制，操作系统能够有效地管理并发事件，例如在UART设备和用户进程之间的Producer-Consumer模式下运行。 这种架构在复杂的现代操作系统中，通过更高效的中断处理和并发管理，确保了系统的稳定性和性能。\nTop and Bottom Top Half是中断处理的第一阶段，主要特征是快速响应和处理紧急任务。它的核心任务是在中断发生时立即执行，以尽快恢复系统的正常运行。Top Half通常处理的工作包括：\n识别中断来源：当一个中断发生时，操作系统首先要知道哪个设备触发了中断。例如，是键盘输入了字符，还是网卡接收到了数据包。 保存重要数据：例如，从设备的硬件寄存器中读取数据并保存到一个临时缓冲区。这一步骤非常重要，因为它确保了中断源数据不会丢失。 清除或屏蔽中断：为了避免重复中断，Top Half通常会执行必要的操作以清除当前的中断状态，或暂时屏蔽相同的中断。 执行上下文：Top Half是在中断上下文中执行的。这意味着它并不运行在任何特定进程的上下文中，而是独立于进程执行。因此，Top Half不允许进行阻塞操作，因为它不能切换到其他任务。\n时间要求：由于中断的发生会打断系统的正常运行，Top Half的执行时间必须尽可能短，以便快速释放CPU资源，恢复系统的正常调度和任务执行。\nBottom Half是中断处理的第二阶段，它主要负责处理那些可以延迟执行的任务。由于Top Half必须尽快执行，复杂或耗时的操作会被推迟到Bottom Half中进行。这些任务可能包括：\n数据处理：例如，从Top Half中保存的缓冲区中读取数据并将其传递给用户进程或进行进一步处理。这个过程可能包括解析协议、与其他系统组件交互等复杂操作。 与用户进程交互：Bottom Half通常负责将处理后的数据传递给用户进程或内核的其他部分。比如，当一个文件系统接收到一个磁盘读请求时，Bottom Half可能会将数据从磁盘读取到内存，并准备好供用户进程访问。 执行上下文：与Top Half不同，Bottom Half在普通进程上下文中执行。这意味着它运行在系统的普通调度机制下，可以被中断，可以阻塞，可以与其他进程一起共享CPU时间。因为它在进程上下文中执行，所以Bottom Half可以访问系统的所有资源，并执行任何复杂的操作。\n时间要求：Bottom Half没有严格的时间要求，因为它的任务可以延迟执行，只要在合理的时间内完成即可。\nTop Half与Bottom Half的关系:\n协作与分工：Top Half和Bottom Half是通过一种协作分工的机制来共同完成中断处理的任务。Top Half快速响应中断并执行紧急处理，而Bottom Half则处理非紧急任务，确保系统的高效运行。这种分工允许系统能够同时响应多个中断，并在不中断系统正常运行的情况下处理复杂的任务。\n通信机制：在Top Half完成初步处理后，它会通过某种机制（如标志、队列、任务调度等）通知Bottom Half继续处理剩余的任务。Bottom Half通常在系统有空闲时间或需要时被调度执行。\n以UART串口通信为例来具体说明Top Half和Bottom Half的运作：\nTop Half:\n当UART设备接收到一个新的字符时，会触发一个中断。 操作系统立即响应这个中断，进入Top Half的处理程序。 Top Half会从UART的接收寄存器中读取字符，并将其保存到系统的一个缓冲区中（如环形缓冲区）。 读取完数据后，Top Half可能会清除中断标志，以防止再次触发相同的中断。 随后，Top Half结束，系统恢复执行被中断的任务。 Bottom Half:\n当系统空闲或特定条件满足时，Bottom Half被调度执行。 Bottom Half从Top Half保存的缓冲区中读取字符，并执行进一步处理，如将数据传递给用户进程。 如果是用户进程等待的数据（如Shell在等待用户输入），Bottom Half会将数据传递给进程，或者将处理结果显示在控制台上。 Bottom Half处理完成后，系统继续调度其他任务。 键盘输入与UART触发中断 当用户按下键盘上的某个键时，键盘会将该按键对应的字符通过UART串口传输到计算机主板上的UART设备。UART设备接收到来自键盘的字节数据，并将其存储在UART的接收寄存器中（Receive Holding Register, RHR）。与此同时，UART设备会触发一个硬件中断，通知操作系统有新的数据需要处理。\n当UART设备产生中断时，操作系统立即暂停当前正在执行的任务，进入中断处理程序的Top Half阶段。Top Half的主要任务是快速响应中断，将数据从UART设备中读取并保存到缓冲区中。详细过程如下：\n进入中断处理程序：操作系统通过查看UART中断状态，确认是键盘输入引发的中断。 读取UART寄存器数据：Top Half从UART的接收寄存器中读取一个字节的数据（即用户按下的字符），并将其存放到操作系统内存中的一个缓冲区。这个缓冲区通常是环形缓冲区（circular buffer），用于临时保存键盘输入的数据。 屏蔽/清除中断：为了避免重复中断，Top Half会清除中断标志或暂时屏蔽UART中断，防止再度中断。 结束Top Half：完成数据读取后，Top Half立即返回，恢复系统对其他任务的执行，确保中断处理时间尽可能短。 注意：在这个阶段，Top Half并不会进行复杂的处理任务，如将数据交给用户进程或显示在屏幕上。它只是确保数据从UART寄存器中取出，并保存到缓冲区中。\nTop Half完成后，操作系统会在合适的时机调用Bottom Half来处理剩下的任务。Bottom Half的主要职责是从缓冲区中读取数据，并将其传递给需要的进程（如Shell）。该阶段允许进行更复杂和耗时的操作，具体步骤如下：\n调度Bottom Half：系统在某个空闲时刻或当缓冲区中有足够的数据时，调度Bottom Half执行。 读取缓冲区数据：Bottom Half从Top Half保存的环形缓冲区中提取字符。这个字符是用户通过键盘输入的。 处理字符数据：读取到的字符可能会被进一步处理，例如检查是否有特殊控制字符（如回车或退格）。如果是普通字符，数据通常会被直接传递给需要处理的进程或设备。 用户进程交互：Bottom Half将从缓冲区中读取的字符提供给用户进程。例如，当前正在运行的Shell进程可能正在等待用户输入的命令。当Bottom Half处理完这些字符后，Shell可以继续执行用户的命令。 对于用户进程（如Shell）来说，键盘输入和UART设备的交互过程如下：\nShell等待输入：Shell进程通常会通过调用read系统调用，等待用户的输入。在这个过程中，Shell进程会进入休眠状态（sleep），直到有新的输入数据可用。 唤醒Shell进程：当用户输入字符并且Bottom Half完成了数据的处理后，Shell进程会被唤醒。具体来说，当环形缓冲区中有新的数据时，系统会通过wakeup机制唤醒之前休眠的Shell进程。 Shell读取数据：唤醒后的Shell进程通过read函数从缓冲区中读取用户输入的字符，并将其作为命令的一部分继续处理。 处理并显示：Shell读取并处理用户输入后，会在控制台上显示相应的反馈结果。例如，用户输入了命令“ls”，Shell读取该命令并将结果显示在屏幕上。 UART读取键盘输入的整个过程结合了中断机制和驱动程序的并发处理。操作系统通过中断管理器（如PLIC）响应外设的中断请求，并通过Top Half和Bottom Half机制高效地处理设备输入，同时保持系统的流畅运行。整个过程的并发管理依靠缓冲区和同步机制来解耦设备和用户进程的交互：\nTop Half快速响应中断并将键盘输入数据保存到缓冲区中。 Bottom Half在适当时机处理缓冲区中的数据，并将其交付给用户进程。 用户进程通过read函数获取输入数据，处理命令并显示结果。 这套机制使得操作系统能够并行处理键盘输入和其他任务，实现高效的多任务操作。\nTop Half与Bottom Half的接口 Top Half处理紧急的中断事件，如UART中断的情况。它的接口可以被内核或用户进程调用，典型的接口包括：\n读写接口：如read() 和 write()系统调用，用户进程可以调用这些接口从设备（如UART）读取数据或写入数据。这些接口的调用最终会触发Top Half处理输入/输出事件。 设备接口：在驱动程序中，Top Half直接与设备的寄存器和中断进行交互，它通过接口与硬件通信。例如，在UART驱动中，Top Half会读取UART的接收寄存器（如RHR寄存器）以获取数据，并将这些数据传递到内存缓冲区。 通常情况下，Bottom Half不会直接暴露给用户进程，因为它的工作是在内核中完成的。它的主要职责是处理Top Half推迟的任务，比如数据的进一步处理、协议解析等。虽然Bottom Half不直接提供传统意义上的接口，但它与Top Half及内核的其他部分之间会有某种隐式接口或协作机制。\nBottom Half的使用方式：\n延迟任务机制：Bottom Half通过某种延迟任务机制（如软中断softirq、任务队列tasklet、工作队列workqueue）被调度执行，这些机制不需要直接暴露给用户，而是在Top Half标记完成后，由操作系统自动安排执行。例如： 软中断（Softirq）：一种轻量级的延迟任务处理方式，用于处理网络数据包等延迟任务。 工作队列（Workqueue）：工作队列可以安排更复杂的任务，通常用于处理需要长时间执行的非紧急操作。 缓冲区的读写操作：Bottom Half主要通过访问共享的缓冲区进行操作。例如，Top Half将数据写入环形缓冲区，Bottom Half读取该缓冲区的数据并处理。虽然这些操作是通过内核函数调用完成的，但它们不直接作为接口提供给用户进程。 Top Half和Bottom Half的接口区别：\nTop Half的接口： 直接与用户进程交互：Top Half通常提供系统调用接口，如read() 和 write()，这些接口由用户进程调用，直接与外部设备（如UART）交互。 紧急处理：Top Half处理硬件中断的紧急部分，执行快速数据采集和传递任务，保证设备响应迅速。 Bottom Half的“接口”： 间接与内核交互：Bottom Half通过软中断、任务队列等机制被调度执行，不与用户进程直接交互。它主要处理延迟任务，如协议处理、数据打包等。 非紧急处理：Bottom Half处理不需要立即完成的任务，确保Top Half能够快速恢复正常工作，而不会因为复杂任务而延迟。 虽然Bottom Half不直接暴露给用户进程，开发者可以通过内核API间接与Bottom Half交互，例如：\n延迟任务调度：如果需要让某个任务在Bottom Half中处理，可以使用**任务队列（Workqueue）*或者*软中断（Softirq） API。例如，通过内核的queue_work()函数，将任务安排到工作队列中，等待系统空闲时执行。 数据传递：Bottom Half负责将处理后的数据传递给Top Half或内核的其他部分。举例来说，Bottom Half处理完某个网络数据包后，会通过某种内核回调机制将数据传递给网络协议栈，进一步传递到用户进程。 总结：\nTop Half的接口：直接暴露给用户进程或内核的其他部分，通常包括读写设备的系统调用接口以及硬件寄存器的访问接口。它通过这些接口处理设备的紧急任务。 Bottom Half的接口：Bottom Half不直接暴露给用户进程，而是通过内核的延迟任务机制（如软中断、工作队列）来处理非紧急任务。这些机制由Top Half或内核自动安排，开发者可以通过内核API使用这些机制，但并不会直接与用户进程交互。 Bottom Half的作用更多是通过内核机制间接使用，如果需要处理复杂的任务或与外部接口交互，通常是由Top Half处理好紧急任务后，Bottom Half通过缓冲区和延迟任务机制来完成实际的操作。\n控制台输入过程（详细分点说明） 初始化阶段：\nmain 函数调用 consoleinit: main 函数在系统启动时调用 consoleinit（kernel/console.c:184），该函数负责初始化UART硬件，使UART能够处理接收和发送字符的中断。\n在consoleinit中，UART硬件配置为：\n每当接收到一个字节时触发接收中断。 每当一个字节被发送完毕时触发发送完成中断。 UART等待输入：\nUART硬件等待输入字符:\nUART的FIFO（First In, First Out）缓冲区存储从外部接收到的字符。UART会将接收到的每个字符存放到 RHR（Receiver Holding Register）寄存器中，供软件读取。 LSR（Line Status Register）会更新状态位，指示UART接收到新字符，准备供软件读取。\n用户输入触发中断：\n用户按键输入传递到UART:\n当用户通过键盘输入字符时，输入会通过QEMU模拟的UART硬件发送到xv6中的UART设备。 当UART接收到字符时，触发接收中断，并向RISC-V内核发出中断请求。 RISC-V的陷阱处理程序接收到该中断后，调用 devintr（kernel/trap.c:177）处理。 处理中断：\ndevintr 确定中断源: devintr 从RISC-V的 scause 寄存器中读取中断信息，发现中断来自外部设备。 devintr 通过与PLIC（Platform-Level Interrupt Controller）的交互，确定中断来源于UART设备。 调用UART中断处理程序：\nuartintr 处理UART中断: devintr 确定中断来自UART后，调用 uartintr（kernel/uart.c:180）。 uartintr 从UART的RHR读取所有等待处理的输入字符。 这些输入字符被传递给 consoleintr（kernel/console.c:138）。 字符处理与缓冲：\nconsoleintr 处理用户输入字符: consoleintr 将接收到的字符积累到缓冲区 cons.buf 中。 该函数对特殊字符（如退格、Ctrl+U）进行处理。 如果用户输入了换行符（代表一整行输入完成），consoleintr 会唤醒等待的 consoleread 函数。 整行输入读取：\nconsoleread 等待整行输入: 当用户进程（如shell）调用 read 系统调用时，系统进入 consoleread 函数（kernel/console.c:82）。 consoleread 会检查 cons.buf 中的内容，如果未接收到整行输入，调用 sleep 使进程进入休眠状态，等待输入完成。 当整行输入完成并且 consoleintr 唤醒 consoleread 时，consoleread 会将 cons.buf 中的整行内容复制到用户空间。 返回输入结果： 返回用户输入数据:\nconsoleread 将缓冲区中的一整行数据复制到用户进程的缓冲区中，并通过 read 系统调用将该数据返回到用户进程。 用户进程（如shell）会继续处理该输入行，执行相应操作。\n总结：\n输入流程开始于用户按键输入，字符通过QEMU的UART硬件传递到xv6的UART设备，触发中断。 中断处理程序 devintr 和 uartintr 从UART硬件读取输入字符，consoleintr 将字符缓存在 cons.buf 中。 当整行输入完成后，consoleread 负责将输入行复制到用户空间，并返回给用户进程。 控制台输出过程（函数作用与调用关系） write 系统调用\n作用: 用户进程通过 write 系统调用向控制台输出数据。\n调用关系: write 系统调用从用户空间接收数据，并将其交给内核，由内核负责将数据写入到控制台设备的文件描述符。\nuartputc (kernel/uart.c:87)\n作用: 这是控制台输出的核心函数，用于将每个字符输出到UART设备。\n缓冲机制: 将输出字符添加到输出缓冲区 uart_tx_buf，这样写进程不需要等待UART硬件逐个字节传输数据。 调用 uartstart: 在字符被放入缓冲区后，uartputc 会调用 uartstart 来启动UART的输出传输。 等待情况: 如果缓冲区已满，uartputc 将等待直到缓冲区有空余空间再继续。 uartstart (kernel/uart.c:126)\n作用: 检查UART是否正在发送数据，如果没有则开始传输下一个字符。\n设备启动: uartstart 会从输出缓冲区中取出下一个要发送的字符，写入到 THR（Transmitter Holding Register） 中，启动UART的传输。\n调用关系:\nuartstart可以由两种方式调用：\n由 uartputc 调用，用于启动第一个字符的传输。 由 uartintr 调用，用于继续传输剩余缓冲区中的字符。 uartintr (kernel/uart.c:180)\n作用: UART中断处理程序，用于在传输完成时处理下一个字符。\n调用 uartstart: 当UART完成了一个字节的传输，会触发中断，uartintr 会调用 uartstart 来继续发送缓冲区中的下一个字节。 中断机制: UART每次传输完成一个字节，都会触发中断，这使得 uartintr 能够继续从 uart_tx_buf 缓冲区中读取字符并发送。\n控制台输出的调用关系图 用户进程调用 write 系统调用 用户进程通过 write 向控制台文件描述符写入多个字符。 write 调用 uartputc write 系统调用内部将每个字符传递给 uartputc。 uartputc 将每个字符加入到 uart_tx_buf 缓冲区中，确保写进程不必等待字符传输完成。 uartputc 调用 uartstart uartputc 启动传输第一个字符，将其交给UART硬件。 uartstart 检查UART是否空闲，如果空闲，则从 uart_tx_buf 中取出下一个字符，并写入 THR 寄存器，启动字符传输。 UART硬件发送完成，触发中断 UART每次完成一个字符传输时，会触发中断，通知CPU传输完成。 中断处理程序 uartintr 处理传输完成 中断触发后，RISC-V内核的陷阱处理程序调用 devintr，随后调用 uartintr。 uartintr 负责从 uart_tx_buf 中读取下一个字符，调用 uartstart 启动下一个字符的传输。 uartstart 启动下一字符传输 uartstart 再次从 uart_tx_buf 中取出下一个字符，写入UART的 THR 寄存器中，继续传输。 控制台输出的完整流程：\n用户进程通过 write 系统调用 向控制台写入多个字符。 write 调用 uartputc，将字符逐个加入输出缓冲区 uart_tx_buf。 uartputc 调用 uartstart，启动UART传输第一个字符。 UART硬件发送完一个字符后，触发中断。 中断处理程序 uartintr 调用 uartstart，继续发送输出缓冲区中的下一个字符。 UART持续传输，直到所有缓冲字符都被发送完毕。 控制台输出是通过将数据放入输出缓冲区并异步发送的过程。write 系统调用将数据交给 uartputc，它会立即返回，避免进程阻塞。通过 uartstart 和中断机制，UART硬件逐步发送缓冲区中的字符，最终完成数据传输。这种设计通过缓冲和中断解耦了设备的慢速I/O操作与进程的执行，从而提高了系统的效率和响应能力。\nUART（通用异步收发传输器）具有双向通信功能，既能接收字符，也能发送字符。这是UART作为一个串行通信设备的核心功能，它不仅用于从外部设备接收输入，还能将系统内部的数据发送到外部设备。下面我详细解释一下这两者的关系，以及发送字符到UART的作用。\n接收字符（输入）： UART负责从外部设备接收数据，比如从键盘输入或从其他串行设备接收到数据。 在你提到的控制台输入的过程中，UART通过硬件接收字符，触发中断，最终让xv6内核处理输入字符。 发送字符（输出）： UART也负责将系统内部的数据传输到外部设备。例如，操作系统需要把用户进程的输出（比如shell的输出）显示在控制台屏幕上，这时就需要通过UART发送数据。 当系统向控制台写入输出数据时，它通过UART传输这些字符，最终显示在终端或其他外部设备上。 定时器中断的作用 定时器中断在操作系统中起着至关重要的作用，它的主要功能是确保系统时钟的精确性，以及实现任务的调度。在xv6操作系统中，定时器中断同样是保持系统正常运行的关键机制，特别是在多任务处理和进程调度方面。下面我将详细说明定时器中断的具体作用。\n维持系统时钟\n定时器中断的首要功能是维持操作系统的时钟，这是所有现代操作系统的基础。操作系统通过定时器中断来跟踪系统的当前时间，并执行与时间相关的操作，比如：\n时间片（Time Slice）管理：在多任务系统中，每个进程或线程会分配一定的时间片进行执行。当时间片用完后，定时器中断会触发，从而通知操作系统进行任务切换。 系统时间维护：定时器中断以固定的间隔触发，操作系统利用这种固定周期的中断来更新系统的时间，使得系统可以精确地管理秒、分钟、小时等时间单位。 定时事件：定时器中断可以用于触发定时任务（比如操作系统内核中的心跳机制，定期检查系统健康状态），也可以用于实现用户进程中的定时功能，如sleep函数。 实现进程调度\n在xv6操作系统中，定时器中断起到了触发进程调度的作用。通过定时器中断，操作系统可以定期检查当前运行的进程，并决定是否切换到另一个进程：\n时间片轮转调度（Round-Robin Scheduling）：在多任务操作系统中，定时器中断用于实现时间片轮转调度算法。操作系统会为每个进程分配一个时间片（比如几毫秒），当时间片用完时，定时器中断会触发操作系统的调度器（scheduler）运行，选择另一个进程执行。\n例如，在xv6中，当定时器中断发生时，系统会调用 yield 函数来触发任务切换。yield 将当前进程的状态保存，并调度其他进程运行。如果当前进程是一个计算密集型进程（即需要大量CPU时间的进程），定时器中断确保该进程不会长期占用CPU，而是让其他进程也有机会执行。\n公平性：定时器中断通过时间片机制保证系统中每个进程能够公平地获取CPU资源。如果没有定时器中断，一个高优先级或耗时长的进程可能会长时间占用CPU，导致其他进程饥饿。\n打断无限循环或阻塞操作\n在一些情况下，进程可能会进入长时间的阻塞或无限循环。定时器中断的存在可以强制打断这些进程，从而避免系统陷入死锁或卡死状态：\n防止单一进程长期占用CPU：如果一个进程陷入无限循环，定时器中断会周期性触发进程切换，确保系统可以继续运行其他任务。 进程间的并发处理：定时器中断帮助操作系统在进程间实现并发运行，防止某个进程因长时间执行而阻止其他进程获取CPU。 保证系统响应性\n定时器中断还确保系统能够及时响应外部事件，例如I/O操作、用户输入等。即使系统正处于一个计算密集型任务中，定时器中断可以强制让操作系统暂停当前任务，检查是否有需要优先处理的事件：\nI/O并发：定时器中断可以在处理耗时的I/O操作时保证系统不会卡死。例如，当系统等待外部设备的输入时，定时器中断允许其他进程继续执行，而不是让整个系统停滞在I/O操作上。 定期触发系统任务\n定时器中断不仅用于任务调度，还用于定期触发系统级任务。这些任务通常是在操作系统后台运行，用于维护系统的健康状态或执行周期性的操作。例如：\n垃圾回收：操作系统可能会在定时器中断的帮助下，定期清理不再使用的资源或内存。 系统监控：操作系统会使用定时器中断定期执行监控任务，检查系统负载、资源使用情况，或者检查进程是否存在异常行为。 定时器中断的典型过程：\n设置定时器：在xv6启动时，start.c 中的代码设置了定时器硬件，通过编程CLINT（Core-Local Interruptor）来生成定期的中断信号。 定时器中断触发：定时器达到设定的时间后，触发中断信号。这个中断会导致CPU暂停当前的任务，跳转到定时器中断处理程序。 处理定时器中断： 定时器中断触发后，处理程序 timervec 会保存CPU的寄存器状态，设置下一个定时器中断，并请求RISC-V发起一个软件中断。 软件中断交给xv6的内核处理，执行内核的任务调度机制。 任务调度：通过中断的方式，xv6可以决定是否需要切换到另一个进程，确保所有进程都能公平地获取CPU时间。 定时器中断是xv6以及所有现代操作系统的关键组成部分。它不仅维持了系统的时间管理，还通过触发进程切换来实现多任务处理。定时器中断确保了系统的并发性、响应性和公平性，在用户进程之间高效调度资源，同时保持系统流畅运行。\nCLINT 的详细解释 CLINT（Core-Local Interruptor） 是 RISC-V 架构中的一个硬件模块，负责生成核局部中断（Local Interrupts），包括定时器中断和软件中断。它在每个 CPU 核上单独存在，用于处理与该核相关的中断，主要包括定时器中断和软件中断。\nCLINT 在 RISC-V 系统中的主要作用是提供一种机制，使每个 CPU 核能够：\n生成定时器中断：CLINT 包含一个计时器寄存器，通过编程这些寄存器，操作系统可以设置一个定时器。定时器到达指定时间后，CLINT 会向相应的 CPU 核发出一个定时器中断信号。 触发软件中断：CLINT 可以通过写入寄存器，向本地 CPU 核发出一个软件中断。这种中断通常用于进程间通信（IPC），比如操作系统内核通知某个核执行特定的任务。 管理中断：CLINT 负责处理与核本地相关的中断，它允许每个 CPU 核独立处理自己的定时器中断和软件中断。 CLINT 的工作范围在 CPU 核本地，特别是与定时器和软件中断相关的控制操作。在以下几个方面，CLINT 发挥了关键作用：\n定时器中断管理： CLINT 内部有一个定时器寄存器 mtime，用于存储当前的时间计数值，还有一个 mtimecmp 寄存器，表示下一个定时器中断的时间。 当 mtime 增加到等于或超过 mtimecmp 的值时，CLINT 生成一个定时器中断，通知 CPU 核中断处理程序（如 timervec）。 软件中断管理： 操作系统可以通过向 CLINT 的软件中断寄存器写入特定的值来触发一个软件中断。这个中断可以用于通知 CPU 执行特定的内核任务或进程间通信。 核本地中断控制： CLINT 在每个 CPU 核上独立运行，管理与该核相关的本地中断，而不影响其他 CPU 核。这样可以实现并行的中断处理，确保多核系统的高效运行。 要编程 CLINT，我们需要与其提供的寄存器进行交互。通常，这涉及对 mtime 和 mtimecmp 寄存器的读写操作。\n初始化定时器中断\n在 xV6 中，定时器中断的设置是在系统初始化阶段完成的，通常在 start.c 文件中实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void start() { // ... 初始化代码 ... // 设置定时器：编程 CLINT int interval = 1000000; // 定时器间隔，时间单位依赖于时钟频率 *(uint64*)CLINT_MTIMECMP(hartid) = *(uint64*)CLINT_MTIME + interval; // 启用定时器中断 w_mie(r_mie() | MIE_MTIE); // 允许机器模式定时器中断 // 设置 mtvec 指向定时器中断处理程序 timervec w_mtvec((uint64)timervec); // 进入内核主程序 main(); } CLINT 寄存器\nmtime 寄存器：用于计时，它会不断递增，表示当前的时间。\nmtimecmp 寄存器：用于设定定时器中断的触发时间。操作系统将 mtimecmp 设为一个未来的时间值，当 mtime 达到该值时，CLINT 触发定时器中断。\n处理定时器中断\n当 mtime 达到或超过 mtimecmp 的值时，CLINT 触发一个定时器中断。定时器中断会导致 CPU 跳转到中断处理向量（timervec），在此基础上进行以下步骤：\n保存当前状态：timervec 保存当前寄存器状态，以免打断正在执行的任务。 重新编程定时器：将 mtimecmp 更新到一个新的未来时间，以便设置下一个定时器中断。 触发软件中断：timervec 请求生成一个软件中断，通过正常的陷阱机制交给内核进一步处理。 在 devintr 中处理软件中断\n软件中断是由定时器中断处理程序触发的。devintr 函数（kernel/trap.c:204）处理这种软件中断，具体工作如下：\n识别中断类型：通过检查 RISC-V 的 scause 寄存器，devintr 确定中断是由定时器引发的。 执行调度程序：如果定时器中断触发了软件中断，devintr 会调用内核的调度函数 yield，从而可能将 CPU 切换到另一个进程。 总结：\nCLINT 是 RISC-V 系统中的一个核心组件，负责处理核本地的中断，尤其是定时器中断和软件中断。 定时器中断的编程 通过设置 mtime 和 mtimecmp 寄存器来完成，定期生成中断信号，通知操作系统进行调度和其他周期性任务。 定时器中断的作用 是维持系统时钟、调度进程、打断无限循环或阻塞操作，确保系统响应性和公平性。 CLINT 通过提供定时器和软件中断功能，在多核系统中帮助操作系统高效地管理和调度任务。\n","date":"2024-09-16T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0819interrupts/bg_hu_f27adb99a3c633b6.jpg","permalink":"https://echudet.github.io/p/mit6s0819interrupts/","title":"MIT6S081：9Interrupts"},{"content":"Lecture 8 Page Fault 课程杂记 在XV6中，一旦用户空间进程触发了page fault，会导致进程被杀掉。目前为止，内存地址映射相对来说比较静态。不管是user page table还是kernel page table，都是在最开始的时候设置好，之后就不会再做任何变动。 page fault可以让这里的地址映射关系变得动态起来。通过page fault，内核可以更新page table，这是一个非常强大的功能。 内核需要什么样的信息才能够响应page fault 需要出错的虚拟地址，或者是触发page fault的源。(引起page fault的内存地址) 出错的原因。(引起page fault的原因类型) 触发page fault的指令的地址。(引起page fault时的程序计数器值)\u0026ndash;\u0026gt;因为在page fault handler中我们或许想要修复page table，并重新执行对应的指令。 Lazy page allocation。在XV6中，sbrk的实现默认是eager allocation。这表示了，一旦调用了sbrk，内核会立即分配应用程序所需要的物理内存。 sbrk是XV6提供的系统调用，它使得用户应用程序能扩大自己的heap。当调用sbrk时，它的参数是整数，代表了你想要申请的page数量。sbrk会扩展heap的上边界（也就是会扩大heap）。 当一个应用程序启动的时候，sbrk指向的是heap的最底端，同时也是stack的最顶端。这个位置通过代表进程的数据结构中的sz字段表示，这里以p-\u0026gt;sz表示。 当sbrk实际发生或者被调用的时候，内核会分配一些物理内存，并将这些内存映射到用户应用程序的地址空间，然后将内存内容初始化为0，再返回sbrk系统调用。应用程序还可以通过给sbrk传入负数作为参数，来减少或者压缩它的地址空间。 利用lazy allocation。核心思想非常简单，sbrk系统调基本上不做任何事情，唯一需要做的事情就是提升p-\u0026gt;sz，将p-\u0026gt;sz增加n，其中n是需要新分配的内存page数量。但是内核在这个时间点并不会分配任何物理内存。之后在某个时间点，应用程序使用到了新申请的那部分内存，这时会触发page fault，因为我们还没有将新的内存映射到page table。所以，如果我们解析一个大于旧的p-\u0026gt;sz，但是又小于新的*p-\u0026gt;sz（注，也就是旧的p-\u0026gt;sz + n）*的虚拟地址，我们希望内核能够分配一个内存page，并且重新执行指令。 当你查看一个用户程序的地址空间时，存在text区域，data区域，同时还有一个BSS区域（注，BSS区域包含了未被初始化或者初始化为0的全局或者静态变量）。当编译器在生成二进制文件时，编译器会填入这三个区域。text区域是程序的指令，data区域存放的是初始化了的全局变量，BSS包含了未被初始化或者初始化为0的全局变量。\n在一个正常的操作系统中，如果执行exec，exec会申请地址空间，里面会存放text和data。因为BSS里面保存了未被初始化的全局变量，这里或许有许多许多个page，但是所有的page内容都为0。\n通常可以调优的地方是，我有如此多的内容全是0的page，在物理内存中，我只需要分配一个page，这个page的内容全是0。然后将所有虚拟地址空间的全0的page都map到这一个物理page上。这样至少在程序启动的时候能节省大量的物理内存分配。\n因为所有的虚拟地址空间page都期望page的内容是全0，所以这里的PTE都是只读的。\n之后在某个时间点，应用程序尝试写BSS中的一个page时，比如说需要更改一两个变量的值，我们会得到page fault。那么，对于这个特定场景中的page fault,我们应该创建一个新的page，将其内容设置为0，并重新执行指令。之后我们需要更新这个page的mapping关系，首先PTE要设置成可读可写，然后将其指向新的物理page。这里相当于更新了PTE，之后我们可以重新执行指令。\ncopy-on-write fork，有时也称为COW fork。当我们创建子进程时，与其创建，分配并拷贝内容到新的物理内存，其实我们可以直接共享父进程的物理内存page。我们可以设置子进程的PTE指向父进程对应的物理内存page。子进程想要修改这些内存的内容，相应的更新应该对父进程不可见。为了确保进程间的隔离性，我们可以将这里的父进程和子进程的PTE的标志位都设置成只读的。\n父进程和子进程都会继续运行，而父进程或者子进程都可能会执行store指令来更新一些全局变量，这时就会触发page fault，因为现在在向一个只读的PTE写数据。\n新分配的物理内存page只对子进程的地址空间可见，所以我们可以将相应的PTE设置成可读写，并且我们可以重新执行store指令。实际上，对于触发刚刚page fault的物理page，因为现在只对父进程可见，相应的PTE对于父进程也变成可读写的了。\n当内核在管理这些page table时，对于copy-on-write相关的page，内核可以设置相应的bit位，这样当发生page fault时，我们可以发现如果copy-on-write bit位设置了，我们就可以执行相应的操作了。否则的话，比如说lazy allocation，我们就做一些其他的处理操作。\n学生提问：当发生page fault时，我们其实是在向一个只读的地址执行写操作。内核如何能分辨现在是一个copy-on-write fork的场景，而不是应用程序在向一个正常的只读地址写数据。是不是说默认情况下，用户程序的PTE都是可读写的，除非在copy-on-write fork的场景下才可能出现只读的PTE？\nFrans教授：内核必须要能够识别这是一个copy-on-write场景。几乎所有的page table硬件都支持了这一点。我们之前并没有提到相关的内容，下图是一个常见的多级page table。对于PTE的标志位，我之前介绍过第0bit到第7bit，但是没有介绍最后两位RSW。这两位保留给supervisor software使用，supervisor softeware指的就是内核。内核可以随意使用这两个bit位。所以可以做的一件事情就是，将bit8标识为当前是一个copy-on-write page。\n我们回到exec，在未修改的XV6中，操作系统会加载程序内存的text，data区域，并且以eager的方式将这些区域加载进page table。但是根据我们在lazy allocation和zero-filled on demand的经验，为什么我们要以eager的方式将程序加载到内存中？为什么不再等等，直到应用程序实际需要这些指令的时候再加载内存？程序的二进制文件可能非常的巨大，将它全部从磁盘加载到内存中将会是一个代价很高的操作。又或者data区域的大小远大于常见的场景所需要的大小，我们并不一定需要将整个二进制都加载到内存中。\n对于exec，在虚拟地址空间中，我们为text和data分配好地址段，但是相应的PTE并不对应任何物理内存page。对于这些PTE，我们只需要将valid bit位设置为0即可。\n在最坏的情况下，用户程序使用了text和data中的所有内容，那么我们将会在应用程序的每个page都收到一个page fault。但是如果我们幸运的话，用户程序并没有使用所有的text区域或者data区域，那么我们一方面可以节省一些物理内存，另一方面我们可以让exec运行的更快（注，因为不需要为整个程序分配内存）。\n我们将要读取的文件，它的text和data区域可能大于物理内存的容量。又或者多个应用程序按照demand paging的方式启动，它们二进制文件的和大于实际物理内存的容量。对于demand paging来说，假设内存已经耗尽了或者说OOM了，这个时候如果得到了一个page fault，需要从文件系统拷贝中拷贝一些内容到内存中，但这时你又没有任何可用的物理内存page。一个选择是撤回page（evict page）。比如说将部分内存page中的内容写回到文件系统再撤回page。一旦你撤回并释放了page，那么你就有了一个新的空闲的page，你可以使用这个刚刚空闲出来的page，分配给刚刚的page fault handler，再重新执行指令。\n什么样的page可以被撤回？并且该使用什么样的策略来撤回page？\n最常用的策略，Least Recently Used，或者叫LRU。没有被Access过的page可以直接撤回，定时的将Access bit恢复成0。\n一个学生提问：为什么需要恢复这个bit？\nFrans教授：如果你想知道page最近是否被使用过，你需要定时比如每100毫秒或者每秒清除Access bit，如果在下一个100毫秒这个page被访问过，那你就知道这个page在上一个100毫秒中被使用了。而Access bit为0的page在上100毫秒未被使用。这样你就可以统计每个内存page使用的频度，这是一个成熟的LRU实现的基础。（注，可以通过Access bit来决定内存page 在LRU中的排名）\n这节课最后要讨论的内容，也是后面的一个实验，就是memory mapped files。这里的核心思想是，将完整或者部分文件加载到内存中，这样就可以通过内存地址相关的load或者store指令来操纵文件。\n为了支持这个功能，一个现代的操作系统会提供一个叫做mmap的系统调用。这个系统调用会接收一个虚拟内存地址（VA），长度（len），protection，一些标志位，一个打开文件的文件描述符，和偏移量（offset）。\n这里的语义就是，从文件描述符对应的文件的偏移量的位置开始，映射长度为len的内容到虚拟内存地址VA，同时我们需要加上一些保护，比如只读或者读写。\n假设文件内容是读写并且内核实现mmap的方式是eager方式（不过大部分系统都不会这么做），内核会从文件的offset位置开始，将数据拷贝到内存，设置好PTE指向物理内存的位置。之后应用程序就可以使用load或者store指令来修改内存中对应的文件内容。当完成操作之后，会有一个对应的unmap系统调用，参数是虚拟地址（VA），长度（len）。来表明应用程序已经完成了对文件的操作，在unmap时间点，我们需要将dirty block写回到文件中。我们可以很容易的找到哪些block是dirty的，因为它们在PTE中的dirty bit为1。\n","date":"2024-09-15T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0818page-fault/bg_hu_e57aabeb71ba7158.jpg","permalink":"https://echudet.github.io/p/mit6s0818page-fault/","title":"MIT6S081：8Page Fault"},{"content":"Lecture 6 Isolation \u0026amp; system call entry/exit 课程杂记 Traps and system calls\n有三种事件会导致 CPU 搁置普通指令的执行，强制将控制权转移给处理该事件的特殊代码。\n系统调用-\u0026gt;用户程序执行 ecall 指令要求内核为其做某事时 异常-\u0026gt;一条指令（用户或内核）做了一些非法的事情，如除以零 中断-\u0026gt;当一个设备发出需要注意的信号时，例如当磁盘硬件完成一个读写 请求时 代码在执行时发生 trap，之后都会被恢复，而且不需要意识到发生了什么特殊的事情\n执行顺序：trap 迫使控制权转移到内核；内核保存寄存器和其他状态，以便恢复执行；内核执行适当的处理程序代码（例如，系统调用实现或设备驱动程序）；内核恢复保存的状态，并从 trap 中 返回；代码从原来的地方恢复。\nXv6 trap 处理分为四个阶段：RISC-V CPU 采取的硬件行为，为内核 C 代码准备的汇编入口，处理 trap的 C 处理程序，以及系统调用或设备驱动服务\n三种 trap 类型之间的 共性表明，内核可以用单一的代码入口处理所有的 trap，但事实证明，为三种不同的情况， 即来自用户空间的 trap、来自内核空间的 trap 和定时器中断，设置单独的汇编入口和 C trap 处理程序是很方便的。\n每个 RISC-V CPU都有一组控制寄存器，内核写入这些寄存器来告诉CPU 如何处理 trap， 内核可以通过读取这些寄存器来发现已经发生的 trap。这里是最重要的寄存器的概述。\nstvec：内核在这里写下 trap 处理程序的地址；RISC-V 到这里来处理 trap。\nsepc：当 trap 发生时，RISC-V 会将程序计数器保存在这里（因为 PC 会被 stvec 覆 盖）。sret(从 trap 中返回)指令将 sepc 复制到 pc 中。内核可以写 sepc 来控制 sret 的返回到哪里。\nscause：RISC -V 在这里放了一个数字，描述了 trap 的原因。\nsscratch：内核在这里放置了一个值，这个值会方便 trap 恢复/储存用户上下文。\nsstatus：sstatus 中的 SIE 位控制设备中断是否被启用，如果内核清除 SIE，RISCV 将推迟设备中断，直到内核设置 SIE。SPP 位表示 trap 是来自用户模式还是监督者模式，并控制 sret 返回到什么模式。\n当需要执行 trap 时，RISC-V 硬件对所有的 trap 类型（除定时器中断外）进行以下操作：\n如果该 trap 是设备中断，且 sstatus SIE 位为 0，则不要执行以下任何操作。 通过清除 SIE 来禁用中断。 复制 pc 到 sepc 34 将当前模式(用户或监督者)保存在 sstatus 的 SPP 位。 在 scause 设置该次 trap 的原因。 将模式转换为监督者。 将 stvec 复制到 pc。 执行新的 pc。 因为 RISC-V 硬件在 trap 过程中不切换页表，所以用户页表必须包含 uservec 的映射， 即 stvec 指向的 trap 处理程序地址。uservec 必须切换 satp，使其指向内核页表；为了在切 换后继续执行指令，uservec 必须被映射到内核页表与用户页表相同的地址。\nXv6 用一个包含 uservec 的 trampoline 页来满足这些条件。Xv6 在内核页表和每个用户 页表中的同一个虚拟地址上映射了 trampoline 页。这个虚拟地址就是 TRAMPOLINE （如我 们在图 2.3 和图 3.3 中看到的）。trampoline.S 中包含 trampoline 的内容，（执行用户代码 时）stvec 设置为 uservec（kernel/trampoline.S:16）。\ntrapframe 包含指向当前进程的内核栈、当前 CPU 的 hartid、usertrap 的地址和内核页 表的地址的指针，uservec 将这些值设置到相应的寄存器中，并将 satp 切换到内核页表和 刷新 TLB，然后调用 usertrap。\n用户空间 trap 处理流程详解\n当 RISC-V 系统中用户空间程序发生 trap（如系统调用、非法操作或设备中断）时，处理流程如下：\n1. Trap 发生时的初始状态 用户程序在用户模式下运行，并使用用户页表。 程序可能触发 trap，原因可能是系统调用（ecall 指令）、非法指令或硬件中断。 2. 切换到 uservec（kernel/trampoline.S:16） CPU 检测到 trap 并自动跳转到 stvec 指向的地址，即 uservec，这是一个位于内核中的 trap 处理程序的入口地址。 问题：此时仍在使用用户页表，用户页表可能无法映射内核的所有区域，直接访问内核会导致失败。 3. uservec 的操作 uservec 是一个位于 trampoline 页的代码，trampoline 页同时映射在用户页表和内核页表中的相同虚拟地址上。 uservec 通过 csrrw 指令将寄存器 a0 与 sscratch 互换。此时，a0 变为可用，并指向了 sscratch 中的内核数据（通常为进程的 trapframe）。 uservec 保存用户程序的寄存器到进程的 trapframe 中。trapframe 是专门为每个进程分配的一页内存，映射在用户地址空间中（虚拟地址 TRAPFRAME 下）。 4. 切换到内核页表 uservec 设置寄存器，使得 satp 寄存器指向内核页表，并刷新 TLB（Translation Lookaside Buffer，地址转换缓冲）。 切换页表后，系统安全地进入内核地址空间，可以正常访问内核的所有内存和功能。 5. 调用 usertrap（kernel/trap.c:37） uservec 完成初步处理后，跳转到 usertrap，它负责进一步处理 trap。 usertrap 首先更改 stvec 指向 kernelvec，以处理在内核模式下可能发生的其他 trap。 保存 sepc（用户程序计数器），因为在内核中可能会发生进程切换。 根据 trap 的类型进行处理： 如果是系统调用，调用 syscall 处理。 如果是设备中断，调用 devintr 处理。 如果是异常，可能会杀死当前进程。 6. 准备返回用户空间 usertrap 完成处理后，准备返回用户空间。\n调用\n1 usertrapret （kernel/trap.c:90），它负责恢复用户空间的运行环境：\n设置 stvec 重新指向 uservec，以准备下一次 trap。 恢复 sepc 到保存的用户程序计数器。 跳转到 trampoline 页的 userret，准备切换回用户页表。 7. userret 的操作 1 userret 负责最后的状态恢复和切换回用户页表：\nuserret 将 satp 切换回用户页表，确保系统回到用户模式。 恢复用户寄存器，包括 a0 和 sscratch 的最终交换，确保在下一次 trap 发生时这些寄存器可用。 使用 sret 指令返回用户空间，从保存的用户程序计数器处继续执行用户代码。 关键点总结 Trampoline 页：这是内核和用户页表中映射的特殊区域，确保了在页表切换前后的代码能正常运行。 sscratch 寄存器：用于在 trap 处理过程中保存和交换关键的寄存器值。 trapframe：保存所有用户寄存器的状态，便于在用户和内核模式之间切换时进行恢复和保存。 当系统调用函数返回时，syscall 将其返回值记录在 p-\u0026gt;trapframe-\u0026gt;a0 中。用户空间 的 exec()将会返回该值，因为 RISC-V 上的 C 调用通常将返回值放在 a0 中。系统调用返回 负数表示错误，0 或正数表示成功。如果系统调用号无效，syscall 会打印错误并返回 1。\n内核 trap 代码将用 户寄存器保存到当前进程的 trapframe 中，内核代码可以在那里找到它们。函数 argint、 argaddr 和 argfd 从 trapframe 中以整数、指针或文件描述符的形式检索第 n 个系统调用参 数。它们都调用 argraw 在 trapframe 中检索相应的数据(kernel/syscall.c:35)。\nXv6 对异常的响应是相当固定：如果一个异常发生在用户空间，内核就会杀死故障进程。 如果一个异常发生在内核中，内核就会 panic。真正的操作系统通常会以更有趣的方式进行 响应。\n当 CPU 不能将虚拟地址翻译成物理地址时，CPU 会产生一个页面故障异常(page-fault exception)。 RISC-V 有三种不同的页故障：load 页故障（当加载指令不能翻译其虚拟地址 时）、stote 页故障（当存储指令不能翻译其虚拟地址时）和指令页故障（当指令的地址不能 翻译时）。scause 寄存器中的值表示页面故障的类型，stval 寄存器中包含无法翻译的地址。\n写时复制（Copy-On-Write, COW）Fork\n在传统的 fork 操作中，父进程的整个内存空间（包括数据段、堆和栈）都会被复制给子进程。这种方法非常消耗资源，因为即使子进程只使用了父进程的一小部分内存，内核仍然需要进行全量的内存复制。\nCOW fork 提出了一个更高效的解决方案。通过在初始时不复制物理内存，而是让父进程和子进程共享同一块物理内存，操作系统可以大幅节省内存和时间。但这样做有一个问题：如果父进程和子进程都试图写入同一块内存，可能会互相干扰。为了避免这种情况，操作系统将这块共享的内存页标记为只读。\n当父进程或子进程试图写入这块内存时，CPU 会检测到这个写操作违反了只读权限，进而触发一个**页面故障（page fault）**异常。操作系统的页面故障处理程序会意识到这个异常来自于一个 COW 页面，并且会执行以下操作：\n分配一个新的物理内存页。 将原来共享的页面内容复制到新分配的页面。 更新进程的页表，将这个新的页面映射为可写。 重新执行引发页面故障的指令。 这样，父进程和子进程各自获得了独立的、可写的内存，而这一切都是透明的，对应用程序来说是无感知的。\n懒分配（Lazy Allocation）\n懒分配是一种内存管理优化技术，它基于这样一个事实：应用程序往往请求比实际需要更多的内存。在懒分配的实现中，当应用程序调用 sbrk（或其他类似的内存分配系统调用）时，操作系统不会立即分配实际的物理内存，而只是扩大了进程的虚拟地址空间，并将新地址标记为无效。\n当应用程序尝试访问这些新地址时，会因为没有对应的物理内存而触发页面故障。此时，操作系统会为该页面实际分配物理内存，并将其映射到进程的页表中。这种按需分配的策略，确保了物理内存只在应用程序真正需要时才被分配，从而节省了内存资源。\n从磁盘分页（Paging from Disk）\n当系统内存不足时，操作系统可能需要将不常使用的内存页暂时移出物理内存，写入磁盘。这个过程叫做交换（swapping）。当一个被交换到磁盘的内存页再次被访问时，CPU 会触发一个页面故障，因为页表中的对应条目已经被标记为无效。\n操作系统会检查页面故障发生的地址，并判断该地址是否属于一个被换出的页面。如果是这样，内核会从磁盘中读取该页面，加载到物理内存，并更新页表，使其重新有效。随后，操作系统会恢复应用程序的执行，重新执行触发页面故障的指令。\n自动扩展堆栈：如果进程需要更多的栈空间，而它当前的栈已经耗尽，操作系统可以通过页面故障来检测到这一情况，然后分配新的内存页来扩展栈。\n内存映射文件：文件的内容可以直接映射到进程的内存空间中。如果映射的部分文件被访问而未加载，操作系统会通过页面故障来加载相应的文件部分。\n","date":"2024-09-14T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0816isolation-system-call-entry/exit/bg_hu_8475b6e7bc88e764.jpg","permalink":"https://echudet.github.io/p/mit6s0816isolation-system-call-entry/exit/","title":"MIT6S081：6Isolation \u0026 system call entry/exit"},{"content":"Lecture 5 Convention for RISC-V \u0026amp; ASM 课程杂记 C-\u0026gt;ASM\n处理器有一套指令集ISA C -\u0026gt; ASM (.S files)-\u0026gt; binary (obj/o files) RISC-V(Reduced ISA, simple Instr) open source Simple Instr Integrated Devices x86-64(ISA-\u0026gt;PC[Intel, AMP]) Complex ISA, 15k Instr backwards compatible \u0026amp; cmd demand high efficient ARM(RISC) Andriod IOS(Apple) gdb\nRISC-V\n寄存器是用来进行任何运算和数据读取的最快的方式，这就是为什么使用它们很重要，也是为什么我们更喜欢使用寄存器而不是内存。当我们调用函数时，你可以看到这里有a0 - a7寄存器。通常我们在谈到寄存器的时候，我们会用它们的ABI名字。不仅是因为这样描述更清晰和标准，同时也因为在写汇编代码的时候使用的也是ABI名字。第一列中的寄存器名字并不是超级重要，它唯一重要的场景是在RISC-V的Compressed Instruction中。基本上来说，RISC-V中通常的指令是64bit，但是在Compressed Instruction中指令是16bit。在Compressed Instruction中我们使用更少的寄存器，也就是x8 - x15寄存器。我猜你们可能会有疑问，为什么s1寄存器和其他的s寄存器是分开的，因为s1在Compressed Instruction是有效的，而s2-11却不是。除了Compressed Instruction，寄存器都是通过它们的ABI名字来引用。\na0到a7寄存器是用来作为函数的参数。如果一个函数有超过8个参数，我们就需要用内存了。从这里也可以看出，当可以使用寄存器的时候，我们不会使用内存，我们只在不得不使用内存的场景才使用它。\n对于Stack来说，是从高地址开始向低地址使用。所以栈总是向下增长。当我们想要创建一个新的Stack Frame的时候，总是对当前的Stack Pointer做减法。一个函数的Stack Frame包含了保存的寄存器，本地变量，并且，如果函数的参数多于8个，额外的参数会出现在Stack中。所以Stack Frame大小并不总是一样，即使在这个图里面看起来是一样大的。不同的函数有不同数量的本地变量，不同的寄存器，所以Stack Frame的大小是不一样的。但是有关Stack Frame有两件事情是确定的：\nReturn address总是会出现在Stack Frame的第一位 指向前一个Stack Frame的指针也会出现在栈中的固定位置 有关Stack Frame中有两个重要的寄存器，第一个是SP（Stack Pointer），它指向Stack的底部并代表了当前Stack Frame的位置。第二个是FP（Frame Pointer），它指向当前Stack Frame的顶部。因为Return address和指向前一个Stack Frame的的指针都在当前Stack Frame的固定位置，所以可以通过当前的FP寄存器寻址到这两个数据。\n","date":"2024-09-08T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0815convention-for-risc-v-asm/bg_hu_af564ed7c9e74ef9.jpg","permalink":"https://echudet.github.io/p/mit6s0815convention-for-risc-v-asm/","title":"MIT6S081：5Convention for RISC-V \u0026 ASM"},{"content":"Lecture 4 Page Tables(vm) 课程杂记 vm -\u0026gt; virtual memory ，map virtual to physical 隔离性\n正确设置的VM可以实现隔离性\ndefault下，不存在memory isolation\naddress spaces，地址空间 给包括内核所有程序专属空间，这些空间相互独立\n如何在物理内存上划分虚拟空间\n通过page table实现，kalloc储存余有空间的地址\n而page table 页表由处理器硬件或内存管理单元MMU实现，映射内存的VA-\u0026gt;PG表存储在MMU中，也有表存储在内存中用来找到MMU中的表（e.g. SATP寄存器）\nMMU是不定存储的，必要时读取表\n改写SATP寄存器是内核的权利，用户无法改写\npaging hw(risc-v)\nSATP-64bits,可存储2^64个地址，过大\n实际上，它仅为page创建地址，一个page为4kb,\nindex和offset，index指page序号（指向内存中的4kb），offset指page中具体的偏移\n实际上，高25bit并未使用（在RISC-V中)，中27位用于index，低12位用于offset(2^12 bits = 4 kb)，而物理内存可以比虚拟内存大，XV86为44位address + 12 offset，此处56为设计师设计的，并无具体含义\n虽然page table可存储2^27个条目，但它并非一次性存储在各个程序中，而是多级结构，先将27位划分为三个9bit，再指向不同的page directory（4096bytes），其中一个条目为8bytes，故其中含有512个PTE，VM的PT中最高级的9bit指向PD，得到一个物理page号，其指向中级PD，再得到一个PPN指向低级PD，再得到真正的物理地址\nPTE低十位一般用于标志位，\nRSW , Dirty, Accessed , Global, User, Execute, Write, Read, Valid\n9 \u0026mdash;-\u0026gt; 0\n当无法翻译的时候，会出现page fault\nsatp在此多级过程中，会提供最高级PD的地址，然后高位9bit会在PD中寻找相应的PPN\n这三个PD都存放于物理地址\n从PD得到的PPN（非目标物理地址）是44bit的，会在后面补上12个0，再指向下一个PD\n读一次地址要访问三次地址，代价较高，故有页表缓存（TLB），又称PTE缓存。TLB会存储VA-\u0026gt;PQ的映射\n当os切换page table时，会清空TLB缓存\n在RISC-V中，清空TLB指令为sfence_vma\n在RISC-V中，walk函数实现了与硬件MMU相同的功能\n三级页表一般是由硬件实现，而非操作系统\npage table 提供了一层抽象，这里的映射完全由操作系统控制，可实现多种功能，如page fault，os可以更新page table，并重新加载指令\nCPU每个核都有MMU和TLB\nxv6 vm code + layout kernelbase地址于0x800000时（kernel虚拟地址下的），产生的信息若地址低于它，则流向不同io设备，否则流向dram （当然可能也是unused，因为没那么多设备）\n总体物理地址的layout，是由主板决定的，并非仅是CPU决定\nGuardPage 不会映射，起保护Kernel Stack的作用，\nkernel stack 与 kernel data 映射于一处\n每一个用户进程都有一个kernel stack\n用户进程会占用kernel的虚拟内存的free memory，\n用户的虚拟内存和kernel的一样大，但用户的空间利用率低\nkernel中大部分虚拟内存和真实内存是一一对应的，因此在启动虚拟内存映射时不会对kernel 的运行造成影响\n每个进程都有用户栈和内核栈（p-\u0026gt;kstack）。当进程运行用户指令时，只有其用户栈被使用，其内核栈则是空的。然而当进程（通过系统调用或中断）进入内核时，内核代码就在进程的内核栈中执行；进程处于内核中时，其用户栈仍然保存着数据，只是暂时处于不活跃状态。进程的线程交替地使用着用户栈和内核栈。要注意内核栈是用户代码无法使用的，这样即使一个进程破坏了自己的用户栈，内核也能保持运行。\nxv6 使用结构体 struct proc 来维护一个进程的状态，其中最为重要的状态是进程的页表，内核栈，当前运行状态。我们接下来会用 p-\u0026gt;xxx 来指代 proc 结构中的元素。 当进程使用系统调用时，处理器转入内核栈中，提升硬件的特权级，然后运行系统调用对应的内核代码。当系统调用完成时，又从内核空间回到用户空间：降低硬件特权级，转入用户栈，恢复执行系统调用指令后面的那条用户指令。线程可以在内核中“阻塞”，等待 I/O, 在 I/O 结束后再恢复运行。 p-\u0026gt;state 指示了进程的状态：新建、准备运行、运行、等待 I/O 或退出状态中。 p-\u0026gt;pgdir 以 x86 硬件要求的格式保存了进程的页表。xv6 让分页硬件在进程运行时使用 p-\u0026gt;pgdir。进程的页表还记录了保存进程内存的物理页的地址。 当 PC 开机时，它会初始化自己然后从磁盘中载入 boot loader 到内存并运行。然后，boot loader 把 xv6 内核从磁盘中载入并从 entry（1040）开始运行。x86 的分页硬件在此时还没有开始工作；所以这时的虚拟地址是直接映射到物理地址上的。\nboot loader 把 xv6 内核装载到物理地址 0x100000 处。之所以没有装载到内核指令和内核数据应该出现的 0x80100000，是因为小型机器上很可能没有这么大的物理内存。而之所以在 0x100000 而不是 0x0 则是因为地址 0xa0000 到 0x100000 是属于 I/O 设备的。\n为了让内核的剩余部分能够运行，entry 的代码设置了页表，将 0x80000000（称为 KERNBASE（0207））开始的虚拟地址映射到物理地址 0x0 处。注意，页表经常会这样把两段不同的虚拟内存映射到相同的一段物理内存，我们将会看到更多类似的例子。\n“页表经常会这样把两段不同的虚拟内存映射到相同的一段物理内存”指的是以下两段虚拟内存：\n虚拟地址 0x0 到 0x400000：这段虚拟地址范围直接映射到物理地址 0x0 到 0x400000。在 entry 代码开始运行时，为了能够正常执行，这个映射是必要的，因为在系统启动时，虚拟地址和物理地址是直接对应的。 虚拟地址 KERNBASE 到 KERNBASE + 0x400000：这段虚拟地址范围对应的是高地址的内核空间，KERNBASE 通常为 0x80000000。为了让内核在其预期的高虚拟地址空间运行，entry 代码将这一段虚拟地址也映射到了物理地址 0x0 到 0x400000。 这两段虚拟地址虽然不同，但它们都映射到了同一段物理内存，即物理地址 0x0 到 0x400000。这样做的目的是为了在内核加载的早期阶段能够在物理内存较小的情况下让内核正常工作。随着系统的进一步初始化和内存管理机制的完善，低地址的映射会被移除，内核最终会运行在高地址的虚拟内存空间中。\n在xv6的启动过程中，分页硬件起到了至关重要的作用。简单来说，分页硬件负责将程序使用的虚拟地址翻译成实际的物理地址，这是实现虚拟内存管理的核心机制。分页硬件的作用：\n地址翻译：分页硬件的主要任务是将CPU在执行指令时产生的虚拟地址，按照页表的规定，转换为实际的物理地址。这使得每个进程可以拥有一个独立的虚拟地址空间，而不需要关心实际的物理内存布局。 内存保护：分页硬件可以通过页表中的权限位，控制对内存的访问权限。例如，它可以设置某些内存区域只读、不可执行，或者仅允许内核访问，从而提供内存保护机制，防止进程之间互相干扰。 支持进程隔离：分页硬件使得不同进程可以拥有各自的虚拟地址空间，这些空间彼此独立。即使多个进程使用相同的虚拟地址，它们在物理内存中映射到的实际地址也不同，从而实现了进程间的隔离。 分页硬件的启动步骤如下：\n设置页表：在启动分页硬件之前，内核首先要设置好页表。页表是一种数据结构，它记录了虚拟地址到物理地址的映射关系。xv6在启动过程中会建立一个初始的页表，这个页表会映射内核代码和数据到合适的物理地址。 加载页表地址到控制寄存器 %cr3：内核通过将页表的物理地址加载到控制寄存器 %cr3 中，告诉CPU当前要使用哪一个页表来进行地址翻译。这个步骤是让分页硬件知道应该使用哪一张表来进行虚拟地址到物理地址的转换。 启用分页：在设置好页表并将其地址加载到 %cr3 后，内核会设置控制寄存器 %cr0 中的分页启用位（CR0_PG），这一步骤正式启动了分页硬件。此时，CPU开始使用页表进行地址翻译，系统的内存管理机制进入正常工作状态。 切换到高地址执行：启用分页后，内核可以安全地从低地址跳转到高地址继续执行。由于页表已经将高地址映射到正确的物理内存位置，内核可以在它的预期虚拟地址空间中正常工作。 在整个启动过程中，除了分页硬件外，还有几个关键的硬件组件也在发挥重要作用。这些硬件协同工作，确保操作系统能够成功启动并运行：\nCPU（中央处理器）\n指令执行：CPU是整个启动过程的核心，它负责执行从boot loader到内核的所有指令。启动过程开始时，CPU从固化在硬件中的地址（通常是BIOS或EFI）开始执行，然后逐步加载和运行boot loader，再到加载和执行内核。\n模式切换：CPU在启动过程中会从实模式切换到保护模式，最后进入分页模式。在实模式下，CPU只处理简单的地址计算，但在保护模式和分页模式下，CPU可以利用复杂的内存管理和保护机制。\nBIOS/UEFI\n硬件初始化：BIOS（基本输入输出系统）或UEFI（统一可扩展固件接口）是计算机开机时首先运行的固件，它负责初始化计算机的硬件组件，如CPU、内存、硬盘和I/O设备。\n引导加载器加载：BIOS/UEFI在完成硬件初始化后，会将控制权交给boot loader，这通常是通过读取特定存储设备上的引导扇区来实现的。\n内存（RAM）\n代码和数据存储：内存用于存储启动过程中加载的所有代码和数据，包括boot loader、内核代码、页表以及其他重要的数据结构。在内核启动并运行时，内存还会被用来存储进程的代码、数据和栈等。\n栈操作：CPU在执行指令时使用内存中的栈来保存函数调用的信息、局部变量和返回地址。栈的正确初始化和管理对于顺利执行程序至关重要。\n硬盘控制器\n加载内核：硬盘控制器负责从硬盘中读取boot loader和内核。boot loader在启动时会通过硬盘控制器将内核从存储设备加载到内存的指定位置，以便CPU能够执行内核代码。 I/O设备\n基本输入输出：在启动过程中，I/O设备（如键盘、显示器、串口等）通常会被初始化，以便在启动过程的早期阶段，系统可以输出调试信息或接受输入。 中断控制器\n中断管理：中断控制器负责管理和处理硬件中断。在启动过程中，操作系统会设置中断向量表，以确保系统能够正确处理来自外部设备的中断请求。虽然在启动过程的早期阶段，系统可能不会处理复杂的中断，但中断控制器的初始化和配置是操作系统启动的一个重要步骤。 创建第一个进程的过程可以分为几个重要步骤，以下是通俗但详细的讲解：\n初始化阶段：首先，在系统启动时，main 函数会初始化一些必要的设备和子系统。然后，它调用了 userinit 函数来创建系统中的第一个进程。 分配进程槽位：userinit 函数调用了 allocproc，这是一个用于在内核中分配新进程的函数。allocproc 的任务是找到一个未被使用的进程槽位（结构体 struct proc），然后将其状态从未使用（UNUSED）标记为胚胎状态（EMBRYO），表示它正在被使用并分配给一个新进程。 设置内核栈：接下来，allocproc 为新进程分配一个内核栈，并设置好内核栈和一系列内核寄存器，使得当进程第一次运行时，它会“返回”到用户空间。这个过程通过 forkret 函数实现，该函数的作用是让内核线程从某个点开始执行，最后跳转到 trapret，准备好用户态的寄存器并跳转到用户代码执行。 初始化页表：为了让进程能够执行自己的代码，userinit 还需要为进程创建一个页表，这个页表最初只映射内核的内存区域。然后，userinit 调用了 inituvm，为进程分配了一页物理内存，并将初始化代码（initcode.S）拷贝到这段内存中。这个代码是用来启动进程的。 设置用户模式寄存器：在这个阶段，userinit 设置了进程的陷阱帧（trapframe），这是一个用于保存用户模式寄存器的结构。这里的关键是确保进程以用户模式（而不是内核模式）启动，并且能够处理硬件中断。 进程变为可运行状态：一旦所有这些设置完成，userinit 将进程的状态设置为 RUNNABLE，这意味着该进程现在可以被调度器选中运行了。 调度和运行：在 main 函数调用了 userinit 之后，系统调用 scheduler 函数来开始运行进程。调度器会找到一个处于 RUNNABLE 状态的进程，将其状态设为 RUNNING，然后通过 swtch 函数切换上下文，开始运行这个进程的内核线程。 用户代码的执行：最终，这个第一个进程的执行从 initcode.S 的第一个指令开始，该指令位于虚拟地址0。因为之前已经为该进程建立了页表，这些虚拟地址会被翻译为实际的物理内存地址，从而顺利执行进程的用户代码。 通过这些步骤，系统成功创建并运行了第一个用户进程，这也是系统启动后的第一个进程。\n新进程分配的内核栈最初包含一些关键内容，这些内容是为了使新进程能够正确地启动和运行。具体来说，内核栈的初始内容包括：\n上下文信息（context）：这是用于保存进程的寄存器状态的结构，包括指令指针（eip）和栈指针（esp）等。这些信息是为了保证进程能够从正确的位置继续执行。 返回地址：allocproc 会将返回地址设置为 forkret，这意味着当进程第一次运行时，它将从 forkret 开始执行。forkret 函数会进行一些初始化工作，然后返回到 trapret 函数。 trapret 地址：这是栈上的另一个重要内容，它在 forkret 之后被调用，用来从用户态进入内核态时恢复用户寄存器，并最终返回到用户代码的执行。 这些内容的作用是确保当新进程第一次被调度器选中运行时，它能够正确地进行从内核态到用户态的过渡，并开始执行用户代码。这种设置使得创建新进程和通过 fork 复制进程的过程可以共用同样的机制，从而简化了进程管理的逻辑。\n在一个进程中，除了内核栈以外，还包括以下重要内容：\n用户栈（User Stack）： 作用：用户栈用于存储用户态下的函数调用、局部变量和函数参数。每当进程进入用户态并执行用户代码时，它会使用用户栈。用户栈在每次系统调用或中断发生时会被保存，以便在返回到用户态时恢复。 进程控制块（Process Control Block, PCB）： 作用：这是内核用于管理和调度进程的一个重要数据结构。PCB 包含了进程的各种状态信息，如进程ID（PID）、进程状态（如 RUNNING, SLEEPING, ZOMBIE 等）、程序计数器（指示下一条指令的位置）、寄存器内容、打开的文件描述符列表、优先级、调度信息、以及内存管理信息（如页表指针）。 关键作用：PCB 是内核追踪进程的核心结构，它保存了进程执行所需的所有上下文信息。当进程被切换时，PCB 是用来保存和恢复上下文的关键。 虚拟地址空间（Virtual Address Space）： 作用：每个进程都有自己独立的虚拟地址空间。这个空间通常包括代码段（Text Segment）、数据段（Data Segment）、堆（Heap）和栈（Stack）。代码段包含可执行代码；数据段存放已初始化和未初始化的全局变量；堆用于动态内存分配；栈用于函数调用和局部变量。 关键作用：虚拟地址空间为每个进程提供了一个独立的运行环境，确保进程之间的内存隔离和安全。 页表（Page Table）： 作用：页表是内存管理单元（MMU）使用的数据结构，用于将进程的虚拟地址映射到物理地址。每个进程有自己的页表，内核利用它来管理内存和地址转换。 关键作用：页表实现了虚拟内存，使得进程可以假装拥有连续的内存地址，同时内核可以灵活地分配实际的物理内存。 文件描述符表（File Descriptor Table）： 作用：这是进程打开的所有文件的列表。每个打开的文件（或其他 I/O 资源，如管道、网络连接等）都有一个与之关联的文件描述符。 关键作用：文件描述符表使得进程可以进行文件 I/O 操作，如读写文件、网络通信等。每个文件描述符对应一个内核对象，内核通过这个对象管理文件操作。 信号处理表（Signal Handling Table）： 作用：用于定义进程如何处理各种信号。信号是操作系统向进程传递事件通知的一种方式（如中断、非法操作等）。 关键作用：信号处理表指定了进程遇到特定信号时要采取的操作（例如，忽略信号、执行默认操作或调用用户定义的处理函数）。 进程工作目录和根目录： 作用：进程有其当前工作目录（Current Working Directory, CWD）和根目录（Root Directory）。这些目录用于文件系统操作，指定文件路径是相对还是绝对。 关键作用：管理文件系统路径的上下文，使得进程能够访问和操作文件系统中的文件和目录。 共享内存和映射区域： 作用：这是进程可能使用的共享内存区域或通过内存映射文件的区域（如动态库映射）。 关键作用：提供进程间通信（IPC）机制，并允许进程高效访问大文件或共享数据段。 这些组件共同组成了一个进程的完整运行环境。它们协作使得操作系统可以多任务运行，安全地管理多个进程，以及在不同进程之间进行上下文切换。\n在创建第一个进程的过程中，初始化页表是至关重要的，因为页表用于将虚拟地址映射到物理地址。每个进程都有自己的虚拟地址空间，操作系统使用页表来管理这些地址与实际物理内存的对应关系。\n初始化页表的主要目的是为新进程提供一个独立的虚拟地址空间，这样进程可以运行在它自己的地址空间中，而不会干扰其他进程。同时，页表的设置允许操作系统通过分页机制有效管理内存，确保安全性和内存隔离。\n页表的重要性：在系统中，所有进程都运行在各自的虚拟地址空间中，而操作系统通过页表将这些虚拟地址转换为实际的物理地址。初始化页表为进程的运行提供了正确的地址映射，从而确保了进程能够访问内核和用户空间的必要资源。 当我们说“这个页表最初只映射内核的内存区域”时，指的是在初始阶段，页表只设置了内核空间的映射，而没有映射用户空间的地址。内核的地址空间包括内核代码、数据、堆、栈（包括内核栈）等内核所需的内存区域。\n内核栈：内核栈是内核在执行时使用的栈，是每个进程在内核态运行时的重要部分。在进程执行系统调用或发生中断时，CPU 会从用户模式切换到内核模式，使用内核栈来处理内核态代码。这部分内存是内核地址空间的一部分，因此会被映射。 所以，初始页表仅映射了内核的内存区域，意味着进程在最初的设置中只能访问内核空间的内存。这是为了保证进程一开始可以正常地在内核中执行。\n这里提到的“代码是用来启动进程的”指的是 initcode.S 这段汇编代码，它是用于初始化并启动第一个用户进程的。\ninitcode.S 的来源：initcode.S 是一个小型的汇编程序，它作为内核的一部分编译并链接到内核映像中。在构建内核时，链接器会将这段代码作为二进制数据嵌入到内核映像中。内核通过两个符号 _binary_initcode_start 和 _binary_initcode_size 标识这段代码的起始位置和大小。 作用：这段代码的主要作用是为第一个进程（initproc）提供一段简单的用户态代码。当第一个进程开始执行时，它会运行这段代码来初始化用户态的环境。其目的是确保进程能够顺利启动并进入用户态执行。这是内核为启动第一个进程提供的基础代码。\n如何加载：在创建第一个进程时，内核会通过 inituvm 函数为该进程分配一页物理内存，并将 initcode.S 的二进制代码拷贝到这段内存中。这段内存被映射到进程的虚拟地址空间的地址0。当进程第一次运行时，处理器会从地址0开始执行 initcode.S 的指令。 总结一下，初始化页表是为了设置进程的虚拟地址空间，而最初的页表只映射内核区域（包括内核栈），以便在内核态能够正常工作。initcode.S 是一段汇编代码，它从内核编译时嵌入内核中，被加载到第一个进程的内存中，用于启动并运行该进程。\n每个进程的内核栈不是相通的，而是独立的。但是，内核空间在所有进程之间是共享的，但各进程对其访问是受控的。\n内核栈：\n独立性：每个进程都有自己独立的内核栈。当一个进程从用户态切换到内核态（例如执行系统调用或处理中断）时，它使用自己的内核栈来保存调用信息、局部变量以及处理中断或系统调用所需的其他数据。由于每个进程的内核栈是独立的，进程之间不会互相影响。这种独立性确保了进程的内核态执行环境的安全性和稳定性。 切换上下文时的保存与恢复：当操作系统在多个进程之间切换时（即进行上下文切换），它会保存当前进程的内核栈状态，并恢复下一个要运行的进程的内核栈。这种机制保证了每个进程可以独立执行，而不会干扰其他进程的内核栈。 内核空间：\n共享性：内核空间是所有进程共享的地址空间的一部分。它包含了内核代码、全局数据结构、设备驱动程序、内核栈等。这意味着所有进程都可以访问相同的内核代码和全局内核数据。内核空间通常位于每个进程的虚拟地址空间的高地址部分。 安全性：虽然内核空间在所有进程之间是共享的，但普通用户进程无法直接访问内核空间。这是通过硬件机制和内核态/用户态的权限控制来实现的。只有在进程运行于内核态时（例如在系统调用或中断处理期间），进程才能访问内核空间的内容。 统一的映射：所有进程的页表都会映射到相同的内核空间区域。这意味着当系统切换进程时，尽管进程的用户空间地址不同，但内核空间地址是一致的。这使得内核能够高效地管理和调度多个进程，而不必为每个进程维护独立的内核空间。 总结：\n内核栈：每个进程有自己的内核栈，互不相通，保证了内核态的独立性和安全性。 内核空间：内核空间在所有进程之间是共享的，但受权限控制，只有在内核态下进程才能访问内核空间。 这种设计确保了系统的安全性和稳定性，同时也保证了操作系统的效率。\np-\u0026gt;context-\u0026gt;eip 是进程控制块（struct proc）中的一个字段，用于保存进程在内核态的执行状态，特别是保存进程执行时的程序计数器（即指令指针）。p-\u0026gt;context-\u0026gt;eip` 指的是什么？\neip：eip 是 x86 架构中的指令指针寄存器（也称为程序计数器）。它保存了 CPU 下一条将要执行的指令的地址。在上下文切换时，操作系统会保存当前进程的 eip 值，以便在恢复这个进程时，它可以从之前中断的地方继续执行。 p-\u0026gt;context-\u0026gt;eip：p-\u0026gt;context 是一个指向进程上下文结构的指针，这个结构保存了内核态下的 CPU 寄存器状态，其中包括 eip。当进程被切换出去时，eip 会被保存到 p-\u0026gt;context-\u0026gt;eip，当进程被切换回来的时候，eip 会从 p-\u0026gt;context-\u0026gt;eip 中恢复，这样进程就能继续从上次停止的地方执行。 p-\u0026gt;context 是一个用于保存进程在内核态执行时 CPU 寄存器状态的结构。它通常包含以下内容：\neip（指令指针）：保存下一条将要执行的指令的地址。 esp（栈指针）：保存当前的栈指针，用于指示当前的栈顶位置。 ebx、ecx、edx、esi、edi、ebp：这些都是通用寄存器，用于保存进程执行时的寄存器状态。 eflags：保存 CPU 的状态标志寄存器，用于记录当前进程执行时的状态（如中断是否启用、运算结果的标志等）。 当操作系统进行上下文切换时，会将当前进程的所有 CPU 寄存器状态保存到 p-\u0026gt;context 中。然后操作系统会加载下一个进程的 p-\u0026gt;context，恢复所有寄存器状态，使 CPU 继续执行该进程。这种机制确保了多任务操作系统可以高效地在多个进程之间切换，而不丢失每个进程的执行状态。\n指令指针 eip 的作用尤其重要，因为它决定了进程接下来会执行哪条指令。没有正确保存和恢复 eip，进程可能会从错误的地址继续执行，导致系统不稳定甚至崩溃。\n总的来说，p-\u0026gt;context 结构中的内容是进程上下文切换的核心，它记录了进程在内核态下所有关键的寄存器状态，以确保进程可以在被中断后正确恢复并继续执行。\nLab时遇到的问题 “通过一个没有映射的页保护用户栈”是什么意思，怎么做到的\n这句话的意思是：xv6 通过在用户栈的末尾放置一个没有映射的页（即一个虚拟地址空间内的页表条目不指向任何物理内存），来保护用户栈免受溢出错误的影响。\n具体来说，当程序运行时，用户栈是从高地址向低地址方向增长的。如果程序意外地超出了栈的范围（比如递归太深或者分配了过多的局部变量），它可能会尝试访问栈的下方区域。如果这块区域没有得到适当保护，程序就可能覆盖掉其他内存区域的数据，导致不可预测的行为。\n为了防止这种情况发生，xv6 在用户栈的下方添加了一个“没有映射”的页。这意味着该页的页表条目（PTE）没有指向任何实际的物理内存。如果程序尝试访问这个没有映射的页（例如由于栈溢出），CPU 的分页硬件就会检测到这个访问，并产生一个页面错误（page fault），从而终止程序或让操作系统采取相应的处理措施。这就能有效地防止栈溢出带来的潜在危害。\n建立一个地址空间的过程可以分为以下几个步骤：\n调用 kvmalloc()\n位置：main() 调用 kvmalloc，位于代码的第1757行。\n作用：kvmalloc() 是用于创建并切换到一个拥有内核所需映射的页表。它确保在操作系统启动时，内核可以正常运行。\n关键操作：该函数调用 setupkvm() 来完成大部分工作。\n调用 setupkvm()\n位置：第1737行。\n作用：setupkvm() 是负责实际创建内核所需的页表映射的函数。\n具体步骤：\n分配页目录页：setupkvm() 首先会分配一页内存来存储页目录，这是分页的最高层结构，用于指向页表。 调用 mappages()：使用 mappages() 函数，创建映射表，这些映射表可以在 kmap 数组中找到。kmap 数组中包含了各种需要被映射的内存区域。 调用 mappages()\n位置：第1679行。\n作用：mappages() 用来在页表中建立虚拟内存到物理内存的映射。\n具体步骤：\n逐页映射：mappages() 是逐页处理的，即一页一页地创建映射。对于每个虚拟地址，都需要建立相应的物理内存映射。 调用 walkpgdir()：mappages() 通过调用 walkpgdir() 来找到该虚拟地址对应的页表项（PTE）。 设置 PTE：在页表项中记录物理页号、访问权限（如 PTE_W、PTE_U）和 PTE_P 标志位来表示该页表项是否有效。 调用 walkpgdir()\n位置：第1654行。\n作用：walkpgdir() 模拟 x86 硬件分页机制，找到虚拟地址对应的页表项（PTE）。\n具体步骤：\n定位页目录项：walkpgdir() 使用虚拟地址的前10位来查找页目录中对应的条目（位于第1659行）。 检查是否需要分配页表页：如果页目录条目不存在且 alloc 参数为真，则分配一个新的页表页，并将其物理地址存入页目录中。 定位页表项：然后使用虚拟地址的中间10位，找到该地址在页表中的对应条目（PTE）。 返回PTE地址：最后，walkpgdir() 返回该PTE的地址，供 mappages() 初始化。 初始化页表项 (PTE)\n位置：在 mappages() 的第1691行。\n作用：mappages() 在 walkpgdir() 返回的 PTE 地址处，初始化 PTE 条目。\n具体内容：\n保存物理页号：PTE 中记录相应的物理内存页号。 设置权限标志：根据需要设置 PTE 的权限标志（如 PTE_W 表示可写，PTE_U 表示用户访问权限）。 标记页表项有效：通过设置 PTE_P 标志位，表明该页表项有效。 完成内核的基础映射\nsetupkvm()完成了kmap数组中内核地址空间的映射，包括： 内核指令和数据段的映射。 PHYSTOP 以下的物理内存的映射。 I/O 设备地址空间的映射。 未建立用户空间映射\n注意：在这个阶段，setupkvm() 仅仅完成了内核所需的映射，并未为用户空间内存建立映射。用户空间的映射将在稍后的执行过程中进行。 切换到新页表\n当 kvmalloc() 完成时，它会切换到新创建的页表，这确保了内核可以使用新建立的映射来执行代码。 如何管理空闲物理内存页的分配与释放。\n物理内存分配器中的数据结构\n空闲链表：分配器使用一个由空闲的物理内存页构成的链表来管理内存。链表中的每个元素是一个struct run结构体，用于表示一个空闲页。\n自包含的结构：为了节省空间，分配器将struct run结构体存放在每个空闲页的开头部分，因为这些页本身是空闲的，没有其他数据。\n内存的安全保护\nSpin L\tock：分配器使用自旋锁（spin lock）来保护空闲链表的操作，防止并发操作引起的数据不一致问题。这个锁和链表一起封装在一个结构体中，以确保锁可以保护到相关的所有数据。 分配器的初始化\nkinit1和kinit2函数：分配器初始化分两步完成，kinit1用于在系统启动初期分配前4MB的内存，因为此时不支持锁和高于4MB的内存操作。kinit2允许使用锁，并解锁了更多的内存供系统使用。\nPHYSTOP假设：在x86架构上，操作系统假设系统有240MB的物理内存（PHYSTOP），从内核结束地址到PHYSTOP之间的内存作为空闲内存池。freerange函数通过调用kfree将这部分内存页逐一加入空闲链表。\n地址对齐和释放\n对齐处理：freerange函数使用PGROUNDUP确保内存页是以4096字节对齐的地址进行释放的，因为PTE（页表项）只能指向对齐的物理地址。\nkfree函数：kfree负责将物理页重新加入空闲链表。它首先将释放的内存页内容设置为全1，以确保访问已释放内存时立刻崩溃，而不是继续使用无效数据。\n地址映射\n虚拟地址和物理地址：分配器通过虚拟地址找到对应的物理页。这是因为在系统的内存管理中，物理内存通常被映射到高地址的虚拟内存空间中。 类型转换\n双重用途的地址：分配器在操作地址时，既将地址当作整数进行运算，也将其当作指针来操作内存中的数据。这种用法导致了大量的类型转换，同时也说明了分配器在释放或分配内存时可能会隐式地改变内存的类型。 用户地址空间中的栈与堆\n堆与栈的布局：堆位于栈的上方，栈本身占用一页内存，包含进程启动时的初始数据。栈的顶部包含命令行参数及其指针数组，并模仿函数main(argc, argv)的调用环境。\n保护页：为了防止栈的溢出，栈的下方有一个未映射的保护页。如果栈增长超过这一页，会触发异常，从而保护系统不受非法访问的影响。\n操作系统中 exec 系统调用的工作原理，exec 用于创建和初始化用户进程的地址空间。\nELF 格式与 exec 的基本流程\nELF 文件：exec 通过读取并解析 ELF (Executable and Linkable Format) 文件来初始化用户地址空间。ELF 是一种通用的二进制文件格式，exec 首先检查文件的 ELF 头部以验证文件格式是否正确。\n魔法数字检查：ELF 文件以 4 个“魔法数字”（0x7F, 'E', 'L', 'F'）开头。exec 首先通过检查这四个字节来确定文件是否是有效的 ELF 文件。如果魔法数字匹配，exec 继续处理文件，否则放弃执行。\n内存分配与段加载\n页表设置：exec 通过 setupkvm 函数创建一个新的页表，其中不包含用户部分的映射。这为用户地址空间的初始化做好了准备。\n内存分配与加载：exec 使用 allocuvm 为 ELF 文件的每个段分配内存。接着，loaduvm 将段的内容从文件中加载到内存中。loaduvm 通过 walkpgdir 找到内存中相应的物理地址，并通过 readi 从文件读取数据。\n处理 Program Header\n段头解释：ELF 文件包含多个段头（Program Header），每个段头描述了一个需要载入内存的程序段。exec 依次处理每个段头，分配相应的内存并加载数据。\nfilesz 与 memsz：段头中的 filesz 是从文件中读取的字节数，memsz 是分配的内存大小。如果 filesz 小于 memsz，表示从文件读取的数据不足以填满分配的内存，未填充的部分通常用于存储未初始化的全局变量，并被填充为 0。\n用户栈的分配与初始化\n栈内存分配：exec 为用户栈分配一页内存，并在栈顶放置传入的参数字符串。参数字符串的指针被保存在 ustack 中，最后在参数列表 (argv) 的末尾放置一个空指针，以便程序知道参数列表的结束。\n伪造的返回地址：在栈上，ustack 的前三个位置依次存放伪造的返回地址、参数个数 (argc)、以及参数列表的指针 (argv)。这使得当程序从 main 函数开始执行时，能够正确访问这些参数。\n栈保护页\n防止栈溢出：在分配栈页下方，exec 会设置一个无法访问的保护页。这个保护页用于防止栈意外增长超出分配的页数，从而引发内存访问错误。\n处理大参数：如果传入的参数过大以至于超出栈的页，exec 通过 copyout 函数检测并返回错误，以避免栈溢出。\n错误处理与内存释放\n错误处理：如果 exec 在创建新的内存映像过程中发现错误（例如无效的程序段），它会跳转到 bad 标签处，释放已经分配的内存，并返回 -1，表示调用失败。\n内存映像切换：只有在确认新内存映像已经正确构建后，exec 才会释放旧的内存映像。这是为了确保如果 exec 失败时，能够安全地返回错误代码 -1。成功完成后，exec 返回 0，表示执行成功。\n总结\nexec 是一个关键的系统调用，用于将用户进程的地址空间初始化为一个新的执行映像。它通过加载 ELF 文件、分配内存、设置栈空间等操作，确保用户进程可以正确启动并运行。 栈只分配一页内存，这在某些情况下可能会显得太小，特别是当程序需要处理大量的参数或者深度递归时。这种设计是为了简化内存管理和避免过度浪费，但也带来了某些局限性。如果需要处理超过一页内存的大参数，系统有以下几种方法来应对：\n增加栈的大小\n在某些系统中，可以动态增加栈的大小。具体方法包括：\n手动扩展：程序可以通过系统调用（例如 mmap）手动分配更多的栈空间。\n自动扩展：某些操作系统会在检测到栈溢出时自动扩展栈空间。这个过程通常是由内核负责的，通过捕获页面错误并分配新页面来实现栈的自动扩展。\n使用堆内存\n堆内存分配：程序可以使用堆（heap）来处理大数据，而不是依赖栈。通过调用 malloc 或者 calloc 这样的标准库函数，程序可以在堆上分配足够大的内存块来存储参数或其他数据。\n传递指针：对于大参数，可以将参数放在堆中，然后在栈上只传递一个指向堆中数据的指针。这样可以有效减轻栈的压力，同时仍然能够处理大数据。\n参数从文件读取\n文件输入：如果参数数据特别大，可以将其放在文件中，程序启动后从文件中读取。这种方式避免了将大量数据直接传递给程序，而是通过文件 I/O 进行处理，灵活性较高。 分块处理\n分块加载：对于过大的参数，可以考虑将其分块加载到内存中。每次只处理其中的一部分数据，处理完后再读取下一部分。这样可以降低对栈的要求，同时保持程序对大数据的处理能力。 环境变量\n环境变量传递：有时可以通过环境变量传递一些参数，环境变量不受栈大小的限制，且可以在程序运行时随时读取。 系统配置\n修改系统配置：某些操作系统允许通过修改配置文件或编译参数来增加默认的栈大小。这种方法可以在需要处理大量参数的应用中增大栈空间，避免因栈空间不足导致的错误。 “超级页”的特点\n“超级页”（Superpages）指的是在内存管理中使用较大的页面大小，而不是标准的4KB页。超级页通常为4MB或更大，具体特点包括：\n减少页表开销：使用超级页可以显著减少页表的大小和复杂度，因为每个超级页覆盖的地址空间比标准页大得多。这意味着需要维护的页表项（PTE）更少，从而减轻了页表的管理负担。 提高TLB（Translation Lookaside Buffer）效率：TLB 是一个缓存，用于加速虚拟地址到物理地址的转换。由于超级页减少了页表项的数量，TLB 更有可能缓存所有需要的页表项，从而减少TLB缺失的几率，提高内存访问效率。 适合大内存应用：在拥有大量内存的机器上，使用超级页非常有效，因为它们可以减少页表的复杂性，提高内存管理的整体性能。 浪费内存的风险：当应用程序的内存需求较小时，超级页可能会导致内存浪费。例如，一个只需要8KB内存的程序，如果被分配了4MB的超级页，将会浪费大量未使用的内存。 ","date":"2024-09-07T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0814page-tablesvm/124146068_hu_10166488fcbc5321.jpg","permalink":"https://echudet.github.io/p/mit6s0814page-tablesvm/","title":"MIT6S081：4Page Tables(vm)"},{"content":"Lecture 3 OS Organization \u0026amp; System Calls OS 隔离性、防御性 为了不变成一个恶意程序，Shell在发现自己运行了一段时间之后，需要让别的程序也有机会能运行。这种机制有时候称为协同调度（Cooperative Scheduling）。 这个特性是非常有用的，不论应用程序在执行什么操作，multiplexing都会迫使应用程序时不时的释放CPU，这样其他的应用程序才能运行。 使用操作系统的一个原因，甚至可以说是主要原因就是为了实现multiplexing和内存隔离。如果你不使用操作系统，并且应用程序直接与硬件交互，就很难实现这两点。所以，将操作系统设计成一个库，并不是一种常见的设计。你或许可以在一些实时操作系统中看到这样的设计，因为在这些实时操作系统中，应用程序之间彼此相互信任。但是在大部分的其他操作系统中，都会强制实现硬件资源的隔离。 需要通过硬件来实现这的强隔离性。这里的硬件支持包括了两部分，第一部分是user/kernel mode，kernel mode在RISC-V中被称为Supervisor mode但是其实是同一个东西；第二部分是page table或者虚拟内存（Virtual Memory）。 处理器会有两种操作模式，第一种是user mode，第二种是kernel mode。当运行在kernel mode时，CPU可以运行特定权限的指令（privileged instructions）；当运行在user mode时，CPU只能运行普通权限的指令（unprivileged instructions）。 实际上RISC-V还有第三种模式称为machine mode。在大多数场景下，我们会忽略这种模式，所以我也不太会介绍这种模式。 在处理器的一个bit，当它为1的时候是user mode，当它为0时是kernel mode。当处理器在解析指令时，如果指令是特殊权限指令，并且该bit被设置为1，处理器会拒绝执行这条指令，就像在运算时不能除以0一样。 让整个操作系统代码都运行在kernel mode。大多数的Unix操作系统实现都运行在kernel mode。比如，XV6中，所有的操作系统服务都在kernel mode中，这种形式被称为Monolithic Kernel Design（宏内核 ）。 另一种设计主要关注点是减少内核中的代码，它被称为Micro Kernel Design（微内核 ）。在这种模式下，希望在kernel mode中运行尽可能少的代码。所以这种设计下还是有内核，但是内核只有非常少的几个模块，例如，内核通常会有一些IPC的实现或者是Message passing；非常少的虚拟内存的支持，可能只支持了page table；以及分时复用CPU的一些支持。微内核的目的在于将大部分的操作系统运行在内核之外。因为在内核中的代码的数量较小，更少的代码意味着更少的Bug。 在实际中，两种内核设计都会出现，出于历史原因大部分的桌面操作系统是宏内核，如果你运行需要大量内核计算的应用程序，例如在数据中心服务器上的操作系统，通常也是使用的宏内核，主要的原因是Linux提供了很好的性能。但是很多嵌入式系统，例如Minix，Cell，这些都是微内核设计。这两种设计都很流行，如果你从头开始写一个操作系统，你可能会从一个微内核设计开始。 System Calls 有一种方式能够让应用程序可以将控制权转移给内核（Entering Kernel）。在RISC-V中，有一个专门的指令用来实现这个功能，叫做ECALL。ECALL接收一个数字参数，当一个用户程序想要将程序执行的控制权转移到内核，它只需要执行ECALL指令，并传入一个数字。这里的数字参数代表了应用程序想要调用的System Call。\n不论是Shell还是其他的应用程序，当它在用户空间执行fork时，它并不是直接调用操作系统中对应的函数，而是调用ECALL指令，并将fork对应的数字作为参数传给ECALL。之后再通过ECALL跳转到内核。\n左边是用户空间，右边是内核空间。在内核侧，有一个位于syscall.c的函数syscall，每一个从应用程序发起的系统调用都会调用到这个syscall函数，syscall函数会检查ECALL的参数，通过这个参数内核可以知道需要调用的是fork\n内核有时候也被称为可被信任的计算空间（Trusted Computing Base），在一些安全的术语中也被称为TCB。\n在这段代码中，p-\u0026gt;trapframe 和 p-\u0026gt;trapframe-\u0026gt;a0 是与系统调用处理相关的两个重要部分。为了理解它们的意义，我们需要明确 trapframe 是什么，以及 a0 在其中的作用。\np-\u0026gt;trapframe 的意义\ntrapframe 结构：trapframe 是一个结构体，它保存了当进程从用户态切换到内核态时，CPU 寄存器的状态。这通常发生在进程进行系统调用、发生中断或异常时。操作系统会在内核栈中创建一个 trapframe，用于保存当前的 CPU 状态（包括通用寄存器、指令指针、栈指针等）。 p-\u0026gt;trapframe：这里的 p 是指向进程控制块（struct proc）的指针，而 p-\u0026gt;trapframe 则是该进程的 trapframe。当进程进入内核态时，操作系统会使用 p-\u0026gt;trapframe 来保存进程的 CPU 寄存器状态。 p-\u0026gt;trapframe-\u0026gt;a0 的意义\na0：在 trapframe 结构体中，a0 是保存系统调用返回值的寄存器。通常在 xv6 操作系统中，a0 是一个寄存器，用于保存从系统调用返回时的结果。每个系统调用执行完后，返回的结果会存放在 trapframe-\u0026gt;a0 中，然后在从内核返回用户态时，这个值会被传递给用户进程。 p-\u0026gt;trapframe-\u0026gt;a0 = syscall[num]()：这行代码表示执行一个系统调用，并将系统调用的返回值保存到 p-\u0026gt;trapframe-\u0026gt;a0 中。syscall[num]() 是一个函数指针数组，num 是系统调用号，根据这个号找到对应的系统调用函数并执行。执行后的返回值被存入 a0 中，稍后当进程返回用户态时，这个值将被返回给进程的用户代码。 总结\np-\u0026gt;trapframe：保存了当前进程的 CPU 寄存器状态，当进程从用户态进入内核态时使用。 p-\u0026gt;trapframe-\u0026gt;a0：用于保存系统调用的返回值，这个值会在系统调用执行完毕后传递回用户态的进程。 这种机制保证了用户进程在进行系统调用时能够正确地与内核交互，并获取系统调用的结果。\np-\u0026gt;context 和 p-\u0026gt;trapframe 是两种不同的数据结构，它们分别用于保存不同阶段的进程状态，具有不同的用途和作用。不过，它们也有相似之处，都是为了实现进程的正确执行和状态保存。\np-\u0026gt;context 与 p-\u0026gt;trapframe 的不同之处\n用途和作用不同: p-\u0026gt;context： 作用：p-\u0026gt;context 主要用于保存进程在 内核态 的上下文（即寄存器状态），特别是当进程在内核态运行且发生上下文切换时（例如调度器将进程挂起以运行另一个进程）。它保存的是内核态寄存器的状态，包括程序计数器（eip）、栈指针（esp）等。这个上下文会在进程被重新调度运行时恢复，以确保进程从上次被挂起的地方继续执行。 使用场景：p-\u0026gt;context 的保存与恢复主要发生在进程的上下文切换过程中，即当操作系统在多个进程之间切换时。p-\u0026gt;context 保存的是内核态的运行状态，而不是用户态的状态。 p-\u0026gt;trapframe： 作用：p-\u0026gt;trapframe 则主要用于保存进程从 用户态 切换到 内核态 时的 CPU 寄存器状态。它保存的寄存器状态包括通用寄存器、程序计数器、栈指针、段寄存器等，基本上就是用户态的所有寄存器状态。这是为了当系统调用、中断或异常发生时，能够保存用户态的状态，然后在处理完毕后可以恢复并返回用户态继续执行。 使用场景：p-\u0026gt;trapframe 的保存与恢复发生在用户态和内核态之间的切换过程中。例如，当进程执行系统调用时，CPU 的寄存器状态会被保存到 trapframe，以便在系统调用完成后正确返回用户态。 保存内容不同： p-\u0026gt;context：保存的是内核态的寄存器状态，特别是与内核栈相关的状态信息，例如 eip（指向内核代码中的某个位置）、esp（指向内核栈中的某个位置）等。 p-\u0026gt;trapframe：保存的是用户态的寄存器状态以及一些与系统调用和中断相关的状态信息，例如用户态的 eip、esp 以及系统调用号、参数等。 使用的时机不同： p-\u0026gt;context：在进程调度时使用，当系统切换到其他进程时，当前进程的内核态上下文会被保存到 p-\u0026gt;context，并在下次调度回该进程时恢复。 p-\u0026gt;trapframe：在系统调用或中断发生时使用，当进程从用户态进入内核态时，trapframe 用于保存当前的用户态寄存器状态，以便在系统调用或中断处理完毕后能够正确地返回用户态。 p-\u0026gt;context 与 p-\u0026gt;trapframe 的相同之处\n都是用来保存状态的： 无论是 p-\u0026gt;context 还是 p-\u0026gt;trapframe，它们的主要作用都是为了保存进程在某个时刻的 CPU 状态，以便在之后能够正确恢复进程的执行。 都与进程切换有关： 虽然 p-\u0026gt;context 和 p-\u0026gt;trapframe 用于不同的切换场景（进程上下文切换 vs 用户态和内核态切换），但它们都涉及到进程在不同执行阶段的切换。 总结\np-\u0026gt;context 保存的是进程在 内核态 下的上下文信息，主要用于进程的上下文切换（即调度器在不同进程之间切换时）。 p-\u0026gt;trapframe 保存的是进程在 用户态 和 内核态 切换时的状态信息，主要用于处理系统调用、中断或异常时的状态保存和恢复。 二者的作用和使用场景不同，但它们的共同目标都是为了确保进程的执行能够在不同的切换过程中被正确保存和恢复。\nxv6的组织结构 xv6 使用了传统的内核概念 - 一个向其他运行中程序提供服务的特殊程序。每一个运行中程序（称之为进程）都拥有包含指令、数据、栈的内存空间。指令实现了程序的运算，数据是用于运算过程的变量，栈管理了程序的过程调用。 内核使用了 CPU 的硬件保护机制来保证用户进程只能访问自己的内存空间。内核拥有实现保护机制所需的硬件权限(hardware privileges)，而用户程序没有这些权限。当一个用户程序进行一次系统调用时，硬件会提升特权级并且开始执行一些内核中预定义的功能。 ","date":"2024-09-01T22:02:50+08:00","image":"https://echudet.github.io/p/mit6s0813os-organization-system-calls/2ed7cfb8882411ebb6edd017c2d2eca2_hu_93fa47012019c58e.png","permalink":"https://echudet.github.io/p/mit6s0813os-organization-system-calls/","title":"MIT6S081：3OS Organization \u0026 System Calls"},{"content":"Lecture 1 OS Interface Inode 是什么 Inode 是 UNIX 类文件系统中一个非常核心的概念，负责管理文件和目录的元数据。\nInode 是文件系统中的一个数据结构，用来存储文件的元信息（metadata），而不包含文件的内容和文件名。每一个文件或者目录都会分配一个唯一的 Inode，Inode 中存储了与文件或目录相关的以下信息：\n文件类型：是普通文件、目录还是设备文件（在 stat 结构体中的 type 字段表示）。 文件的大小：以字节为单位存储（在 stat 结构体中的 size 字段表示）。 硬链接数：有多少个不同的文件名指向这个 Inode（在 stat 结构体中的 nlink 字段表示）。 文件的访问权限：谁可以读取、写入或执行这个文件。 文件的时间戳：包括文件的创建时间、最后访问时间、最后修改时间等。 数据块指针：指向文件内容在硬盘上存储位置的指针。 Inode 在文件系统中起着关键的作用，因为它连接了文件名和文件内容之间的关系。理解这一点可以从以下几个方面来说明：\n文件与文件名的分离：文件名只是一个指向 Inode 的引用，这意味着在文件系统中，多个不同的文件名可以指向同一个 Inode（也就是硬链接）。这就是为什么可以使用 link 系统调用创建多个文件名指向同一个文件。 文件的元数据管理：所有与文件相关的元数据都存储在 Inode 中。这包括文件类型、大小、权限、所有者等。当你需要获取文件的这些信息时，系统会查找文件的 Inode，而不需要访问文件的内容。 文件内容的定位：Inode 包含指向文件内容在磁盘上位置的指针。当你打开一个文件时，系统通过文件名查找到对应的 Inode，然后通过 Inode 中的数据块指针找到文件的实际内容。 删除文件：当你使用 unlink 系统调用删除文件时，系统只删除了文件名与 Inode 的关联。如果这个 Inode 没有其他链接（即 nlink 为 0），并且没有进程在使用这个文件，系统才会真正释放与该 Inode 相关联的磁盘空间。 比如运行了下面这段代码：\n1 2 open(\u0026#34;a\u0026#34;, O_CREATE|O_WRONLY); link(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); 这个例子创建了一个文件 a，并且为这个文件创建了一个硬链接 b。在这两行代码执行之后，文件 a 和 b 都指向同一个 Inode，它们的内容完全相同，任何一个文件的修改都会反映在另一个文件中。\n通过 Inode，你可以理解文件系统如何管理文件的元数据以及如何实现硬链接、文件删除等功能。简而言之，Inode 是文件系统中的核心结构，负责文件元数据的管理和文件内容的定位。\n特殊文件和普通文件的区别 指向设备的特殊文件和普通文件在文件系统中的处理方式和用途上有很大的不同。它们的主要区别如下：\n文件类型和作用\n普通文件：这是最常见的文件类型，用于存储用户的数据，例如文本文件、图像文件、程序文件等。普通文件包含实际的数据内容，文件系统通过 Inode 和数据块将文件名映射到硬盘上的实际数据。\n特殊文件（设备文件）：特殊文件不包含用户数据，而是用于与硬件设备交互。设备文件通过文件系统接口将文件操作（如读写）映射到设备驱动程序。例如，/dev/sda 可能是一个硬盘设备文件，通过对这个文件进行操作，实际上是对对应的硬盘设备进行操作。\nInode 中的差异\n普通文件的 Inode：普通文件的 Inode 包含文件的元数据（如文件类型、大小、权限等）和指向数据块的指针。这些指针指向存储在硬盘上文件内容的具体位置。\n特殊文件的 Inode：设备文件的 Inode 也包含元数据，但它的指针部分不指向存储在硬盘上的数据块，而是包含与设备相关的标识信息，即主设备号（major number）和次设备号（minor number）。这些编号告诉内核应该使用哪个设备驱动程序以及如何与具体的设备进行通信。\n操作方式\n普通文件操作：当你对普通文件进行读写操作时，文件系统会通过 Inode 查找文件内容在磁盘上的位置，并执行实际的数据读写。\n特殊文件操作：当你对设备文件进行读写操作时，文件系统并不直接处理磁盘数据，而是将这些操作请求转发给对应的设备驱动程序。设备驱动程序根据操作类型（读或写）与实际的硬件设备进行交互。比如，对 /dev/null 文件的写操作，实际上什么也不做，而对 /dev/sda 的写操作，可能会导致硬盘上的数据被写入。\n使用场景\n普通文件：用于存储和读取用户的应用数据，如文档、图像、程序代码等。\n特殊文件：用于操作和控制硬件设备，比如：\n字符设备文件（如 /dev/tty，对应终端设备）：处理字符流设备的读写，通常一次一个字符进行操作。 块设备文件（如 /dev/sda，对应硬盘设备）：处理块设备的读写操作，通常一次一个块（通常为512字节或更多）进行操作。 系统调用的区别\n普通文件：可以通过常规的系统调用如 open、read、write、close 等操作普通文件。\n特殊文件：虽然设备文件也可以通过 open、read、write 等系统调用操作，但这些调用最终会被内核处理，并转交给设备的驱动程序，由驱动程序与硬件直接交互。\n普通文件用于存储数据，文件系统会直接对其内容进行操作。而设备文件是与硬件交互的接口，操作这些文件时，实际上是在与对应的设备驱动程序交互，进而与实际的硬件设备交互。这种设计允许设备在 UNIX 文件系统中的统一表示，使得设备的操作与文件操作类似，保持了 UNIX 的一贯简洁和一致性。\n设备文件 设备文件本身不能被修改，因为它不包含实际的数据内容。设备文件只是一个接口，通过它可以对硬件设备进行操作。例如，当你写数据到一个设备文件时，这些数据实际上是发送到对应的硬件设备（如硬盘）上，而不是存储在设备文件本身中。\n设备文件在硬盘上几乎不占用存储空间。因为设备文件只是一个特殊的文件系统条目，主要存储了设备的标识信息（如主设备号和次设备号），以及一些基础的元数据（例如文件权限、创建时间等）。这些信息占用的空间非常小，通常只需要几个字节到几十个字节。\n与普通文件不同，设备文件没有指向实际数据块的指针，也没有存储用户数据的空间。所以，尽管设备文件存在于文件系统中，但它们在硬盘上占用的存储空间是微乎其微的。\n","date":"2024-08-31T00:00:00Z","image":"https://echudet.github.io/p/mit6s0811os-interface/56d1b86047724f56a2793bda4a87bccb_hu_fe8cf904c77f765b.jpg","permalink":"https://echudet.github.io/p/mit6s0811os-interface/","title":"MIT6S081：1OS Interface"}]